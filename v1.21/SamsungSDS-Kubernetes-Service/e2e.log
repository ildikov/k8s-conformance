I0518 04:30:10.912239      19 e2e.go:129] Starting e2e run "46774d82-87bb-430e-bae9-92efa50bf752" on Ginkgo node 1
{"msg":"Test Suite starting","total":337,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1621312209 - Will randomize all specs
Will run 337 of 5771 specs

May 18 04:30:10.965: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 04:30:10.967: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 18 04:30:10.987: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 18 04:30:11.022: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 18 04:30:11.022: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
May 18 04:30:11.022: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 18 04:30:11.028: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
May 18 04:30:11.028: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 18 04:30:11.028: INFO: e2e test version: v1.21.1
May 18 04:30:11.030: INFO: kube-apiserver version: v1.21.1
May 18 04:30:11.030: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 04:30:11.035: INFO: Cluster IP family: ipv4
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:30:11.035: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename security-context-test
W0518 04:30:11.076774      19 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
May 18 04:30:11.076: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:30:11.087: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-451dfa16-e88f-4c3b-8281-36206604df9b" in namespace "security-context-test-3869" to be "Succeeded or Failed"
May 18 04:30:11.090: INFO: Pod "busybox-privileged-false-451dfa16-e88f-4c3b-8281-36206604df9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.619988ms
May 18 04:30:13.093: INFO: Pod "busybox-privileged-false-451dfa16-e88f-4c3b-8281-36206604df9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005946806s
May 18 04:30:13.093: INFO: Pod "busybox-privileged-false-451dfa16-e88f-4c3b-8281-36206604df9b" satisfied condition "Succeeded or Failed"
May 18 04:30:13.102: INFO: Got logs for pod "busybox-privileged-false-451dfa16-e88f-4c3b-8281-36206604df9b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:30:13.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3869" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":1,"skipped":0,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:30:13.109: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:30:13.151: INFO: The status of Pod test-webserver-d19ad909-bd08-475f-bd91-8a3d8daf1560 is Pending, waiting for it to be Running (with Ready = true)
May 18 04:30:15.156: INFO: The status of Pod test-webserver-d19ad909-bd08-475f-bd91-8a3d8daf1560 is Pending, waiting for it to be Running (with Ready = true)
May 18 04:30:17.156: INFO: The status of Pod test-webserver-d19ad909-bd08-475f-bd91-8a3d8daf1560 is Running (Ready = false)
May 18 04:30:19.157: INFO: The status of Pod test-webserver-d19ad909-bd08-475f-bd91-8a3d8daf1560 is Running (Ready = false)
May 18 04:30:21.157: INFO: The status of Pod test-webserver-d19ad909-bd08-475f-bd91-8a3d8daf1560 is Running (Ready = false)
May 18 04:30:23.159: INFO: The status of Pod test-webserver-d19ad909-bd08-475f-bd91-8a3d8daf1560 is Running (Ready = false)
May 18 04:30:25.155: INFO: The status of Pod test-webserver-d19ad909-bd08-475f-bd91-8a3d8daf1560 is Running (Ready = false)
May 18 04:30:27.163: INFO: The status of Pod test-webserver-d19ad909-bd08-475f-bd91-8a3d8daf1560 is Running (Ready = false)
May 18 04:30:29.167: INFO: The status of Pod test-webserver-d19ad909-bd08-475f-bd91-8a3d8daf1560 is Running (Ready = false)
May 18 04:30:31.170: INFO: The status of Pod test-webserver-d19ad909-bd08-475f-bd91-8a3d8daf1560 is Running (Ready = false)
May 18 04:30:33.164: INFO: The status of Pod test-webserver-d19ad909-bd08-475f-bd91-8a3d8daf1560 is Running (Ready = false)
May 18 04:30:35.155: INFO: The status of Pod test-webserver-d19ad909-bd08-475f-bd91-8a3d8daf1560 is Running (Ready = true)
May 18 04:30:35.157: INFO: Container started at 2021-05-18 04:30:14 +0000 UTC, pod became ready at 2021-05-18 04:30:33 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:30:35.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2892" for this suite.

• [SLOW TEST:22.057 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":337,"completed":2,"skipped":10,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:30:35.166: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
May 18 04:30:35.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-114 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
May 18 04:30:35.669: INFO: stderr: ""
May 18 04:30:35.669: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 18 04:30:35.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-114 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
May 18 04:30:35.887: INFO: stderr: ""
May 18 04:30:35.887: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
May 18 04:30:35.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-114 delete pods e2e-test-httpd-pod'
May 18 04:30:37.085: INFO: stderr: ""
May 18 04:30:37.085: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:30:37.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-114" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":337,"completed":3,"skipped":16,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:30:37.093: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 04:30:37.131: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d252add-e250-46b3-809a-5e36271d1764" in namespace "projected-673" to be "Succeeded or Failed"
May 18 04:30:37.135: INFO: Pod "downwardapi-volume-5d252add-e250-46b3-809a-5e36271d1764": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719983ms
May 18 04:30:39.141: INFO: Pod "downwardapi-volume-5d252add-e250-46b3-809a-5e36271d1764": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009795788s
May 18 04:30:41.147: INFO: Pod "downwardapi-volume-5d252add-e250-46b3-809a-5e36271d1764": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015282396s
May 18 04:30:43.154: INFO: Pod "downwardapi-volume-5d252add-e250-46b3-809a-5e36271d1764": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023176593s
May 18 04:30:45.165: INFO: Pod "downwardapi-volume-5d252add-e250-46b3-809a-5e36271d1764": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.033471179s
STEP: Saw pod success
May 18 04:30:45.165: INFO: Pod "downwardapi-volume-5d252add-e250-46b3-809a-5e36271d1764" satisfied condition "Succeeded or Failed"
May 18 04:30:45.169: INFO: Trying to get logs from node node2 pod downwardapi-volume-5d252add-e250-46b3-809a-5e36271d1764 container client-container: <nil>
STEP: delete the pod
May 18 04:30:45.195: INFO: Waiting for pod downwardapi-volume-5d252add-e250-46b3-809a-5e36271d1764 to disappear
May 18 04:30:45.201: INFO: Pod downwardapi-volume-5d252add-e250-46b3-809a-5e36271d1764 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:30:45.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-673" for this suite.

• [SLOW TEST:8.125 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":337,"completed":4,"skipped":36,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:30:45.219: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 04:30:45.993: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 18 04:30:48.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756909045, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756909045, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756909046, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756909045, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 04:30:51.026: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 18 04:30:51.063: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:30:51.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6748" for this suite.
STEP: Destroying namespace "webhook-6748-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.931 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":337,"completed":5,"skipped":36,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:30:51.150: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 04:30:51.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2d9b394-be00-4905-94cc-c02de12a37f7" in namespace "projected-6810" to be "Succeeded or Failed"
May 18 04:30:51.216: INFO: Pod "downwardapi-volume-a2d9b394-be00-4905-94cc-c02de12a37f7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.824551ms
May 18 04:30:53.222: INFO: Pod "downwardapi-volume-a2d9b394-be00-4905-94cc-c02de12a37f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017516753s
STEP: Saw pod success
May 18 04:30:53.222: INFO: Pod "downwardapi-volume-a2d9b394-be00-4905-94cc-c02de12a37f7" satisfied condition "Succeeded or Failed"
May 18 04:30:53.225: INFO: Trying to get logs from node node2 pod downwardapi-volume-a2d9b394-be00-4905-94cc-c02de12a37f7 container client-container: <nil>
STEP: delete the pod
May 18 04:30:53.248: INFO: Waiting for pod downwardapi-volume-a2d9b394-be00-4905-94cc-c02de12a37f7 to disappear
May 18 04:30:53.250: INFO: Pod downwardapi-volume-a2d9b394-be00-4905-94cc-c02de12a37f7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:30:53.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6810" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":337,"completed":6,"skipped":39,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:30:53.258: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-f9714ac0-e644-47a5-b361-9b59c76f936a
STEP: Creating a pod to test consume secrets
May 18 04:30:53.298: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3b3effc5-45f1-43a6-8331-f45e1e45628d" in namespace "projected-3846" to be "Succeeded or Failed"
May 18 04:30:53.300: INFO: Pod "pod-projected-secrets-3b3effc5-45f1-43a6-8331-f45e1e45628d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.27989ms
May 18 04:30:55.305: INFO: Pod "pod-projected-secrets-3b3effc5-45f1-43a6-8331-f45e1e45628d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006921401s
STEP: Saw pod success
May 18 04:30:55.305: INFO: Pod "pod-projected-secrets-3b3effc5-45f1-43a6-8331-f45e1e45628d" satisfied condition "Succeeded or Failed"
May 18 04:30:55.307: INFO: Trying to get logs from node node2 pod pod-projected-secrets-3b3effc5-45f1-43a6-8331-f45e1e45628d container projected-secret-volume-test: <nil>
STEP: delete the pod
May 18 04:30:55.321: INFO: Waiting for pod pod-projected-secrets-3b3effc5-45f1-43a6-8331-f45e1e45628d to disappear
May 18 04:30:55.323: INFO: Pod pod-projected-secrets-3b3effc5-45f1-43a6-8331-f45e1e45628d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:30:55.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3846" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":7,"skipped":41,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:30:55.329: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-76mx
STEP: Creating a pod to test atomic-volume-subpath
May 18 04:30:55.373: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-76mx" in namespace "subpath-1437" to be "Succeeded or Failed"
May 18 04:30:55.381: INFO: Pod "pod-subpath-test-configmap-76mx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.667565ms
May 18 04:30:57.384: INFO: Pod "pod-subpath-test-configmap-76mx": Phase="Running", Reason="", readiness=true. Elapsed: 2.011538181s
May 18 04:30:59.390: INFO: Pod "pod-subpath-test-configmap-76mx": Phase="Running", Reason="", readiness=true. Elapsed: 4.017178088s
May 18 04:31:01.396: INFO: Pod "pod-subpath-test-configmap-76mx": Phase="Running", Reason="", readiness=true. Elapsed: 6.022573596s
May 18 04:31:03.401: INFO: Pod "pod-subpath-test-configmap-76mx": Phase="Running", Reason="", readiness=true. Elapsed: 8.027989604s
May 18 04:31:05.406: INFO: Pod "pod-subpath-test-configmap-76mx": Phase="Running", Reason="", readiness=true. Elapsed: 10.032980814s
May 18 04:31:07.422: INFO: Pod "pod-subpath-test-configmap-76mx": Phase="Running", Reason="", readiness=true. Elapsed: 12.049108874s
May 18 04:31:09.427: INFO: Pod "pod-subpath-test-configmap-76mx": Phase="Running", Reason="", readiness=true. Elapsed: 14.054303883s
May 18 04:31:11.432: INFO: Pod "pod-subpath-test-configmap-76mx": Phase="Running", Reason="", readiness=true. Elapsed: 16.059313893s
May 18 04:31:13.439: INFO: Pod "pod-subpath-test-configmap-76mx": Phase="Running", Reason="", readiness=true. Elapsed: 18.066097295s
May 18 04:31:15.444: INFO: Pod "pod-subpath-test-configmap-76mx": Phase="Running", Reason="", readiness=true. Elapsed: 20.071104805s
May 18 04:31:17.449: INFO: Pod "pod-subpath-test-configmap-76mx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.076037816s
STEP: Saw pod success
May 18 04:31:17.449: INFO: Pod "pod-subpath-test-configmap-76mx" satisfied condition "Succeeded or Failed"
May 18 04:31:17.451: INFO: Trying to get logs from node node2 pod pod-subpath-test-configmap-76mx container test-container-subpath-configmap-76mx: <nil>
STEP: delete the pod
May 18 04:31:17.472: INFO: Waiting for pod pod-subpath-test-configmap-76mx to disappear
May 18 04:31:17.475: INFO: Pod pod-subpath-test-configmap-76mx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-76mx
May 18 04:31:17.475: INFO: Deleting pod "pod-subpath-test-configmap-76mx" in namespace "subpath-1437"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:31:17.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1437" for this suite.

• [SLOW TEST:22.156 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":337,"completed":8,"skipped":42,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:31:17.485: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:31:17.526: INFO: The status of Pod pod-secrets-526e6eac-cf4b-4781-8880-b3e26046f104 is Pending, waiting for it to be Running (with Ready = true)
May 18 04:31:19.531: INFO: The status of Pod pod-secrets-526e6eac-cf4b-4781-8880-b3e26046f104 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:31:19.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9347" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":337,"completed":9,"skipped":47,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:31:19.577: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:31:19.607: INFO: Creating simple deployment test-new-deployment
May 18 04:31:19.618: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 18 04:31:21.675: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9864  0f3a842b-c12e-416e-95c8-47e58b263ba5 18094 3 2021-05-18 04:31:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-05-18 04:31:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-18 04:31:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003775a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-18 04:31:20 +0000 UTC,LastTransitionTime:2021-05-18 04:31:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-05-18 04:31:20 +0000 UTC,LastTransitionTime:2021-05-18 04:31:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 18 04:31:21.688: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-9864  e3b1c137-427c-41a4-9e7a-f28e4eb6f11b 18101 3 2021-05-18 04:31:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 0f3a842b-c12e-416e-95c8-47e58b263ba5 0xc003775e27 0xc003775e28}] []  [{kube-controller-manager Update apps/v1 2021-05-18 04:31:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f3a842b-c12e-416e-95c8-47e58b263ba5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003775e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 18 04:31:21.695: INFO: Pod "test-new-deployment-847dcfb7fb-fjfgx" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-fjfgx test-new-deployment-847dcfb7fb- deployment-9864  3d2db9f4-70ad-40d2-b7b5-bdc3db271022 18100 0 2021-05-18 04:31:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb e3b1c137-427c-41a4-9e7a-f28e4eb6f11b 0xc003884257 0xc003884258}] []  [{kube-controller-manager Update v1 2021-05-18 04:31:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b1c137-427c-41a4-9e7a-f28e4eb6f11b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-18 04:31:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkk6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkk6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 04:31:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 04:31:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 04:31:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 04:31:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2021-05-18 04:31:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 04:31:21.696: INFO: Pod "test-new-deployment-847dcfb7fb-qvm5s" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-qvm5s test-new-deployment-847dcfb7fb- deployment-9864  d1b9edbb-c250-4011-bdfd-946c1bc85608 18086 0 2021-05-18 04:31:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.30.104.10/32 cni.projectcalico.org/podIPs:172.30.104.10/32] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb e3b1c137-427c-41a4-9e7a-f28e4eb6f11b 0xc003884437 0xc003884438}] []  [{kube-controller-manager Update v1 2021-05-18 04:31:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b1c137-427c-41a4-9e7a-f28e4eb6f11b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 04:31:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 04:31:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.104.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mzp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mzp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 04:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 04:31:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 04:31:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 04:31:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.104.10,StartTime:2021-05-18 04:31:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 04:31:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://a4d26e7706eef57c9fa42b306f05e65aaffbf9be6edacaeaff930bbc6e1c0893,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.104.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:31:21.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9864" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":337,"completed":10,"skipped":64,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:31:21.718: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:31:28.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3172" for this suite.

• [SLOW TEST:7.062 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":337,"completed":11,"skipped":86,"failed":0}
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:31:28.780: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:31:28.821: INFO: Pod name sample-pod: Found 0 pods out of 1
May 18 04:31:33.841: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
May 18 04:31:33.846: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
May 18 04:31:33.859: INFO: observed ReplicaSet test-rs in namespace replicaset-5322 with ReadyReplicas 1, AvailableReplicas 1
May 18 04:31:33.872: INFO: observed ReplicaSet test-rs in namespace replicaset-5322 with ReadyReplicas 1, AvailableReplicas 1
May 18 04:31:33.901: INFO: observed ReplicaSet test-rs in namespace replicaset-5322 with ReadyReplicas 1, AvailableReplicas 1
May 18 04:31:33.909: INFO: observed ReplicaSet test-rs in namespace replicaset-5322 with ReadyReplicas 1, AvailableReplicas 1
May 18 04:31:35.612: INFO: observed ReplicaSet test-rs in namespace replicaset-5322 with ReadyReplicas 2, AvailableReplicas 2
May 18 04:31:36.045: INFO: observed Replicaset test-rs in namespace replicaset-5322 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:31:36.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5322" for this suite.

• [SLOW TEST:7.286 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":337,"completed":12,"skipped":86,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:31:36.068: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
May 18 04:31:36.134: INFO: Pod name sample-pod: Found 0 pods out of 1
May 18 04:31:41.140: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:31:41.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2509" for this suite.

• [SLOW TEST:5.138 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":337,"completed":13,"skipped":135,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:31:41.206: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 18 04:31:41.251: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6327  8d076742-9efc-4489-8e75-b2042138eb5f 18346 0 2021-05-18 04:31:41 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-05-18 04:31:41 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99sw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99sw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 04:31:41.256: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 18 04:31:43.262: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 18 04:31:43.262: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6327 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 04:31:43.262: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Verifying customized DNS server is configured on pod...
May 18 04:31:43.359: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6327 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 04:31:43.359: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 04:31:43.435: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:31:43.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6327" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":337,"completed":14,"skipped":137,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:31:43.464: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-370f7952-0f19-46a4-a6a0-afb90e04539b in namespace container-probe-6368
May 18 04:31:45.522: INFO: Started pod test-webserver-370f7952-0f19-46a4-a6a0-afb90e04539b in namespace container-probe-6368
STEP: checking the pod's current state and verifying that restartCount is present
May 18 04:31:45.524: INFO: Initial restart count of pod test-webserver-370f7952-0f19-46a4-a6a0-afb90e04539b is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:35:46.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6368" for this suite.

• [SLOW TEST:242.907 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":337,"completed":15,"skipped":150,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:35:46.371: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-5969
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-5969
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5969
May 18 04:35:46.432: INFO: Found 0 stateful pods, waiting for 1
May 18 04:35:56.437: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 18 04:35:56.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 18 04:35:56.637: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 18 04:35:56.637: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 18 04:35:56.637: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 18 04:35:56.639: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 18 04:36:06.646: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 18 04:36:06.646: INFO: Waiting for statefulset status.replicas updated to 0
May 18 04:36:06.666: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 18 04:36:06.666: INFO: ss-0  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  }]
May 18 04:36:06.666: INFO: 
May 18 04:36:06.666: INFO: StatefulSet ss has not reached scale 3, at 1
May 18 04:36:07.671: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986588795s
May 18 04:36:08.676: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98180365s
May 18 04:36:09.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975783611s
May 18 04:36:10.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970732468s
May 18 04:36:11.691: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.966131022s
May 18 04:36:12.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96080958s
May 18 04:36:13.705: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.952559051s
May 18 04:36:14.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.94692641s
May 18 04:36:15.715: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.702158ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5969
May 18 04:36:16.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:36:16.882: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 18 04:36:16.882: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 18 04:36:16.882: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 18 04:36:16.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:36:17.035: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 18 04:36:17.035: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 18 04:36:17.035: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 18 04:36:17.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:36:17.220: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 18 04:36:17.220: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 18 04:36:17.220: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 18 04:36:17.223: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 18 04:36:27.234: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 18 04:36:27.234: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 18 04:36:27.234: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 18 04:36:27.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 18 04:36:27.536: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 18 04:36:27.536: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 18 04:36:27.536: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 18 04:36:27.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 18 04:36:27.940: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 18 04:36:27.940: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 18 04:36:27.940: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 18 04:36:27.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 18 04:36:28.459: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 18 04:36:28.459: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 18 04:36:28.459: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 18 04:36:28.459: INFO: Waiting for statefulset status.replicas updated to 0
May 18 04:36:28.467: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 18 04:36:38.481: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 18 04:36:38.482: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 18 04:36:38.482: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 18 04:36:38.499: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 18 04:36:38.499: INFO: ss-0  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  }]
May 18 04:36:38.499: INFO: ss-1  node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:38.499: INFO: ss-2  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:38.499: INFO: 
May 18 04:36:38.499: INFO: StatefulSet ss has not reached scale 0, at 3
May 18 04:36:39.508: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 18 04:36:39.508: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  }]
May 18 04:36:39.508: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:39.508: INFO: ss-2  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:39.508: INFO: 
May 18 04:36:39.508: INFO: StatefulSet ss has not reached scale 0, at 3
May 18 04:36:40.519: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 18 04:36:40.519: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  }]
May 18 04:36:40.519: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:40.519: INFO: ss-2  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:40.519: INFO: 
May 18 04:36:40.519: INFO: StatefulSet ss has not reached scale 0, at 3
May 18 04:36:41.527: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 18 04:36:41.527: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  }]
May 18 04:36:41.527: INFO: ss-2  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:41.527: INFO: 
May 18 04:36:41.527: INFO: StatefulSet ss has not reached scale 0, at 2
May 18 04:36:42.540: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 18 04:36:42.540: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  }]
May 18 04:36:42.540: INFO: ss-2  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:42.540: INFO: 
May 18 04:36:42.540: INFO: StatefulSet ss has not reached scale 0, at 2
May 18 04:36:43.549: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 18 04:36:43.549: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:35:46 +0000 UTC  }]
May 18 04:36:43.549: INFO: ss-2  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:43.549: INFO: 
May 18 04:36:43.549: INFO: StatefulSet ss has not reached scale 0, at 2
May 18 04:36:44.580: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 18 04:36:44.580: INFO: ss-2  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:44.580: INFO: 
May 18 04:36:44.580: INFO: StatefulSet ss has not reached scale 0, at 1
May 18 04:36:45.588: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 18 04:36:45.588: INFO: ss-2  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:45.588: INFO: 
May 18 04:36:45.588: INFO: StatefulSet ss has not reached scale 0, at 1
May 18 04:36:46.594: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 18 04:36:46.594: INFO: ss-2  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:46.594: INFO: 
May 18 04:36:46.594: INFO: StatefulSet ss has not reached scale 0, at 1
May 18 04:36:47.600: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 18 04:36:47.601: INFO: ss-2  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 04:36:06 +0000 UTC  }]
May 18 04:36:47.601: INFO: 
May 18 04:36:47.601: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5969
May 18 04:36:48.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:36:48.746: INFO: rc: 1
May 18 04:36:48.746: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 18 04:36:58.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:36:58.816: INFO: rc: 1
May 18 04:36:58.816: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:37:08.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:37:08.950: INFO: rc: 1
May 18 04:37:08.950: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:37:18.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:37:19.138: INFO: rc: 1
May 18 04:37:19.138: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:37:29.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:37:29.212: INFO: rc: 1
May 18 04:37:29.212: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:37:39.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:37:39.297: INFO: rc: 1
May 18 04:37:39.297: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:37:49.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:37:49.366: INFO: rc: 1
May 18 04:37:49.366: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:37:59.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:37:59.444: INFO: rc: 1
May 18 04:37:59.444: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:38:09.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:38:09.512: INFO: rc: 1
May 18 04:38:09.512: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:38:19.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:38:19.587: INFO: rc: 1
May 18 04:38:19.587: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:38:29.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:38:29.661: INFO: rc: 1
May 18 04:38:29.661: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:38:39.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:38:39.753: INFO: rc: 1
May 18 04:38:39.753: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:38:49.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:38:49.828: INFO: rc: 1
May 18 04:38:49.828: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:38:59.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:38:59.910: INFO: rc: 1
May 18 04:38:59.910: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:39:09.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:39:10.043: INFO: rc: 1
May 18 04:39:10.043: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:39:20.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:39:20.114: INFO: rc: 1
May 18 04:39:20.114: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:39:30.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:39:30.339: INFO: rc: 1
May 18 04:39:30.339: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:39:40.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:39:40.462: INFO: rc: 1
May 18 04:39:40.462: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:39:50.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:39:50.608: INFO: rc: 1
May 18 04:39:50.608: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:40:00.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:40:00.692: INFO: rc: 1
May 18 04:40:00.692: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:40:10.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:40:10.824: INFO: rc: 1
May 18 04:40:10.824: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:40:20.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:40:20.959: INFO: rc: 1
May 18 04:40:20.959: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:40:30.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:40:31.215: INFO: rc: 1
May 18 04:40:31.216: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:40:41.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:40:41.606: INFO: rc: 1
May 18 04:40:41.606: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:40:51.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:40:51.732: INFO: rc: 1
May 18 04:40:51.732: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:41:01.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:41:01.864: INFO: rc: 1
May 18 04:41:01.864: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:41:11.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:41:11.933: INFO: rc: 1
May 18 04:41:11.933: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:41:21.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:41:22.001: INFO: rc: 1
May 18 04:41:22.001: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:41:32.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:41:32.073: INFO: rc: 1
May 18 04:41:32.074: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:41:42.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:41:42.145: INFO: rc: 1
May 18 04:41:42.145: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 18 04:41:52.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 04:41:52.212: INFO: rc: 1
May 18 04:41:52.212: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
May 18 04:41:52.212: INFO: Scaling statefulset ss to 0
May 18 04:41:52.223: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
May 18 04:41:52.225: INFO: Deleting all statefulset in ns statefulset-5969
May 18 04:41:52.227: INFO: Scaling statefulset ss to 0
May 18 04:41:52.234: INFO: Waiting for statefulset status.replicas updated to 0
May 18 04:41:52.236: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:41:52.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5969" for this suite.

• [SLOW TEST:365.886 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":337,"completed":16,"skipped":194,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:41:52.257: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-7b6737a7-ba3e-4a50-aac9-242186c72f75
STEP: Creating a pod to test consume configMaps
May 18 04:41:52.296: INFO: Waiting up to 5m0s for pod "pod-configmaps-5944fafd-2e3c-494f-afc3-91faa1877fb3" in namespace "configmap-2513" to be "Succeeded or Failed"
May 18 04:41:52.300: INFO: Pod "pod-configmaps-5944fafd-2e3c-494f-afc3-91faa1877fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024482ms
May 18 04:41:54.305: INFO: Pod "pod-configmaps-5944fafd-2e3c-494f-afc3-91faa1877fb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008572094s
STEP: Saw pod success
May 18 04:41:54.305: INFO: Pod "pod-configmaps-5944fafd-2e3c-494f-afc3-91faa1877fb3" satisfied condition "Succeeded or Failed"
May 18 04:41:54.307: INFO: Trying to get logs from node node2 pod pod-configmaps-5944fafd-2e3c-494f-afc3-91faa1877fb3 container agnhost-container: <nil>
STEP: delete the pod
May 18 04:41:54.329: INFO: Waiting for pod pod-configmaps-5944fafd-2e3c-494f-afc3-91faa1877fb3 to disappear
May 18 04:41:54.332: INFO: Pod pod-configmaps-5944fafd-2e3c-494f-afc3-91faa1877fb3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:41:54.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2513" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":17,"skipped":202,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:41:54.338: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:41:54.378: INFO: Creating ReplicaSet my-hostname-basic-5e914775-be86-4b14-ac37-cd044201879a
May 18 04:41:54.383: INFO: Pod name my-hostname-basic-5e914775-be86-4b14-ac37-cd044201879a: Found 0 pods out of 1
May 18 04:41:59.386: INFO: Pod name my-hostname-basic-5e914775-be86-4b14-ac37-cd044201879a: Found 1 pods out of 1
May 18 04:41:59.386: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5e914775-be86-4b14-ac37-cd044201879a" is running
May 18 04:41:59.388: INFO: Pod "my-hostname-basic-5e914775-be86-4b14-ac37-cd044201879a-n2nr6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-18 04:41:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-18 04:41:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-18 04:41:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-18 04:41:54 +0000 UTC Reason: Message:}])
May 18 04:41:59.388: INFO: Trying to dial the pod
May 18 04:42:04.403: INFO: Controller my-hostname-basic-5e914775-be86-4b14-ac37-cd044201879a: Got expected result from replica 1 [my-hostname-basic-5e914775-be86-4b14-ac37-cd044201879a-n2nr6]: "my-hostname-basic-5e914775-be86-4b14-ac37-cd044201879a-n2nr6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:42:04.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8678" for this suite.

• [SLOW TEST:10.072 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":337,"completed":18,"skipped":222,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:42:04.410: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
May 18 04:42:04.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7398 create -f -'
May 18 04:42:04.650: INFO: stderr: ""
May 18 04:42:04.650: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 18 04:42:05.654: INFO: Selector matched 1 pods for map[app:agnhost]
May 18 04:42:05.654: INFO: Found 0 / 1
May 18 04:42:06.654: INFO: Selector matched 1 pods for map[app:agnhost]
May 18 04:42:06.654: INFO: Found 1 / 1
May 18 04:42:06.654: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 18 04:42:06.656: INFO: Selector matched 1 pods for map[app:agnhost]
May 18 04:42:06.656: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 18 04:42:06.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7398 patch pod agnhost-primary-zzt2w -p {"metadata":{"annotations":{"x":"y"}}}'
May 18 04:42:06.727: INFO: stderr: ""
May 18 04:42:06.727: INFO: stdout: "pod/agnhost-primary-zzt2w patched\n"
STEP: checking annotations
May 18 04:42:06.730: INFO: Selector matched 1 pods for map[app:agnhost]
May 18 04:42:06.730: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:42:06.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7398" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":337,"completed":19,"skipped":230,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:42:06.736: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:42:06.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5843" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":337,"completed":20,"skipped":250,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:42:06.806: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:42:06.858: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"41b00bf2-a713-4bb5-8e3e-18aacf43be3e", Controller:(*bool)(0xc00089d6fe), BlockOwnerDeletion:(*bool)(0xc00089d6ff)}}
May 18 04:42:06.873: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4f78ff4f-a663-4684-99af-50cf7d15f343", Controller:(*bool)(0xc0001695ae), BlockOwnerDeletion:(*bool)(0xc0001695af)}}
May 18 04:42:06.893: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"75682f06-e692-4a30-9598-a1235217469d", Controller:(*bool)(0xc00089db2e), BlockOwnerDeletion:(*bool)(0xc00089db2f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:42:11.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6630" for this suite.

• [SLOW TEST:5.115 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":337,"completed":21,"skipped":255,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:42:11.921: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:42:13.991: INFO: Deleting pod "var-expansion-b6a32a3a-74e1-4a8d-9ada-5dcdedd63b34" in namespace "var-expansion-6824"
May 18 04:42:13.997: INFO: Wait up to 5m0s for pod "var-expansion-b6a32a3a-74e1-4a8d-9ada-5dcdedd63b34" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:42:26.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6824" for this suite.

• [SLOW TEST:14.094 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":337,"completed":22,"skipped":258,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:42:26.015: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 04:42:26.055: INFO: Waiting up to 5m0s for pod "downwardapi-volume-71a658fe-1661-4d7c-b8fb-b7156f353600" in namespace "downward-api-4456" to be "Succeeded or Failed"
May 18 04:42:26.062: INFO: Pod "downwardapi-volume-71a658fe-1661-4d7c-b8fb-b7156f353600": Phase="Pending", Reason="", readiness=false. Elapsed: 6.957868ms
May 18 04:42:28.069: INFO: Pod "downwardapi-volume-71a658fe-1661-4d7c-b8fb-b7156f353600": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01379507s
STEP: Saw pod success
May 18 04:42:28.069: INFO: Pod "downwardapi-volume-71a658fe-1661-4d7c-b8fb-b7156f353600" satisfied condition "Succeeded or Failed"
May 18 04:42:28.072: INFO: Trying to get logs from node node2 pod downwardapi-volume-71a658fe-1661-4d7c-b8fb-b7156f353600 container client-container: <nil>
STEP: delete the pod
May 18 04:42:28.094: INFO: Waiting for pod downwardapi-volume-71a658fe-1661-4d7c-b8fb-b7156f353600 to disappear
May 18 04:42:28.101: INFO: Pod downwardapi-volume-71a658fe-1661-4d7c-b8fb-b7156f353600 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:42:28.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4456" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":337,"completed":23,"skipped":271,"failed":0}
SSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:42:28.110: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-74425844-e4ff-40d1-a59e-a6dc7a6ebf46
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:42:28.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-807" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":337,"completed":24,"skipped":278,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:42:28.206: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:42:28.247: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7525
I0518 04:42:28.266356      19 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7525, replica count: 1
I0518 04:42:29.317400      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0518 04:42:30.318106      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 18 04:42:30.429: INFO: Created: latency-svc-29rwm
May 18 04:42:30.443: INFO: Got endpoints: latency-svc-29rwm [24.637388ms]
May 18 04:42:30.467: INFO: Created: latency-svc-gp8zp
May 18 04:42:30.483: INFO: Got endpoints: latency-svc-gp8zp [40.269918ms]
May 18 04:42:30.484: INFO: Created: latency-svc-xvct8
May 18 04:42:30.492: INFO: Got endpoints: latency-svc-xvct8 [48.450281ms]
May 18 04:42:30.517: INFO: Created: latency-svc-hrmc8
May 18 04:42:30.522: INFO: Got endpoints: latency-svc-hrmc8 [78.786143ms]
May 18 04:42:30.530: INFO: Created: latency-svc-trs76
May 18 04:42:30.538: INFO: Got endpoints: latency-svc-trs76 [94.82397ms]
May 18 04:42:30.545: INFO: Created: latency-svc-6rm94
May 18 04:42:30.557: INFO: Got endpoints: latency-svc-6rm94 [112.983288ms]
May 18 04:42:30.557: INFO: Created: latency-svc-4k59m
May 18 04:42:30.566: INFO: Got endpoints: latency-svc-4k59m [121.832448ms]
May 18 04:42:30.574: INFO: Created: latency-svc-9vd9g
May 18 04:42:30.581: INFO: Got endpoints: latency-svc-9vd9g [136.92598ms]
May 18 04:42:30.584: INFO: Created: latency-svc-8snqv
May 18 04:42:30.594: INFO: Got endpoints: latency-svc-8snqv [150.514818ms]
May 18 04:42:30.597: INFO: Created: latency-svc-v4nfh
May 18 04:42:30.605: INFO: Got endpoints: latency-svc-v4nfh [161.00737ms]
May 18 04:42:30.606: INFO: Created: latency-svc-6wtb6
May 18 04:42:30.614: INFO: Got endpoints: latency-svc-6wtb6 [170.478227ms]
May 18 04:42:30.615: INFO: Created: latency-svc-tsqdf
May 18 04:42:30.623: INFO: Got endpoints: latency-svc-tsqdf [179.260088ms]
May 18 04:42:30.624: INFO: Created: latency-svc-m2x9m
May 18 04:42:30.632: INFO: Got endpoints: latency-svc-m2x9m [187.714949ms]
May 18 04:42:30.639: INFO: Created: latency-svc-xmhtb
May 18 04:42:30.644: INFO: Got endpoints: latency-svc-xmhtb [199.803794ms]
May 18 04:42:30.647: INFO: Created: latency-svc-w59hf
May 18 04:42:30.653: INFO: Got endpoints: latency-svc-w59hf [209.280351ms]
May 18 04:42:30.654: INFO: Created: latency-svc-gwf2z
May 18 04:42:30.658: INFO: Got endpoints: latency-svc-gwf2z [214.431427ms]
May 18 04:42:30.661: INFO: Created: latency-svc-7tprn
May 18 04:42:30.667: INFO: Got endpoints: latency-svc-7tprn [183.13517ms]
May 18 04:42:30.671: INFO: Created: latency-svc-htrr2
May 18 04:42:30.678: INFO: Got endpoints: latency-svc-htrr2 [185.127661ms]
May 18 04:42:30.684: INFO: Created: latency-svc-6n8zv
May 18 04:42:30.689: INFO: Created: latency-svc-rm9lc
May 18 04:42:30.689: INFO: Got endpoints: latency-svc-6n8zv [167.192942ms]
May 18 04:42:30.698: INFO: Got endpoints: latency-svc-rm9lc [160.192073ms]
May 18 04:42:30.703: INFO: Created: latency-svc-rv92z
May 18 04:42:30.708: INFO: Got endpoints: latency-svc-rv92z [151.705912ms]
May 18 04:42:30.709: INFO: Created: latency-svc-vtp7r
May 18 04:42:30.718: INFO: Created: latency-svc-g9fjj
May 18 04:42:30.718: INFO: Got endpoints: latency-svc-vtp7r [152.693508ms]
May 18 04:42:30.724: INFO: Got endpoints: latency-svc-g9fjj [142.844053ms]
May 18 04:42:30.727: INFO: Created: latency-svc-nlgzw
May 18 04:42:30.732: INFO: Got endpoints: latency-svc-nlgzw [137.752175ms]
May 18 04:42:30.737: INFO: Created: latency-svc-l6bnw
May 18 04:42:30.745: INFO: Got endpoints: latency-svc-l6bnw [139.836566ms]
May 18 04:42:30.750: INFO: Created: latency-svc-6q4f4
May 18 04:42:30.759: INFO: Created: latency-svc-khx5w
May 18 04:42:30.759: INFO: Got endpoints: latency-svc-6q4f4 [145.062342ms]
May 18 04:42:30.763: INFO: Got endpoints: latency-svc-khx5w [140.205364ms]
May 18 04:42:30.768: INFO: Created: latency-svc-xdn79
May 18 04:42:30.771: INFO: Got endpoints: latency-svc-xdn79 [139.700867ms]
May 18 04:42:30.776: INFO: Created: latency-svc-4rmcf
May 18 04:42:30.784: INFO: Got endpoints: latency-svc-4rmcf [140.048365ms]
May 18 04:42:30.793: INFO: Created: latency-svc-4h9x8
May 18 04:42:30.802: INFO: Got endpoints: latency-svc-4h9x8 [148.373728ms]
May 18 04:42:30.804: INFO: Created: latency-svc-6t6ds
May 18 04:42:30.808: INFO: Got endpoints: latency-svc-6t6ds [149.457023ms]
May 18 04:42:30.812: INFO: Created: latency-svc-6nhrr
May 18 04:42:30.820: INFO: Created: latency-svc-zw6pb
May 18 04:42:30.820: INFO: Got endpoints: latency-svc-6nhrr [153.375304ms]
May 18 04:42:30.828: INFO: Got endpoints: latency-svc-zw6pb [150.526717ms]
May 18 04:42:30.829: INFO: Created: latency-svc-kvcbv
May 18 04:42:30.835: INFO: Created: latency-svc-6c6sr
May 18 04:42:30.835: INFO: Got endpoints: latency-svc-kvcbv [145.421241ms]
May 18 04:42:30.842: INFO: Got endpoints: latency-svc-6c6sr [143.569149ms]
May 18 04:42:30.847: INFO: Created: latency-svc-pvsv4
May 18 04:42:30.853: INFO: Got endpoints: latency-svc-pvsv4 [145.150141ms]
May 18 04:42:30.855: INFO: Created: latency-svc-xzzgj
May 18 04:42:30.860: INFO: Got endpoints: latency-svc-xzzgj [142.022456ms]
May 18 04:42:30.864: INFO: Created: latency-svc-wd9pq
May 18 04:42:30.871: INFO: Got endpoints: latency-svc-wd9pq [147.82683ms]
May 18 04:42:30.872: INFO: Created: latency-svc-8q7dd
May 18 04:42:30.891: INFO: Got endpoints: latency-svc-8q7dd [159.013679ms]
May 18 04:42:30.892: INFO: Created: latency-svc-fdvwg
May 18 04:42:30.898: INFO: Created: latency-svc-lr6xc
May 18 04:42:30.905: INFO: Created: latency-svc-n9wds
May 18 04:42:30.914: INFO: Created: latency-svc-c7b44
May 18 04:42:30.925: INFO: Created: latency-svc-gzpgd
May 18 04:42:30.932: INFO: Created: latency-svc-kzvg9
May 18 04:42:30.932: INFO: Got endpoints: latency-svc-fdvwg [187.268751ms]
May 18 04:42:30.939: INFO: Created: latency-svc-pgjd2
May 18 04:42:30.945: INFO: Created: latency-svc-lcqff
May 18 04:42:30.951: INFO: Created: latency-svc-ncpvq
May 18 04:42:30.957: INFO: Created: latency-svc-tnpr4
May 18 04:42:30.965: INFO: Created: latency-svc-9cz9d
May 18 04:42:30.971: INFO: Created: latency-svc-7jplp
May 18 04:42:30.976: INFO: Created: latency-svc-npgwj
May 18 04:42:30.983: INFO: Got endpoints: latency-svc-lr6xc [223.309687ms]
May 18 04:42:30.983: INFO: Created: latency-svc-xv9hp
May 18 04:42:30.996: INFO: Created: latency-svc-bvhrh
May 18 04:42:31.002: INFO: Created: latency-svc-w9x79
May 18 04:42:31.011: INFO: Created: latency-svc-p428h
May 18 04:42:31.032: INFO: Got endpoints: latency-svc-n9wds [268.614082ms]
May 18 04:42:31.041: INFO: Created: latency-svc-c6trs
May 18 04:42:31.084: INFO: Got endpoints: latency-svc-c7b44 [312.976481ms]
May 18 04:42:31.095: INFO: Created: latency-svc-k99fm
May 18 04:42:31.133: INFO: Got endpoints: latency-svc-gzpgd [349.034317ms]
May 18 04:42:31.140: INFO: Created: latency-svc-57g65
May 18 04:42:31.184: INFO: Got endpoints: latency-svc-kzvg9 [382.467266ms]
May 18 04:42:31.194: INFO: Created: latency-svc-b7qhv
May 18 04:42:31.249: INFO: Got endpoints: latency-svc-pgjd2 [441.097601ms]
May 18 04:42:31.264: INFO: Created: latency-svc-wkmj9
May 18 04:42:31.288: INFO: Got endpoints: latency-svc-lcqff [467.864079ms]
May 18 04:42:31.301: INFO: Created: latency-svc-5dlk6
May 18 04:42:31.333: INFO: Got endpoints: latency-svc-ncpvq [504.667212ms]
May 18 04:42:31.345: INFO: Created: latency-svc-8xpsl
May 18 04:42:31.387: INFO: Got endpoints: latency-svc-tnpr4 [551.837098ms]
May 18 04:42:31.396: INFO: Created: latency-svc-n257f
May 18 04:42:31.432: INFO: Got endpoints: latency-svc-9cz9d [589.626727ms]
May 18 04:42:31.444: INFO: Created: latency-svc-fc4k4
May 18 04:42:31.484: INFO: Got endpoints: latency-svc-7jplp [630.320343ms]
May 18 04:42:31.496: INFO: Created: latency-svc-x5x8b
May 18 04:42:31.533: INFO: Got endpoints: latency-svc-npgwj [672.405351ms]
May 18 04:42:31.547: INFO: Created: latency-svc-zqxgc
May 18 04:42:31.590: INFO: Got endpoints: latency-svc-xv9hp [718.901241ms]
May 18 04:42:31.611: INFO: Created: latency-svc-8zgpc
May 18 04:42:31.640: INFO: Got endpoints: latency-svc-bvhrh [749.109304ms]
May 18 04:42:31.654: INFO: Created: latency-svc-4snsk
May 18 04:42:31.684: INFO: Got endpoints: latency-svc-w9x79 [752.372289ms]
May 18 04:42:31.699: INFO: Created: latency-svc-9dxrx
May 18 04:42:31.732: INFO: Got endpoints: latency-svc-p428h [749.544102ms]
May 18 04:42:31.746: INFO: Created: latency-svc-ll9dz
May 18 04:42:31.792: INFO: Got endpoints: latency-svc-c6trs [759.562756ms]
May 18 04:42:31.820: INFO: Created: latency-svc-jg2kg
May 18 04:42:31.835: INFO: Got endpoints: latency-svc-k99fm [751.134394ms]
May 18 04:42:31.856: INFO: Created: latency-svc-p229h
May 18 04:42:31.882: INFO: Got endpoints: latency-svc-57g65 [749.252103ms]
May 18 04:42:31.889: INFO: Created: latency-svc-sj7g2
May 18 04:42:31.933: INFO: Got endpoints: latency-svc-b7qhv [749.332903ms]
May 18 04:42:31.944: INFO: Created: latency-svc-pgd4k
May 18 04:42:31.982: INFO: Got endpoints: latency-svc-wkmj9 [732.737478ms]
May 18 04:42:31.992: INFO: Created: latency-svc-65rsw
May 18 04:42:32.031: INFO: Got endpoints: latency-svc-5dlk6 [743.22313ms]
May 18 04:42:32.043: INFO: Created: latency-svc-gzbpq
May 18 04:42:32.083: INFO: Got endpoints: latency-svc-8xpsl [750.325098ms]
May 18 04:42:32.093: INFO: Created: latency-svc-6c4p7
May 18 04:42:32.133: INFO: Got endpoints: latency-svc-n257f [746.678914ms]
May 18 04:42:32.141: INFO: Created: latency-svc-p8lzn
May 18 04:42:32.181: INFO: Got endpoints: latency-svc-fc4k4 [749.262303ms]
May 18 04:42:32.189: INFO: Created: latency-svc-rx8nq
May 18 04:42:32.233: INFO: Got endpoints: latency-svc-x5x8b [748.768906ms]
May 18 04:42:32.240: INFO: Created: latency-svc-bqfgv
May 18 04:42:32.283: INFO: Got endpoints: latency-svc-zqxgc [750.381798ms]
May 18 04:42:32.291: INFO: Created: latency-svc-rw9tq
May 18 04:42:32.335: INFO: Got endpoints: latency-svc-8zgpc [744.540925ms]
May 18 04:42:32.342: INFO: Created: latency-svc-nbcgh
May 18 04:42:32.383: INFO: Got endpoints: latency-svc-4snsk [742.734433ms]
May 18 04:42:32.397: INFO: Created: latency-svc-x6tqc
May 18 04:42:32.431: INFO: Got endpoints: latency-svc-9dxrx [746.693614ms]
May 18 04:42:32.442: INFO: Created: latency-svc-tdgmp
May 18 04:42:32.483: INFO: Got endpoints: latency-svc-ll9dz [750.887496ms]
May 18 04:42:32.496: INFO: Created: latency-svc-sdqbw
May 18 04:42:32.531: INFO: Got endpoints: latency-svc-jg2kg [738.745051ms]
May 18 04:42:32.539: INFO: Created: latency-svc-rg82f
May 18 04:42:32.586: INFO: Got endpoints: latency-svc-p229h [750.0194ms]
May 18 04:42:32.594: INFO: Created: latency-svc-fh59d
May 18 04:42:32.631: INFO: Got endpoints: latency-svc-sj7g2 [748.550206ms]
May 18 04:42:32.643: INFO: Created: latency-svc-pbs2d
May 18 04:42:32.682: INFO: Got endpoints: latency-svc-pgd4k [748.122108ms]
May 18 04:42:32.689: INFO: Created: latency-svc-qjw9p
May 18 04:42:32.734: INFO: Got endpoints: latency-svc-65rsw [752.504189ms]
May 18 04:42:32.743: INFO: Created: latency-svc-pdmql
May 18 04:42:32.787: INFO: Got endpoints: latency-svc-gzbpq [755.894173ms]
May 18 04:42:32.797: INFO: Created: latency-svc-hzbv4
May 18 04:42:32.836: INFO: Got endpoints: latency-svc-6c4p7 [753.033786ms]
May 18 04:42:32.846: INFO: Created: latency-svc-2lgp8
May 18 04:42:32.881: INFO: Got endpoints: latency-svc-p8lzn [747.587411ms]
May 18 04:42:32.896: INFO: Created: latency-svc-766rv
May 18 04:42:32.933: INFO: Got endpoints: latency-svc-rx8nq [751.895391ms]
May 18 04:42:32.941: INFO: Created: latency-svc-dtt5s
May 18 04:42:32.982: INFO: Got endpoints: latency-svc-bqfgv [749.405602ms]
May 18 04:42:32.992: INFO: Created: latency-svc-scfp8
May 18 04:42:33.032: INFO: Got endpoints: latency-svc-rw9tq [748.490207ms]
May 18 04:42:33.050: INFO: Created: latency-svc-hlfs8
May 18 04:42:33.087: INFO: Got endpoints: latency-svc-nbcgh [752.214289ms]
May 18 04:42:33.096: INFO: Created: latency-svc-dk5dw
May 18 04:42:33.132: INFO: Got endpoints: latency-svc-x6tqc [748.661606ms]
May 18 04:42:33.156: INFO: Created: latency-svc-jt6b7
May 18 04:42:33.187: INFO: Got endpoints: latency-svc-tdgmp [755.564475ms]
May 18 04:42:33.200: INFO: Created: latency-svc-g9d27
May 18 04:42:33.234: INFO: Got endpoints: latency-svc-sdqbw [750.819396ms]
May 18 04:42:33.243: INFO: Created: latency-svc-f7xln
May 18 04:42:33.281: INFO: Got endpoints: latency-svc-rg82f [750.602697ms]
May 18 04:42:33.297: INFO: Created: latency-svc-5s9pf
May 18 04:42:33.334: INFO: Got endpoints: latency-svc-fh59d [748.134808ms]
May 18 04:42:33.344: INFO: Created: latency-svc-9rnbc
May 18 04:42:33.384: INFO: Got endpoints: latency-svc-pbs2d [752.577488ms]
May 18 04:42:33.393: INFO: Created: latency-svc-tsp72
May 18 04:42:33.433: INFO: Got endpoints: latency-svc-qjw9p [751.210994ms]
May 18 04:42:33.454: INFO: Created: latency-svc-scmlg
May 18 04:42:33.485: INFO: Got endpoints: latency-svc-pdmql [750.442498ms]
May 18 04:42:33.499: INFO: Created: latency-svc-47jxn
May 18 04:42:33.538: INFO: Got endpoints: latency-svc-hzbv4 [750.677797ms]
May 18 04:42:33.551: INFO: Created: latency-svc-zk64n
May 18 04:42:33.582: INFO: Got endpoints: latency-svc-2lgp8 [745.394621ms]
May 18 04:42:33.591: INFO: Created: latency-svc-jtc97
May 18 04:42:33.632: INFO: Got endpoints: latency-svc-766rv [750.870496ms]
May 18 04:42:33.641: INFO: Created: latency-svc-t5464
May 18 04:42:33.682: INFO: Got endpoints: latency-svc-dtt5s [748.533406ms]
May 18 04:42:33.692: INFO: Created: latency-svc-x87ct
May 18 04:42:33.732: INFO: Got endpoints: latency-svc-scfp8 [749.463702ms]
May 18 04:42:33.741: INFO: Created: latency-svc-b7vcv
May 18 04:42:33.783: INFO: Got endpoints: latency-svc-hlfs8 [751.458393ms]
May 18 04:42:33.790: INFO: Created: latency-svc-5nzv2
May 18 04:42:33.831: INFO: Got endpoints: latency-svc-dk5dw [744.044227ms]
May 18 04:42:33.842: INFO: Created: latency-svc-6lfs9
May 18 04:42:33.885: INFO: Got endpoints: latency-svc-jt6b7 [753.200386ms]
May 18 04:42:33.892: INFO: Created: latency-svc-vc9zj
May 18 04:42:33.933: INFO: Got endpoints: latency-svc-g9d27 [746.185117ms]
May 18 04:42:33.942: INFO: Created: latency-svc-zkhjm
May 18 04:42:33.984: INFO: Got endpoints: latency-svc-f7xln [749.412903ms]
May 18 04:42:33.992: INFO: Created: latency-svc-7jhpw
May 18 04:42:34.035: INFO: Got endpoints: latency-svc-5s9pf [753.991382ms]
May 18 04:42:34.048: INFO: Created: latency-svc-nflvr
May 18 04:42:34.084: INFO: Got endpoints: latency-svc-9rnbc [750.524697ms]
May 18 04:42:34.097: INFO: Created: latency-svc-ftlmc
May 18 04:42:34.136: INFO: Got endpoints: latency-svc-tsp72 [752.24389ms]
May 18 04:42:34.144: INFO: Created: latency-svc-s9csj
May 18 04:42:34.183: INFO: Got endpoints: latency-svc-scmlg [749.738101ms]
May 18 04:42:34.194: INFO: Created: latency-svc-v79k8
May 18 04:42:34.235: INFO: Got endpoints: latency-svc-47jxn [749.474802ms]
May 18 04:42:34.257: INFO: Created: latency-svc-ss4dk
May 18 04:42:34.281: INFO: Got endpoints: latency-svc-zk64n [743.571229ms]
May 18 04:42:34.299: INFO: Created: latency-svc-7s989
May 18 04:42:34.339: INFO: Got endpoints: latency-svc-jtc97 [756.944869ms]
May 18 04:42:34.366: INFO: Created: latency-svc-8lwpf
May 18 04:42:34.382: INFO: Got endpoints: latency-svc-t5464 [749.7451ms]
May 18 04:42:34.394: INFO: Created: latency-svc-dhh82
May 18 04:42:34.437: INFO: Got endpoints: latency-svc-x87ct [754.925277ms]
May 18 04:42:34.453: INFO: Created: latency-svc-7rvxb
May 18 04:42:34.485: INFO: Got endpoints: latency-svc-b7vcv [753.478984ms]
May 18 04:42:34.498: INFO: Created: latency-svc-xhz4k
May 18 04:42:34.534: INFO: Got endpoints: latency-svc-5nzv2 [750.773296ms]
May 18 04:42:34.543: INFO: Created: latency-svc-ct94g
May 18 04:42:34.588: INFO: Got endpoints: latency-svc-6lfs9 [756.58587ms]
May 18 04:42:34.605: INFO: Created: latency-svc-kqjl9
May 18 04:42:34.634: INFO: Got endpoints: latency-svc-vc9zj [748.640006ms]
May 18 04:42:34.648: INFO: Created: latency-svc-httf5
May 18 04:42:34.686: INFO: Got endpoints: latency-svc-zkhjm [752.841787ms]
May 18 04:42:34.693: INFO: Created: latency-svc-l4xwt
May 18 04:42:34.747: INFO: Got endpoints: latency-svc-7jhpw [763.020541ms]
May 18 04:42:34.774: INFO: Created: latency-svc-9d5nb
May 18 04:42:34.794: INFO: Got endpoints: latency-svc-nflvr [759.058859ms]
May 18 04:42:34.802: INFO: Created: latency-svc-4cgzw
May 18 04:42:34.838: INFO: Got endpoints: latency-svc-ftlmc [753.372184ms]
May 18 04:42:34.845: INFO: Created: latency-svc-kqgsk
May 18 04:42:34.885: INFO: Got endpoints: latency-svc-s9csj [749.422103ms]
May 18 04:42:34.893: INFO: Created: latency-svc-rc4jb
May 18 04:42:34.931: INFO: Got endpoints: latency-svc-v79k8 [748.621206ms]
May 18 04:42:34.945: INFO: Created: latency-svc-dnz26
May 18 04:42:34.983: INFO: Got endpoints: latency-svc-ss4dk [748.096808ms]
May 18 04:42:34.991: INFO: Created: latency-svc-2z79n
May 18 04:42:35.035: INFO: Got endpoints: latency-svc-7s989 [753.090586ms]
May 18 04:42:35.043: INFO: Created: latency-svc-l7hb2
May 18 04:42:35.085: INFO: Got endpoints: latency-svc-8lwpf [745.853218ms]
May 18 04:42:35.092: INFO: Created: latency-svc-dr6fm
May 18 04:42:35.140: INFO: Got endpoints: latency-svc-dhh82 [758.091263ms]
May 18 04:42:35.150: INFO: Created: latency-svc-cfsv9
May 18 04:42:35.181: INFO: Got endpoints: latency-svc-7rvxb [744.859723ms]
May 18 04:42:35.193: INFO: Created: latency-svc-25l4p
May 18 04:42:35.235: INFO: Got endpoints: latency-svc-xhz4k [750.013099ms]
May 18 04:42:35.244: INFO: Created: latency-svc-8767p
May 18 04:42:35.281: INFO: Got endpoints: latency-svc-ct94g [747.084113ms]
May 18 04:42:35.290: INFO: Created: latency-svc-46zpf
May 18 04:42:35.335: INFO: Got endpoints: latency-svc-kqjl9 [746.625915ms]
May 18 04:42:35.343: INFO: Created: latency-svc-ll87q
May 18 04:42:35.381: INFO: Got endpoints: latency-svc-httf5 [747.563211ms]
May 18 04:42:35.391: INFO: Created: latency-svc-57hc5
May 18 04:42:35.435: INFO: Got endpoints: latency-svc-l4xwt [749.200304ms]
May 18 04:42:35.450: INFO: Created: latency-svc-gbx8f
May 18 04:42:35.482: INFO: Got endpoints: latency-svc-9d5nb [735.504566ms]
May 18 04:42:35.511: INFO: Created: latency-svc-z5vlt
May 18 04:42:35.531: INFO: Got endpoints: latency-svc-4cgzw [736.073063ms]
May 18 04:42:35.541: INFO: Created: latency-svc-j4gc9
May 18 04:42:35.586: INFO: Got endpoints: latency-svc-kqgsk [747.75301ms]
May 18 04:42:35.594: INFO: Created: latency-svc-spjmj
May 18 04:42:35.638: INFO: Got endpoints: latency-svc-rc4jb [753.011086ms]
May 18 04:42:35.652: INFO: Created: latency-svc-5vgpt
May 18 04:42:35.701: INFO: Got endpoints: latency-svc-dnz26 [769.892309ms]
May 18 04:42:35.711: INFO: Created: latency-svc-bqdxt
May 18 04:42:35.737: INFO: Got endpoints: latency-svc-2z79n [754.288681ms]
May 18 04:42:35.744: INFO: Created: latency-svc-lfch4
May 18 04:42:35.791: INFO: Got endpoints: latency-svc-l7hb2 [756.180571ms]
May 18 04:42:35.810: INFO: Created: latency-svc-m86wp
May 18 04:42:35.831: INFO: Got endpoints: latency-svc-dr6fm [745.967118ms]
May 18 04:42:35.843: INFO: Created: latency-svc-6jh2t
May 18 04:42:35.891: INFO: Got endpoints: latency-svc-cfsv9 [751.137695ms]
May 18 04:42:35.902: INFO: Created: latency-svc-nqdsh
May 18 04:42:35.939: INFO: Got endpoints: latency-svc-25l4p [757.923563ms]
May 18 04:42:35.947: INFO: Created: latency-svc-9wflf
May 18 04:42:35.983: INFO: Got endpoints: latency-svc-8767p [747.330212ms]
May 18 04:42:35.991: INFO: Created: latency-svc-xsbnl
May 18 04:42:36.032: INFO: Got endpoints: latency-svc-46zpf [750.710597ms]
May 18 04:42:36.047: INFO: Created: latency-svc-44b4g
May 18 04:42:36.082: INFO: Got endpoints: latency-svc-ll87q [746.874514ms]
May 18 04:42:36.093: INFO: Created: latency-svc-gnjc6
May 18 04:42:36.131: INFO: Got endpoints: latency-svc-57hc5 [749.8427ms]
May 18 04:42:36.140: INFO: Created: latency-svc-tznnh
May 18 04:42:36.185: INFO: Got endpoints: latency-svc-gbx8f [749.9549ms]
May 18 04:42:36.193: INFO: Created: latency-svc-gpwdr
May 18 04:42:36.236: INFO: Got endpoints: latency-svc-z5vlt [753.416984ms]
May 18 04:42:36.244: INFO: Created: latency-svc-w9wm4
May 18 04:42:36.284: INFO: Got endpoints: latency-svc-j4gc9 [753.357085ms]
May 18 04:42:36.297: INFO: Created: latency-svc-rs7j4
May 18 04:42:36.341: INFO: Got endpoints: latency-svc-spjmj [755.507575ms]
May 18 04:42:36.349: INFO: Created: latency-svc-sbn47
May 18 04:42:36.384: INFO: Got endpoints: latency-svc-5vgpt [745.71022ms]
May 18 04:42:36.396: INFO: Created: latency-svc-wkhgk
May 18 04:42:36.450: INFO: Got endpoints: latency-svc-bqdxt [748.703106ms]
May 18 04:42:36.462: INFO: Created: latency-svc-kmctg
May 18 04:42:36.487: INFO: Got endpoints: latency-svc-lfch4 [749.515602ms]
May 18 04:42:36.496: INFO: Created: latency-svc-p59d7
May 18 04:42:36.533: INFO: Got endpoints: latency-svc-m86wp [742.346935ms]
May 18 04:42:36.545: INFO: Created: latency-svc-s58z6
May 18 04:42:36.581: INFO: Got endpoints: latency-svc-6jh2t [750.614497ms]
May 18 04:42:36.591: INFO: Created: latency-svc-tkt27
May 18 04:42:36.638: INFO: Got endpoints: latency-svc-nqdsh [746.702515ms]
May 18 04:42:36.653: INFO: Created: latency-svc-klqtv
May 18 04:42:36.684: INFO: Got endpoints: latency-svc-9wflf [744.781724ms]
May 18 04:42:36.693: INFO: Created: latency-svc-n4vjw
May 18 04:42:36.738: INFO: Got endpoints: latency-svc-xsbnl [755.783473ms]
May 18 04:42:36.776: INFO: Created: latency-svc-kwdmb
May 18 04:42:36.789: INFO: Got endpoints: latency-svc-44b4g [756.874068ms]
May 18 04:42:36.803: INFO: Created: latency-svc-wjv26
May 18 04:42:36.846: INFO: Got endpoints: latency-svc-gnjc6 [764.197535ms]
May 18 04:42:36.868: INFO: Created: latency-svc-mvnwb
May 18 04:42:36.887: INFO: Got endpoints: latency-svc-tznnh [755.759874ms]
May 18 04:42:36.899: INFO: Created: latency-svc-89qht
May 18 04:42:36.933: INFO: Got endpoints: latency-svc-gpwdr [747.997709ms]
May 18 04:42:36.947: INFO: Created: latency-svc-bllzj
May 18 04:42:36.981: INFO: Got endpoints: latency-svc-w9wm4 [745.39612ms]
May 18 04:42:36.993: INFO: Created: latency-svc-5rp8l
May 18 04:42:37.032: INFO: Got endpoints: latency-svc-rs7j4 [748.082108ms]
May 18 04:42:37.041: INFO: Created: latency-svc-hkgtz
May 18 04:42:37.085: INFO: Got endpoints: latency-svc-sbn47 [743.568129ms]
May 18 04:42:37.102: INFO: Created: latency-svc-kb4cc
May 18 04:42:37.133: INFO: Got endpoints: latency-svc-wkhgk [749.034205ms]
May 18 04:42:37.141: INFO: Created: latency-svc-p5lph
May 18 04:42:37.184: INFO: Got endpoints: latency-svc-kmctg [733.812074ms]
May 18 04:42:37.192: INFO: Created: latency-svc-6d9sk
May 18 04:42:37.231: INFO: Got endpoints: latency-svc-p59d7 [744.400925ms]
May 18 04:42:37.239: INFO: Created: latency-svc-74b6c
May 18 04:42:37.286: INFO: Got endpoints: latency-svc-s58z6 [752.607188ms]
May 18 04:42:37.299: INFO: Created: latency-svc-4rcdp
May 18 04:42:37.335: INFO: Got endpoints: latency-svc-tkt27 [753.478484ms]
May 18 04:42:37.344: INFO: Created: latency-svc-v2p5d
May 18 04:42:37.385: INFO: Got endpoints: latency-svc-klqtv [746.843514ms]
May 18 04:42:37.393: INFO: Created: latency-svc-w8tn5
May 18 04:42:37.434: INFO: Got endpoints: latency-svc-n4vjw [749.648301ms]
May 18 04:42:37.449: INFO: Created: latency-svc-6jqwv
May 18 04:42:37.488: INFO: Got endpoints: latency-svc-kwdmb [749.368503ms]
May 18 04:42:37.504: INFO: Created: latency-svc-wpq8z
May 18 04:42:37.539: INFO: Got endpoints: latency-svc-wjv26 [750.389198ms]
May 18 04:42:37.547: INFO: Created: latency-svc-w7rgm
May 18 04:42:37.586: INFO: Got endpoints: latency-svc-mvnwb [740.375643ms]
May 18 04:42:37.597: INFO: Created: latency-svc-pw77b
May 18 04:42:37.635: INFO: Got endpoints: latency-svc-89qht [747.78031ms]
May 18 04:42:37.643: INFO: Created: latency-svc-lh6j5
May 18 04:42:37.683: INFO: Got endpoints: latency-svc-bllzj [750.038899ms]
May 18 04:42:37.691: INFO: Created: latency-svc-9r2l8
May 18 04:42:37.734: INFO: Got endpoints: latency-svc-5rp8l [752.583488ms]
May 18 04:42:37.741: INFO: Created: latency-svc-sdpxl
May 18 04:42:37.786: INFO: Got endpoints: latency-svc-hkgtz [753.636483ms]
May 18 04:42:37.794: INFO: Created: latency-svc-f2zdg
May 18 04:42:37.831: INFO: Got endpoints: latency-svc-kb4cc [746.124618ms]
May 18 04:42:37.846: INFO: Created: latency-svc-bkpfg
May 18 04:42:37.883: INFO: Got endpoints: latency-svc-p5lph [749.477702ms]
May 18 04:42:37.890: INFO: Created: latency-svc-g2g7j
May 18 04:42:37.931: INFO: Got endpoints: latency-svc-6d9sk [747.157113ms]
May 18 04:42:37.940: INFO: Created: latency-svc-pfwpx
May 18 04:42:37.981: INFO: Got endpoints: latency-svc-74b6c [750.371398ms]
May 18 04:42:37.991: INFO: Created: latency-svc-8w5mv
May 18 04:42:38.031: INFO: Got endpoints: latency-svc-4rcdp [745.051523ms]
May 18 04:42:38.040: INFO: Created: latency-svc-7z8gj
May 18 04:42:38.083: INFO: Got endpoints: latency-svc-v2p5d [748.648906ms]
May 18 04:42:38.104: INFO: Created: latency-svc-22s79
May 18 04:42:38.131: INFO: Got endpoints: latency-svc-w8tn5 [746.150618ms]
May 18 04:42:38.139: INFO: Created: latency-svc-6g4jz
May 18 04:42:38.183: INFO: Got endpoints: latency-svc-6jqwv [748.492406ms]
May 18 04:42:38.191: INFO: Created: latency-svc-qv8ck
May 18 04:42:38.230: INFO: Got endpoints: latency-svc-wpq8z [742.271935ms]
May 18 04:42:38.243: INFO: Created: latency-svc-lstqr
May 18 04:42:38.284: INFO: Got endpoints: latency-svc-w7rgm [744.420525ms]
May 18 04:42:38.335: INFO: Got endpoints: latency-svc-pw77b [748.864105ms]
May 18 04:42:38.381: INFO: Got endpoints: latency-svc-lh6j5 [746.384916ms]
May 18 04:42:38.431: INFO: Got endpoints: latency-svc-9r2l8 [747.79451ms]
May 18 04:42:38.483: INFO: Got endpoints: latency-svc-sdpxl [748.870505ms]
May 18 04:42:38.536: INFO: Got endpoints: latency-svc-f2zdg [750.690997ms]
May 18 04:42:38.584: INFO: Got endpoints: latency-svc-bkpfg [752.737788ms]
May 18 04:42:38.648: INFO: Got endpoints: latency-svc-g2g7j [765.591829ms]
May 18 04:42:38.690: INFO: Got endpoints: latency-svc-pfwpx [758.966459ms]
May 18 04:42:38.735: INFO: Got endpoints: latency-svc-8w5mv [753.942982ms]
May 18 04:42:38.803: INFO: Got endpoints: latency-svc-7z8gj [771.808201ms]
May 18 04:42:38.831: INFO: Got endpoints: latency-svc-22s79 [747.487211ms]
May 18 04:42:38.881: INFO: Got endpoints: latency-svc-6g4jz [749.8292ms]
May 18 04:42:38.937: INFO: Got endpoints: latency-svc-qv8ck [754.36718ms]
May 18 04:42:38.986: INFO: Got endpoints: latency-svc-lstqr [755.463975ms]
May 18 04:42:38.986: INFO: Latencies: [40.269918ms 48.450281ms 78.786143ms 94.82397ms 112.983288ms 121.832448ms 136.92598ms 137.752175ms 139.700867ms 139.836566ms 140.048365ms 140.205364ms 142.022456ms 142.844053ms 143.569149ms 145.062342ms 145.150141ms 145.421241ms 147.82683ms 148.373728ms 149.457023ms 150.514818ms 150.526717ms 151.705912ms 152.693508ms 153.375304ms 159.013679ms 160.192073ms 161.00737ms 167.192942ms 170.478227ms 179.260088ms 183.13517ms 185.127661ms 187.268751ms 187.714949ms 199.803794ms 209.280351ms 214.431427ms 223.309687ms 268.614082ms 312.976481ms 349.034317ms 382.467266ms 441.097601ms 467.864079ms 504.667212ms 551.837098ms 589.626727ms 630.320343ms 672.405351ms 718.901241ms 732.737478ms 733.812074ms 735.504566ms 736.073063ms 738.745051ms 740.375643ms 742.271935ms 742.346935ms 742.734433ms 743.22313ms 743.568129ms 743.571229ms 744.044227ms 744.400925ms 744.420525ms 744.540925ms 744.781724ms 744.859723ms 745.051523ms 745.394621ms 745.39612ms 745.71022ms 745.853218ms 745.967118ms 746.124618ms 746.150618ms 746.185117ms 746.384916ms 746.625915ms 746.678914ms 746.693614ms 746.702515ms 746.843514ms 746.874514ms 747.084113ms 747.157113ms 747.330212ms 747.487211ms 747.563211ms 747.587411ms 747.75301ms 747.78031ms 747.79451ms 747.997709ms 748.082108ms 748.096808ms 748.122108ms 748.134808ms 748.490207ms 748.492406ms 748.533406ms 748.550206ms 748.621206ms 748.640006ms 748.648906ms 748.661606ms 748.703106ms 748.768906ms 748.864105ms 748.870505ms 749.034205ms 749.109304ms 749.200304ms 749.252103ms 749.262303ms 749.332903ms 749.368503ms 749.405602ms 749.412903ms 749.422103ms 749.463702ms 749.474802ms 749.477702ms 749.515602ms 749.544102ms 749.648301ms 749.738101ms 749.7451ms 749.8292ms 749.8427ms 749.9549ms 750.013099ms 750.0194ms 750.038899ms 750.325098ms 750.371398ms 750.381798ms 750.389198ms 750.442498ms 750.524697ms 750.602697ms 750.614497ms 750.677797ms 750.690997ms 750.710597ms 750.773296ms 750.819396ms 750.870496ms 750.887496ms 751.134394ms 751.137695ms 751.210994ms 751.458393ms 751.895391ms 752.214289ms 752.24389ms 752.372289ms 752.504189ms 752.577488ms 752.583488ms 752.607188ms 752.737788ms 752.841787ms 753.011086ms 753.033786ms 753.090586ms 753.200386ms 753.357085ms 753.372184ms 753.416984ms 753.478484ms 753.478984ms 753.636483ms 753.942982ms 753.991382ms 754.288681ms 754.36718ms 754.925277ms 755.463975ms 755.507575ms 755.564475ms 755.759874ms 755.783473ms 755.894173ms 756.180571ms 756.58587ms 756.874068ms 756.944869ms 757.923563ms 758.091263ms 758.966459ms 759.058859ms 759.562756ms 763.020541ms 764.197535ms 765.591829ms 769.892309ms 771.808201ms]
May 18 04:42:38.986: INFO: 50 %ile: 748.490207ms
May 18 04:42:38.986: INFO: 90 %ile: 755.463975ms
May 18 04:42:38.986: INFO: 99 %ile: 769.892309ms
May 18 04:42:38.986: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:42:38.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7525" for this suite.

• [SLOW TEST:10.787 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":337,"completed":25,"skipped":317,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:42:38.994: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-2583
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2583
STEP: Creating statefulset with conflicting port in namespace statefulset-2583
STEP: Waiting until pod test-pod will start running in namespace statefulset-2583
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2583
May 18 04:42:43.058: INFO: Observed stateful pod in namespace: statefulset-2583, name: ss-0, uid: 9afe52d3-4caa-4bcc-a563-f289f0e26816, status phase: Pending. Waiting for statefulset controller to delete.
May 18 04:42:43.442: INFO: Observed stateful pod in namespace: statefulset-2583, name: ss-0, uid: 9afe52d3-4caa-4bcc-a563-f289f0e26816, status phase: Failed. Waiting for statefulset controller to delete.
May 18 04:42:43.455: INFO: Observed stateful pod in namespace: statefulset-2583, name: ss-0, uid: 9afe52d3-4caa-4bcc-a563-f289f0e26816, status phase: Failed. Waiting for statefulset controller to delete.
May 18 04:42:43.461: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2583
STEP: Removing pod with conflicting port in namespace statefulset-2583
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2583 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
May 18 04:42:47.498: INFO: Deleting all statefulset in ns statefulset-2583
May 18 04:42:47.501: INFO: Scaling statefulset ss to 0
May 18 04:42:57.523: INFO: Waiting for statefulset status.replicas updated to 0
May 18 04:42:57.525: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:42:57.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2583" for this suite.

• [SLOW TEST:18.552 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":337,"completed":26,"skipped":320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:42:57.546: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 18 04:42:57.573: INFO: Waiting up to 5m0s for pod "pod-c80bff12-60cc-4ad2-bdfd-41ae9a706256" in namespace "emptydir-5529" to be "Succeeded or Failed"
May 18 04:42:57.578: INFO: Pod "pod-c80bff12-60cc-4ad2-bdfd-41ae9a706256": Phase="Pending", Reason="", readiness=false. Elapsed: 4.801279ms
May 18 04:42:59.583: INFO: Pod "pod-c80bff12-60cc-4ad2-bdfd-41ae9a706256": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00950109s
STEP: Saw pod success
May 18 04:42:59.583: INFO: Pod "pod-c80bff12-60cc-4ad2-bdfd-41ae9a706256" satisfied condition "Succeeded or Failed"
May 18 04:42:59.585: INFO: Trying to get logs from node node2 pod pod-c80bff12-60cc-4ad2-bdfd-41ae9a706256 container test-container: <nil>
STEP: delete the pod
May 18 04:42:59.596: INFO: Waiting for pod pod-c80bff12-60cc-4ad2-bdfd-41ae9a706256 to disappear
May 18 04:42:59.598: INFO: Pod pod-c80bff12-60cc-4ad2-bdfd-41ae9a706256 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:42:59.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5529" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":27,"skipped":388,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:42:59.605: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
May 18 04:42:59.643: INFO: The status of Pod labelsupdate29e8d7c7-076d-417f-8946-0f99e7a3f0d2 is Pending, waiting for it to be Running (with Ready = true)
May 18 04:43:01.648: INFO: The status of Pod labelsupdate29e8d7c7-076d-417f-8946-0f99e7a3f0d2 is Running (Ready = true)
May 18 04:43:02.166: INFO: Successfully updated pod "labelsupdate29e8d7c7-076d-417f-8946-0f99e7a3f0d2"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:43:04.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4324" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":337,"completed":28,"skipped":400,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:43:04.203: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-ba443413-9408-496f-9e89-052e3d2dc8cb in namespace container-probe-5894
May 18 04:43:06.256: INFO: Started pod liveness-ba443413-9408-496f-9e89-052e3d2dc8cb in namespace container-probe-5894
STEP: checking the pod's current state and verifying that restartCount is present
May 18 04:43:06.258: INFO: Initial restart count of pod liveness-ba443413-9408-496f-9e89-052e3d2dc8cb is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:47:07.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5894" for this suite.

• [SLOW TEST:242.877 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":337,"completed":29,"skipped":402,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:47:07.080: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
May 18 04:47:07.136: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:47:10.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4980" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":337,"completed":30,"skipped":418,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:47:10.607: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5484.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5484.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5484.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5484.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5484.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5484.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 18 04:47:16.736: INFO: DNS probes using dns-5484/dns-test-0875687f-b094-4979-baa4-7c784088f639 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:47:16.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5484" for this suite.

• [SLOW TEST:6.215 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":337,"completed":31,"skipped":419,"failed":0}
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:47:16.822: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-d9965abf-b96c-4d84-8fc3-d7b79d9b0a84 in namespace container-probe-1671
May 18 04:47:20.966: INFO: Started pod liveness-d9965abf-b96c-4d84-8fc3-d7b79d9b0a84 in namespace container-probe-1671
STEP: checking the pod's current state and verifying that restartCount is present
May 18 04:47:20.969: INFO: Initial restart count of pod liveness-d9965abf-b96c-4d84-8fc3-d7b79d9b0a84 is 0
May 18 04:47:39.040: INFO: Restart count of pod container-probe-1671/liveness-d9965abf-b96c-4d84-8fc3-d7b79d9b0a84 is now 1 (18.071106573s elapsed)
May 18 04:47:59.110: INFO: Restart count of pod container-probe-1671/liveness-d9965abf-b96c-4d84-8fc3-d7b79d9b0a84 is now 2 (38.140475186s elapsed)
May 18 04:48:19.177: INFO: Restart count of pod container-probe-1671/liveness-d9965abf-b96c-4d84-8fc3-d7b79d9b0a84 is now 3 (58.20743521s elapsed)
May 18 04:48:39.259: INFO: Restart count of pod container-probe-1671/liveness-d9965abf-b96c-4d84-8fc3-d7b79d9b0a84 is now 4 (1m18.289877164s elapsed)
May 18 04:49:45.450: INFO: Restart count of pod container-probe-1671/liveness-d9965abf-b96c-4d84-8fc3-d7b79d9b0a84 is now 5 (2m24.48064368s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:49:45.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1671" for this suite.

• [SLOW TEST:148.657 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":337,"completed":32,"skipped":419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:49:45.479: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 18 04:49:45.790: INFO: Pod name wrapped-volume-race-fdf98458-f979-4de9-856c-45665a6ba2f9: Found 1 pods out of 5
May 18 04:49:50.811: INFO: Pod name wrapped-volume-race-fdf98458-f979-4de9-856c-45665a6ba2f9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fdf98458-f979-4de9-856c-45665a6ba2f9 in namespace emptydir-wrapper-3001, will wait for the garbage collector to delete the pods
May 18 04:50:00.890: INFO: Deleting ReplicationController wrapped-volume-race-fdf98458-f979-4de9-856c-45665a6ba2f9 took: 3.755383ms
May 18 04:50:00.991: INFO: Terminating ReplicationController wrapped-volume-race-fdf98458-f979-4de9-856c-45665a6ba2f9 pods took: 101.066341ms
STEP: Creating RC which spawns configmap-volume pods
May 18 04:50:11.154: INFO: Pod name wrapped-volume-race-6876d4fd-0398-4618-a78e-8c4886a3885b: Found 0 pods out of 5
May 18 04:50:16.170: INFO: Pod name wrapped-volume-race-6876d4fd-0398-4618-a78e-8c4886a3885b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6876d4fd-0398-4618-a78e-8c4886a3885b in namespace emptydir-wrapper-3001, will wait for the garbage collector to delete the pods
May 18 04:50:26.251: INFO: Deleting ReplicationController wrapped-volume-race-6876d4fd-0398-4618-a78e-8c4886a3885b took: 3.794782ms
May 18 04:50:26.352: INFO: Terminating ReplicationController wrapped-volume-race-6876d4fd-0398-4618-a78e-8c4886a3885b pods took: 100.827343ms
STEP: Creating RC which spawns configmap-volume pods
May 18 04:50:34.670: INFO: Pod name wrapped-volume-race-053bf0d2-7cf9-4963-8ebd-f496256b3aea: Found 0 pods out of 5
May 18 04:50:39.689: INFO: Pod name wrapped-volume-race-053bf0d2-7cf9-4963-8ebd-f496256b3aea: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-053bf0d2-7cf9-4963-8ebd-f496256b3aea in namespace emptydir-wrapper-3001, will wait for the garbage collector to delete the pods
May 18 04:50:51.767: INFO: Deleting ReplicationController wrapped-volume-race-053bf0d2-7cf9-4963-8ebd-f496256b3aea took: 3.504985ms
May 18 04:50:51.868: INFO: Terminating ReplicationController wrapped-volume-race-053bf0d2-7cf9-4963-8ebd-f496256b3aea pods took: 100.521144ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:51:04.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3001" for this suite.

• [SLOW TEST:79.413 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":337,"completed":33,"skipped":454,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:51:04.892: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:51:04.935: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 18 04:51:09.943: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 18 04:51:09.943: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 18 04:51:11.971: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3768  19904ea8-7c29-416f-9f69-d5493406c461 23784 1 2021-05-18 04:51:09 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-05-18 04:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-18 04:51:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00265b028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-18 04:51:09 +0000 UTC,LastTransitionTime:2021-05-18 04:51:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5b4d99b59b" has successfully progressed.,LastUpdateTime:2021-05-18 04:51:11 +0000 UTC,LastTransitionTime:2021-05-18 04:51:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 18 04:51:11.973: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-3768  1b36ccc1-2d16-4b38-95d3-a2e71d95cea7 23773 1 2021-05-18 04:51:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 19904ea8-7c29-416f-9f69-d5493406c461 0xc00265b3e7 0xc00265b3e8}] []  [{kube-controller-manager Update apps/v1 2021-05-18 04:51:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"19904ea8-7c29-416f-9f69-d5493406c461\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00265b5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 18 04:51:11.975: INFO: Pod "test-cleanup-deployment-5b4d99b59b-4rwd2" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-4rwd2 test-cleanup-deployment-5b4d99b59b- deployment-3768  17034411-1d3a-4d1b-8278-8a7236330fed 23772 0 2021-05-18 04:51:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[cni.projectcalico.org/podIP:172.30.166.143/32 cni.projectcalico.org/podIPs:172.30.166.143/32] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b 1b36ccc1-2d16-4b38-95d3-a2e71d95cea7 0xc00265b987 0xc00265b988}] []  [{kube-controller-manager Update v1 2021-05-18 04:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b36ccc1-2d16-4b38-95d3-a2e71d95cea7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 04:51:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 04:51:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.166.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5wc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5wc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 04:51:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 04:51:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 04:51:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 04:51:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.166.143,StartTime:2021-05-18 04:51:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 04:51:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://b099ba185461c658eda1362048dc6765d1762256e06d5a3efe9491ed5d097abc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.166.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:51:11.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3768" for this suite.

• [SLOW TEST:7.089 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":337,"completed":34,"skipped":456,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:51:11.982: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:51:23.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6316" for this suite.

• [SLOW TEST:11.180 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":337,"completed":35,"skipped":500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:51:23.163: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:51:23.187: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:51:23.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2948" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":337,"completed":36,"skipped":548,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:51:23.737: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-5904e8bc-da82-43c9-bf5d-e905dd7adbe9
May 18 04:51:23.791: INFO: Pod name my-hostname-basic-5904e8bc-da82-43c9-bf5d-e905dd7adbe9: Found 0 pods out of 1
May 18 04:51:28.798: INFO: Pod name my-hostname-basic-5904e8bc-da82-43c9-bf5d-e905dd7adbe9: Found 1 pods out of 1
May 18 04:51:28.798: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5904e8bc-da82-43c9-bf5d-e905dd7adbe9" are running
May 18 04:51:28.801: INFO: Pod "my-hostname-basic-5904e8bc-da82-43c9-bf5d-e905dd7adbe9-m4f7w" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-18 04:51:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-18 04:51:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-18 04:51:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-18 04:51:23 +0000 UTC Reason: Message:}])
May 18 04:51:28.802: INFO: Trying to dial the pod
May 18 04:51:33.813: INFO: Controller my-hostname-basic-5904e8bc-da82-43c9-bf5d-e905dd7adbe9: Got expected result from replica 1 [my-hostname-basic-5904e8bc-da82-43c9-bf5d-e905dd7adbe9-m4f7w]: "my-hostname-basic-5904e8bc-da82-43c9-bf5d-e905dd7adbe9-m4f7w", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:51:33.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1593" for this suite.

• [SLOW TEST:10.083 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":337,"completed":37,"skipped":569,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:51:33.820: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:51:33.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9284" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":337,"completed":38,"skipped":570,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:51:33.865: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
May 18 04:51:33.907: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May 18 04:51:35.911: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May 18 04:51:37.913: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:51:38.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9445" for this suite.

• [SLOW TEST:5.081 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":337,"completed":39,"skipped":617,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:51:38.947: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-cabc528d-eaa4-4b93-b421-ea061479551b
STEP: Creating a pod to test consume configMaps
May 18 04:51:39.002: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a9873c7a-e177-4734-a10b-91c0cfe9890c" in namespace "projected-2056" to be "Succeeded or Failed"
May 18 04:51:39.011: INFO: Pod "pod-projected-configmaps-a9873c7a-e177-4734-a10b-91c0cfe9890c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.116058ms
May 18 04:51:41.019: INFO: Pod "pod-projected-configmaps-a9873c7a-e177-4734-a10b-91c0cfe9890c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017198427s
STEP: Saw pod success
May 18 04:51:41.019: INFO: Pod "pod-projected-configmaps-a9873c7a-e177-4734-a10b-91c0cfe9890c" satisfied condition "Succeeded or Failed"
May 18 04:51:41.022: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-a9873c7a-e177-4734-a10b-91c0cfe9890c container agnhost-container: <nil>
STEP: delete the pod
May 18 04:51:41.061: INFO: Waiting for pod pod-projected-configmaps-a9873c7a-e177-4734-a10b-91c0cfe9890c to disappear
May 18 04:51:41.069: INFO: Pod pod-projected-configmaps-a9873c7a-e177-4734-a10b-91c0cfe9890c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:51:41.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2056" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":40,"skipped":684,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:51:41.078: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
May 18 04:51:41.136: INFO: created test-event-1
May 18 04:51:41.140: INFO: created test-event-2
May 18 04:51:41.143: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 18 04:51:41.146: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 18 04:51:41.159: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:51:41.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3793" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":337,"completed":41,"skipped":699,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:51:41.171: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 18 04:51:41.225: INFO: Waiting up to 1m0s for all nodes to be ready
May 18 04:52:41.261: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
May 18 04:52:41.282: INFO: Created pod: pod0-sched-preemption-low-priority
May 18 04:52:41.307: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:52:57.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6125" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:76.212 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":337,"completed":42,"skipped":734,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:52:57.383: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
May 18 04:54:57.932: INFO: Successfully updated pod "var-expansion-a9eee0ca-d59a-4ed9-aefa-433e8c2490ba"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 18 04:54:59.942: INFO: Deleting pod "var-expansion-a9eee0ca-d59a-4ed9-aefa-433e8c2490ba" in namespace "var-expansion-1277"
May 18 04:54:59.946: INFO: Wait up to 5m0s for pod "var-expansion-a9eee0ca-d59a-4ed9-aefa-433e8c2490ba" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:55:35.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1277" for this suite.

• [SLOW TEST:158.583 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":337,"completed":43,"skipped":747,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:55:35.965: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
May 18 04:55:36.011: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 18 04:55:38.018: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 18 04:55:40.022: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
May 18 04:55:40.038: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 18 04:55:42.050: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 18 04:55:44.050: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 18 04:55:44.074: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 18 04:55:44.077: INFO: Pod pod-with-poststart-exec-hook still exists
May 18 04:55:46.077: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 18 04:55:46.081: INFO: Pod pod-with-poststart-exec-hook still exists
May 18 04:55:48.079: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 18 04:55:48.082: INFO: Pod pod-with-poststart-exec-hook still exists
May 18 04:55:50.077: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 18 04:55:50.084: INFO: Pod pod-with-poststart-exec-hook still exists
May 18 04:55:52.077: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 18 04:55:52.083: INFO: Pod pod-with-poststart-exec-hook still exists
May 18 04:55:54.077: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 18 04:55:54.081: INFO: Pod pod-with-poststart-exec-hook still exists
May 18 04:55:56.078: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 18 04:55:56.085: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:55:56.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3731" for this suite.

• [SLOW TEST:20.130 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":337,"completed":44,"skipped":757,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:55:56.095: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 18 04:55:56.147: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:55:56.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4332" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":337,"completed":45,"skipped":766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:55:56.178: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-534ab79d-ecdd-46d0-aa3a-c094f76648b8
STEP: Creating configMap with name cm-test-opt-upd-42bf0161-5e2b-4ea0-ad4c-af8b1209032c
STEP: Creating the pod
May 18 04:55:56.221: INFO: The status of Pod pod-projected-configmaps-d3d9788a-f7d6-46eb-9981-b79523f88c78 is Pending, waiting for it to be Running (with Ready = true)
May 18 04:55:58.227: INFO: The status of Pod pod-projected-configmaps-d3d9788a-f7d6-46eb-9981-b79523f88c78 is Pending, waiting for it to be Running (with Ready = true)
May 18 04:56:00.227: INFO: The status of Pod pod-projected-configmaps-d3d9788a-f7d6-46eb-9981-b79523f88c78 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-534ab79d-ecdd-46d0-aa3a-c094f76648b8
STEP: Updating configmap cm-test-opt-upd-42bf0161-5e2b-4ea0-ad4c-af8b1209032c
STEP: Creating configMap with name cm-test-opt-create-a76ac86a-b5d3-407e-82a7-3d70a5ae689b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:57:18.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2006" for this suite.

• [SLOW TEST:82.430 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":337,"completed":46,"skipped":801,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:57:18.609: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:57:18.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6765" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":337,"completed":47,"skipped":809,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:57:18.649: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 04:57:19.120: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 04:57:22.142: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:57:22.146: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:57:25.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9225" for this suite.
STEP: Destroying namespace "webhook-9225-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.701 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":337,"completed":48,"skipped":826,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:57:25.350: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
May 18 04:57:25.444: INFO: Waiting up to 5m0s for pod "var-expansion-7b122ddb-438d-464b-8547-73867e644be4" in namespace "var-expansion-5489" to be "Succeeded or Failed"
May 18 04:57:25.464: INFO: Pod "var-expansion-7b122ddb-438d-464b-8547-73867e644be4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.379008ms
May 18 04:57:27.470: INFO: Pod "var-expansion-7b122ddb-438d-464b-8547-73867e644be4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026236414s
May 18 04:57:29.475: INFO: Pod "var-expansion-7b122ddb-438d-464b-8547-73867e644be4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031171624s
STEP: Saw pod success
May 18 04:57:29.475: INFO: Pod "var-expansion-7b122ddb-438d-464b-8547-73867e644be4" satisfied condition "Succeeded or Failed"
May 18 04:57:29.477: INFO: Trying to get logs from node node2 pod var-expansion-7b122ddb-438d-464b-8547-73867e644be4 container dapi-container: <nil>
STEP: delete the pod
May 18 04:57:29.495: INFO: Waiting for pod var-expansion-7b122ddb-438d-464b-8547-73867e644be4 to disappear
May 18 04:57:29.498: INFO: Pod var-expansion-7b122ddb-438d-464b-8547-73867e644be4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:57:29.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5489" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":337,"completed":49,"skipped":844,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:57:29.506: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 04:57:29.530: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Creating first CR 
May 18 04:57:32.072: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-18T04:57:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-18T04:57:32Z]] name:name1 resourceVersion:24995 uid:72186130-08ee-40b2-b4a6-b52fe61420a0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 18 04:57:42.083: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-18T04:57:42Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-18T04:57:42Z]] name:name2 resourceVersion:25024 uid:918d71d4-ca61-416b-bded-89d2df41b55c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 18 04:57:52.089: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-18T04:57:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-18T04:57:52Z]] name:name1 resourceVersion:25041 uid:72186130-08ee-40b2-b4a6-b52fe61420a0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 18 04:58:02.104: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-18T04:57:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-18T04:58:02Z]] name:name2 resourceVersion:25056 uid:918d71d4-ca61-416b-bded-89d2df41b55c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 18 04:58:12.124: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-18T04:57:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-18T04:57:52Z]] name:name1 resourceVersion:25071 uid:72186130-08ee-40b2-b4a6-b52fe61420a0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 18 04:58:22.134: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-18T04:57:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-18T04:58:02Z]] name:name2 resourceVersion:25086 uid:918d71d4-ca61-416b-bded-89d2df41b55c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:58:32.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6162" for this suite.

• [SLOW TEST:63.152 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":337,"completed":50,"skipped":851,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:58:32.658: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
May 18 04:58:32.711: INFO: The status of Pod annotationupdate2de32073-0987-45f3-bb5b-cbc1a806f348 is Pending, waiting for it to be Running (with Ready = true)
May 18 04:58:34.715: INFO: The status of Pod annotationupdate2de32073-0987-45f3-bb5b-cbc1a806f348 is Running (Ready = true)
May 18 04:58:35.235: INFO: Successfully updated pod "annotationupdate2de32073-0987-45f3-bb5b-cbc1a806f348"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:58:39.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7063" for this suite.

• [SLOW TEST:6.614 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":337,"completed":51,"skipped":856,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:58:39.272: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:59:07.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7442" for this suite.

• [SLOW TEST:28.076 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":337,"completed":52,"skipped":911,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:59:07.349: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
May 18 04:59:07.391: INFO: Waiting up to 5m0s for pod "security-context-7e8d9ecd-1309-4531-8840-cc3f2581a59e" in namespace "security-context-5611" to be "Succeeded or Failed"
May 18 04:59:07.406: INFO: Pod "security-context-7e8d9ecd-1309-4531-8840-cc3f2581a59e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.342035ms
May 18 04:59:09.412: INFO: Pod "security-context-7e8d9ecd-1309-4531-8840-cc3f2581a59e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020252641s
STEP: Saw pod success
May 18 04:59:09.412: INFO: Pod "security-context-7e8d9ecd-1309-4531-8840-cc3f2581a59e" satisfied condition "Succeeded or Failed"
May 18 04:59:09.414: INFO: Trying to get logs from node node2 pod security-context-7e8d9ecd-1309-4531-8840-cc3f2581a59e container test-container: <nil>
STEP: delete the pod
May 18 04:59:09.427: INFO: Waiting for pod security-context-7e8d9ecd-1309-4531-8840-cc3f2581a59e to disappear
May 18 04:59:09.429: INFO: Pod security-context-7e8d9ecd-1309-4531-8840-cc3f2581a59e no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 04:59:09.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5611" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":337,"completed":53,"skipped":937,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 04:59:09.436: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0518 04:59:09.477955      19 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:05:01.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6670" for this suite.

• [SLOW TEST:352.080 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":337,"completed":54,"skipped":947,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:05:01.516: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
May 18 05:05:04.089: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8128 pod-service-account-24821ab5-6a20-40f5-95d4-ba75f5f9aa12 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 18 05:05:04.586: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8128 pod-service-account-24821ab5-6a20-40f5-95d4-ba75f5f9aa12 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 18 05:05:04.747: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8128 pod-service-account-24821ab5-6a20-40f5-95d4-ba75f5f9aa12 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:05:04.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8128" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":337,"completed":55,"skipped":970,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:05:04.939: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May 18 05:05:04.970: INFO: Waiting up to 5m0s for pod "downward-api-ff5a499f-0620-4bbd-97a7-3d5b6364dcfc" in namespace "downward-api-72" to be "Succeeded or Failed"
May 18 05:05:04.972: INFO: Pod "downward-api-ff5a499f-0620-4bbd-97a7-3d5b6364dcfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297189ms
May 18 05:05:06.977: INFO: Pod "downward-api-ff5a499f-0620-4bbd-97a7-3d5b6364dcfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0072005s
STEP: Saw pod success
May 18 05:05:06.977: INFO: Pod "downward-api-ff5a499f-0620-4bbd-97a7-3d5b6364dcfc" satisfied condition "Succeeded or Failed"
May 18 05:05:06.980: INFO: Trying to get logs from node node1 pod downward-api-ff5a499f-0620-4bbd-97a7-3d5b6364dcfc container dapi-container: <nil>
STEP: delete the pod
May 18 05:05:07.011: INFO: Waiting for pod downward-api-ff5a499f-0620-4bbd-97a7-3d5b6364dcfc to disappear
May 18 05:05:07.014: INFO: Pod downward-api-ff5a499f-0620-4bbd-97a7-3d5b6364dcfc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:05:07.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-72" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":337,"completed":56,"skipped":975,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:05:07.023: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:05:07.079: INFO: The status of Pod busybox-host-aliasesd7f196be-8ebe-4c2a-a54f-4bb9a1c62da6 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:05:09.087: INFO: The status of Pod busybox-host-aliasesd7f196be-8ebe-4c2a-a54f-4bb9a1c62da6 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:05:09.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-661" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":57,"skipped":1007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:05:09.110: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8553, will wait for the garbage collector to delete the pods
May 18 05:05:13.213: INFO: Deleting Job.batch foo took: 4.47848ms
May 18 05:05:13.314: INFO: Terminating Job.batch foo pods took: 100.935842ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:05:54.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8553" for this suite.

• [SLOW TEST:45.513 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":337,"completed":58,"skipped":1029,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:05:54.623: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-59feee8d-38c0-48eb-ad73-bd1458760585
STEP: Creating a pod to test consume secrets
May 18 05:05:54.658: INFO: Waiting up to 5m0s for pod "pod-secrets-a15f2f22-4901-490f-8cdd-02d03bc1e330" in namespace "secrets-5164" to be "Succeeded or Failed"
May 18 05:05:54.662: INFO: Pod "pod-secrets-a15f2f22-4901-490f-8cdd-02d03bc1e330": Phase="Pending", Reason="", readiness=false. Elapsed: 4.472679ms
May 18 05:05:56.668: INFO: Pod "pod-secrets-a15f2f22-4901-490f-8cdd-02d03bc1e330": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009889988s
STEP: Saw pod success
May 18 05:05:56.668: INFO: Pod "pod-secrets-a15f2f22-4901-490f-8cdd-02d03bc1e330" satisfied condition "Succeeded or Failed"
May 18 05:05:56.671: INFO: Trying to get logs from node node2 pod pod-secrets-a15f2f22-4901-490f-8cdd-02d03bc1e330 container secret-volume-test: <nil>
STEP: delete the pod
May 18 05:05:56.701: INFO: Waiting for pod pod-secrets-a15f2f22-4901-490f-8cdd-02d03bc1e330 to disappear
May 18 05:05:56.703: INFO: Pod pod-secrets-a15f2f22-4901-490f-8cdd-02d03bc1e330 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:05:56.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5164" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":59,"skipped":1059,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:05:56.713: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-d0157c42-024b-4caa-b65b-5e5f59a65708
STEP: Creating a pod to test consume configMaps
May 18 05:05:56.763: INFO: Waiting up to 5m0s for pod "pod-configmaps-1734d7e4-922c-44a0-bb7a-6ac006d8b39c" in namespace "configmap-9149" to be "Succeeded or Failed"
May 18 05:05:56.774: INFO: Pod "pod-configmaps-1734d7e4-922c-44a0-bb7a-6ac006d8b39c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.96785ms
May 18 05:05:58.784: INFO: Pod "pod-configmaps-1734d7e4-922c-44a0-bb7a-6ac006d8b39c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020672339s
May 18 05:06:00.790: INFO: Pod "pod-configmaps-1734d7e4-922c-44a0-bb7a-6ac006d8b39c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027536441s
STEP: Saw pod success
May 18 05:06:00.790: INFO: Pod "pod-configmaps-1734d7e4-922c-44a0-bb7a-6ac006d8b39c" satisfied condition "Succeeded or Failed"
May 18 05:06:00.794: INFO: Trying to get logs from node node2 pod pod-configmaps-1734d7e4-922c-44a0-bb7a-6ac006d8b39c container agnhost-container: <nil>
STEP: delete the pod
May 18 05:06:00.815: INFO: Waiting for pod pod-configmaps-1734d7e4-922c-44a0-bb7a-6ac006d8b39c to disappear
May 18 05:06:00.817: INFO: Pod pod-configmaps-1734d7e4-922c-44a0-bb7a-6ac006d8b39c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:06:00.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9149" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":60,"skipped":1063,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:06:00.823: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
May 18 05:06:00.863: INFO: The status of Pod annotationupdatedfcd7fde-3474-48b7-8813-329011b62faf is Pending, waiting for it to be Running (with Ready = true)
May 18 05:06:02.868: INFO: The status of Pod annotationupdatedfcd7fde-3474-48b7-8813-329011b62faf is Pending, waiting for it to be Running (with Ready = true)
May 18 05:06:04.867: INFO: The status of Pod annotationupdatedfcd7fde-3474-48b7-8813-329011b62faf is Running (Ready = true)
May 18 05:06:05.392: INFO: Successfully updated pod "annotationupdatedfcd7fde-3474-48b7-8813-329011b62faf"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:06:07.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8538" for this suite.

• [SLOW TEST:6.596 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":337,"completed":61,"skipped":1063,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:06:07.419: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:06:07.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8327 create -f -'
May 18 05:06:07.681: INFO: stderr: ""
May 18 05:06:07.681: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 18 05:06:07.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8327 create -f -'
May 18 05:06:07.891: INFO: stderr: ""
May 18 05:06:07.891: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 18 05:06:08.900: INFO: Selector matched 1 pods for map[app:agnhost]
May 18 05:06:08.900: INFO: Found 0 / 1
May 18 05:06:09.899: INFO: Selector matched 1 pods for map[app:agnhost]
May 18 05:06:09.899: INFO: Found 0 / 1
May 18 05:06:10.898: INFO: Selector matched 1 pods for map[app:agnhost]
May 18 05:06:10.898: INFO: Found 1 / 1
May 18 05:06:10.898: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 18 05:06:10.902: INFO: Selector matched 1 pods for map[app:agnhost]
May 18 05:06:10.902: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 18 05:06:10.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8327 describe pod agnhost-primary-6gkx5'
May 18 05:06:11.040: INFO: stderr: ""
May 18 05:06:11.040: INFO: stdout: "Name:         agnhost-primary-6gkx5\nNamespace:    kubectl-8327\nPriority:     0\nNode:         node2/172.28.128.13\nStart Time:   Tue, 18 May 2021 05:06:07 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 172.30.104.57/32\n              cni.projectcalico.org/podIPs: 172.30.104.57/32\nStatus:       Running\nIP:           172.30.104.57\nIPs:\n  IP:           172.30.104.57\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://dd5270d16c9692154af242abf219f9a10c76f894662d5dc1ebbdd92fd6400352\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 18 May 2021 05:06:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-b7h5v (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-b7h5v:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 60s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 60s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-8327/agnhost-primary-6gkx5 to node2\n  Normal  Pulled     3s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    3s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
May 18 05:06:11.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8327 describe rc agnhost-primary'
May 18 05:06:11.188: INFO: stderr: ""
May 18 05:06:11.188: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8327\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-6gkx5\n"
May 18 05:06:11.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8327 describe service agnhost-primary'
May 18 05:06:11.313: INFO: stderr: ""
May 18 05:06:11.313: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8327\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.24.182.149\nIPs:               172.24.182.149\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.104.57:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 18 05:06:11.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8327 describe node master'
May 18 05:06:11.477: INFO: stderr: ""
May 18 05:06:11.477: INFO: stdout: "Name:               master\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.28.128.11/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 172.30.219.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 18 May 2021 01:19:29 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 18 May 2021 05:06:05 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 18 May 2021 02:04:10 +0000   Tue, 18 May 2021 02:04:10 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 18 May 2021 05:04:46 +0000   Tue, 18 May 2021 01:19:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 18 May 2021 05:04:46 +0000   Tue, 18 May 2021 03:58:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 18 May 2021 05:04:46 +0000   Tue, 18 May 2021 01:19:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 18 May 2021 05:04:46 +0000   Tue, 18 May 2021 01:19:29 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.28.128.11\n  Hostname:    master\nCapacity:\n  cpu:                3\n  ephemeral-storage:  41152736Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3880648Ki\n  pods:               110\nAllocatable:\n  cpu:                2900m\n  ephemeral-storage:  37926361435\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2808086Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 63fc159b073c42bd81b9ac3c209a8ecd\n  System UUID:                FA2A646A-F50B-C548-86DB-8CADE6659268\n  Boot ID:                    8a11eb2b-6f9e-4096-bd73-24cb82f946e8\n  Kernel Version:             3.10.0-957.21.3.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.2\n  Kubelet Version:            v1.21.1\n  Kube-Proxy Version:         v1.21.1\nPodCIDR:                      172.30.0.0/24\nPodCIDRs:                     172.30.0.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-76d8c67897-jfznz                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h2m\n  kube-system                 calico-node-q9fw4                                          250m (8%)     0 (0%)      0 (0%)           0 (0%)         3h2m\n  kube-system                 coredns-6db894fbbc-bck4c                                   100m (3%)     0 (0%)      70Mi (2%)        170Mi (6%)     3h44m\n  kube-system                 coredns-6db894fbbc-kgwjt                                   100m (3%)     0 (0%)      70Mi (2%)        170Mi (6%)     3h44m\n  kube-system                 etcd-master                                                100m (3%)     0 (0%)      100Mi (3%)       0 (0%)         3h46m\n  kube-system                 kube-apiserver-master                                      250m (8%)     0 (0%)      0 (0%)           0 (0%)         3h46m\n  kube-system                 kube-controller-manager-master                             200m (6%)     0 (0%)      0 (0%)           0 (0%)         3h46m\n  kube-system                 kube-proxy-pmm29                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h46m\n  kube-system                 kube-scheduler-master                                      100m (3%)     0 (0%)      0 (0%)           0 (0%)         3h46m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0e6496dadced4aae-4vmh2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         36m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1100m (37%)  0 (0%)\n  memory             240Mi (8%)   340Mi (12%)\n  ephemeral-storage  100Mi (0%)   0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
May 18 05:06:11.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8327 describe namespace kubectl-8327'
May 18 05:06:11.599: INFO: stderr: ""
May 18 05:06:11.599: INFO: stdout: "Name:         kubectl-8327\nLabels:       e2e-framework=kubectl\n              e2e-run=46774d82-87bb-430e-bae9-92efa50bf752\n              kubernetes.io/metadata.name=kubectl-8327\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:06:11.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8327" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":337,"completed":62,"skipped":1068,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:06:11.610: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:06:11.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-440" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":337,"completed":63,"skipped":1090,"failed":0}

------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:06:11.681: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:06:11.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7747" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":337,"completed":64,"skipped":1090,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:06:11.767: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
May 18 05:06:11.810: INFO: Waiting up to 5m0s for pod "client-containers-6b79b465-45ba-4397-827b-9d1405ef75ec" in namespace "containers-1663" to be "Succeeded or Failed"
May 18 05:06:11.821: INFO: Pod "client-containers-6b79b465-45ba-4397-827b-9d1405ef75ec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.395852ms
May 18 05:06:13.825: INFO: Pod "client-containers-6b79b465-45ba-4397-827b-9d1405ef75ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015362263s
STEP: Saw pod success
May 18 05:06:13.826: INFO: Pod "client-containers-6b79b465-45ba-4397-827b-9d1405ef75ec" satisfied condition "Succeeded or Failed"
May 18 05:06:13.827: INFO: Trying to get logs from node node1 pod client-containers-6b79b465-45ba-4397-827b-9d1405ef75ec container agnhost-container: <nil>
STEP: delete the pod
May 18 05:06:13.842: INFO: Waiting for pod client-containers-6b79b465-45ba-4397-827b-9d1405ef75ec to disappear
May 18 05:06:13.844: INFO: Pod client-containers-6b79b465-45ba-4397-827b-9d1405ef75ec no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:06:13.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1663" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":337,"completed":65,"skipped":1106,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:06:13.849: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:06:15.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2518" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]","total":337,"completed":66,"skipped":1121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:06:15.912: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:06:15.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8514" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":337,"completed":67,"skipped":1170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:06:15.990: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
May 18 05:06:16.023: INFO: Waiting up to 5m0s for pod "pod-686484cb-041f-42f3-b146-7f3cfde6be76" in namespace "emptydir-8442" to be "Succeeded or Failed"
May 18 05:06:16.026: INFO: Pod "pod-686484cb-041f-42f3-b146-7f3cfde6be76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.454784ms
May 18 05:06:18.030: INFO: Pod "pod-686484cb-041f-42f3-b146-7f3cfde6be76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006772102s
STEP: Saw pod success
May 18 05:06:18.030: INFO: Pod "pod-686484cb-041f-42f3-b146-7f3cfde6be76" satisfied condition "Succeeded or Failed"
May 18 05:06:18.032: INFO: Trying to get logs from node node2 pod pod-686484cb-041f-42f3-b146-7f3cfde6be76 container test-container: <nil>
STEP: delete the pod
May 18 05:06:18.049: INFO: Waiting for pod pod-686484cb-041f-42f3-b146-7f3cfde6be76 to disappear
May 18 05:06:18.051: INFO: Pod pod-686484cb-041f-42f3-b146-7f3cfde6be76 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:06:18.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8442" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":68,"skipped":1238,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:06:18.058: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-j998
STEP: Creating a pod to test atomic-volume-subpath
May 18 05:06:18.100: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-j998" in namespace "subpath-775" to be "Succeeded or Failed"
May 18 05:06:18.110: INFO: Pod "pod-subpath-test-secret-j998": Phase="Pending", Reason="", readiness=false. Elapsed: 10.364753ms
May 18 05:06:20.116: INFO: Pod "pod-subpath-test-secret-j998": Phase="Running", Reason="", readiness=true. Elapsed: 2.01610916s
May 18 05:06:22.121: INFO: Pod "pod-subpath-test-secret-j998": Phase="Running", Reason="", readiness=true. Elapsed: 4.021439068s
May 18 05:06:24.127: INFO: Pod "pod-subpath-test-secret-j998": Phase="Running", Reason="", readiness=true. Elapsed: 6.027172675s
May 18 05:06:26.133: INFO: Pod "pod-subpath-test-secret-j998": Phase="Running", Reason="", readiness=true. Elapsed: 8.03338468s
May 18 05:06:28.147: INFO: Pod "pod-subpath-test-secret-j998": Phase="Running", Reason="", readiness=true. Elapsed: 10.046937151s
May 18 05:06:30.157: INFO: Pod "pod-subpath-test-secret-j998": Phase="Running", Reason="", readiness=true. Elapsed: 12.05656754s
May 18 05:06:32.163: INFO: Pod "pod-subpath-test-secret-j998": Phase="Running", Reason="", readiness=true. Elapsed: 14.062604145s
May 18 05:06:34.168: INFO: Pod "pod-subpath-test-secret-j998": Phase="Running", Reason="", readiness=true. Elapsed: 16.068464652s
May 18 05:06:36.176: INFO: Pod "pod-subpath-test-secret-j998": Phase="Running", Reason="", readiness=true. Elapsed: 18.075897951s
May 18 05:06:38.181: INFO: Pod "pod-subpath-test-secret-j998": Phase="Running", Reason="", readiness=true. Elapsed: 20.080903261s
May 18 05:06:40.187: INFO: Pod "pod-subpath-test-secret-j998": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.086688667s
STEP: Saw pod success
May 18 05:06:40.187: INFO: Pod "pod-subpath-test-secret-j998" satisfied condition "Succeeded or Failed"
May 18 05:06:40.189: INFO: Trying to get logs from node node1 pod pod-subpath-test-secret-j998 container test-container-subpath-secret-j998: <nil>
STEP: delete the pod
May 18 05:06:40.208: INFO: Waiting for pod pod-subpath-test-secret-j998 to disappear
May 18 05:06:40.211: INFO: Pod pod-subpath-test-secret-j998 no longer exists
STEP: Deleting pod pod-subpath-test-secret-j998
May 18 05:06:40.211: INFO: Deleting pod "pod-subpath-test-secret-j998" in namespace "subpath-775"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:06:40.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-775" for this suite.

• [SLOW TEST:22.164 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":337,"completed":69,"skipped":1262,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:06:40.222: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:06:51.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-207" for this suite.

• [SLOW TEST:11.101 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":337,"completed":70,"skipped":1263,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:06:51.323: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:06:51.716: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:06:54.736: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:06:54.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9684" for this suite.
STEP: Destroying namespace "webhook-9684-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":337,"completed":71,"skipped":1275,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:06:54.803: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
May 18 05:07:14.958: INFO: EndpointSlice for Service endpointslice-2275/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:07:24.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2275" for this suite.

• [SLOW TEST:30.172 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":337,"completed":72,"skipped":1296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:07:24.976: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-0b48a7e0-2a7b-4d92-af09-2d872d70b019
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:07:25.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-506" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":337,"completed":73,"skipped":1329,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:07:25.023: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 18 05:07:25.066: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3271  eb664cdc-ae5e-45ae-850b-98fdcd1db7cc 26688 0 2021-05-18 05:07:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-18 05:07:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 05:07:25.067: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3271  eb664cdc-ae5e-45ae-850b-98fdcd1db7cc 26689 0 2021-05-18 05:07:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-18 05:07:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 18 05:07:25.077: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3271  eb664cdc-ae5e-45ae-850b-98fdcd1db7cc 26690 0 2021-05-18 05:07:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-18 05:07:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 05:07:25.077: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3271  eb664cdc-ae5e-45ae-850b-98fdcd1db7cc 26691 0 2021-05-18 05:07:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-18 05:07:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:07:25.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3271" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":337,"completed":74,"skipped":1331,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:07:25.083: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
May 18 05:07:25.115: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 18 05:07:25.115: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 18 05:07:25.129: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 18 05:07:25.129: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 18 05:07:25.165: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 18 05:07:25.165: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 18 05:07:25.185: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 18 05:07:25.185: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 18 05:07:27.260: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 18 05:07:27.260: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 18 05:07:27.272: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
May 18 05:07:27.280: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 0
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:27.281: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:27.286: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:27.286: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:27.350: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:27.350: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:27.354: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:27.355: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:27.369: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1
May 18 05:07:27.369: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1
May 18 05:07:29.030: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:29.030: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:29.060: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1
STEP: listing Deployments
May 18 05:07:29.066: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
May 18 05:07:29.077: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
May 18 05:07:29.084: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 18 05:07:29.089: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 18 05:07:29.109: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 18 05:07:29.143: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 18 05:07:29.150: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 18 05:07:29.168: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 18 05:07:31.468: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 18 05:07:31.488: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
May 18 05:07:31.544: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
May 18 05:07:31.549: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 18 05:07:33.610: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
May 18 05:07:33.650: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1
May 18 05:07:33.650: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1
May 18 05:07:33.650: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1
May 18 05:07:33.650: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1
May 18 05:07:33.650: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1
May 18 05:07:33.650: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 1
May 18 05:07:33.650: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:33.650: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 3
May 18 05:07:33.651: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 3
May 18 05:07:33.651: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 2
May 18 05:07:33.651: INFO: observed Deployment test-deployment in namespace deployment-8028 with ReadyReplicas 3
STEP: deleting the Deployment
May 18 05:07:33.658: INFO: observed event type MODIFIED
May 18 05:07:33.658: INFO: observed event type MODIFIED
May 18 05:07:33.658: INFO: observed event type MODIFIED
May 18 05:07:33.658: INFO: observed event type MODIFIED
May 18 05:07:33.658: INFO: observed event type MODIFIED
May 18 05:07:33.658: INFO: observed event type MODIFIED
May 18 05:07:33.658: INFO: observed event type MODIFIED
May 18 05:07:33.658: INFO: observed event type MODIFIED
May 18 05:07:33.658: INFO: observed event type MODIFIED
May 18 05:07:33.658: INFO: observed event type MODIFIED
May 18 05:07:33.658: INFO: observed event type MODIFIED
May 18 05:07:33.658: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 18 05:07:33.667: INFO: Log out all the ReplicaSets if there is no deployment created
May 18 05:07:33.669: INFO: ReplicaSet "test-deployment-748588b7cd":
&ReplicaSet{ObjectMeta:{test-deployment-748588b7cd  deployment-8028  fea7fb9f-dc85-4782-812b-b7aebad7a9f6 26933 4 2021-05-18 05:07:27 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 598a401b-7e6d-4dd2-8297-25fa3fd30919 0xc0015c91f7 0xc0015c91f8}] []  [{kube-controller-manager Update apps/v1 2021-05-18 05:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"598a401b-7e6d-4dd2-8297-25fa3fd30919\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 748588b7cd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.4.1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0015c92b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 18 05:07:33.672: INFO: ReplicaSet "test-deployment-7b4c744884":
&ReplicaSet{ObjectMeta:{test-deployment-7b4c744884  deployment-8028  46ff2004-25bc-40c8-be8a-c443b1df6793 26783 3 2021-05-18 05:07:25 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 598a401b-7e6d-4dd2-8297-25fa3fd30919 0xc0015c93f7 0xc0015c93f8}] []  [{kube-controller-manager Update apps/v1 2021-05-18 05:07:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"598a401b-7e6d-4dd2-8297-25fa3fd30919\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b4c744884,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0015c94f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 18 05:07:33.678: INFO: ReplicaSet "test-deployment-85d87c6f4b":
&ReplicaSet{ObjectMeta:{test-deployment-85d87c6f4b  deployment-8028  0af76c51-b0ef-4049-aab7-a4b38d38ff0e 26924 2 2021-05-18 05:07:29 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 598a401b-7e6d-4dd2-8297-25fa3fd30919 0xc0015c9557 0xc0015c9558}] []  [{kube-controller-manager Update apps/v1 2021-05-18 05:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"598a401b-7e6d-4dd2-8297-25fa3fd30919\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 85d87c6f4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0015c95e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

May 18 05:07:33.681: INFO: pod: "test-deployment-85d87c6f4b-2c2mw":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-2c2mw test-deployment-85d87c6f4b- deployment-8028  45573789-75a5-4b3e-8663-c394e1fa039f 26886 0 2021-05-18 05:07:29 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[cni.projectcalico.org/podIP:172.30.166.154/32 cni.projectcalico.org/podIPs:172.30.166.154/32] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b 0af76c51-b0ef-4049-aab7-a4b38d38ff0e 0xc0036463b7 0xc0036463b8}] []  [{kube-controller-manager Update v1 2021-05-18 05:07:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0af76c51-b0ef-4049-aab7-a4b38d38ff0e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 05:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 05:07:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.166.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pztsb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pztsb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:07:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.166.154,StartTime:2021-05-18 05:07:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:07:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://79dcfa1ba9c670102819cfe5843d194be37c67669ca813cdc65129da0c04f265,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.166.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 18 05:07:33.681: INFO: pod: "test-deployment-85d87c6f4b-685cl":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-685cl test-deployment-85d87c6f4b- deployment-8028  f6166641-94ba-482f-b3c1-a5facb7d54e2 26923 0 2021-05-18 05:07:31 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[cni.projectcalico.org/podIP:172.30.166.156/32 cni.projectcalico.org/podIPs:172.30.166.156/32] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b 0af76c51-b0ef-4049-aab7-a4b38d38ff0e 0xc0036465b7 0xc0036465b8}] []  [{kube-controller-manager Update v1 2021-05-18 05:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0af76c51-b0ef-4049-aab7-a4b38d38ff0e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 05:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 05:07:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.166.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9pdqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9pdqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:07:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:07:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:07:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.166.156,StartTime:2021-05-18 05:07:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:07:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://6c0dd37707b4a9a0a72375ad5d45a79f5329654bcaddfff01282ff38a2505896,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.166.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:07:33.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8028" for this suite.

• [SLOW TEST:8.622 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":337,"completed":75,"skipped":1336,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:07:33.705: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
May 18 05:07:33.765: INFO: Waiting up to 5m0s for pod "var-expansion-67dd99e4-96c5-4955-b082-39868e032479" in namespace "var-expansion-8324" to be "Succeeded or Failed"
May 18 05:07:33.776: INFO: Pod "var-expansion-67dd99e4-96c5-4955-b082-39868e032479": Phase="Pending", Reason="", readiness=false. Elapsed: 10.757251ms
May 18 05:07:35.783: INFO: Pod "var-expansion-67dd99e4-96c5-4955-b082-39868e032479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01820735s
STEP: Saw pod success
May 18 05:07:35.783: INFO: Pod "var-expansion-67dd99e4-96c5-4955-b082-39868e032479" satisfied condition "Succeeded or Failed"
May 18 05:07:35.786: INFO: Trying to get logs from node node1 pod var-expansion-67dd99e4-96c5-4955-b082-39868e032479 container dapi-container: <nil>
STEP: delete the pod
May 18 05:07:35.809: INFO: Waiting for pod var-expansion-67dd99e4-96c5-4955-b082-39868e032479 to disappear
May 18 05:07:35.812: INFO: Pod var-expansion-67dd99e4-96c5-4955-b082-39868e032479 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:07:35.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8324" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":337,"completed":76,"skipped":1343,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:07:35.824: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-364b028a-4eda-400c-a164-7531c4b704dd
STEP: Creating a pod to test consume secrets
May 18 05:07:35.867: INFO: Waiting up to 5m0s for pod "pod-secrets-97ac588c-1e71-4146-a900-e601594b85db" in namespace "secrets-7911" to be "Succeeded or Failed"
May 18 05:07:35.876: INFO: Pod "pod-secrets-97ac588c-1e71-4146-a900-e601594b85db": Phase="Pending", Reason="", readiness=false. Elapsed: 8.889059ms
May 18 05:07:37.882: INFO: Pod "pod-secrets-97ac588c-1e71-4146-a900-e601594b85db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014341767s
STEP: Saw pod success
May 18 05:07:37.882: INFO: Pod "pod-secrets-97ac588c-1e71-4146-a900-e601594b85db" satisfied condition "Succeeded or Failed"
May 18 05:07:37.884: INFO: Trying to get logs from node node2 pod pod-secrets-97ac588c-1e71-4146-a900-e601594b85db container secret-volume-test: <nil>
STEP: delete the pod
May 18 05:07:37.900: INFO: Waiting for pod pod-secrets-97ac588c-1e71-4146-a900-e601594b85db to disappear
May 18 05:07:37.902: INFO: Pod pod-secrets-97ac588c-1e71-4146-a900-e601594b85db no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:07:37.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7911" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":337,"completed":77,"skipped":1369,"failed":0}

------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:07:37.908: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
May 18 05:07:37.950: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
May 18 05:07:39.961: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
May 18 05:07:41.976: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:07:43.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-6132" for this suite.

• [SLOW TEST:6.081 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":337,"completed":78,"skipped":1369,"failed":0}
SSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:07:43.989: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:07:44.014: INFO: Creating pod...
May 18 05:07:44.023: INFO: Pod Quantity: 1 Status: Pending
May 18 05:07:45.028: INFO: Pod Quantity: 1 Status: Pending
May 18 05:07:46.028: INFO: Pod Status: Running
May 18 05:07:46.028: INFO: Creating service...
May 18 05:07:46.037: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/pods/agnhost/proxy/some/path/with/DELETE
May 18 05:07:46.046: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 18 05:07:46.046: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/pods/agnhost/proxy/some/path/with/GET
May 18 05:07:46.050: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 18 05:07:46.050: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/pods/agnhost/proxy/some/path/with/HEAD
May 18 05:07:46.052: INFO: http.Client request:HEAD | StatusCode:200
May 18 05:07:46.052: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/pods/agnhost/proxy/some/path/with/OPTIONS
May 18 05:07:46.055: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 18 05:07:46.055: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/pods/agnhost/proxy/some/path/with/PATCH
May 18 05:07:46.058: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 18 05:07:46.058: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/pods/agnhost/proxy/some/path/with/POST
May 18 05:07:46.061: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 18 05:07:46.061: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/pods/agnhost/proxy/some/path/with/PUT
May 18 05:07:46.063: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 18 05:07:46.063: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/services/test-service/proxy/some/path/with/DELETE
May 18 05:07:46.067: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 18 05:07:46.067: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/services/test-service/proxy/some/path/with/GET
May 18 05:07:46.071: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 18 05:07:46.071: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/services/test-service/proxy/some/path/with/HEAD
May 18 05:07:46.074: INFO: http.Client request:HEAD | StatusCode:200
May 18 05:07:46.074: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/services/test-service/proxy/some/path/with/OPTIONS
May 18 05:07:46.078: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 18 05:07:46.078: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/services/test-service/proxy/some/path/with/PATCH
May 18 05:07:46.080: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 18 05:07:46.080: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/services/test-service/proxy/some/path/with/POST
May 18 05:07:46.084: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 18 05:07:46.084: INFO: Starting http.Client for https://172.24.0.1:443/api/v1/namespaces/proxy-369/services/test-service/proxy/some/path/with/PUT
May 18 05:07:46.097: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:07:46.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-369" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":337,"completed":79,"skipped":1373,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:07:46.107: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 18 05:07:46.163: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:07:46.168: INFO: Number of nodes with available pods: 0
May 18 05:07:46.168: INFO: Node node1 is running more than one daemon pod
May 18 05:07:47.177: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:07:47.181: INFO: Number of nodes with available pods: 0
May 18 05:07:47.181: INFO: Node node1 is running more than one daemon pod
May 18 05:07:48.173: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:07:48.175: INFO: Number of nodes with available pods: 1
May 18 05:07:48.175: INFO: Node node2 is running more than one daemon pod
May 18 05:07:49.173: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:07:49.180: INFO: Number of nodes with available pods: 2
May 18 05:07:49.180: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 18 05:07:49.205: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:07:49.209: INFO: Number of nodes with available pods: 1
May 18 05:07:49.209: INFO: Node node2 is running more than one daemon pod
May 18 05:07:50.217: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:07:50.221: INFO: Number of nodes with available pods: 1
May 18 05:07:50.221: INFO: Node node2 is running more than one daemon pod
May 18 05:07:51.220: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:07:51.223: INFO: Number of nodes with available pods: 2
May 18 05:07:51.223: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3896, will wait for the garbage collector to delete the pods
May 18 05:07:51.290: INFO: Deleting DaemonSet.extensions daemon-set took: 5.587375ms
May 18 05:07:51.390: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.521844ms
May 18 05:08:04.595: INFO: Number of nodes with available pods: 0
May 18 05:08:04.595: INFO: Number of running nodes: 0, number of available pods: 0
May 18 05:08:04.598: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27276"},"items":null}

May 18 05:08:04.600: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27276"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:08:04.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3896" for this suite.

• [SLOW TEST:18.510 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":337,"completed":80,"skipped":1376,"failed":0}
S
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:08:04.617: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:08:04.658: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5b325eef-9fc3-4a58-9f58-866e39952935" in namespace "security-context-test-9560" to be "Succeeded or Failed"
May 18 05:08:04.666: INFO: Pod "busybox-user-65534-5b325eef-9fc3-4a58-9f58-866e39952935": Phase="Pending", Reason="", readiness=false. Elapsed: 8.297862ms
May 18 05:08:06.669: INFO: Pod "busybox-user-65534-5b325eef-9fc3-4a58-9f58-866e39952935": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011225481s
May 18 05:08:08.675: INFO: Pod "busybox-user-65534-5b325eef-9fc3-4a58-9f58-866e39952935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016874789s
May 18 05:08:08.675: INFO: Pod "busybox-user-65534-5b325eef-9fc3-4a58-9f58-866e39952935" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:08:08.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9560" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":81,"skipped":1377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:08:08.682: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:08:35.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4940" for this suite.

• [SLOW TEST:27.306 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":337,"completed":82,"skipped":1403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:08:35.988: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6203.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6203.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6203.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6203.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6203.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6203.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 18 05:08:40.053: INFO: DNS probes using dns-6203/dns-test-5fa02ae9-1c67-4506-bc5c-1461bfe5ee77 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:08:40.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6203" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":337,"completed":83,"skipped":1428,"failed":0}
SSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:08:40.077: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-7457
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7457
STEP: Deleting pre-stop pod
May 18 05:08:51.157: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:08:51.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7457" for this suite.

• [SLOW TEST:11.112 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":337,"completed":84,"skipped":1435,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:08:51.189: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-4f5cac4a-4e19-4414-ae4c-a977c9717d21
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:08:53.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1341" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":337,"completed":85,"skipped":1444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:08:53.291: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:06.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-328" for this suite.

• [SLOW TEST:13.096 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":337,"completed":86,"skipped":1481,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:06.388: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-5892/configmap-test-ac29b2d6-80dd-4aff-9d9f-c18f68c7f947
STEP: Creating a pod to test consume configMaps
May 18 05:09:06.420: INFO: Waiting up to 5m0s for pod "pod-configmaps-e4616a5d-e935-431e-97e7-129169014054" in namespace "configmap-5892" to be "Succeeded or Failed"
May 18 05:09:06.422: INFO: Pod "pod-configmaps-e4616a5d-e935-431e-97e7-129169014054": Phase="Pending", Reason="", readiness=false. Elapsed: 1.780092ms
May 18 05:09:08.427: INFO: Pod "pod-configmaps-e4616a5d-e935-431e-97e7-129169014054": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006916002s
STEP: Saw pod success
May 18 05:09:08.427: INFO: Pod "pod-configmaps-e4616a5d-e935-431e-97e7-129169014054" satisfied condition "Succeeded or Failed"
May 18 05:09:08.429: INFO: Trying to get logs from node node2 pod pod-configmaps-e4616a5d-e935-431e-97e7-129169014054 container env-test: <nil>
STEP: delete the pod
May 18 05:09:08.443: INFO: Waiting for pod pod-configmaps-e4616a5d-e935-431e-97e7-129169014054 to disappear
May 18 05:09:08.445: INFO: Pod pod-configmaps-e4616a5d-e935-431e-97e7-129169014054 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:08.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5892" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":337,"completed":87,"skipped":1491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:08.452: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
May 18 05:09:08.473: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
May 18 05:09:08.788: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
May 18 05:09:10.829: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756911348, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756911348, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756911348, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756911348, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 18 05:09:13.964: INFO: Waited 1.118798028s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
May 18 05:09:14.116: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:14.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9825" for this suite.

• [SLOW TEST:6.561 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":337,"completed":88,"skipped":1513,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:15.013: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 18 05:09:15.053: INFO: Waiting up to 5m0s for pod "pod-ad01330b-5ddb-4285-94de-031b413fb15a" in namespace "emptydir-9598" to be "Succeeded or Failed"
May 18 05:09:15.068: INFO: Pod "pod-ad01330b-5ddb-4285-94de-031b413fb15a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.957432ms
May 18 05:09:17.073: INFO: Pod "pod-ad01330b-5ddb-4285-94de-031b413fb15a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019341245s
May 18 05:09:19.078: INFO: Pod "pod-ad01330b-5ddb-4285-94de-031b413fb15a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024750653s
STEP: Saw pod success
May 18 05:09:19.078: INFO: Pod "pod-ad01330b-5ddb-4285-94de-031b413fb15a" satisfied condition "Succeeded or Failed"
May 18 05:09:19.080: INFO: Trying to get logs from node node2 pod pod-ad01330b-5ddb-4285-94de-031b413fb15a container test-container: <nil>
STEP: delete the pod
May 18 05:09:19.096: INFO: Waiting for pod pod-ad01330b-5ddb-4285-94de-031b413fb15a to disappear
May 18 05:09:19.098: INFO: Pod pod-ad01330b-5ddb-4285-94de-031b413fb15a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:19.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9598" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":89,"skipped":1526,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:19.105: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-b1eb559a-85df-412e-b139-996d36fce5f4
STEP: Creating a pod to test consume configMaps
May 18 05:09:19.138: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ac4a24a-079d-4081-9f81-140bf938a75d" in namespace "configmap-8743" to be "Succeeded or Failed"
May 18 05:09:19.155: INFO: Pod "pod-configmaps-7ac4a24a-079d-4081-9f81-140bf938a75d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.529825ms
May 18 05:09:21.161: INFO: Pod "pod-configmaps-7ac4a24a-079d-4081-9f81-140bf938a75d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022231832s
STEP: Saw pod success
May 18 05:09:21.161: INFO: Pod "pod-configmaps-7ac4a24a-079d-4081-9f81-140bf938a75d" satisfied condition "Succeeded or Failed"
May 18 05:09:21.163: INFO: Trying to get logs from node node2 pod pod-configmaps-7ac4a24a-079d-4081-9f81-140bf938a75d container agnhost-container: <nil>
STEP: delete the pod
May 18 05:09:21.181: INFO: Waiting for pod pod-configmaps-7ac4a24a-079d-4081-9f81-140bf938a75d to disappear
May 18 05:09:21.183: INFO: Pod pod-configmaps-7ac4a24a-079d-4081-9f81-140bf938a75d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:21.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8743" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":337,"completed":90,"skipped":1532,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:21.189: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
May 18 05:09:21.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-6183 create -f -'
May 18 05:09:21.458: INFO: stderr: ""
May 18 05:09:21.458: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 18 05:09:21.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-6183 diff -f -'
May 18 05:09:21.681: INFO: rc: 1
May 18 05:09:21.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-6183 delete -f -'
May 18 05:09:21.752: INFO: stderr: ""
May 18 05:09:21.752: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:21.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6183" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":337,"completed":91,"skipped":1537,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:21.762: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
May 18 05:09:23.817: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:25.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3068" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":337,"completed":92,"skipped":1547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:25.868: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:09:25.907: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 18 05:09:28.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-6768 --namespace=crd-publish-openapi-6768 create -f -'
May 18 05:09:29.601: INFO: stderr: ""
May 18 05:09:29.601: INFO: stdout: "e2e-test-crd-publish-openapi-7718-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 18 05:09:29.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-6768 --namespace=crd-publish-openapi-6768 delete e2e-test-crd-publish-openapi-7718-crds test-cr'
May 18 05:09:29.682: INFO: stderr: ""
May 18 05:09:29.682: INFO: stdout: "e2e-test-crd-publish-openapi-7718-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 18 05:09:29.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-6768 --namespace=crd-publish-openapi-6768 apply -f -'
May 18 05:09:29.883: INFO: stderr: ""
May 18 05:09:29.883: INFO: stdout: "e2e-test-crd-publish-openapi-7718-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 18 05:09:29.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-6768 --namespace=crd-publish-openapi-6768 delete e2e-test-crd-publish-openapi-7718-crds test-cr'
May 18 05:09:29.962: INFO: stderr: ""
May 18 05:09:29.962: INFO: stdout: "e2e-test-crd-publish-openapi-7718-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 18 05:09:29.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-6768 explain e2e-test-crd-publish-openapi-7718-crds'
May 18 05:09:30.173: INFO: stderr: ""
May 18 05:09:30.173: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7718-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:34.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6768" for this suite.

• [SLOW TEST:9.071 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":337,"completed":93,"skipped":1577,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:34.939: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8421
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8421
STEP: creating replication controller externalsvc in namespace services-8421
I0518 05:09:34.999739      19 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8421, replica count: 2
I0518 05:09:38.051038      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 18 05:09:38.076: INFO: Creating new exec pod
May 18 05:09:40.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-8421 exec execpodx84z6 -- /bin/sh -x -c nslookup nodeport-service.services-8421.svc.cluster.local'
May 18 05:09:40.277: INFO: stderr: "+ nslookup nodeport-service.services-8421.svc.cluster.local\n"
May 18 05:09:40.277: INFO: stdout: "Server:\t\t172.24.0.10\nAddress:\t172.24.0.10#53\n\nnodeport-service.services-8421.svc.cluster.local\tcanonical name = externalsvc.services-8421.svc.cluster.local.\nName:\texternalsvc.services-8421.svc.cluster.local\nAddress: 172.24.230.192\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8421, will wait for the garbage collector to delete the pods
May 18 05:09:40.337: INFO: Deleting ReplicationController externalsvc took: 6.325172ms
May 18 05:09:40.438: INFO: Terminating ReplicationController externalsvc pods took: 100.885842ms
May 18 05:09:50.959: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:50.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8421" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:16.050 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":337,"completed":94,"skipped":1585,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:50.989: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May 18 05:09:51.055: INFO: Waiting up to 5m0s for pod "downward-api-ddba3443-2212-4399-a6ef-6d748167cc81" in namespace "downward-api-7084" to be "Succeeded or Failed"
May 18 05:09:51.062: INFO: Pod "downward-api-ddba3443-2212-4399-a6ef-6d748167cc81": Phase="Pending", Reason="", readiness=false. Elapsed: 7.125267ms
May 18 05:09:53.069: INFO: Pod "downward-api-ddba3443-2212-4399-a6ef-6d748167cc81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014104968s
STEP: Saw pod success
May 18 05:09:53.069: INFO: Pod "downward-api-ddba3443-2212-4399-a6ef-6d748167cc81" satisfied condition "Succeeded or Failed"
May 18 05:09:53.072: INFO: Trying to get logs from node node2 pod downward-api-ddba3443-2212-4399-a6ef-6d748167cc81 container dapi-container: <nil>
STEP: delete the pod
May 18 05:09:53.090: INFO: Waiting for pod downward-api-ddba3443-2212-4399-a6ef-6d748167cc81 to disappear
May 18 05:09:53.095: INFO: Pod downward-api-ddba3443-2212-4399-a6ef-6d748167cc81 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:53.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7084" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":337,"completed":95,"skipped":1607,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:53.108: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
May 18 05:09:53.166: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
May 18 05:09:53.182: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:53.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4500" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":337,"completed":96,"skipped":1617,"failed":0}
S
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:53.212: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-1242/secret-test-f5cd92e6-bd20-4292-8824-b62f700ee437
STEP: Creating a pod to test consume secrets
May 18 05:09:53.272: INFO: Waiting up to 5m0s for pod "pod-configmaps-d8c4ec83-6988-4916-bd2c-b7a0741c01de" in namespace "secrets-1242" to be "Succeeded or Failed"
May 18 05:09:53.281: INFO: Pod "pod-configmaps-d8c4ec83-6988-4916-bd2c-b7a0741c01de": Phase="Pending", Reason="", readiness=false. Elapsed: 9.030659ms
May 18 05:09:55.286: INFO: Pod "pod-configmaps-d8c4ec83-6988-4916-bd2c-b7a0741c01de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014134269s
STEP: Saw pod success
May 18 05:09:55.286: INFO: Pod "pod-configmaps-d8c4ec83-6988-4916-bd2c-b7a0741c01de" satisfied condition "Succeeded or Failed"
May 18 05:09:55.288: INFO: Trying to get logs from node node2 pod pod-configmaps-d8c4ec83-6988-4916-bd2c-b7a0741c01de container env-test: <nil>
STEP: delete the pod
May 18 05:09:55.307: INFO: Waiting for pod pod-configmaps-d8c4ec83-6988-4916-bd2c-b7a0741c01de to disappear
May 18 05:09:55.309: INFO: Pod pod-configmaps-d8c4ec83-6988-4916-bd2c-b7a0741c01de no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:55.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1242" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":337,"completed":97,"skipped":1618,"failed":0}
S
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:55.316: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
May 18 05:09:55.346: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:09:59.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9988" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":337,"completed":98,"skipped":1619,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:09:59.385: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:10:15.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9425" for this suite.

• [SLOW TEST:16.122 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":337,"completed":99,"skipped":1627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:10:15.507: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4449
STEP: creating service affinity-nodeport-transition in namespace services-4449
STEP: creating replication controller affinity-nodeport-transition in namespace services-4449
I0518 05:10:15.551887      19 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-4449, replica count: 3
I0518 05:10:18.602922      19 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 18 05:10:18.612: INFO: Creating new exec pod
May 18 05:10:23.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-4449 exec execpod-affinityp4gbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
May 18 05:10:23.791: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 18 05:10:23.791: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:10:23.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-4449 exec execpod-affinityp4gbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.24.127.111 80'
May 18 05:10:23.967: INFO: stderr: "+ nc -v -t -w 2 172.24.127.111 80\nConnection to 172.24.127.111 80 port [tcp/http] succeeded!\n+ echo hostName\n"
May 18 05:10:23.967: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:10:23.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-4449 exec execpod-affinityp4gbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 32607'
May 18 05:10:24.159: INFO: stderr: "+ nc -v -t -w 2 172.28.128.12 32607\n+ echo hostName\nConnection to 172.28.128.12 32607 port [tcp/*] succeeded!\n"
May 18 05:10:24.159: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:10:24.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-4449 exec execpod-affinityp4gbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.13 32607'
May 18 05:10:24.335: INFO: stderr: "+ nc -v -t -w 2 172.28.128.13 32607\n+ echo hostName\nConnection to 172.28.128.13 32607 port [tcp/*] succeeded!\n"
May 18 05:10:24.335: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:10:24.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-4449 exec execpod-affinityp4gbf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.28.128.12:32607/ ; done'
May 18 05:10:24.624: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n"
May 18 05:10:24.624: INFO: stdout: "\naffinity-nodeport-transition-m4pfr\naffinity-nodeport-transition-h5kcg\naffinity-nodeport-transition-m4pfr\naffinity-nodeport-transition-m4pfr\naffinity-nodeport-transition-m4pfr\naffinity-nodeport-transition-m4pfr\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-h5kcg\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-m4pfr\naffinity-nodeport-transition-h5kcg\naffinity-nodeport-transition-m4pfr\naffinity-nodeport-transition-m4pfr\naffinity-nodeport-transition-m4pfr"
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-m4pfr
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-h5kcg
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-m4pfr
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-m4pfr
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-m4pfr
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-m4pfr
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-h5kcg
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-m4pfr
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-h5kcg
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-m4pfr
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-m4pfr
May 18 05:10:24.624: INFO: Received response from host: affinity-nodeport-transition-m4pfr
May 18 05:10:24.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-4449 exec execpod-affinityp4gbf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.28.128.12:32607/ ; done'
May 18 05:10:24.919: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:32607/\n"
May 18 05:10:24.919: INFO: stdout: "\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q\naffinity-nodeport-transition-cwt2q"
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Received response from host: affinity-nodeport-transition-cwt2q
May 18 05:10:24.919: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4449, will wait for the garbage collector to delete the pods
May 18 05:10:24.985: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.498984ms
May 18 05:10:25.086: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.891243ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:10:34.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4449" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:19.125 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":337,"completed":100,"skipped":1657,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:10:34.631: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
May 18 05:10:34.677: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 18 05:10:36.683: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
May 18 05:10:36.698: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 18 05:10:38.707: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 18 05:10:38.732: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 18 05:10:38.742: INFO: Pod pod-with-poststart-http-hook still exists
May 18 05:10:40.745: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 18 05:10:40.752: INFO: Pod pod-with-poststart-http-hook still exists
May 18 05:10:42.742: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 18 05:10:42.747: INFO: Pod pod-with-poststart-http-hook still exists
May 18 05:10:44.743: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 18 05:10:44.748: INFO: Pod pod-with-poststart-http-hook still exists
May 18 05:10:46.743: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 18 05:10:46.746: INFO: Pod pod-with-poststart-http-hook still exists
May 18 05:10:48.742: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 18 05:10:48.747: INFO: Pod pod-with-poststart-http-hook still exists
May 18 05:10:50.742: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 18 05:10:50.747: INFO: Pod pod-with-poststart-http-hook still exists
May 18 05:10:52.743: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 18 05:10:52.748: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:10:52.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-274" for this suite.

• [SLOW TEST:18.123 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":337,"completed":101,"skipped":1673,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:10:52.755: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 18 05:10:52.849: INFO: The status of Pod pod-update-0cb2ce26-b4aa-4f18-ad13-fa1216fdf3c1 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:10:54.855: INFO: The status of Pod pod-update-0cb2ce26-b4aa-4f18-ad13-fa1216fdf3c1 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 18 05:10:55.370: INFO: Successfully updated pod "pod-update-0cb2ce26-b4aa-4f18-ad13-fa1216fdf3c1"
STEP: verifying the updated pod is in kubernetes
May 18 05:10:55.376: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:10:55.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9247" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":337,"completed":102,"skipped":1686,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:10:55.388: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 18 05:10:55.424: INFO: Waiting up to 1m0s for all nodes to be ready
May 18 05:11:55.447: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
May 18 05:11:55.461: INFO: Created pod: pod0-sched-preemption-low-priority
May 18 05:11:55.491: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:12:03.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2910" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:68.199 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":337,"completed":103,"skipped":1697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:12:03.588: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:12:03.651: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 18 05:12:03.656: INFO: Number of nodes with available pods: 0
May 18 05:12:03.656: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 18 05:12:03.679: INFO: Number of nodes with available pods: 0
May 18 05:12:03.679: INFO: Node node1 is running more than one daemon pod
May 18 05:12:04.688: INFO: Number of nodes with available pods: 0
May 18 05:12:04.688: INFO: Node node1 is running more than one daemon pod
May 18 05:12:05.686: INFO: Number of nodes with available pods: 1
May 18 05:12:05.686: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 18 05:12:05.705: INFO: Number of nodes with available pods: 1
May 18 05:12:05.705: INFO: Number of running nodes: 0, number of available pods: 1
May 18 05:12:06.710: INFO: Number of nodes with available pods: 0
May 18 05:12:06.710: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 18 05:12:06.720: INFO: Number of nodes with available pods: 0
May 18 05:12:06.720: INFO: Node node1 is running more than one daemon pod
May 18 05:12:07.724: INFO: Number of nodes with available pods: 0
May 18 05:12:07.724: INFO: Node node1 is running more than one daemon pod
May 18 05:12:08.724: INFO: Number of nodes with available pods: 0
May 18 05:12:08.724: INFO: Node node1 is running more than one daemon pod
May 18 05:12:09.725: INFO: Number of nodes with available pods: 0
May 18 05:12:09.725: INFO: Node node1 is running more than one daemon pod
May 18 05:12:10.729: INFO: Number of nodes with available pods: 0
May 18 05:12:10.729: INFO: Node node1 is running more than one daemon pod
May 18 05:12:11.731: INFO: Number of nodes with available pods: 1
May 18 05:12:11.731: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7932, will wait for the garbage collector to delete the pods
May 18 05:12:11.801: INFO: Deleting DaemonSet.extensions daemon-set took: 8.292162ms
May 18 05:12:11.902: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.066642ms
May 18 05:12:15.509: INFO: Number of nodes with available pods: 0
May 18 05:12:15.509: INFO: Number of running nodes: 0, number of available pods: 0
May 18 05:12:15.511: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28994"},"items":null}

May 18 05:12:15.512: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28994"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:12:15.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7932" for this suite.

• [SLOW TEST:11.946 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":337,"completed":104,"skipped":1724,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:12:15.534: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 18 05:12:15.572: INFO: The status of Pod pod-update-activedeadlineseconds-d20ea262-8f68-43e7-9db5-13440b6bc01c is Pending, waiting for it to be Running (with Ready = true)
May 18 05:12:17.576: INFO: The status of Pod pod-update-activedeadlineseconds-d20ea262-8f68-43e7-9db5-13440b6bc01c is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 18 05:12:18.089: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d20ea262-8f68-43e7-9db5-13440b6bc01c"
May 18 05:12:18.090: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d20ea262-8f68-43e7-9db5-13440b6bc01c" in namespace "pods-4561" to be "terminated due to deadline exceeded"
May 18 05:12:18.096: INFO: Pod "pod-update-activedeadlineseconds-d20ea262-8f68-43e7-9db5-13440b6bc01c": Phase="Running", Reason="", readiness=true. Elapsed: 6.016473ms
May 18 05:12:20.110: INFO: Pod "pod-update-activedeadlineseconds-d20ea262-8f68-43e7-9db5-13440b6bc01c": Phase="Running", Reason="", readiness=true. Elapsed: 2.020755138s
May 18 05:12:22.116: INFO: Pod "pod-update-activedeadlineseconds-d20ea262-8f68-43e7-9db5-13440b6bc01c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.026492545s
May 18 05:12:22.116: INFO: Pod "pod-update-activedeadlineseconds-d20ea262-8f68-43e7-9db5-13440b6bc01c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:12:22.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4561" for this suite.

• [SLOW TEST:6.588 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":337,"completed":105,"skipped":1762,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:12:22.123: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:12:22.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8777" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":337,"completed":106,"skipped":1768,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:12:22.183: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:12:22.494: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:12:25.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 18 05:12:29.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=webhook-5482 attach --namespace=webhook-5482 to-be-attached-pod -i -c=container1'
May 18 05:12:29.622: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:12:29.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5482" for this suite.
STEP: Destroying namespace "webhook-5482-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.491 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":337,"completed":107,"skipped":1768,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:12:29.674: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 18 05:12:30.515: INFO: starting watch
STEP: patching
STEP: updating
May 18 05:12:30.522: INFO: waiting for watch events with expected annotations
May 18 05:12:30.522: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:12:30.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2477" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":337,"completed":108,"skipped":1780,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:12:30.579: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 05:12:30.608: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fce18a5b-6566-48f6-a7fe-b1d658ef1474" in namespace "downward-api-8196" to be "Succeeded or Failed"
May 18 05:12:30.610: INFO: Pod "downwardapi-volume-fce18a5b-6566-48f6-a7fe-b1d658ef1474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.527188ms
May 18 05:12:32.627: INFO: Pod "downwardapi-volume-fce18a5b-6566-48f6-a7fe-b1d658ef1474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019560244s
May 18 05:12:34.634: INFO: Pod "downwardapi-volume-fce18a5b-6566-48f6-a7fe-b1d658ef1474": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026033547s
STEP: Saw pod success
May 18 05:12:34.634: INFO: Pod "downwardapi-volume-fce18a5b-6566-48f6-a7fe-b1d658ef1474" satisfied condition "Succeeded or Failed"
May 18 05:12:34.636: INFO: Trying to get logs from node node2 pod downwardapi-volume-fce18a5b-6566-48f6-a7fe-b1d658ef1474 container client-container: <nil>
STEP: delete the pod
May 18 05:12:34.667: INFO: Waiting for pod downwardapi-volume-fce18a5b-6566-48f6-a7fe-b1d658ef1474 to disappear
May 18 05:12:34.669: INFO: Pod downwardapi-volume-fce18a5b-6566-48f6-a7fe-b1d658ef1474 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:12:34.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8196" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":109,"skipped":1780,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:12:34.677: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
May 18 05:12:34.721: INFO: Waiting up to 5m0s for pod "pod-9c33040c-962e-4646-8062-81b107bf7d0e" in namespace "emptydir-2005" to be "Succeeded or Failed"
May 18 05:12:34.728: INFO: Pod "pod-9c33040c-962e-4646-8062-81b107bf7d0e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.72887ms
May 18 05:12:36.734: INFO: Pod "pod-9c33040c-962e-4646-8062-81b107bf7d0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012506876s
May 18 05:12:38.738: INFO: Pod "pod-9c33040c-962e-4646-8062-81b107bf7d0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016591891s
STEP: Saw pod success
May 18 05:12:38.738: INFO: Pod "pod-9c33040c-962e-4646-8062-81b107bf7d0e" satisfied condition "Succeeded or Failed"
May 18 05:12:38.740: INFO: Trying to get logs from node node2 pod pod-9c33040c-962e-4646-8062-81b107bf7d0e container test-container: <nil>
STEP: delete the pod
May 18 05:12:38.755: INFO: Waiting for pod pod-9c33040c-962e-4646-8062-81b107bf7d0e to disappear
May 18 05:12:38.758: INFO: Pod pod-9c33040c-962e-4646-8062-81b107bf7d0e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:12:38.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2005" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":110,"skipped":1785,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:12:38.764: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
May 18 05:12:38.805: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May 18 05:12:40.811: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May 18 05:12:42.812: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 18 05:12:43.840: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:12:43.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-681" for this suite.

• [SLOW TEST:5.153 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":337,"completed":111,"skipped":1789,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:12:43.919: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 18 05:12:47.998: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8156 PodName:var-expansion-898e3c73-b3df-481f-8306-0164ccb836bc ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:12:47.998: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: test for file in mounted path
May 18 05:12:48.087: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8156 PodName:var-expansion-898e3c73-b3df-481f-8306-0164ccb836bc ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:12:48.087: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: updating the annotation value
May 18 05:12:48.689: INFO: Successfully updated pod "var-expansion-898e3c73-b3df-481f-8306-0164ccb836bc"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 18 05:12:48.694: INFO: Deleting pod "var-expansion-898e3c73-b3df-481f-8306-0164ccb836bc" in namespace "var-expansion-8156"
May 18 05:12:48.699: INFO: Wait up to 5m0s for pod "var-expansion-898e3c73-b3df-481f-8306-0164ccb836bc" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:13:22.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8156" for this suite.

• [SLOW TEST:38.796 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":337,"completed":112,"skipped":1835,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:13:22.715: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:13:22.746: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:13:28.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6669" for this suite.

• [SLOW TEST:6.236 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":337,"completed":113,"skipped":1843,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:13:28.952: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1548
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
May 18 05:13:28.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-5604 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
May 18 05:13:29.122: INFO: stderr: ""
May 18 05:13:29.122: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 18 05:13:34.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-5604 get pod e2e-test-httpd-pod -o json'
May 18 05:13:34.238: INFO: stderr: ""
May 18 05:13:34.238: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.104.35/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.104.35/32\"\n        },\n        \"creationTimestamp\": \"2021-05-18T05:13:29Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5604\",\n        \"resourceVersion\": \"29539\",\n        \"uid\": \"1aa14243-0da2-420f-8662-674a2043891e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-x8s5w\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 60\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 60\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-x8s5w\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-18T05:13:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-18T05:13:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-18T05:13:31Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-18T05:13:29Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://6cf832af38e5df21fe09846ef9a9eac4cd855b0a38fa27e3c834711105818d42\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-05-18T05:13:30Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.28.128.13\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.104.35\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.104.35\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-05-18T05:13:29Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 18 05:13:34.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-5604 replace -f -'
May 18 05:13:34.489: INFO: stderr: ""
May 18 05:13:34.489: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1552
May 18 05:13:34.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-5604 delete pods e2e-test-httpd-pod'
May 18 05:13:44.555: INFO: stderr: ""
May 18 05:13:44.555: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:13:44.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5604" for this suite.

• [SLOW TEST:15.616 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":337,"completed":114,"skipped":1859,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:13:44.567: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 18 05:13:44.607: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 18 05:13:44.617: INFO: Waiting for terminating namespaces to be deleted...
May 18 05:13:44.620: INFO: 
Logging pods the apiserver thinks is on node node1 before test
May 18 05:13:44.625: INFO: calico-node-bzct9 from kube-system started at 2021-05-18 02:09:57 +0000 UTC (1 container statuses recorded)
May 18 05:13:44.625: INFO: 	Container calico-node ready: true, restart count 0
May 18 05:13:44.625: INFO: kube-proxy-vxmpl from kube-system started at 2021-05-18 02:09:57 +0000 UTC (1 container statuses recorded)
May 18 05:13:44.625: INFO: 	Container kube-proxy ready: true, restart count 0
May 18 05:13:44.625: INFO: sonobuoy-e2e-job-d53ed4c4301a4fe4 from sonobuoy started at 2021-05-18 04:30:03 +0000 UTC (2 container statuses recorded)
May 18 05:13:44.625: INFO: 	Container e2e ready: true, restart count 0
May 18 05:13:44.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 18 05:13:44.625: INFO: sonobuoy-systemd-logs-daemon-set-0e6496dadced4aae-5wgz6 from sonobuoy started at 2021-05-18 04:30:04 +0000 UTC (2 container statuses recorded)
May 18 05:13:44.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 18 05:13:44.625: INFO: 	Container systemd-logs ready: true, restart count 0
May 18 05:13:44.625: INFO: 
Logging pods the apiserver thinks is on node node2 before test
May 18 05:13:44.629: INFO: calico-node-ddzct from kube-system started at 2021-05-18 04:29:32 +0000 UTC (1 container statuses recorded)
May 18 05:13:44.629: INFO: 	Container calico-node ready: true, restart count 0
May 18 05:13:44.629: INFO: kube-proxy-d6j4t from kube-system started at 2021-05-18 02:10:08 +0000 UTC (1 container statuses recorded)
May 18 05:13:44.629: INFO: 	Container kube-proxy ready: true, restart count 0
May 18 05:13:44.629: INFO: sonobuoy from sonobuoy started at 2021-05-18 04:30:02 +0000 UTC (1 container statuses recorded)
May 18 05:13:44.629: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 18 05:13:44.629: INFO: sonobuoy-systemd-logs-daemon-set-0e6496dadced4aae-7bgms from sonobuoy started at 2021-05-18 04:30:03 +0000 UTC (2 container statuses recorded)
May 18 05:13:44.629: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 18 05:13:44.629: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-46396792-5c7b-4223-acff-0212d367d875 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.28.128.13 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-46396792-5c7b-4223-acff-0212d367d875 off the node node2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-46396792-5c7b-4223-acff-0212d367d875
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:18:50.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8792" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:306.225 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":337,"completed":115,"skipped":1865,"failed":0}
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:18:50.793: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1308
STEP: creating the pod
May 18 05:18:50.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8189 create -f -'
May 18 05:18:51.038: INFO: stderr: ""
May 18 05:18:51.038: INFO: stdout: "pod/pause created\n"
May 18 05:18:51.038: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 18 05:18:51.038: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8189" to be "running and ready"
May 18 05:18:51.044: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016172ms
May 18 05:18:53.052: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.014524967s
May 18 05:18:53.052: INFO: Pod "pause" satisfied condition "running and ready"
May 18 05:18:53.052: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
May 18 05:18:53.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8189 label pods pause testing-label=testing-label-value'
May 18 05:18:53.174: INFO: stderr: ""
May 18 05:18:53.174: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 18 05:18:53.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8189 get pod pause -L testing-label'
May 18 05:18:53.293: INFO: stderr: ""
May 18 05:18:53.293: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 18 05:18:53.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8189 label pods pause testing-label-'
May 18 05:18:53.431: INFO: stderr: ""
May 18 05:18:53.431: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 18 05:18:53.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8189 get pod pause -L testing-label'
May 18 05:18:53.559: INFO: stderr: ""
May 18 05:18:53.559: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: using delete to clean up resources
May 18 05:18:53.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8189 delete --grace-period=0 --force -f -'
May 18 05:18:53.703: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 18 05:18:53.703: INFO: stdout: "pod \"pause\" force deleted\n"
May 18 05:18:53.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8189 get rc,svc -l name=pause --no-headers'
May 18 05:18:53.870: INFO: stderr: "No resources found in kubectl-8189 namespace.\n"
May 18 05:18:53.870: INFO: stdout: ""
May 18 05:18:53.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8189 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 18 05:18:54.102: INFO: stderr: ""
May 18 05:18:54.102: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:18:54.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8189" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":337,"completed":116,"skipped":1865,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:18:54.112: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
May 18 05:18:54.171: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 18 05:18:56.177: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 18 05:18:58.178: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
May 18 05:18:58.201: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 18 05:19:00.206: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
May 18 05:19:00.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 18 05:19:00.219: INFO: Pod pod-with-prestop-exec-hook still exists
May 18 05:19:02.220: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 18 05:19:02.224: INFO: Pod pod-with-prestop-exec-hook still exists
May 18 05:19:04.221: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 18 05:19:04.229: INFO: Pod pod-with-prestop-exec-hook still exists
May 18 05:19:06.220: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 18 05:19:06.229: INFO: Pod pod-with-prestop-exec-hook still exists
May 18 05:19:08.221: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 18 05:19:08.224: INFO: Pod pod-with-prestop-exec-hook still exists
May 18 05:19:10.220: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 18 05:19:10.225: INFO: Pod pod-with-prestop-exec-hook still exists
May 18 05:19:12.220: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 18 05:19:12.230: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:19:12.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1841" for this suite.

• [SLOW TEST:18.137 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":337,"completed":117,"skipped":1867,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:19:12.249: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:19:12.695: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:19:15.722: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:19:15.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5620" for this suite.
STEP: Destroying namespace "webhook-5620-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":337,"completed":118,"skipped":1870,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:19:15.818: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-sbh2
STEP: Creating a pod to test atomic-volume-subpath
May 18 05:19:15.876: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-sbh2" in namespace "subpath-2504" to be "Succeeded or Failed"
May 18 05:19:15.883: INFO: Pod "pod-subpath-test-projected-sbh2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.785069ms
May 18 05:19:17.887: INFO: Pod "pod-subpath-test-projected-sbh2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010748284s
May 18 05:19:19.891: INFO: Pod "pod-subpath-test-projected-sbh2": Phase="Running", Reason="", readiness=true. Elapsed: 4.0145182s
May 18 05:19:21.896: INFO: Pod "pod-subpath-test-projected-sbh2": Phase="Running", Reason="", readiness=true. Elapsed: 6.019765909s
May 18 05:19:23.907: INFO: Pod "pod-subpath-test-projected-sbh2": Phase="Running", Reason="", readiness=true. Elapsed: 8.030852491s
May 18 05:19:25.915: INFO: Pod "pod-subpath-test-projected-sbh2": Phase="Running", Reason="", readiness=true. Elapsed: 10.038819688s
May 18 05:19:27.920: INFO: Pod "pod-subpath-test-projected-sbh2": Phase="Running", Reason="", readiness=true. Elapsed: 12.043845898s
May 18 05:19:29.927: INFO: Pod "pod-subpath-test-projected-sbh2": Phase="Running", Reason="", readiness=true. Elapsed: 14.0506839s
May 18 05:19:31.937: INFO: Pod "pod-subpath-test-projected-sbh2": Phase="Running", Reason="", readiness=true. Elapsed: 16.061238985s
May 18 05:19:33.945: INFO: Pod "pod-subpath-test-projected-sbh2": Phase="Running", Reason="", readiness=true. Elapsed: 18.068748783s
May 18 05:19:35.952: INFO: Pod "pod-subpath-test-projected-sbh2": Phase="Running", Reason="", readiness=true. Elapsed: 20.075892184s
May 18 05:19:37.958: INFO: Pod "pod-subpath-test-projected-sbh2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.082322687s
STEP: Saw pod success
May 18 05:19:37.958: INFO: Pod "pod-subpath-test-projected-sbh2" satisfied condition "Succeeded or Failed"
May 18 05:19:37.961: INFO: Trying to get logs from node node1 pod pod-subpath-test-projected-sbh2 container test-container-subpath-projected-sbh2: <nil>
STEP: delete the pod
May 18 05:19:37.981: INFO: Waiting for pod pod-subpath-test-projected-sbh2 to disappear
May 18 05:19:37.983: INFO: Pod pod-subpath-test-projected-sbh2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-sbh2
May 18 05:19:37.983: INFO: Deleting pod "pod-subpath-test-projected-sbh2" in namespace "subpath-2504"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:19:37.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2504" for this suite.

• [SLOW TEST:22.176 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":337,"completed":119,"skipped":1871,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:19:37.994: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:149
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 18 05:19:38.042: INFO: starting watch
STEP: patching
STEP: updating
May 18 05:19:38.050: INFO: waiting for watch events with expected annotations
May 18 05:19:38.050: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:19:38.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2471" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":337,"completed":120,"skipped":1880,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:19:38.079: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 05:19:38.111: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e90b4836-ddc4-4d8f-8893-ab12cf807a24" in namespace "downward-api-2803" to be "Succeeded or Failed"
May 18 05:19:38.115: INFO: Pod "downwardapi-volume-e90b4836-ddc4-4d8f-8893-ab12cf807a24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.55718ms
May 18 05:19:40.118: INFO: Pod "downwardapi-volume-e90b4836-ddc4-4d8f-8893-ab12cf807a24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007516999s
STEP: Saw pod success
May 18 05:19:40.118: INFO: Pod "downwardapi-volume-e90b4836-ddc4-4d8f-8893-ab12cf807a24" satisfied condition "Succeeded or Failed"
May 18 05:19:40.121: INFO: Trying to get logs from node node2 pod downwardapi-volume-e90b4836-ddc4-4d8f-8893-ab12cf807a24 container client-container: <nil>
STEP: delete the pod
May 18 05:19:40.139: INFO: Waiting for pod downwardapi-volume-e90b4836-ddc4-4d8f-8893-ab12cf807a24 to disappear
May 18 05:19:40.142: INFO: Pod downwardapi-volume-e90b4836-ddc4-4d8f-8893-ab12cf807a24 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:19:40.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2803" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":337,"completed":121,"skipped":1902,"failed":0}
SSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:19:40.151: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
May 18 05:19:40.189: INFO: Major version: 1
STEP: Confirm minor version
May 18 05:19:40.189: INFO: cleanMinorVersion: 21
May 18 05:19:40.189: INFO: Minor version: 21
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:19:40.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-9953" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":337,"completed":122,"skipped":1905,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:19:40.198: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-56d405b1-ab2a-4247-8775-0553ab64256d
STEP: Creating a pod to test consume configMaps
May 18 05:19:40.234: INFO: Waiting up to 5m0s for pod "pod-configmaps-628c8513-b042-4d37-88b1-c9d1dabb6b2d" in namespace "configmap-8200" to be "Succeeded or Failed"
May 18 05:19:40.241: INFO: Pod "pod-configmaps-628c8513-b042-4d37-88b1-c9d1dabb6b2d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.54117ms
May 18 05:19:42.248: INFO: Pod "pod-configmaps-628c8513-b042-4d37-88b1-c9d1dabb6b2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013624471s
STEP: Saw pod success
May 18 05:19:42.248: INFO: Pod "pod-configmaps-628c8513-b042-4d37-88b1-c9d1dabb6b2d" satisfied condition "Succeeded or Failed"
May 18 05:19:42.251: INFO: Trying to get logs from node node2 pod pod-configmaps-628c8513-b042-4d37-88b1-c9d1dabb6b2d container agnhost-container: <nil>
STEP: delete the pod
May 18 05:19:42.270: INFO: Waiting for pod pod-configmaps-628c8513-b042-4d37-88b1-c9d1dabb6b2d to disappear
May 18 05:19:42.280: INFO: Pod pod-configmaps-628c8513-b042-4d37-88b1-c9d1dabb6b2d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:19:42.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8200" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":337,"completed":123,"skipped":1908,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:19:42.288: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:19:44.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6294" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":337,"completed":124,"skipped":1916,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:19:44.344: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-f893ab62-66c7-45a9-8a76-18bb9c590d2f
STEP: Creating a pod to test consume secrets
May 18 05:19:44.384: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b3c60bcb-5e0e-4eb3-ba99-3a9a35a63cf6" in namespace "projected-7640" to be "Succeeded or Failed"
May 18 05:19:44.393: INFO: Pod "pod-projected-secrets-b3c60bcb-5e0e-4eb3-ba99-3a9a35a63cf6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.830959ms
May 18 05:19:46.398: INFO: Pod "pod-projected-secrets-b3c60bcb-5e0e-4eb3-ba99-3a9a35a63cf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013857669s
STEP: Saw pod success
May 18 05:19:46.398: INFO: Pod "pod-projected-secrets-b3c60bcb-5e0e-4eb3-ba99-3a9a35a63cf6" satisfied condition "Succeeded or Failed"
May 18 05:19:46.400: INFO: Trying to get logs from node node2 pod pod-projected-secrets-b3c60bcb-5e0e-4eb3-ba99-3a9a35a63cf6 container secret-volume-test: <nil>
STEP: delete the pod
May 18 05:19:46.417: INFO: Waiting for pod pod-projected-secrets-b3c60bcb-5e0e-4eb3-ba99-3a9a35a63cf6 to disappear
May 18 05:19:46.419: INFO: Pod pod-projected-secrets-b3c60bcb-5e0e-4eb3-ba99-3a9a35a63cf6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:19:46.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7640" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":337,"completed":125,"skipped":1956,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:19:46.427: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-7cb8ffb6-495f-4189-b4c1-e4d44854155c
STEP: Creating a pod to test consume secrets
May 18 05:19:46.475: INFO: Waiting up to 5m0s for pod "pod-secrets-d6920712-ccb9-45ae-98c1-3084609afd12" in namespace "secrets-1869" to be "Succeeded or Failed"
May 18 05:19:46.480: INFO: Pod "pod-secrets-d6920712-ccb9-45ae-98c1-3084609afd12": Phase="Pending", Reason="", readiness=false. Elapsed: 5.049077ms
May 18 05:19:48.488: INFO: Pod "pod-secrets-d6920712-ccb9-45ae-98c1-3084609afd12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013133274s
STEP: Saw pod success
May 18 05:19:48.488: INFO: Pod "pod-secrets-d6920712-ccb9-45ae-98c1-3084609afd12" satisfied condition "Succeeded or Failed"
May 18 05:19:48.492: INFO: Trying to get logs from node node2 pod pod-secrets-d6920712-ccb9-45ae-98c1-3084609afd12 container secret-volume-test: <nil>
STEP: delete the pod
May 18 05:19:48.513: INFO: Waiting for pod pod-secrets-d6920712-ccb9-45ae-98c1-3084609afd12 to disappear
May 18 05:19:48.519: INFO: Pod pod-secrets-d6920712-ccb9-45ae-98c1-3084609afd12 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:19:48.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1869" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":126,"skipped":1961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:19:48.531: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-70740eee-8adc-4f70-be92-78b11b792ded
STEP: Creating a pod to test consume configMaps
May 18 05:19:48.589: INFO: Waiting up to 5m0s for pod "pod-configmaps-4a63f531-200f-4c1f-8276-806a5367078f" in namespace "configmap-9358" to be "Succeeded or Failed"
May 18 05:19:48.593: INFO: Pod "pod-configmaps-4a63f531-200f-4c1f-8276-806a5367078f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935682ms
May 18 05:19:50.601: INFO: Pod "pod-configmaps-4a63f531-200f-4c1f-8276-806a5367078f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011761379s
STEP: Saw pod success
May 18 05:19:50.601: INFO: Pod "pod-configmaps-4a63f531-200f-4c1f-8276-806a5367078f" satisfied condition "Succeeded or Failed"
May 18 05:19:50.604: INFO: Trying to get logs from node node2 pod pod-configmaps-4a63f531-200f-4c1f-8276-806a5367078f container agnhost-container: <nil>
STEP: delete the pod
May 18 05:19:50.629: INFO: Waiting for pod pod-configmaps-4a63f531-200f-4c1f-8276-806a5367078f to disappear
May 18 05:19:50.633: INFO: Pod pod-configmaps-4a63f531-200f-4c1f-8276-806a5367078f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:19:50.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9358" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":337,"completed":127,"skipped":1986,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:19:50.644: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2593
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 18 05:19:50.684: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 18 05:19:50.734: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:19:52.740: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:19:54.739: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:19:56.737: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:19:58.739: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:20:00.740: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:20:02.739: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:20:04.739: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:20:06.741: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:20:08.741: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:20:10.742: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 18 05:20:10.747: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 18 05:20:14.771: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 18 05:20:14.771: INFO: Breadth first check of 172.30.166.172 on host 172.28.128.12...
May 18 05:20:14.774: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.104.47:9080/dial?request=hostname&protocol=http&host=172.30.166.172&port=8080&tries=1'] Namespace:pod-network-test-2593 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:20:14.775: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:20:14.953: INFO: Waiting for responses: map[]
May 18 05:20:14.953: INFO: reached 172.30.166.172 after 0/1 tries
May 18 05:20:14.953: INFO: Breadth first check of 172.30.104.46 on host 172.28.128.13...
May 18 05:20:14.958: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.104.47:9080/dial?request=hostname&protocol=http&host=172.30.104.46&port=8080&tries=1'] Namespace:pod-network-test-2593 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:20:14.958: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:20:15.121: INFO: Waiting for responses: map[]
May 18 05:20:15.121: INFO: reached 172.30.104.46 after 0/1 tries
May 18 05:20:15.121: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:20:15.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2593" for this suite.

• [SLOW TEST:24.490 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":337,"completed":128,"skipped":1999,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:20:15.134: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 18 05:20:15.205: INFO: Pod name pod-release: Found 0 pods out of 1
May 18 05:20:20.211: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:20:21.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-79" for this suite.

• [SLOW TEST:6.101 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":337,"completed":129,"skipped":2015,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:20:21.234: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9837
STEP: creating service affinity-clusterip-transition in namespace services-9837
STEP: creating replication controller affinity-clusterip-transition in namespace services-9837
I0518 05:20:21.292199      19 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-9837, replica count: 3
I0518 05:20:24.343479      19 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 18 05:20:24.349: INFO: Creating new exec pod
May 18 05:20:27.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-9837 exec execpod-affinitywt4kz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
May 18 05:20:27.886: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 18 05:20:27.886: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:20:27.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-9837 exec execpod-affinitywt4kz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.24.49.249 80'
May 18 05:20:28.083: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.24.49.249 80\nConnection to 172.24.49.249 80 port [tcp/http] succeeded!\n"
May 18 05:20:28.083: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:20:28.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-9837 exec execpod-affinitywt4kz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.49.249:80/ ; done'
May 18 05:20:28.375: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n"
May 18 05:20:28.375: INFO: stdout: "\naffinity-clusterip-transition-xxdsp\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-tx99l\naffinity-clusterip-transition-tx99l\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-tx99l\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-tx99l\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-tx99l\naffinity-clusterip-transition-tx99l\naffinity-clusterip-transition-xxdsp\naffinity-clusterip-transition-tx99l\naffinity-clusterip-transition-sfj26"
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-xxdsp
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-tx99l
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-tx99l
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-tx99l
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-tx99l
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-tx99l
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-tx99l
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-xxdsp
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-tx99l
May 18 05:20:28.375: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-9837 exec execpod-affinitywt4kz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.49.249:80/ ; done'
May 18 05:20:28.642: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.49.249:80/\n"
May 18 05:20:28.642: INFO: stdout: "\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26\naffinity-clusterip-transition-sfj26"
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Received response from host: affinity-clusterip-transition-sfj26
May 18 05:20:28.642: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9837, will wait for the garbage collector to delete the pods
May 18 05:20:28.715: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.572483ms
May 18 05:20:28.815: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.255446ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:20:41.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9837" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:19.829 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":337,"completed":130,"skipped":2048,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:20:41.063: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
May 18 05:20:41.117: INFO: Waiting up to 5m0s for pod "pod-a8ba2f49-e9ae-48ea-a7fc-39df173c9325" in namespace "emptydir-9620" to be "Succeeded or Failed"
May 18 05:20:41.137: INFO: Pod "pod-a8ba2f49-e9ae-48ea-a7fc-39df173c9325": Phase="Pending", Reason="", readiness=false. Elapsed: 20.307408ms
May 18 05:20:43.143: INFO: Pod "pod-a8ba2f49-e9ae-48ea-a7fc-39df173c9325": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026032614s
STEP: Saw pod success
May 18 05:20:43.143: INFO: Pod "pod-a8ba2f49-e9ae-48ea-a7fc-39df173c9325" satisfied condition "Succeeded or Failed"
May 18 05:20:43.145: INFO: Trying to get logs from node node2 pod pod-a8ba2f49-e9ae-48ea-a7fc-39df173c9325 container test-container: <nil>
STEP: delete the pod
May 18 05:20:43.170: INFO: Waiting for pod pod-a8ba2f49-e9ae-48ea-a7fc-39df173c9325 to disappear
May 18 05:20:43.172: INFO: Pod pod-a8ba2f49-e9ae-48ea-a7fc-39df173c9325 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:20:43.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9620" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":131,"skipped":2055,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:20:43.182: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 18 05:20:43.217: INFO: Waiting up to 5m0s for pod "pod-bf32d26f-96cf-4479-99fb-a10ce3ca2487" in namespace "emptydir-9262" to be "Succeeded or Failed"
May 18 05:20:43.223: INFO: Pod "pod-bf32d26f-96cf-4479-99fb-a10ce3ca2487": Phase="Pending", Reason="", readiness=false. Elapsed: 5.670674ms
May 18 05:20:45.227: INFO: Pod "pod-bf32d26f-96cf-4479-99fb-a10ce3ca2487": Phase="Running", Reason="", readiness=true. Elapsed: 2.010092887s
May 18 05:20:47.233: INFO: Pod "pod-bf32d26f-96cf-4479-99fb-a10ce3ca2487": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015903493s
STEP: Saw pod success
May 18 05:20:47.233: INFO: Pod "pod-bf32d26f-96cf-4479-99fb-a10ce3ca2487" satisfied condition "Succeeded or Failed"
May 18 05:20:47.235: INFO: Trying to get logs from node node2 pod pod-bf32d26f-96cf-4479-99fb-a10ce3ca2487 container test-container: <nil>
STEP: delete the pod
May 18 05:20:47.252: INFO: Waiting for pod pod-bf32d26f-96cf-4479-99fb-a10ce3ca2487 to disappear
May 18 05:20:47.255: INFO: Pod pod-bf32d26f-96cf-4479-99fb-a10ce3ca2487 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:20:47.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9262" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":132,"skipped":2057,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:20:47.261: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:21:18.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4242" for this suite.
STEP: Destroying namespace "nsdeletetest-9505" for this suite.
May 18 05:21:18.407: INFO: Namespace nsdeletetest-9505 was already deleted
STEP: Destroying namespace "nsdeletetest-8356" for this suite.

• [SLOW TEST:31.149 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":337,"completed":133,"skipped":2069,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:21:18.411: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 18 05:21:18.442: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 18 05:21:18.446: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 18 05:21:18.446: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 18 05:21:18.451: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 18 05:21:18.451: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 18 05:21:18.463: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 18 05:21:18.463: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 18 05:21:25.527: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:21:25.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6268" for this suite.

• [SLOW TEST:7.152 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":337,"completed":134,"skipped":2077,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:21:25.563: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
May 18 05:21:25.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7088 create -f -'
May 18 05:21:26.006: INFO: stderr: ""
May 18 05:21:26.007: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 18 05:21:26.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7088 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 18 05:21:26.163: INFO: stderr: ""
May 18 05:21:26.163: INFO: stdout: "update-demo-nautilus-2sv9h update-demo-nautilus-h7tbh "
May 18 05:21:26.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7088 get pods update-demo-nautilus-2sv9h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 18 05:21:26.267: INFO: stderr: ""
May 18 05:21:26.267: INFO: stdout: ""
May 18 05:21:26.267: INFO: update-demo-nautilus-2sv9h is created but not running
May 18 05:21:31.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7088 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 18 05:21:31.349: INFO: stderr: ""
May 18 05:21:31.349: INFO: stdout: "update-demo-nautilus-2sv9h update-demo-nautilus-h7tbh "
May 18 05:21:31.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7088 get pods update-demo-nautilus-2sv9h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 18 05:21:31.442: INFO: stderr: ""
May 18 05:21:31.442: INFO: stdout: "true"
May 18 05:21:31.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7088 get pods update-demo-nautilus-2sv9h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 18 05:21:31.507: INFO: stderr: ""
May 18 05:21:31.507: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 18 05:21:31.507: INFO: validating pod update-demo-nautilus-2sv9h
May 18 05:21:31.520: INFO: got data: {
  "image": "nautilus.jpg"
}

May 18 05:21:31.520: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 18 05:21:31.520: INFO: update-demo-nautilus-2sv9h is verified up and running
May 18 05:21:31.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7088 get pods update-demo-nautilus-h7tbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 18 05:21:31.586: INFO: stderr: ""
May 18 05:21:31.586: INFO: stdout: "true"
May 18 05:21:31.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7088 get pods update-demo-nautilus-h7tbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 18 05:21:31.663: INFO: stderr: ""
May 18 05:21:31.663: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 18 05:21:31.663: INFO: validating pod update-demo-nautilus-h7tbh
May 18 05:21:31.667: INFO: got data: {
  "image": "nautilus.jpg"
}

May 18 05:21:31.667: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 18 05:21:31.667: INFO: update-demo-nautilus-h7tbh is verified up and running
STEP: using delete to clean up resources
May 18 05:21:31.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7088 delete --grace-period=0 --force -f -'
May 18 05:21:31.745: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 18 05:21:31.745: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 18 05:21:31.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7088 get rc,svc -l name=update-demo --no-headers'
May 18 05:21:31.880: INFO: stderr: "No resources found in kubectl-7088 namespace.\n"
May 18 05:21:31.880: INFO: stdout: ""
May 18 05:21:31.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-7088 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 18 05:21:31.995: INFO: stderr: ""
May 18 05:21:31.996: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:21:31.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7088" for this suite.

• [SLOW TEST:6.446 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":337,"completed":135,"skipped":2087,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:21:32.009: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:21:32.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9450" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":337,"completed":136,"skipped":2103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:21:32.068: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
May 18 05:21:32.118: INFO: created test-pod-1
May 18 05:21:32.128: INFO: created test-pod-2
May 18 05:21:32.134: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:21:32.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6721" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":337,"completed":137,"skipped":2128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:21:32.209: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
May 18 05:21:32.247: INFO: Waiting up to 5m0s for pod "client-containers-cc40d3bb-11e2-44d1-af6d-69841d4249d1" in namespace "containers-8323" to be "Succeeded or Failed"
May 18 05:21:32.284: INFO: Pod "client-containers-cc40d3bb-11e2-44d1-af6d-69841d4249d1": Phase="Pending", Reason="", readiness=false. Elapsed: 36.476435ms
May 18 05:21:34.290: INFO: Pod "client-containers-cc40d3bb-11e2-44d1-af6d-69841d4249d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042163442s
STEP: Saw pod success
May 18 05:21:34.290: INFO: Pod "client-containers-cc40d3bb-11e2-44d1-af6d-69841d4249d1" satisfied condition "Succeeded or Failed"
May 18 05:21:34.292: INFO: Trying to get logs from node node2 pod client-containers-cc40d3bb-11e2-44d1-af6d-69841d4249d1 container agnhost-container: <nil>
STEP: delete the pod
May 18 05:21:34.311: INFO: Waiting for pod client-containers-cc40d3bb-11e2-44d1-af6d-69841d4249d1 to disappear
May 18 05:21:34.315: INFO: Pod client-containers-cc40d3bb-11e2-44d1-af6d-69841d4249d1 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:21:34.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8323" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":337,"completed":138,"skipped":2160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:21:34.325: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
May 18 05:21:34.364: INFO: Waiting up to 5m0s for pod "var-expansion-216a0e19-590f-4813-b380-abf8b0e32867" in namespace "var-expansion-2429" to be "Succeeded or Failed"
May 18 05:21:34.369: INFO: Pod "var-expansion-216a0e19-590f-4813-b380-abf8b0e32867": Phase="Pending", Reason="", readiness=false. Elapsed: 5.118377ms
May 18 05:21:36.375: INFO: Pod "var-expansion-216a0e19-590f-4813-b380-abf8b0e32867": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010816484s
STEP: Saw pod success
May 18 05:21:36.375: INFO: Pod "var-expansion-216a0e19-590f-4813-b380-abf8b0e32867" satisfied condition "Succeeded or Failed"
May 18 05:21:36.381: INFO: Trying to get logs from node node2 pod var-expansion-216a0e19-590f-4813-b380-abf8b0e32867 container dapi-container: <nil>
STEP: delete the pod
May 18 05:21:36.397: INFO: Waiting for pod var-expansion-216a0e19-590f-4813-b380-abf8b0e32867 to disappear
May 18 05:21:36.399: INFO: Pod var-expansion-216a0e19-590f-4813-b380-abf8b0e32867 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:21:36.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2429" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":337,"completed":139,"skipped":2185,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:21:36.406: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9999
May 18 05:21:36.447: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 18 05:21:38.452: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
May 18 05:21:38.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-9999 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 18 05:21:38.634: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 18 05:21:38.634: INFO: stdout: "iptables"
May 18 05:21:38.634: INFO: proxyMode: iptables
May 18 05:21:38.643: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 18 05:21:38.645: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-9999
STEP: creating replication controller affinity-nodeport-timeout in namespace services-9999
I0518 05:21:38.665857      19 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9999, replica count: 3
I0518 05:21:41.717766      19 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 18 05:21:41.732: INFO: Creating new exec pod
May 18 05:21:46.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-9999 exec execpod-affinitydl96k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
May 18 05:21:46.932: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 18 05:21:46.932: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:21:46.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-9999 exec execpod-affinitydl96k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.24.127.70 80'
May 18 05:21:47.070: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.24.127.70 80\nConnection to 172.24.127.70 80 port [tcp/http] succeeded!\n"
May 18 05:21:47.070: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:21:47.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-9999 exec execpod-affinitydl96k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 31356'
May 18 05:21:47.244: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.12 31356\nConnection to 172.28.128.12 31356 port [tcp/*] succeeded!\n"
May 18 05:21:47.244: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:21:47.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-9999 exec execpod-affinitydl96k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.13 31356'
May 18 05:21:47.482: INFO: stderr: "+ nc -v -t -w 2 172.28.128.13 31356\n+ echo hostName\nConnection to 172.28.128.13 31356 port [tcp/*] succeeded!\n"
May 18 05:21:47.482: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:21:47.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-9999 exec execpod-affinitydl96k -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.28.128.12:31356/ ; done'
May 18 05:21:47.831: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n"
May 18 05:21:47.831: INFO: stdout: "\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz\naffinity-nodeport-timeout-8drhz"
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Received response from host: affinity-nodeport-timeout-8drhz
May 18 05:21:47.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-9999 exec execpod-affinitydl96k -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.28.128.12:31356/'
May 18 05:21:48.068: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n"
May 18 05:21:48.068: INFO: stdout: "affinity-nodeport-timeout-8drhz"
May 18 05:22:08.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-9999 exec execpod-affinitydl96k -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.28.128.12:31356/'
May 18 05:22:08.218: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.28.128.12:31356/\n"
May 18 05:22:08.218: INFO: stdout: "affinity-nodeport-timeout-5dnpf"
May 18 05:22:08.218: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9999, will wait for the garbage collector to delete the pods
May 18 05:22:08.306: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 4.715479ms
May 18 05:22:08.407: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.789143ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:22:21.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9999" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:44.659 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":337,"completed":140,"skipped":2193,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:22:21.065: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 18 05:22:21.136: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 18 05:22:40.096: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:22:46.826: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:23:07.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9298" for this suite.

• [SLOW TEST:46.306 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":337,"completed":141,"skipped":2195,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:23:07.372: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:23:07.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8879" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":337,"completed":142,"skipped":2210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:23:07.482: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:23:07.511: INFO: Creating deployment "test-recreate-deployment"
May 18 05:23:07.515: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 18 05:23:07.524: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 18 05:23:09.530: INFO: Waiting deployment "test-recreate-deployment" to complete
May 18 05:23:09.533: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912187, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912187, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912187, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912187, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6cb8b65c46\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 18 05:23:11.537: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 18 05:23:11.546: INFO: Updating deployment test-recreate-deployment
May 18 05:23:11.546: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 18 05:23:11.636: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8448  1b4f42fd-2bd9-4165-936f-a5f45a963207 31823 2 2021-05-18 05:23:07 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-18 05:23:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-18 05:23:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fa2578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-18 05:23:11 +0000 UTC,LastTransitionTime:2021-05-18 05:23:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-05-18 05:23:11 +0000 UTC,LastTransitionTime:2021-05-18 05:23:07 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 18 05:23:11.642: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-8448  051f6f78-59ca-4f23-8e3c-9cdff69c8c3c 31820 1 2021-05-18 05:23:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 1b4f42fd-2bd9-4165-936f-a5f45a963207 0xc0059bdea0 0xc0059bdea1}] []  [{kube-controller-manager Update apps/v1 2021-05-18 05:23:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b4f42fd-2bd9-4165-936f-a5f45a963207\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059bdf58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 18 05:23:11.642: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 18 05:23:11.642: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-8448  84b18ce1-2e79-4ea5-b052-c48170db3822 31812 2 2021-05-18 05:23:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 1b4f42fd-2bd9-4165-936f-a5f45a963207 0xc0059bdd37 0xc0059bdd38}] []  [{kube-controller-manager Update apps/v1 2021-05-18 05:23:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b4f42fd-2bd9-4165-936f-a5f45a963207\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059bddf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 18 05:23:11.647: INFO: Pod "test-recreate-deployment-85d47dcb4-d8xpz" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-d8xpz test-recreate-deployment-85d47dcb4- deployment-8448  4b10dea8-ccb5-4cf7-9bc0-7f309d702b03 31824 0 2021-05-18 05:23:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 051f6f78-59ca-4f23-8e3c-9cdff69c8c3c 0xc005ffe470 0xc005ffe471}] []  [{kube-controller-manager Update v1 2021-05-18 05:23:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"051f6f78-59ca-4f23-8e3c-9cdff69c8c3c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-18 05:23:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fcmvt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fcmvt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:23:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:23:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:23:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:23:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2021-05-18 05:23:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:23:11.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8448" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":337,"completed":143,"skipped":2244,"failed":0}

------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:23:11.656: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:23:12.769: INFO: Checking APIGroup: apiregistration.k8s.io
May 18 05:23:12.770: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 18 05:23:12.770: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.770: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 18 05:23:12.770: INFO: Checking APIGroup: apps
May 18 05:23:12.772: INFO: PreferredVersion.GroupVersion: apps/v1
May 18 05:23:12.772: INFO: Versions found [{apps/v1 v1}]
May 18 05:23:12.772: INFO: apps/v1 matches apps/v1
May 18 05:23:12.772: INFO: Checking APIGroup: events.k8s.io
May 18 05:23:12.774: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 18 05:23:12.774: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.774: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 18 05:23:12.774: INFO: Checking APIGroup: authentication.k8s.io
May 18 05:23:12.775: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 18 05:23:12.775: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.775: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 18 05:23:12.775: INFO: Checking APIGroup: authorization.k8s.io
May 18 05:23:12.776: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 18 05:23:12.776: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.776: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 18 05:23:12.776: INFO: Checking APIGroup: autoscaling
May 18 05:23:12.778: INFO: PreferredVersion.GroupVersion: autoscaling/v1
May 18 05:23:12.778: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 18 05:23:12.778: INFO: autoscaling/v1 matches autoscaling/v1
May 18 05:23:12.778: INFO: Checking APIGroup: batch
May 18 05:23:12.779: INFO: PreferredVersion.GroupVersion: batch/v1
May 18 05:23:12.779: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 18 05:23:12.779: INFO: batch/v1 matches batch/v1
May 18 05:23:12.779: INFO: Checking APIGroup: certificates.k8s.io
May 18 05:23:12.780: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 18 05:23:12.780: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.780: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 18 05:23:12.780: INFO: Checking APIGroup: networking.k8s.io
May 18 05:23:12.782: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 18 05:23:12.782: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.782: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 18 05:23:12.782: INFO: Checking APIGroup: extensions
May 18 05:23:12.783: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
May 18 05:23:12.783: INFO: Versions found [{extensions/v1beta1 v1beta1}]
May 18 05:23:12.783: INFO: extensions/v1beta1 matches extensions/v1beta1
May 18 05:23:12.783: INFO: Checking APIGroup: policy
May 18 05:23:12.785: INFO: PreferredVersion.GroupVersion: policy/v1
May 18 05:23:12.785: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
May 18 05:23:12.785: INFO: policy/v1 matches policy/v1
May 18 05:23:12.785: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 18 05:23:12.786: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 18 05:23:12.786: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.786: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 18 05:23:12.786: INFO: Checking APIGroup: storage.k8s.io
May 18 05:23:12.787: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 18 05:23:12.787: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.787: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 18 05:23:12.787: INFO: Checking APIGroup: admissionregistration.k8s.io
May 18 05:23:12.788: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 18 05:23:12.788: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.788: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 18 05:23:12.788: INFO: Checking APIGroup: apiextensions.k8s.io
May 18 05:23:12.790: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 18 05:23:12.790: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.790: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 18 05:23:12.790: INFO: Checking APIGroup: scheduling.k8s.io
May 18 05:23:12.791: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 18 05:23:12.791: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.791: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 18 05:23:12.791: INFO: Checking APIGroup: coordination.k8s.io
May 18 05:23:12.793: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 18 05:23:12.793: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.793: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 18 05:23:12.793: INFO: Checking APIGroup: node.k8s.io
May 18 05:23:12.794: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 18 05:23:12.794: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.794: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 18 05:23:12.794: INFO: Checking APIGroup: discovery.k8s.io
May 18 05:23:12.796: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May 18 05:23:12.796: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.796: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May 18 05:23:12.796: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 18 05:23:12.797: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
May 18 05:23:12.797: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May 18 05:23:12.797: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
May 18 05:23:12.797: INFO: Checking APIGroup: crd.projectcalico.org
May 18 05:23:12.799: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
May 18 05:23:12.799: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
May 18 05:23:12.799: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:23:12.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8918" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":337,"completed":144,"skipped":2244,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:23:12.806: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-b49ef699-8e27-4bbb-8272-26d663153a44
STEP: Creating secret with name secret-projected-all-test-volume-4fd76611-066e-43ab-8f3c-7fd7e780d328
STEP: Creating a pod to test Check all projections for projected volume plugin
May 18 05:23:12.897: INFO: Waiting up to 5m0s for pod "projected-volume-0c6b146d-04fd-40cd-9a8c-6fe637792085" in namespace "projected-586" to be "Succeeded or Failed"
May 18 05:23:12.906: INFO: Pod "projected-volume-0c6b146d-04fd-40cd-9a8c-6fe637792085": Phase="Pending", Reason="", readiness=false. Elapsed: 9.429458ms
May 18 05:23:14.912: INFO: Pod "projected-volume-0c6b146d-04fd-40cd-9a8c-6fe637792085": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014692167s
STEP: Saw pod success
May 18 05:23:14.912: INFO: Pod "projected-volume-0c6b146d-04fd-40cd-9a8c-6fe637792085" satisfied condition "Succeeded or Failed"
May 18 05:23:14.914: INFO: Trying to get logs from node node2 pod projected-volume-0c6b146d-04fd-40cd-9a8c-6fe637792085 container projected-all-volume-test: <nil>
STEP: delete the pod
May 18 05:23:14.930: INFO: Waiting for pod projected-volume-0c6b146d-04fd-40cd-9a8c-6fe637792085 to disappear
May 18 05:23:14.932: INFO: Pod projected-volume-0c6b146d-04fd-40cd-9a8c-6fe637792085 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:23:14.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-586" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":337,"completed":145,"skipped":2265,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:23:14.939: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-df4f7e33-7949-4107-a683-597b647699ed in namespace container-probe-9732
May 18 05:23:17.001: INFO: Started pod liveness-df4f7e33-7949-4107-a683-597b647699ed in namespace container-probe-9732
STEP: checking the pod's current state and verifying that restartCount is present
May 18 05:23:17.003: INFO: Initial restart count of pod liveness-df4f7e33-7949-4107-a683-597b647699ed is 0
May 18 05:23:37.068: INFO: Restart count of pod container-probe-9732/liveness-df4f7e33-7949-4107-a683-597b647699ed is now 1 (20.064751134s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:23:37.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9732" for this suite.

• [SLOW TEST:22.163 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":337,"completed":146,"skipped":2274,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:23:37.102: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 05:23:37.143: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a66e803-0c5c-4d45-a898-5c4a430c54c1" in namespace "projected-5335" to be "Succeeded or Failed"
May 18 05:23:37.148: INFO: Pod "downwardapi-volume-5a66e803-0c5c-4d45-a898-5c4a430c54c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.37608ms
May 18 05:23:39.155: INFO: Pod "downwardapi-volume-5a66e803-0c5c-4d45-a898-5c4a430c54c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011361081s
STEP: Saw pod success
May 18 05:23:39.155: INFO: Pod "downwardapi-volume-5a66e803-0c5c-4d45-a898-5c4a430c54c1" satisfied condition "Succeeded or Failed"
May 18 05:23:39.157: INFO: Trying to get logs from node node2 pod downwardapi-volume-5a66e803-0c5c-4d45-a898-5c4a430c54c1 container client-container: <nil>
STEP: delete the pod
May 18 05:23:39.183: INFO: Waiting for pod downwardapi-volume-5a66e803-0c5c-4d45-a898-5c4a430c54c1 to disappear
May 18 05:23:39.186: INFO: Pod downwardapi-volume-5a66e803-0c5c-4d45-a898-5c4a430c54c1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:23:39.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5335" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":337,"completed":147,"skipped":2275,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:23:39.196: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 05:23:39.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4cc8478c-489e-4ee1-8992-f5ed1a03bfc0" in namespace "projected-4991" to be "Succeeded or Failed"
May 18 05:23:39.245: INFO: Pod "downwardapi-volume-4cc8478c-489e-4ee1-8992-f5ed1a03bfc0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.01545ms
May 18 05:23:41.251: INFO: Pod "downwardapi-volume-4cc8478c-489e-4ee1-8992-f5ed1a03bfc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017872451s
STEP: Saw pod success
May 18 05:23:41.251: INFO: Pod "downwardapi-volume-4cc8478c-489e-4ee1-8992-f5ed1a03bfc0" satisfied condition "Succeeded or Failed"
May 18 05:23:41.254: INFO: Trying to get logs from node node2 pod downwardapi-volume-4cc8478c-489e-4ee1-8992-f5ed1a03bfc0 container client-container: <nil>
STEP: delete the pod
May 18 05:23:41.274: INFO: Waiting for pod downwardapi-volume-4cc8478c-489e-4ee1-8992-f5ed1a03bfc0 to disappear
May 18 05:23:41.277: INFO: Pod downwardapi-volume-4cc8478c-489e-4ee1-8992-f5ed1a03bfc0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:23:41.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4991" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":148,"skipped":2284,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:23:41.285: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
May 18 05:23:41.331: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May 18 05:23:43.341: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May 18 05:23:45.341: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
May 18 05:23:45.373: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May 18 05:23:47.380: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 18 05:23:47.384: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3784 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:23:47.384: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:23:47.552: INFO: Exec stderr: ""
May 18 05:23:47.552: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3784 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:23:47.552: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:23:47.721: INFO: Exec stderr: ""
May 18 05:23:47.722: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3784 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:23:47.722: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:23:47.911: INFO: Exec stderr: ""
May 18 05:23:47.911: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3784 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:23:47.911: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:23:48.063: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 18 05:23:48.063: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3784 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:23:48.063: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:23:48.218: INFO: Exec stderr: ""
May 18 05:23:48.218: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3784 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:23:48.218: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:23:48.409: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 18 05:23:48.409: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3784 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:23:48.409: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:23:48.552: INFO: Exec stderr: ""
May 18 05:23:48.552: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3784 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:23:48.552: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:23:48.699: INFO: Exec stderr: ""
May 18 05:23:48.699: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3784 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:23:48.699: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:23:48.819: INFO: Exec stderr: ""
May 18 05:23:48.820: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3784 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:23:48.820: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:23:48.929: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:23:48.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3784" for this suite.

• [SLOW TEST:7.661 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":149,"skipped":2341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:23:48.947: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
May 18 05:23:48.991: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:24:09.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3589" for this suite.

• [SLOW TEST:20.128 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":337,"completed":150,"skipped":2426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:24:09.076: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-40a63e40-7869-4135-b018-b4c869003bd0
STEP: Creating a pod to test consume secrets
May 18 05:24:09.133: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8cd1a858-7b8a-41a2-9ef0-291f01aeb254" in namespace "projected-2079" to be "Succeeded or Failed"
May 18 05:24:09.136: INFO: Pod "pod-projected-secrets-8cd1a858-7b8a-41a2-9ef0-291f01aeb254": Phase="Pending", Reason="", readiness=false. Elapsed: 3.659283ms
May 18 05:24:11.140: INFO: Pod "pod-projected-secrets-8cd1a858-7b8a-41a2-9ef0-291f01aeb254": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0071325s
May 18 05:24:13.145: INFO: Pod "pod-projected-secrets-8cd1a858-7b8a-41a2-9ef0-291f01aeb254": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012772407s
STEP: Saw pod success
May 18 05:24:13.145: INFO: Pod "pod-projected-secrets-8cd1a858-7b8a-41a2-9ef0-291f01aeb254" satisfied condition "Succeeded or Failed"
May 18 05:24:13.147: INFO: Trying to get logs from node node2 pod pod-projected-secrets-8cd1a858-7b8a-41a2-9ef0-291f01aeb254 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 18 05:24:13.171: INFO: Waiting for pod pod-projected-secrets-8cd1a858-7b8a-41a2-9ef0-291f01aeb254 to disappear
May 18 05:24:13.174: INFO: Pod pod-projected-secrets-8cd1a858-7b8a-41a2-9ef0-291f01aeb254 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:24:13.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2079" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":151,"skipped":2448,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:24:13.181: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:24:13.908: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:24:16.939: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:24:17.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6989" for this suite.
STEP: Destroying namespace "webhook-6989-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":337,"completed":152,"skipped":2455,"failed":0}
S
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:24:17.099: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-180a25a5-0bb5-4b1c-9216-df646afeb198 in namespace container-probe-7090
May 18 05:24:21.183: INFO: Started pod busybox-180a25a5-0bb5-4b1c-9216-df646afeb198 in namespace container-probe-7090
STEP: checking the pod's current state and verifying that restartCount is present
May 18 05:24:21.187: INFO: Initial restart count of pod busybox-180a25a5-0bb5-4b1c-9216-df646afeb198 is 0
May 18 05:25:09.324: INFO: Restart count of pod container-probe-7090/busybox-180a25a5-0bb5-4b1c-9216-df646afeb198 is now 1 (48.136245568s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:25:09.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7090" for this suite.

• [SLOW TEST:52.253 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":337,"completed":153,"skipped":2456,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:25:09.352: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-3ee9d66c-7b3e-45c6-a4c1-88e63c0432f0
STEP: Creating a pod to test consume configMaps
May 18 05:25:09.402: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a8a799a2-c6fc-42f3-b531-8f5d17e3027f" in namespace "projected-3110" to be "Succeeded or Failed"
May 18 05:25:09.405: INFO: Pod "pod-projected-configmaps-a8a799a2-c6fc-42f3-b531-8f5d17e3027f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.693088ms
May 18 05:25:11.410: INFO: Pod "pod-projected-configmaps-a8a799a2-c6fc-42f3-b531-8f5d17e3027f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008115096s
STEP: Saw pod success
May 18 05:25:11.410: INFO: Pod "pod-projected-configmaps-a8a799a2-c6fc-42f3-b531-8f5d17e3027f" satisfied condition "Succeeded or Failed"
May 18 05:25:11.414: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-a8a799a2-c6fc-42f3-b531-8f5d17e3027f container agnhost-container: <nil>
STEP: delete the pod
May 18 05:25:11.437: INFO: Waiting for pod pod-projected-configmaps-a8a799a2-c6fc-42f3-b531-8f5d17e3027f to disappear
May 18 05:25:11.440: INFO: Pod pod-projected-configmaps-a8a799a2-c6fc-42f3-b531-8f5d17e3027f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:25:11.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3110" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":337,"completed":154,"skipped":2466,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:25:11.454: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-ee2b718e-9080-4f53-b6a4-ef4e702b49a5
STEP: Creating the pod
May 18 05:25:11.527: INFO: The status of Pod pod-configmaps-cd5bcf16-8ee9-4b39-bfd2-b3f38fc1445f is Pending, waiting for it to be Running (with Ready = true)
May 18 05:25:13.535: INFO: The status of Pod pod-configmaps-cd5bcf16-8ee9-4b39-bfd2-b3f38fc1445f is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-ee2b718e-9080-4f53-b6a4-ef4e702b49a5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:25:15.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1609" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":337,"completed":155,"skipped":2466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:25:15.579: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:25:16.010: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 18 05:25:18.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912315, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912315, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912316, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912315, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:25:21.044: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:25:21.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6222" for this suite.
STEP: Destroying namespace "webhook-6222-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.585 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":337,"completed":156,"skipped":2498,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:25:21.164: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-7b34c596-b536-415c-be09-6971ed964ee8
STEP: Creating secret with name s-test-opt-upd-0092bb87-01b4-4c60-bc20-44b0a85ee5c3
STEP: Creating the pod
May 18 05:25:21.255: INFO: The status of Pod pod-projected-secrets-504e3619-3eda-4456-9cd0-0e327713f877 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:25:23.264: INFO: The status of Pod pod-projected-secrets-504e3619-3eda-4456-9cd0-0e327713f877 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:25:25.266: INFO: The status of Pod pod-projected-secrets-504e3619-3eda-4456-9cd0-0e327713f877 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-7b34c596-b536-415c-be09-6971ed964ee8
STEP: Updating secret s-test-opt-upd-0092bb87-01b4-4c60-bc20-44b0a85ee5c3
STEP: Creating secret with name s-test-opt-create-3994b82e-c4fa-4ce3-a86b-4f110a468578
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:26:35.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-150" for this suite.

• [SLOW TEST:74.487 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":337,"completed":157,"skipped":2509,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:26:35.652: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-843ebde3-2094-46e3-b7d6-ed06605644ff
STEP: Creating a pod to test consume configMaps
May 18 05:26:35.686: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d52e15c7-5352-4797-953e-0b7ea9555261" in namespace "projected-3173" to be "Succeeded or Failed"
May 18 05:26:35.690: INFO: Pod "pod-projected-configmaps-d52e15c7-5352-4797-953e-0b7ea9555261": Phase="Pending", Reason="", readiness=false. Elapsed: 4.45378ms
May 18 05:26:37.697: INFO: Pod "pod-projected-configmaps-d52e15c7-5352-4797-953e-0b7ea9555261": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011041983s
STEP: Saw pod success
May 18 05:26:37.697: INFO: Pod "pod-projected-configmaps-d52e15c7-5352-4797-953e-0b7ea9555261" satisfied condition "Succeeded or Failed"
May 18 05:26:37.700: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-d52e15c7-5352-4797-953e-0b7ea9555261 container agnhost-container: <nil>
STEP: delete the pod
May 18 05:26:37.730: INFO: Waiting for pod pod-projected-configmaps-d52e15c7-5352-4797-953e-0b7ea9555261 to disappear
May 18 05:26:37.733: INFO: Pod pod-projected-configmaps-d52e15c7-5352-4797-953e-0b7ea9555261 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:26:37.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3173" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":337,"completed":158,"skipped":2518,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:26:37.742: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 05:26:37.780: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7497fe4c-303b-4e66-a433-5154dfbe8369" in namespace "downward-api-9360" to be "Succeeded or Failed"
May 18 05:26:37.788: INFO: Pod "downwardapi-volume-7497fe4c-303b-4e66-a433-5154dfbe8369": Phase="Pending", Reason="", readiness=false. Elapsed: 7.850864ms
May 18 05:26:39.795: INFO: Pod "downwardapi-volume-7497fe4c-303b-4e66-a433-5154dfbe8369": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014502267s
STEP: Saw pod success
May 18 05:26:39.795: INFO: Pod "downwardapi-volume-7497fe4c-303b-4e66-a433-5154dfbe8369" satisfied condition "Succeeded or Failed"
May 18 05:26:39.797: INFO: Trying to get logs from node node1 pod downwardapi-volume-7497fe4c-303b-4e66-a433-5154dfbe8369 container client-container: <nil>
STEP: delete the pod
May 18 05:26:39.816: INFO: Waiting for pod downwardapi-volume-7497fe4c-303b-4e66-a433-5154dfbe8369 to disappear
May 18 05:26:39.821: INFO: Pod downwardapi-volume-7497fe4c-303b-4e66-a433-5154dfbe8369 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:26:39.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9360" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":337,"completed":159,"skipped":2536,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:26:39.829: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 05:26:39.870: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9392dd8-5a4e-4b6a-938a-c78cf4bed0fc" in namespace "projected-6596" to be "Succeeded or Failed"
May 18 05:26:39.874: INFO: Pod "downwardapi-volume-c9392dd8-5a4e-4b6a-938a-c78cf4bed0fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.26998ms
May 18 05:26:41.881: INFO: Pod "downwardapi-volume-c9392dd8-5a4e-4b6a-938a-c78cf4bed0fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011180282s
STEP: Saw pod success
May 18 05:26:41.881: INFO: Pod "downwardapi-volume-c9392dd8-5a4e-4b6a-938a-c78cf4bed0fc" satisfied condition "Succeeded or Failed"
May 18 05:26:41.884: INFO: Trying to get logs from node node1 pod downwardapi-volume-c9392dd8-5a4e-4b6a-938a-c78cf4bed0fc container client-container: <nil>
STEP: delete the pod
May 18 05:26:41.902: INFO: Waiting for pod downwardapi-volume-c9392dd8-5a4e-4b6a-938a-c78cf4bed0fc to disappear
May 18 05:26:41.910: INFO: Pod downwardapi-volume-c9392dd8-5a4e-4b6a-938a-c78cf4bed0fc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:26:41.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6596" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":337,"completed":160,"skipped":2538,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:26:41.924: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:26:41.978: INFO: The status of Pod busybox-scheduling-c9f1a9ed-e70b-47ed-a25b-6b1bfbbd2328 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:26:43.987: INFO: The status of Pod busybox-scheduling-c9f1a9ed-e70b-47ed-a25b-6b1bfbbd2328 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:26:44.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-927" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":337,"completed":161,"skipped":2557,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:26:44.020: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:26:44.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9285" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":337,"completed":162,"skipped":2566,"failed":0}
S
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:26:44.087: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 18 05:26:46.147: INFO: &Pod{ObjectMeta:{send-events-7afdbe8c-e1ed-4478-9587-dab9101dbeaf  events-6171  b970b525-3f01-4da3-b6e1-0a2f9b1e7146 32910 0 2021-05-18 05:26:44 +0000 UTC <nil> <nil> map[name:foo time:123930390] map[cni.projectcalico.org/podIP:172.30.104.7/32 cni.projectcalico.org/podIPs:172.30.104.7/32] [] []  [{calico Update v1 2021-05-18 05:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {e2e.test Update v1 2021-05-18 05:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-18 05:26:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.104.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-262t5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-262t5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:26:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:26:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.104.7,StartTime:2021-05-18 05:26:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:26:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://0abe97a144b47765f7616a5d019508f7b2741c91661f7adca46eb124d8e559ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.104.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 18 05:26:48.155: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 18 05:26:50.165: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:26:50.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6171" for this suite.

• [SLOW TEST:6.101 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":337,"completed":163,"skipped":2567,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:26:50.188: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
May 18 05:26:50.218: INFO: Waiting up to 5m0s for pod "var-expansion-3c39a778-61c3-4c90-86b2-c5b50613bb00" in namespace "var-expansion-3837" to be "Succeeded or Failed"
May 18 05:26:50.220: INFO: Pod "var-expansion-3c39a778-61c3-4c90-86b2-c5b50613bb00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.513888ms
May 18 05:26:52.225: INFO: Pod "var-expansion-3c39a778-61c3-4c90-86b2-c5b50613bb00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007565498s
STEP: Saw pod success
May 18 05:26:52.226: INFO: Pod "var-expansion-3c39a778-61c3-4c90-86b2-c5b50613bb00" satisfied condition "Succeeded or Failed"
May 18 05:26:52.228: INFO: Trying to get logs from node node2 pod var-expansion-3c39a778-61c3-4c90-86b2-c5b50613bb00 container dapi-container: <nil>
STEP: delete the pod
May 18 05:26:52.246: INFO: Waiting for pod var-expansion-3c39a778-61c3-4c90-86b2-c5b50613bb00 to disappear
May 18 05:26:52.248: INFO: Pod var-expansion-3c39a778-61c3-4c90-86b2-c5b50613bb00 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:26:52.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3837" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":337,"completed":164,"skipped":2568,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:26:52.256: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
May 18 05:26:56.305: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9290 PodName:pod-sharedvolume-757fe4f7-4649-4ed0-b545-022681be3ff2 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:26:56.305: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:26:56.396: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:26:56.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9290" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":337,"completed":165,"skipped":2600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:26:56.403: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:26:56.444: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:26:57.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9478" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":337,"completed":166,"skipped":2649,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:26:57.466: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:26:57.513: INFO: Create a RollingUpdate DaemonSet
May 18 05:26:57.516: INFO: Check that daemon pods launch on every node of the cluster
May 18 05:26:57.521: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:26:57.523: INFO: Number of nodes with available pods: 0
May 18 05:26:57.523: INFO: Node node1 is running more than one daemon pod
May 18 05:26:58.536: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:26:58.539: INFO: Number of nodes with available pods: 0
May 18 05:26:58.539: INFO: Node node1 is running more than one daemon pod
May 18 05:26:59.528: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:26:59.530: INFO: Number of nodes with available pods: 2
May 18 05:26:59.530: INFO: Number of running nodes: 2, number of available pods: 2
May 18 05:26:59.530: INFO: Update the DaemonSet to trigger a rollout
May 18 05:26:59.535: INFO: Updating DaemonSet daemon-set
May 18 05:27:15.547: INFO: Roll back the DaemonSet before rollout is complete
May 18 05:27:15.552: INFO: Updating DaemonSet daemon-set
May 18 05:27:15.552: INFO: Make sure DaemonSet rollback is complete
May 18 05:27:15.554: INFO: Wrong image for pod: daemon-set-lknlc. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
May 18 05:27:15.554: INFO: Pod daemon-set-lknlc is not available
May 18 05:27:15.565: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:27:16.573: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:27:17.573: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:27:18.573: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:27:19.572: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:27:20.573: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:27:21.571: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:27:22.571: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:27:23.571: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:27:24.571: INFO: Pod daemon-set-qgg88 is not available
May 18 05:27:24.573: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5382, will wait for the garbage collector to delete the pods
May 18 05:27:24.633: INFO: Deleting DaemonSet.extensions daemon-set took: 3.062886ms
May 18 05:27:24.734: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.851343ms
May 18 05:27:34.639: INFO: Number of nodes with available pods: 0
May 18 05:27:34.639: INFO: Number of running nodes: 0, number of available pods: 0
May 18 05:27:34.641: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33242"},"items":null}

May 18 05:27:34.643: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33242"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:27:34.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5382" for this suite.

• [SLOW TEST:37.190 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":337,"completed":167,"skipped":2657,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:27:34.657: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-579a8510-066e-4512-9de5-a7edd13b1c3a
STEP: Creating a pod to test consume secrets
May 18 05:27:34.697: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-47fec21d-d704-4f6e-8a28-d99a368aa2ca" in namespace "projected-1556" to be "Succeeded or Failed"
May 18 05:27:34.703: INFO: Pod "pod-projected-secrets-47fec21d-d704-4f6e-8a28-d99a368aa2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.116973ms
May 18 05:27:36.708: INFO: Pod "pod-projected-secrets-47fec21d-d704-4f6e-8a28-d99a368aa2ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011127283s
STEP: Saw pod success
May 18 05:27:36.708: INFO: Pod "pod-projected-secrets-47fec21d-d704-4f6e-8a28-d99a368aa2ca" satisfied condition "Succeeded or Failed"
May 18 05:27:36.710: INFO: Trying to get logs from node node2 pod pod-projected-secrets-47fec21d-d704-4f6e-8a28-d99a368aa2ca container projected-secret-volume-test: <nil>
STEP: delete the pod
May 18 05:27:36.736: INFO: Waiting for pod pod-projected-secrets-47fec21d-d704-4f6e-8a28-d99a368aa2ca to disappear
May 18 05:27:36.738: INFO: Pod pod-projected-secrets-47fec21d-d704-4f6e-8a28-d99a368aa2ca no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:27:36.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1556" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":168,"skipped":2669,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:27:36.745: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0518 05:27:42.810911      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 18 05:28:44.824: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:28:44.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4542" for this suite.

• [SLOW TEST:68.090 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":337,"completed":169,"skipped":2689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:28:44.835: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:28:44.869: INFO: Got root ca configmap in namespace "svcaccounts-8633"
May 18 05:28:44.872: INFO: Deleted root ca configmap in namespace "svcaccounts-8633"
STEP: waiting for a new root ca configmap created
May 18 05:28:45.379: INFO: Recreated root ca configmap in namespace "svcaccounts-8633"
May 18 05:28:45.384: INFO: Updated root ca configmap in namespace "svcaccounts-8633"
STEP: waiting for the root ca configmap reconciled
May 18 05:28:45.889: INFO: Reconciled root ca configmap in namespace "svcaccounts-8633"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:28:45.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8633" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":337,"completed":170,"skipped":2719,"failed":0}
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:28:45.896: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
May 18 05:28:45.932: INFO: PodSpec: initContainers in spec.initContainers
May 18 05:29:34.078: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-69ad3185-5d4b-4cb0-bd07-5a4a0c495ffd", GenerateName:"", Namespace:"init-container-6362", SelfLink:"", UID:"d9aa1644-61d2-4ace-bfa3-72dab9692c4a", ResourceVersion:"33801", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63756912525, loc:(*time.Location)(0x9dc0820)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"932093258"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.104.22/32", "cni.projectcalico.org/podIPs":"172.30.104.22/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00477c120), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00477c138)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00477c150), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00477c168)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00477c198), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00477c1b0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-6j4pb", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0036344c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6j4pb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6j4pb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.4.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6j4pb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00337e3d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00239c4d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00337e450)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00337e470)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00337e478), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00337e47c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0015dc070), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912525, loc:(*time.Location)(0x9dc0820)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912525, loc:(*time.Location)(0x9dc0820)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912525, loc:(*time.Location)(0x9dc0820)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912525, loc:(*time.Location)(0x9dc0820)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.28.128.13", PodIP:"172.30.104.22", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.104.22"}}, StartTime:(*v1.Time)(0xc00477c1f8), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00239c5b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00239c620)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"docker-pullable://k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"docker://f62dbf1faad4e1d225cef9e0dac23bd0270ebb98932d7ad096ae38b077ab0912", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003634600), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0036345e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.4.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00337e4f4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:29:34.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6362" for this suite.

• [SLOW TEST:48.215 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":337,"completed":171,"skipped":2722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:29:34.111: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 18 05:29:34.156: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  5a3220e8-c849-44c7-9f81-e7d1d56bbc89 33809 0 2021-05-18 05:29:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-18 05:29:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 05:29:34.156: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  5a3220e8-c849-44c7-9f81-e7d1d56bbc89 33810 0 2021-05-18 05:29:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-18 05:29:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 05:29:34.157: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  5a3220e8-c849-44c7-9f81-e7d1d56bbc89 33811 0 2021-05-18 05:29:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-18 05:29:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 18 05:29:44.190: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  5a3220e8-c849-44c7-9f81-e7d1d56bbc89 33842 0 2021-05-18 05:29:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-18 05:29:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 05:29:44.190: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  5a3220e8-c849-44c7-9f81-e7d1d56bbc89 33843 0 2021-05-18 05:29:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-18 05:29:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 05:29:44.190: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  5a3220e8-c849-44c7-9f81-e7d1d56bbc89 33844 0 2021-05-18 05:29:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-18 05:29:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:29:44.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5304" for this suite.

• [SLOW TEST:10.086 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":337,"completed":172,"skipped":2758,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:29:44.197: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
May 18 05:29:44.224: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 18 05:29:44.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1879 create -f -'
May 18 05:29:44.509: INFO: stderr: ""
May 18 05:29:44.509: INFO: stdout: "service/agnhost-replica created\n"
May 18 05:29:44.509: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 18 05:29:44.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1879 create -f -'
May 18 05:29:44.737: INFO: stderr: ""
May 18 05:29:44.737: INFO: stdout: "service/agnhost-primary created\n"
May 18 05:29:44.737: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 18 05:29:44.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1879 create -f -'
May 18 05:29:44.948: INFO: stderr: ""
May 18 05:29:44.948: INFO: stdout: "service/frontend created\n"
May 18 05:29:44.948: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 18 05:29:44.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1879 create -f -'
May 18 05:29:45.203: INFO: stderr: ""
May 18 05:29:45.203: INFO: stdout: "deployment.apps/frontend created\n"
May 18 05:29:45.203: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 18 05:29:45.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1879 create -f -'
May 18 05:29:45.549: INFO: stderr: ""
May 18 05:29:45.549: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 18 05:29:45.549: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 18 05:29:45.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1879 create -f -'
May 18 05:29:45.968: INFO: stderr: ""
May 18 05:29:45.968: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 18 05:29:45.968: INFO: Waiting for all frontend pods to be Running.
May 18 05:29:51.020: INFO: Waiting for frontend to serve content.
May 18 05:29:51.028: INFO: Trying to add a new entry to the guestbook.
May 18 05:29:51.036: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 18 05:29:51.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1879 delete --grace-period=0 --force -f -'
May 18 05:29:51.148: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 18 05:29:51.148: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 18 05:29:51.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1879 delete --grace-period=0 --force -f -'
May 18 05:29:51.229: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 18 05:29:51.230: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 18 05:29:51.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1879 delete --grace-period=0 --force -f -'
May 18 05:29:51.318: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 18 05:29:51.318: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 18 05:29:51.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1879 delete --grace-period=0 --force -f -'
May 18 05:29:51.389: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 18 05:29:51.389: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 18 05:29:51.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1879 delete --grace-period=0 --force -f -'
May 18 05:29:51.467: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 18 05:29:51.467: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 18 05:29:51.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1879 delete --grace-period=0 --force -f -'
May 18 05:29:51.563: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 18 05:29:51.563: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:29:51.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1879" for this suite.

• [SLOW TEST:7.379 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:336
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":337,"completed":173,"skipped":2770,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:29:51.577: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:29:52.422: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 18 05:29:54.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912592, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912592, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912592, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756912592, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:29:57.469: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:29:57.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1879" for this suite.
STEP: Destroying namespace "webhook-1879-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.996 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":337,"completed":174,"skipped":2783,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:29:57.573: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0518 05:30:07.638939      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 18 05:31:09.654: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:31:09.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8350" for this suite.

• [SLOW TEST:72.089 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":337,"completed":175,"skipped":2802,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:31:09.662: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:31:09.687: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 18 05:31:09.694: INFO: The status of Pod pod-logs-websocket-8104955e-11d3-45b8-a279-079f462eab82 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:31:11.699: INFO: The status of Pod pod-logs-websocket-8104955e-11d3-45b8-a279-079f462eab82 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:31:11.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8028" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":337,"completed":176,"skipped":2807,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:31:11.722: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-4505
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
May 18 05:31:11.763: INFO: Found 0 stateful pods, waiting for 3
May 18 05:31:21.773: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 18 05:31:21.773: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 18 05:31:21.773: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 18 05:31:21.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-4505 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 18 05:31:22.296: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 18 05:31:22.296: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 18 05:31:22.296: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
May 18 05:31:32.399: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 18 05:31:42.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-4505 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 05:31:42.561: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 18 05:31:42.561: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 18 05:31:42.561: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 18 05:31:52.578: INFO: Waiting for StatefulSet statefulset-4505/ss2 to complete update
May 18 05:31:52.578: INFO: Waiting for Pod statefulset-4505/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Rolling back to a previous revision
May 18 05:32:02.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-4505 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 18 05:32:02.776: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 18 05:32:02.776: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 18 05:32:02.776: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 18 05:32:12.808: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 18 05:32:22.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-4505 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 05:32:22.965: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 18 05:32:22.965: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 18 05:32:22.965: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 18 05:32:32.981: INFO: Waiting for StatefulSet statefulset-4505/ss2 to complete update
May 18 05:32:32.981: INFO: Waiting for Pod statefulset-4505/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
May 18 05:32:32.981: INFO: Waiting for Pod statefulset-4505/ss2-1 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
May 18 05:32:32.981: INFO: Waiting for Pod statefulset-4505/ss2-2 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
May 18 05:32:42.989: INFO: Waiting for StatefulSet statefulset-4505/ss2 to complete update
May 18 05:32:42.989: INFO: Waiting for Pod statefulset-4505/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
May 18 05:32:52.991: INFO: Deleting all statefulset in ns statefulset-4505
May 18 05:32:52.994: INFO: Scaling statefulset ss2 to 0
May 18 05:33:13.016: INFO: Waiting for statefulset status.replicas updated to 0
May 18 05:33:13.019: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:33:13.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4505" for this suite.

• [SLOW TEST:121.324 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":337,"completed":177,"skipped":2812,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:33:13.047: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-4520
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4520
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4520
May 18 05:33:13.099: INFO: Found 0 stateful pods, waiting for 1
May 18 05:33:23.104: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 18 05:33:23.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-4520 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 18 05:33:23.277: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 18 05:33:23.277: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 18 05:33:23.277: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 18 05:33:23.280: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 18 05:33:33.285: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 18 05:33:33.285: INFO: Waiting for statefulset status.replicas updated to 0
May 18 05:33:33.296: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999996s
May 18 05:33:34.300: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997092647s
May 18 05:33:35.303: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992657101s
May 18 05:33:36.308: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989516448s
May 18 05:33:37.313: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984775204s
May 18 05:33:38.317: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.980092958s
May 18 05:33:39.322: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975327614s
May 18 05:33:40.328: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.97021897s
May 18 05:33:41.340: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.964055232s
May 18 05:33:42.346: INFO: Verifying statefulset ss doesn't scale past 1 for another 951.529822ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4520
May 18 05:33:43.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-4520 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 05:33:43.512: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 18 05:33:43.512: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 18 05:33:43.512: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 18 05:33:43.515: INFO: Found 1 stateful pods, waiting for 3
May 18 05:33:53.519: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 18 05:33:53.519: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 18 05:33:53.519: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 18 05:33:53.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-4520 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 18 05:33:53.680: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 18 05:33:53.680: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 18 05:33:53.680: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 18 05:33:53.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-4520 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 18 05:33:53.849: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 18 05:33:53.849: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 18 05:33:53.849: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 18 05:33:53.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-4520 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 18 05:33:54.018: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 18 05:33:54.018: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 18 05:33:54.018: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 18 05:33:54.018: INFO: Waiting for statefulset status.replicas updated to 0
May 18 05:33:54.021: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 18 05:34:04.035: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 18 05:34:04.035: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 18 05:34:04.035: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 18 05:34:04.054: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999995s
May 18 05:34:05.059: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988160387s
May 18 05:34:06.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983266743s
May 18 05:34:07.069: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977832501s
May 18 05:34:08.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972754658s
May 18 05:34:09.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968072812s
May 18 05:34:10.085: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962482871s
May 18 05:34:11.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957594327s
May 18 05:34:12.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952773383s
May 18 05:34:13.098: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.756439ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4520
May 18 05:34:14.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-4520 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 05:34:14.274: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 18 05:34:14.274: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 18 05:34:14.274: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 18 05:34:14.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-4520 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 05:34:14.449: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 18 05:34:14.449: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 18 05:34:14.449: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 18 05:34:14.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=statefulset-4520 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 18 05:34:14.586: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 18 05:34:14.586: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 18 05:34:14.586: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 18 05:34:14.586: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
May 18 05:34:34.600: INFO: Deleting all statefulset in ns statefulset-4520
May 18 05:34:34.602: INFO: Scaling statefulset ss to 0
May 18 05:34:34.609: INFO: Waiting for statefulset status.replicas updated to 0
May 18 05:34:34.611: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:34:34.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4520" for this suite.

• [SLOW TEST:81.580 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":337,"completed":178,"skipped":2843,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:34:34.627: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7372 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7372;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7372 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7372;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7372.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7372.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7372.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7372.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7372.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7372.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7372.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7372.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7372.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7372.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7372.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 225.95.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.95.225_udp@PTR;check="$$(dig +tcp +noall +answer +search 225.95.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.95.225_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7372 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7372;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7372 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7372;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7372.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7372.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7372.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7372.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7372.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7372.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7372.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7372.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7372.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7372.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7372.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7372.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 225.95.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.95.225_udp@PTR;check="$$(dig +tcp +noall +answer +search 225.95.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.95.225_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 18 05:34:38.711: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.716: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.720: INFO: Unable to read wheezy_udp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.724: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.728: INFO: Unable to read wheezy_udp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.733: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.737: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.741: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.775: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.780: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.784: INFO: Unable to read jessie_udp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.789: INFO: Unable to read jessie_tcp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.795: INFO: Unable to read jessie_udp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.800: INFO: Unable to read jessie_tcp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.804: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.809: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:38.836: INFO: Lookups using dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7372 wheezy_tcp@dns-test-service.dns-7372 wheezy_udp@dns-test-service.dns-7372.svc wheezy_tcp@dns-test-service.dns-7372.svc wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7372 jessie_tcp@dns-test-service.dns-7372 jessie_udp@dns-test-service.dns-7372.svc jessie_tcp@dns-test-service.dns-7372.svc jessie_udp@_http._tcp.dns-test-service.dns-7372.svc jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc]

May 18 05:34:43.840: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.842: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.844: INFO: Unable to read wheezy_udp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.847: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.850: INFO: Unable to read wheezy_udp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.853: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.855: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.858: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.877: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.879: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.883: INFO: Unable to read jessie_udp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.886: INFO: Unable to read jessie_tcp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.889: INFO: Unable to read jessie_udp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.891: INFO: Unable to read jessie_tcp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.893: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.896: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:43.916: INFO: Lookups using dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7372 wheezy_tcp@dns-test-service.dns-7372 wheezy_udp@dns-test-service.dns-7372.svc wheezy_tcp@dns-test-service.dns-7372.svc wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7372 jessie_tcp@dns-test-service.dns-7372 jessie_udp@dns-test-service.dns-7372.svc jessie_tcp@dns-test-service.dns-7372.svc jessie_udp@_http._tcp.dns-test-service.dns-7372.svc jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc]

May 18 05:34:48.849: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.852: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.854: INFO: Unable to read wheezy_udp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.857: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.860: INFO: Unable to read wheezy_udp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.862: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.865: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.867: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.884: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.887: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.891: INFO: Unable to read jessie_udp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.894: INFO: Unable to read jessie_tcp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.897: INFO: Unable to read jessie_udp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.899: INFO: Unable to read jessie_tcp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.903: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.906: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:48.923: INFO: Lookups using dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7372 wheezy_tcp@dns-test-service.dns-7372 wheezy_udp@dns-test-service.dns-7372.svc wheezy_tcp@dns-test-service.dns-7372.svc wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7372 jessie_tcp@dns-test-service.dns-7372 jessie_udp@dns-test-service.dns-7372.svc jessie_tcp@dns-test-service.dns-7372.svc jessie_udp@_http._tcp.dns-test-service.dns-7372.svc jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc]

May 18 05:34:53.840: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.842: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.848: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.850: INFO: Unable to read wheezy_udp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.853: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.855: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.858: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.876: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.879: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.882: INFO: Unable to read jessie_udp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.884: INFO: Unable to read jessie_tcp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.887: INFO: Unable to read jessie_udp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.889: INFO: Unable to read jessie_tcp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.892: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.894: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:53.910: INFO: Lookups using dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7372 wheezy_tcp@dns-test-service.dns-7372 wheezy_udp@dns-test-service.dns-7372.svc wheezy_tcp@dns-test-service.dns-7372.svc wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7372 jessie_tcp@dns-test-service.dns-7372 jessie_udp@dns-test-service.dns-7372.svc jessie_tcp@dns-test-service.dns-7372.svc jessie_udp@_http._tcp.dns-test-service.dns-7372.svc jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc]

May 18 05:34:58.839: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.842: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.848: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.850: INFO: Unable to read wheezy_udp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.853: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.855: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.857: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.875: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.878: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.880: INFO: Unable to read jessie_udp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.883: INFO: Unable to read jessie_tcp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.886: INFO: Unable to read jessie_udp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.888: INFO: Unable to read jessie_tcp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.892: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.894: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:34:58.911: INFO: Lookups using dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7372 wheezy_tcp@dns-test-service.dns-7372 wheezy_udp@dns-test-service.dns-7372.svc wheezy_tcp@dns-test-service.dns-7372.svc wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7372 jessie_tcp@dns-test-service.dns-7372 jessie_udp@dns-test-service.dns-7372.svc jessie_tcp@dns-test-service.dns-7372.svc jessie_udp@_http._tcp.dns-test-service.dns-7372.svc jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc]

May 18 05:35:03.839: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.842: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.844: INFO: Unable to read wheezy_udp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.847: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.850: INFO: Unable to read wheezy_udp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.853: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.855: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.860: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.877: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.879: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.882: INFO: Unable to read jessie_udp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.884: INFO: Unable to read jessie_tcp@dns-test-service.dns-7372 from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.887: INFO: Unable to read jessie_udp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.889: INFO: Unable to read jessie_tcp@dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.892: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.894: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc from pod dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79: the server could not find the requested resource (get pods dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79)
May 18 05:35:03.910: INFO: Lookups using dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7372 wheezy_tcp@dns-test-service.dns-7372 wheezy_udp@dns-test-service.dns-7372.svc wheezy_tcp@dns-test-service.dns-7372.svc wheezy_udp@_http._tcp.dns-test-service.dns-7372.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7372.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7372 jessie_tcp@dns-test-service.dns-7372 jessie_udp@dns-test-service.dns-7372.svc jessie_tcp@dns-test-service.dns-7372.svc jessie_udp@_http._tcp.dns-test-service.dns-7372.svc jessie_tcp@_http._tcp.dns-test-service.dns-7372.svc]

May 18 05:35:08.913: INFO: DNS probes using dns-7372/dns-test-506a3cf9-1a87-4ffe-bf41-1bdbbc771a79 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:35:09.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7372" for this suite.

• [SLOW TEST:34.395 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":337,"completed":179,"skipped":2847,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:35:09.022: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 18 05:35:09.063: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:35:12.438: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:35:27.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7662" for this suite.

• [SLOW TEST:18.145 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":337,"completed":180,"skipped":2863,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:35:27.168: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:35:31.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6396" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":337,"completed":181,"skipped":2882,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:35:31.235: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-c4395573-8241-46e9-bc68-1d516c90ce74 in namespace container-probe-4177
May 18 05:35:33.283: INFO: Started pod busybox-c4395573-8241-46e9-bc68-1d516c90ce74 in namespace container-probe-4177
STEP: checking the pod's current state and verifying that restartCount is present
May 18 05:35:33.285: INFO: Initial restart count of pod busybox-c4395573-8241-46e9-bc68-1d516c90ce74 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:39:34.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4177" for this suite.

• [SLOW TEST:243.043 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":337,"completed":182,"skipped":2911,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:39:34.278: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:39:45.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7110" for this suite.

• [SLOW TEST:11.087 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":337,"completed":183,"skipped":2919,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:39:45.366: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:39:53.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5750" for this suite.

• [SLOW TEST:8.055 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":337,"completed":184,"skipped":2969,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:39:53.421: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:39:53.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1703" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":337,"completed":185,"skipped":2977,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:39:53.475: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:39:53.510: INFO: Creating deployment "webserver-deployment"
May 18 05:39:53.514: INFO: Waiting for observed generation 1
May 18 05:39:55.529: INFO: Waiting for all required pods to come up
May 18 05:39:55.570: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 18 05:39:59.619: INFO: Waiting for deployment "webserver-deployment" to complete
May 18 05:39:59.623: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 18 05:39:59.628: INFO: Updating deployment webserver-deployment
May 18 05:39:59.628: INFO: Waiting for observed generation 2
May 18 05:40:01.638: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 18 05:40:01.640: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 18 05:40:01.642: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 18 05:40:01.649: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 18 05:40:01.649: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 18 05:40:01.651: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 18 05:40:01.655: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 18 05:40:01.655: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 18 05:40:01.661: INFO: Updating deployment webserver-deployment
May 18 05:40:01.661: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 18 05:40:01.677: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 18 05:40:01.689: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 18 05:40:01.741: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8346  58b34341-0dfb-4a46-9e20-74f22b287bfb 36625 3 2021-05-18 05:39:53 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-18 05:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-18 05:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00172e128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-05-18 05:39:59 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-18 05:40:01 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 18 05:40:01.773: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-8346  622dd357-5da4-48ff-afc0-3bf3665691d7 36617 3 2021-05-18 05:39:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 58b34341-0dfb-4a46-9e20-74f22b287bfb 0xc003646c47 0xc003646c48}] []  [{kube-controller-manager Update apps/v1 2021-05-18 05:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58b34341-0dfb-4a46-9e20-74f22b287bfb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003646cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 18 05:40:01.773: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 18 05:40:01.773: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-8346  645a8afc-b703-49f3-879f-44e731aff9de 36614 3 2021-05-18 05:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 58b34341-0dfb-4a46-9e20-74f22b287bfb 0xc003646d27 0xc003646d28}] []  [{kube-controller-manager Update apps/v1 2021-05-18 05:39:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58b34341-0dfb-4a46-9e20-74f22b287bfb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003646d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 18 05:40:01.803: INFO: Pod "webserver-deployment-795d758f88-57jbx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-57jbx webserver-deployment-795d758f88- deployment-8346  5011eb38-09c2-4dde-bc76-f2dfba71fb4a 36611 0 2021-05-18 05:39:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.104.51/32 cni.projectcalico.org/podIPs:172.30.104.51/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc003647767 0xc003647768}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-18 05:39:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjv9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjv9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2021-05-18 05:39:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.803: INFO: Pod "webserver-deployment-795d758f88-5qs9b" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5qs9b webserver-deployment-795d758f88- deployment-8346  2ada9351-c4b8-4c5b-bee8-3badefde0942 36638 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc0036479f7 0xc0036479f8}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7xgwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7xgwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.803: INFO: Pod "webserver-deployment-795d758f88-78xkv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-78xkv webserver-deployment-795d758f88- deployment-8346  8f3aad83-2057-4582-b2ba-d4c1bfa915ed 36655 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc003647c00 0xc003647c01}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2g6b2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2g6b2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.803: INFO: Pod "webserver-deployment-795d758f88-9zqgz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9zqgz webserver-deployment-795d758f88- deployment-8346  e035288d-804a-48a8-9f54-2370da42ef3d 36647 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc003647d90 0xc003647d91}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hr7f2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hr7f2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.804: INFO: Pod "webserver-deployment-795d758f88-b26n9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-b26n9 webserver-deployment-795d758f88- deployment-8346  ed3bf218-ce7b-4e58-b96d-037808338c85 36668 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc003647f10 0xc003647f11}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bkdxp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bkdxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.804: INFO: Pod "webserver-deployment-795d758f88-bq7fb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bq7fb webserver-deployment-795d758f88- deployment-8346  cdd4cdba-8e45-49e8-ac8e-440304632b52 36597 0 2021-05-18 05:39:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.104.48/32 cni.projectcalico.org/podIPs:172.30.104.48/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc0036e0190 0xc0036e0191}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-18 05:39:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4m55r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4m55r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2021-05-18 05:39:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.804: INFO: Pod "webserver-deployment-795d758f88-bxndd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bxndd webserver-deployment-795d758f88- deployment-8346  0e4fcd7c-b9db-4b02-9a5c-e197f94a25b0 36592 0 2021-05-18 05:39:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.166.139/32 cni.projectcalico.org/podIPs:172.30.166.139/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc0036e0397 0xc0036e0398}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-18 05:39:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-18 05:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qmw9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qmw9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2021-05-18 05:39:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.804: INFO: Pod "webserver-deployment-795d758f88-kllbb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kllbb webserver-deployment-795d758f88- deployment-8346  9c874db5-5a74-4fdb-b5c3-0959b911e3cc 36601 0 2021-05-18 05:39:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.104.50/32 cni.projectcalico.org/podIPs:172.30.104.50/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc0036e0aa7 0xc0036e0aa8}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-18 05:39:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-66xtp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-66xtp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2021-05-18 05:39:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.805: INFO: Pod "webserver-deployment-795d758f88-lfjxj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lfjxj webserver-deployment-795d758f88- deployment-8346  ed23157e-aed8-4876-9790-a0264ab2814a 36646 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc0036e1337 0xc0036e1338}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bf5ps,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bf5ps,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.807: INFO: Pod "webserver-deployment-795d758f88-r9ts6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-r9ts6 webserver-deployment-795d758f88- deployment-8346  b7a40da6-290a-491e-9325-f25745c05b52 36604 0 2021-05-18 05:39:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.104.49/32 cni.projectcalico.org/podIPs:172.30.104.49/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc0036e1620 0xc0036e1621}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-18 05:39:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p4dzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p4dzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2021-05-18 05:39:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.807: INFO: Pod "webserver-deployment-795d758f88-tljp6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tljp6 webserver-deployment-795d758f88- deployment-8346  65f4dab5-710b-4fc7-90a5-795532edc2a8 36642 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc0036e1837 0xc0036e1838}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5v9q6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5v9q6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.809: INFO: Pod "webserver-deployment-795d758f88-v9jfw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-v9jfw webserver-deployment-795d758f88- deployment-8346  fb81312d-de01-481d-868a-c7d15120e4b3 36649 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc0036e19b0 0xc0036e19b1}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8cb97,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8cb97,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.809: INFO: Pod "webserver-deployment-795d758f88-wthnn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wthnn webserver-deployment-795d758f88- deployment-8346  c2377cb4-b833-4e00-877e-dcae4be54378 36650 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 622dd357-5da4-48ff-afc0-3bf3665691d7 0xc0036e1b20 0xc0036e1b21}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"622dd357-5da4-48ff-afc0-3bf3665691d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqrr5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqrr5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:,StartTime:2021-05-18 05:40:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.809: INFO: Pod "webserver-deployment-847dcfb7fb-5ww28" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-5ww28 webserver-deployment-847dcfb7fb- deployment-8346  80374175-a939-45e7-b08a-02e36677a96f 36666 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc0036e1ea7 0xc0036e1ea8}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfsz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfsz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2021-05-18 05:40:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.827: INFO: Pod "webserver-deployment-847dcfb7fb-7xwfq" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-7xwfq webserver-deployment-847dcfb7fb- deployment-8346  3d264375-9430-4972-b64b-51b72eabf1b4 36488 0 2021-05-18 05:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.30.104.47/32 cni.projectcalico.org/podIPs:172.30.104.47/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337e0b7 0xc00337e0b8}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 05:39:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 05:39:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.104.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7fn45,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7fn45,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.104.47,StartTime:2021-05-18 05:39:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:39:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://a7127955ebaad529f61251a27eccdc83cef37b81c021bc4b45ed15ecb043981e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.104.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.827: INFO: Pod "webserver-deployment-847dcfb7fb-9zjr9" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-9zjr9 webserver-deployment-847dcfb7fb- deployment-8346  b023a183-4699-4d21-b038-e3c5ab6d9530 36659 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337e2d7 0xc00337e2d8}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z7nlx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z7nlx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.827: INFO: Pod "webserver-deployment-847dcfb7fb-cz2pf" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-cz2pf webserver-deployment-847dcfb7fb- deployment-8346  fc927982-1047-49d2-b649-1dd369de44ec 36446 0 2021-05-18 05:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.30.166.137/32 cni.projectcalico.org/podIPs:172.30.166.137/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337e417 0xc00337e418}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 05:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 05:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.166.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-942xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-942xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.166.137,StartTime:2021-05-18 05:39:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://d8bdc773fdf6bcf29ccc1f383608891b3001f8d645f63d7130ade01b04c60dfb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.166.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.827: INFO: Pod "webserver-deployment-847dcfb7fb-gzxx6" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-gzxx6 webserver-deployment-847dcfb7fb- deployment-8346  a872a381-1cca-45b5-a55c-a8e0072422f7 36657 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337e617 0xc00337e618}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gh5p4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gh5p4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.828: INFO: Pod "webserver-deployment-847dcfb7fb-h8fbg" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-h8fbg webserver-deployment-847dcfb7fb- deployment-8346  bb09984e-8743-4b14-8c2b-056513a0920e 36636 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337e780 0xc00337e781}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k2s2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k2s2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:,StartTime:2021-05-18 05:40:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.828: INFO: Pod "webserver-deployment-847dcfb7fb-hdzp8" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hdzp8 webserver-deployment-847dcfb7fb- deployment-8346  7ed2145e-72a3-4e32-9637-61915605a88e 36644 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337e947 0xc00337e948}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jsmjf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jsmjf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.828: INFO: Pod "webserver-deployment-847dcfb7fb-hf89f" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hf89f webserver-deployment-847dcfb7fb- deployment-8346  e28d7473-b752-4495-bac9-6977c68880ec 36656 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337eab0 0xc00337eab1}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkjss,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkjss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.828: INFO: Pod "webserver-deployment-847dcfb7fb-hj6gh" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hj6gh webserver-deployment-847dcfb7fb- deployment-8346  082598a8-712a-4484-9d3b-67b0147aa8b4 36664 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337ec10 0xc00337ec11}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pqzrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pqzrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.828: INFO: Pod "webserver-deployment-847dcfb7fb-jsrkb" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-jsrkb webserver-deployment-847dcfb7fb- deployment-8346  cee52e4a-af42-4e6d-b7ab-41f506f584a6 36665 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337ed80 0xc00337ed81}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vww2b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vww2b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.828: INFO: Pod "webserver-deployment-847dcfb7fb-k4kg7" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-k4kg7 webserver-deployment-847dcfb7fb- deployment-8346  3441d7c4-8512-4dae-af15-5dc18e43059b 36473 0 2021-05-18 05:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.30.166.141/32 cni.projectcalico.org/podIPs:172.30.166.141/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337eee0 0xc00337eee1}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 05:39:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 05:39:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.166.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hf6ts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf6ts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.166.141,StartTime:2021-05-18 05:39:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:39:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://cd088312c6c7581649ab9e6ba4589aed251b74c07897d8976b233998f6b72f52,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.166.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.829: INFO: Pod "webserver-deployment-847dcfb7fb-kxzbv" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-kxzbv webserver-deployment-847dcfb7fb- deployment-8346  e1b505c4-aeff-4d55-a309-280b43bc035f 36637 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337f0d7 0xc00337f0d8}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jqw9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jqw9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.833: INFO: Pod "webserver-deployment-847dcfb7fb-l5876" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-l5876 webserver-deployment-847dcfb7fb- deployment-8346  a95eefc7-c75e-47ae-9c42-f48d6db2a5c3 36660 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337f240 0xc00337f241}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9285h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9285h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.833: INFO: Pod "webserver-deployment-847dcfb7fb-m92kb" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-m92kb webserver-deployment-847dcfb7fb- deployment-8346  f79c097e-ca97-4f37-85c1-49890aacb56f 36658 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337f387 0xc00337f388}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5r6q2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5r6q2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.833: INFO: Pod "webserver-deployment-847dcfb7fb-n96cp" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-n96cp webserver-deployment-847dcfb7fb- deployment-8346  da87a275-15ae-44a6-b96d-61181bd95887 36479 0 2021-05-18 05:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.30.166.138/32 cni.projectcalico.org/podIPs:172.30.166.138/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337f4f0 0xc00337f4f1}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 05:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 05:39:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.166.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z97ts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z97ts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.166.138,StartTime:2021-05-18 05:39:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:39:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://772d47f89ceeb7f419e0e92a4313cd9f624a891414a451bdc01dd7d4c06911e1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.166.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.833: INFO: Pod "webserver-deployment-847dcfb7fb-q465v" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-q465v webserver-deployment-847dcfb7fb- deployment-8346  1f255110-d1b2-41ef-a962-4f7215451707 36452 0 2021-05-18 05:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.30.104.43/32 cni.projectcalico.org/podIPs:172.30.104.43/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337f707 0xc00337f708}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 05:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 05:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.104.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8gs8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8gs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.104.43,StartTime:2021-05-18 05:39:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://0c8b4936d6d0abab10f298c7f857ca113d4920d0fbf233e659041e39071fc21e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.104.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.834: INFO: Pod "webserver-deployment-847dcfb7fb-r9hwr" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-r9hwr webserver-deployment-847dcfb7fb- deployment-8346  9f09cf7a-374e-4ad5-b272-4593570ea1b0 36476 0 2021-05-18 05:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.30.166.142/32 cni.projectcalico.org/podIPs:172.30.166.142/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337f917 0xc00337f918}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 05:39:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 05:39:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.166.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hmn9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hmn9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.166.142,StartTime:2021-05-18 05:39:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:39:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://be36d553119ca4a3d1880a39ac571126a09cb5086bddf7aca9229a2e6503e581,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.166.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.834: INFO: Pod "webserver-deployment-847dcfb7fb-vhc5x" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-vhc5x webserver-deployment-847dcfb7fb- deployment-8346  c9e56198-06a5-4916-95e1-cf828bcfd04d 36427 0 2021-05-18 05:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.30.166.136/32 cni.projectcalico.org/podIPs:172.30.166.136/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337fb17 0xc00337fb18}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 05:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 05:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.166.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67s4k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67s4k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.166.136,StartTime:2021-05-18 05:39:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://2271c9fd0463eb833663fdb683fe676b33cfba048b63ed9304ddcfe47cfee5a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.166.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.834: INFO: Pod "webserver-deployment-847dcfb7fb-z4687" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-z4687 webserver-deployment-847dcfb7fb- deployment-8346  05cd9fde-9d5d-4080-a0bc-0905250e9404 36662 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337fd77 0xc00337fd78}] []  [{kube-controller-manager Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85xkg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85xkg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 18 05:40:01.834: INFO: Pod "webserver-deployment-847dcfb7fb-z48kh" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-z48kh webserver-deployment-847dcfb7fb- deployment-8346  8699aa00-1efa-4c1b-9ee3-15d4321ef93b 36467 0 2021-05-18 05:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.30.104.45/32 cni.projectcalico.org/podIPs:172.30.104.45/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 645a8afc-b703-49f3-879f-44e731aff9de 0xc00337ff07 0xc00337ff08}] []  [{kube-controller-manager Update v1 2021-05-18 05:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"645a8afc-b703-49f3-879f-44e731aff9de\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 05:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 05:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.104.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vnp2k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vnp2k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:39:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.13,PodIP:172.30.104.45,StartTime:2021-05-18 05:39:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:39:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://537acdde5bc4f5de021159c5b776d378fa7590eaa1f62fd8d206e612b4ab28c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.104.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:40:01.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8346" for this suite.

• [SLOW TEST:8.393 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":337,"completed":186,"skipped":2983,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:40:01.869: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
May 18 05:40:01.940: INFO: Found Service test-service-5x5mh in namespace services-7862 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May 18 05:40:01.940: INFO: Service test-service-5x5mh created
STEP: Getting /status
May 18 05:40:01.943: INFO: Service test-service-5x5mh has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
May 18 05:40:01.953: INFO: observed Service test-service-5x5mh in namespace services-7862 with annotations: map[] & LoadBalancer: {[]}
May 18 05:40:01.953: INFO: Found Service test-service-5x5mh in namespace services-7862 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May 18 05:40:01.953: INFO: Service test-service-5x5mh has service status patched
STEP: updating the ServiceStatus
May 18 05:40:01.967: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
May 18 05:40:01.973: INFO: Observed Service test-service-5x5mh in namespace services-7862 with annotations: map[] & Conditions: {[]}
May 18 05:40:01.973: INFO: Observed event: &Service{ObjectMeta:{test-service-5x5mh  services-7862  f7b4377c-1c12-4b66-b928-60d13e64bc60 36701 0 2021-05-18 05:40:01 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2021-05-18 05:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}},"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}}}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.24.68.72,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,TopologyKeys:[],IPFamilyPolicy:*SingleStack,ClusterIPs:[172.24.68.72],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:nil,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May 18 05:40:01.973: INFO: Found Service test-service-5x5mh in namespace services-7862 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 18 05:40:01.973: INFO: Service test-service-5x5mh has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
May 18 05:40:01.994: INFO: observed Service test-service-5x5mh in namespace services-7862 with labels: map[test-service-static:true]
May 18 05:40:01.994: INFO: observed Service test-service-5x5mh in namespace services-7862 with labels: map[test-service-static:true]
May 18 05:40:01.994: INFO: observed Service test-service-5x5mh in namespace services-7862 with labels: map[test-service-static:true]
May 18 05:40:01.994: INFO: Found Service test-service-5x5mh in namespace services-7862 with labels: map[test-service:patched test-service-static:true]
May 18 05:40:01.994: INFO: Service test-service-5x5mh patched
STEP: deleting the service
STEP: watching for the Service to be deleted
May 18 05:40:02.010: INFO: Observed event: ADDED
May 18 05:40:02.010: INFO: Observed event: MODIFIED
May 18 05:40:02.010: INFO: Observed event: MODIFIED
May 18 05:40:02.010: INFO: Observed event: MODIFIED
May 18 05:40:02.010: INFO: Found Service test-service-5x5mh in namespace services-7862 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May 18 05:40:02.010: INFO: Service test-service-5x5mh deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:40:02.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7862" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":337,"completed":187,"skipped":3003,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:40:02.027: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:40:03.068: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 18 05:40:05.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 18 05:40:07.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 18 05:40:09.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 18 05:40:11.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913203, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:40:14.104: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:40:14.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7661" for this suite.
STEP: Destroying namespace "webhook-7661-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.252 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":337,"completed":188,"skipped":3004,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:40:14.282: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4874.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4874.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4874.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4874.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4874.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4874.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 18 05:40:18.527: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:18.530: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:18.533: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:18.535: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:18.543: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:18.546: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:18.548: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:18.551: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:18.556: INFO: Lookups using dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local]

May 18 05:40:23.561: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:23.564: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:23.566: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:23.569: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:23.577: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:23.579: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:23.582: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:23.585: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:23.590: INFO: Lookups using dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local]

May 18 05:40:28.560: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:28.563: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:28.566: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:28.568: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:28.576: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:28.579: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:28.582: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:28.585: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:28.590: INFO: Lookups using dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local]

May 18 05:40:33.564: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:33.570: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:33.582: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:33.589: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:33.631: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:33.633: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:33.636: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:33.639: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:33.644: INFO: Lookups using dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local]

May 18 05:40:38.559: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:38.563: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:38.565: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:38.568: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:38.576: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:38.579: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:38.581: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:38.584: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:38.589: INFO: Lookups using dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local]

May 18 05:40:43.559: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:43.562: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:43.564: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:43.567: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:43.574: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:43.577: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:43.580: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:43.582: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local from pod dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2: the server could not find the requested resource (get pods dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2)
May 18 05:40:43.587: INFO: Lookups using dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4874.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4874.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4874.svc.cluster.local jessie_udp@dns-test-service-2.dns-4874.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4874.svc.cluster.local]

May 18 05:40:48.593: INFO: DNS probes using dns-4874/dns-test-dec1552f-44f8-41f7-b3e7-5ab12cef8ad2 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:40:48.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4874" for this suite.

• [SLOW TEST:34.370 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":337,"completed":189,"skipped":3057,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:40:48.652: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7888
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7888
I0518 05:40:48.734140      19 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7888, replica count: 2
May 18 05:40:51.785: INFO: Creating new exec pod
I0518 05:40:51.785588      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 18 05:40:54.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-7888 exec execpodsznwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 18 05:40:54.956: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 18 05:40:54.956: INFO: stdout: ""
May 18 05:40:55.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-7888 exec execpodsznwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 18 05:40:56.158: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 18 05:40:56.158: INFO: stdout: "externalname-service-kv8d6"
May 18 05:40:56.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-7888 exec execpodsznwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.24.31.139 80'
May 18 05:40:56.403: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.24.31.139 80\nConnection to 172.24.31.139 80 port [tcp/http] succeeded!\n"
May 18 05:40:56.403: INFO: stdout: ""
May 18 05:40:57.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-7888 exec execpodsznwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.24.31.139 80'
May 18 05:40:57.541: INFO: stderr: "+ nc -v -t -w 2 172.24.31.139 80\n+ echo hostName\nConnection to 172.24.31.139 80 port [tcp/http] succeeded!\n"
May 18 05:40:57.541: INFO: stdout: "externalname-service-kv8d6"
May 18 05:40:57.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-7888 exec execpodsznwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 32528'
May 18 05:40:57.687: INFO: stderr: "+ nc -v -t -w 2 172.28.128.12 32528\n+ echo hostName\nConnection to 172.28.128.12 32528 port [tcp/*] succeeded!\n"
May 18 05:40:57.687: INFO: stdout: ""
May 18 05:40:58.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-7888 exec execpodsznwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 32528'
May 18 05:40:58.820: INFO: stderr: "+ nc -v -t -w 2 172.28.128.12 32528\n+ echo hostName\nConnection to 172.28.128.12 32528 port [tcp/*] succeeded!\n"
May 18 05:40:58.820: INFO: stdout: ""
May 18 05:40:59.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-7888 exec execpodsznwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 32528'
May 18 05:40:59.839: INFO: stderr: "+ nc -v -t -w 2 172.28.128.12 32528\n+ echo hostName\nConnection to 172.28.128.12 32528 port [tcp/*] succeeded!\n"
May 18 05:40:59.839: INFO: stdout: "externalname-service-drkk2"
May 18 05:40:59.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-7888 exec execpodsznwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.13 32528'
May 18 05:40:59.997: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.13 32528\nConnection to 172.28.128.13 32528 port [tcp/*] succeeded!\n"
May 18 05:40:59.997: INFO: stdout: ""
May 18 05:41:00.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-7888 exec execpodsznwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.13 32528'
May 18 05:41:01.134: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.13 32528\nConnection to 172.28.128.13 32528 port [tcp/*] succeeded!\n"
May 18 05:41:01.134: INFO: stdout: "externalname-service-drkk2"
May 18 05:41:01.134: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:41:01.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7888" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:12.527 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":337,"completed":190,"skipped":3058,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:41:01.179: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0518 05:41:11.321779      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 18 05:42:13.346: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 18 05:42:13.346: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wwks" in namespace "gc-7922"
May 18 05:42:13.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-fr7rj" in namespace "gc-7922"
May 18 05:42:13.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-hns56" in namespace "gc-7922"
May 18 05:42:13.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-hsdgm" in namespace "gc-7922"
May 18 05:42:13.430: INFO: Deleting pod "simpletest-rc-to-be-deleted-jw5f7" in namespace "gc-7922"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:42:13.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7922" for this suite.

• [SLOW TEST:72.288 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":337,"completed":191,"skipped":3074,"failed":0}
SSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:42:13.467: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 18 05:42:13.563: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 18 05:42:13.572: INFO: starting watch
STEP: patching
STEP: updating
May 18 05:42:13.588: INFO: waiting for watch events with expected annotations
May 18 05:42:13.588: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:42:13.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6918" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":337,"completed":192,"skipped":3081,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:42:13.649: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3665
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3665
STEP: creating replication controller externalsvc in namespace services-3665
I0518 05:42:13.721548      19 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3665, replica count: 2
I0518 05:42:16.772446      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 18 05:42:16.792: INFO: Creating new exec pod
May 18 05:42:18.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-3665 exec execpodt62xp -- /bin/sh -x -c nslookup clusterip-service.services-3665.svc.cluster.local'
May 18 05:42:19.310: INFO: stderr: "+ nslookup clusterip-service.services-3665.svc.cluster.local\n"
May 18 05:42:19.310: INFO: stdout: "Server:\t\t172.24.0.10\nAddress:\t172.24.0.10#53\n\nclusterip-service.services-3665.svc.cluster.local\tcanonical name = externalsvc.services-3665.svc.cluster.local.\nName:\texternalsvc.services-3665.svc.cluster.local\nAddress: 172.24.74.128\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3665, will wait for the garbage collector to delete the pods
May 18 05:42:19.366: INFO: Deleting ReplicationController externalsvc took: 2.957587ms
May 18 05:42:19.468: INFO: Terminating ReplicationController externalsvc pods took: 101.070742ms
May 18 05:42:34.600: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:42:34.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3665" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:20.995 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":337,"completed":193,"skipped":3088,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:42:34.645: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-7eefaf3d-af1d-444a-8e68-1f50f53fd207
STEP: Creating a pod to test consume secrets
May 18 05:42:34.702: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cf33d8d3-ca5e-41d0-8571-83755e50fff4" in namespace "projected-5499" to be "Succeeded or Failed"
May 18 05:42:34.710: INFO: Pod "pod-projected-secrets-cf33d8d3-ca5e-41d0-8571-83755e50fff4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.639065ms
May 18 05:42:36.713: INFO: Pod "pod-projected-secrets-cf33d8d3-ca5e-41d0-8571-83755e50fff4": Phase="Running", Reason="", readiness=true. Elapsed: 2.01150988s
May 18 05:42:38.723: INFO: Pod "pod-projected-secrets-cf33d8d3-ca5e-41d0-8571-83755e50fff4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02091347s
STEP: Saw pod success
May 18 05:42:38.723: INFO: Pod "pod-projected-secrets-cf33d8d3-ca5e-41d0-8571-83755e50fff4" satisfied condition "Succeeded or Failed"
May 18 05:42:38.726: INFO: Trying to get logs from node node2 pod pod-projected-secrets-cf33d8d3-ca5e-41d0-8571-83755e50fff4 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 18 05:42:38.763: INFO: Waiting for pod pod-projected-secrets-cf33d8d3-ca5e-41d0-8571-83755e50fff4 to disappear
May 18 05:42:38.767: INFO: Pod pod-projected-secrets-cf33d8d3-ca5e-41d0-8571-83755e50fff4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:42:38.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5499" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":337,"completed":194,"skipped":3114,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:42:38.778: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 18 05:42:38.829: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 18 05:42:38.839: INFO: Waiting for terminating namespaces to be deleted...
May 18 05:42:38.843: INFO: 
Logging pods the apiserver thinks is on node node1 before test
May 18 05:42:38.850: INFO: calico-node-bzct9 from kube-system started at 2021-05-18 02:09:57 +0000 UTC (1 container statuses recorded)
May 18 05:42:38.850: INFO: 	Container calico-node ready: true, restart count 0
May 18 05:42:38.850: INFO: kube-proxy-vxmpl from kube-system started at 2021-05-18 02:09:57 +0000 UTC (1 container statuses recorded)
May 18 05:42:38.851: INFO: 	Container kube-proxy ready: true, restart count 0
May 18 05:42:38.851: INFO: sonobuoy-e2e-job-d53ed4c4301a4fe4 from sonobuoy started at 2021-05-18 04:30:03 +0000 UTC (2 container statuses recorded)
May 18 05:42:38.851: INFO: 	Container e2e ready: true, restart count 0
May 18 05:42:38.851: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 18 05:42:38.851: INFO: sonobuoy-systemd-logs-daemon-set-0e6496dadced4aae-5wgz6 from sonobuoy started at 2021-05-18 04:30:04 +0000 UTC (2 container statuses recorded)
May 18 05:42:38.851: INFO: 	Container sonobuoy-worker ready: false, restart count 7
May 18 05:42:38.851: INFO: 	Container systemd-logs ready: true, restart count 0
May 18 05:42:38.851: INFO: 
Logging pods the apiserver thinks is on node node2 before test
May 18 05:42:38.858: INFO: calico-node-ddzct from kube-system started at 2021-05-18 04:29:32 +0000 UTC (1 container statuses recorded)
May 18 05:42:38.858: INFO: 	Container calico-node ready: true, restart count 0
May 18 05:42:38.858: INFO: kube-proxy-d6j4t from kube-system started at 2021-05-18 02:10:08 +0000 UTC (1 container statuses recorded)
May 18 05:42:38.858: INFO: 	Container kube-proxy ready: true, restart count 0
May 18 05:42:38.858: INFO: execpodt62xp from services-3665 started at 2021-05-18 05:42:16 +0000 UTC (1 container statuses recorded)
May 18 05:42:38.858: INFO: 	Container agnhost-container ready: true, restart count 0
May 18 05:42:38.858: INFO: sonobuoy from sonobuoy started at 2021-05-18 04:30:02 +0000 UTC (1 container statuses recorded)
May 18 05:42:38.858: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 18 05:42:38.858: INFO: sonobuoy-systemd-logs-daemon-set-0e6496dadced4aae-7bgms from sonobuoy started at 2021-05-18 04:30:03 +0000 UTC (2 container statuses recorded)
May 18 05:42:38.858: INFO: 	Container sonobuoy-worker ready: false, restart count 7
May 18 05:42:38.858: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node node1
STEP: verifying the node has the label node node2
May 18 05:42:38.936: INFO: Pod calico-node-bzct9 requesting resource cpu=250m on Node node1
May 18 05:42:38.936: INFO: Pod calico-node-ddzct requesting resource cpu=250m on Node node2
May 18 05:42:38.936: INFO: Pod kube-proxy-d6j4t requesting resource cpu=0m on Node node2
May 18 05:42:38.936: INFO: Pod kube-proxy-vxmpl requesting resource cpu=0m on Node node1
May 18 05:42:38.936: INFO: Pod execpodt62xp requesting resource cpu=0m on Node node2
May 18 05:42:38.936: INFO: Pod sonobuoy requesting resource cpu=0m on Node node2
May 18 05:42:38.936: INFO: Pod sonobuoy-e2e-job-d53ed4c4301a4fe4 requesting resource cpu=0m on Node node1
May 18 05:42:38.936: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e6496dadced4aae-5wgz6 requesting resource cpu=0m on Node node1
May 18 05:42:38.936: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e6496dadced4aae-7bgms requesting resource cpu=0m on Node node2
STEP: Starting Pods to consume most of the cluster CPU.
May 18 05:42:38.936: INFO: Creating a pod which consumes cpu=1176m on Node node1
May 18 05:42:38.946: INFO: Creating a pod which consumes cpu=1176m on Node node2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90717081-b5ef-46d6-abff-5b20280512ae.168012d1fbaebd73], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1594/filler-pod-90717081-b5ef-46d6-abff-5b20280512ae to node1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90717081-b5ef-46d6-abff-5b20280512ae.168012d25064bdfb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90717081-b5ef-46d6-abff-5b20280512ae.168012d2577560c6], Reason = [Created], Message = [Created container filler-pod-90717081-b5ef-46d6-abff-5b20280512ae]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90717081-b5ef-46d6-abff-5b20280512ae.168012d26b3f500c], Reason = [Started], Message = [Started container filler-pod-90717081-b5ef-46d6-abff-5b20280512ae]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2734f4e-b312-4e0a-ae8a-05d78425dbfd.168012d1fbae720f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1594/filler-pod-c2734f4e-b312-4e0a-ae8a-05d78425dbfd to node2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2734f4e-b312-4e0a-ae8a-05d78425dbfd.168012d25a6e155c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2734f4e-b312-4e0a-ae8a-05d78425dbfd.168012d26611f72a], Reason = [Created], Message = [Created container filler-pod-c2734f4e-b312-4e0a-ae8a-05d78425dbfd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2734f4e-b312-4e0a-ae8a-05d78425dbfd.168012d2796ddbd5], Reason = [Started], Message = [Started container filler-pod-c2734f4e-b312-4e0a-ae8a-05d78425dbfd]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168012d2eb99194d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node node1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node node2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:42:44.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1594" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.275 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":337,"completed":195,"skipped":3117,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:42:44.053: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-3241
STEP: creating service affinity-nodeport in namespace services-3241
STEP: creating replication controller affinity-nodeport in namespace services-3241
I0518 05:42:44.114863      19 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-3241, replica count: 3
I0518 05:42:47.167156      19 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 18 05:42:47.177: INFO: Creating new exec pod
May 18 05:42:50.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-3241 exec execpod-affinityj88b2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
May 18 05:42:50.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 18 05:42:50.364: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:42:50.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-3241 exec execpod-affinityj88b2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.24.108.230 80'
May 18 05:42:50.525: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.24.108.230 80\nConnection to 172.24.108.230 80 port [tcp/http] succeeded!\n"
May 18 05:42:50.525: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:42:50.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-3241 exec execpod-affinityj88b2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 30299'
May 18 05:42:50.680: INFO: stderr: "+ nc -v -t -w 2 172.28.128.12 30299\nConnection to 172.28.128.12 30299 port [tcp/*] succeeded!\n+ echo hostName\n"
May 18 05:42:50.680: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:42:50.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-3241 exec execpod-affinityj88b2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.13 30299'
May 18 05:42:50.821: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.13 30299\nConnection to 172.28.128.13 30299 port [tcp/*] succeeded!\n"
May 18 05:42:50.821: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:42:50.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-3241 exec execpod-affinityj88b2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.28.128.12:30299/ ; done'
May 18 05:42:51.030: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.28.128.12:30299/\n"
May 18 05:42:51.030: INFO: stdout: "\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj\naffinity-nodeport-xx4bj"
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Received response from host: affinity-nodeport-xx4bj
May 18 05:42:51.030: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3241, will wait for the garbage collector to delete the pods
May 18 05:42:51.118: INFO: Deleting ReplicationController affinity-nodeport took: 11.05975ms
May 18 05:42:51.220: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.36724ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:43:04.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3241" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:20.614 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":337,"completed":196,"skipped":3134,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:43:04.667: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
May 18 05:43:04.714: INFO: The status of Pod pod-hostip-4dea5d52-cc5d-4a14-afec-0c7e032816a2 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:43:06.720: INFO: The status of Pod pod-hostip-4dea5d52-cc5d-4a14-afec-0c7e032816a2 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:43:08.721: INFO: The status of Pod pod-hostip-4dea5d52-cc5d-4a14-afec-0c7e032816a2 is Running (Ready = true)
May 18 05:43:08.724: INFO: Pod pod-hostip-4dea5d52-cc5d-4a14-afec-0c7e032816a2 has hostIP: 172.28.128.13
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:43:08.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3261" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":337,"completed":197,"skipped":3143,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:43:08.734: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-3192
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3192 to expose endpoints map[]
May 18 05:43:08.785: INFO: successfully validated that service endpoint-test2 in namespace services-3192 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3192
May 18 05:43:08.794: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:43:10.799: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:43:12.798: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3192 to expose endpoints map[pod1:[80]]
May 18 05:43:12.809: INFO: successfully validated that service endpoint-test2 in namespace services-3192 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-3192
May 18 05:43:12.821: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:43:14.827: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:43:16.826: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3192 to expose endpoints map[pod1:[80] pod2:[80]]
May 18 05:43:16.837: INFO: successfully validated that service endpoint-test2 in namespace services-3192 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-3192
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3192 to expose endpoints map[pod2:[80]]
May 18 05:43:16.880: INFO: successfully validated that service endpoint-test2 in namespace services-3192 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-3192
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3192 to expose endpoints map[]
May 18 05:43:16.908: INFO: successfully validated that service endpoint-test2 in namespace services-3192 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:43:16.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3192" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:8.218 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":337,"completed":198,"skipped":3146,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:43:16.952: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:43:17.555: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:43:20.583: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:43:33.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1247" for this suite.
STEP: Destroying namespace "webhook-1247-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.836 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":337,"completed":199,"skipped":3154,"failed":0}
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:43:33.789: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
May 18 05:43:33.895: INFO: observed Pod pod-test in namespace pods-5335 in phase Pending with labels: map[test-pod-static:true] & conditions []
May 18 05:43:33.895: INFO: observed Pod pod-test in namespace pods-5335 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:33 +0000 UTC  }]
May 18 05:43:33.909: INFO: observed Pod pod-test in namespace pods-5335 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:33 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:33 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:33 +0000 UTC  }]
May 18 05:43:34.928: INFO: observed Pod pod-test in namespace pods-5335 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:33 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:33 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:33 +0000 UTC  }]
May 18 05:43:36.073: INFO: Found Pod pod-test in namespace pods-5335 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-18 05:43:33 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
May 18 05:43:36.091: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
May 18 05:43:36.131: INFO: observed event type ADDED
May 18 05:43:36.131: INFO: observed event type MODIFIED
May 18 05:43:36.131: INFO: observed event type MODIFIED
May 18 05:43:36.131: INFO: observed event type MODIFIED
May 18 05:43:36.131: INFO: observed event type MODIFIED
May 18 05:43:36.131: INFO: observed event type MODIFIED
May 18 05:43:36.131: INFO: observed event type MODIFIED
May 18 05:43:36.131: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:43:36.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5335" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":337,"completed":200,"skipped":3154,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:43:36.144: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:44:36.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3354" for this suite.

• [SLOW TEST:60.071 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":337,"completed":201,"skipped":3160,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:44:36.216: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
May 18 05:44:36.244: INFO: created test-podtemplate-1
May 18 05:44:36.249: INFO: created test-podtemplate-2
May 18 05:44:36.252: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 18 05:44:36.254: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 18 05:44:36.272: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:44:36.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6016" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":337,"completed":202,"skipped":3179,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:44:36.284: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:44:41.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3538" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":337,"completed":203,"skipped":3189,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:44:41.251: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 18 05:44:41.295: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:44:45.274: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:44:58.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4880" for this suite.

• [SLOW TEST:17.155 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":337,"completed":204,"skipped":3199,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:44:58.406: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 18 05:44:58.450: INFO: Waiting up to 5m0s for pod "pod-9519b92f-eab3-4bc4-84e1-1783a9f8c0c2" in namespace "emptydir-1875" to be "Succeeded or Failed"
May 18 05:44:58.455: INFO: Pod "pod-9519b92f-eab3-4bc4-84e1-1783a9f8c0c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.33118ms
May 18 05:45:00.460: INFO: Pod "pod-9519b92f-eab3-4bc4-84e1-1783a9f8c0c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00943209s
STEP: Saw pod success
May 18 05:45:00.460: INFO: Pod "pod-9519b92f-eab3-4bc4-84e1-1783a9f8c0c2" satisfied condition "Succeeded or Failed"
May 18 05:45:00.462: INFO: Trying to get logs from node node2 pod pod-9519b92f-eab3-4bc4-84e1-1783a9f8c0c2 container test-container: <nil>
STEP: delete the pod
May 18 05:45:00.488: INFO: Waiting for pod pod-9519b92f-eab3-4bc4-84e1-1783a9f8c0c2 to disappear
May 18 05:45:00.490: INFO: Pod pod-9519b92f-eab3-4bc4-84e1-1783a9f8c0c2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:45:00.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1875" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":205,"skipped":3209,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:45:00.496: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 18 05:45:00.525: INFO: Waiting up to 5m0s for pod "pod-8bc3d593-0184-46e8-aa71-9f2b3ada4c18" in namespace "emptydir-5799" to be "Succeeded or Failed"
May 18 05:45:00.529: INFO: Pod "pod-8bc3d593-0184-46e8-aa71-9f2b3ada4c18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.692178ms
May 18 05:45:02.536: INFO: Pod "pod-8bc3d593-0184-46e8-aa71-9f2b3ada4c18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010959083s
May 18 05:45:04.541: INFO: Pod "pod-8bc3d593-0184-46e8-aa71-9f2b3ada4c18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016406691s
STEP: Saw pod success
May 18 05:45:04.541: INFO: Pod "pod-8bc3d593-0184-46e8-aa71-9f2b3ada4c18" satisfied condition "Succeeded or Failed"
May 18 05:45:04.543: INFO: Trying to get logs from node node2 pod pod-8bc3d593-0184-46e8-aa71-9f2b3ada4c18 container test-container: <nil>
STEP: delete the pod
May 18 05:45:04.567: INFO: Waiting for pod pod-8bc3d593-0184-46e8-aa71-9f2b3ada4c18 to disappear
May 18 05:45:04.569: INFO: Pod pod-8bc3d593-0184-46e8-aa71-9f2b3ada4c18 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:45:04.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5799" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":206,"skipped":3210,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:45:04.576: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2873
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 18 05:45:04.600: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 18 05:45:04.627: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:45:06.632: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:45:08.632: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:45:10.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:45:12.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:45:14.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:45:16.632: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:45:18.643: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:45:20.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:45:22.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:45:24.648: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 18 05:45:24.652: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 18 05:45:26.690: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 18 05:45:26.690: INFO: Going to poll 172.30.166.157 on port 8081 at least 0 times, with a maximum of 34 tries before failing
May 18 05:45:26.692: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.166.157 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2873 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:45:26.692: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:45:27.755: INFO: Found all 1 expected endpoints: [netserver-0]
May 18 05:45:27.755: INFO: Going to poll 172.30.104.10 on port 8081 at least 0 times, with a maximum of 34 tries before failing
May 18 05:45:27.759: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.104.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2873 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:45:27.759: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:45:28.822: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:45:28.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2873" for this suite.

• [SLOW TEST:24.256 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":207,"skipped":3227,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:45:28.832: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
May 18 05:45:28.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 create -f -'
May 18 05:45:29.128: INFO: stderr: ""
May 18 05:45:29.128: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 18 05:45:29.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 18 05:45:29.219: INFO: stderr: ""
May 18 05:45:29.219: INFO: stdout: "update-demo-nautilus-8zn7s update-demo-nautilus-trccg "
May 18 05:45:29.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods update-demo-nautilus-8zn7s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 18 05:45:29.291: INFO: stderr: ""
May 18 05:45:29.291: INFO: stdout: ""
May 18 05:45:29.291: INFO: update-demo-nautilus-8zn7s is created but not running
May 18 05:45:34.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 18 05:45:34.489: INFO: stderr: ""
May 18 05:45:34.489: INFO: stdout: "update-demo-nautilus-8zn7s update-demo-nautilus-trccg "
May 18 05:45:34.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods update-demo-nautilus-8zn7s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 18 05:45:34.611: INFO: stderr: ""
May 18 05:45:34.611: INFO: stdout: "true"
May 18 05:45:34.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods update-demo-nautilus-8zn7s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 18 05:45:34.707: INFO: stderr: ""
May 18 05:45:34.707: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 18 05:45:34.707: INFO: validating pod update-demo-nautilus-8zn7s
May 18 05:45:34.711: INFO: got data: {
  "image": "nautilus.jpg"
}

May 18 05:45:34.711: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 18 05:45:34.711: INFO: update-demo-nautilus-8zn7s is verified up and running
May 18 05:45:34.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods update-demo-nautilus-trccg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 18 05:45:34.802: INFO: stderr: ""
May 18 05:45:34.802: INFO: stdout: "true"
May 18 05:45:34.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods update-demo-nautilus-trccg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 18 05:45:34.868: INFO: stderr: ""
May 18 05:45:34.868: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 18 05:45:34.868: INFO: validating pod update-demo-nautilus-trccg
May 18 05:45:34.871: INFO: got data: {
  "image": "nautilus.jpg"
}

May 18 05:45:34.871: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 18 05:45:34.871: INFO: update-demo-nautilus-trccg is verified up and running
STEP: scaling down the replication controller
May 18 05:45:34.873: INFO: scanned /root for discovery docs: <nil>
May 18 05:45:34.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 18 05:45:35.980: INFO: stderr: ""
May 18 05:45:35.980: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 18 05:45:35.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 18 05:45:36.058: INFO: stderr: ""
May 18 05:45:36.058: INFO: stdout: "update-demo-nautilus-8zn7s update-demo-nautilus-trccg "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 18 05:45:41.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 18 05:45:41.131: INFO: stderr: ""
May 18 05:45:41.131: INFO: stdout: "update-demo-nautilus-trccg "
May 18 05:45:41.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods update-demo-nautilus-trccg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 18 05:45:41.197: INFO: stderr: ""
May 18 05:45:41.197: INFO: stdout: "true"
May 18 05:45:41.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods update-demo-nautilus-trccg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 18 05:45:41.259: INFO: stderr: ""
May 18 05:45:41.259: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 18 05:45:41.259: INFO: validating pod update-demo-nautilus-trccg
May 18 05:45:41.262: INFO: got data: {
  "image": "nautilus.jpg"
}

May 18 05:45:41.262: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 18 05:45:41.262: INFO: update-demo-nautilus-trccg is verified up and running
STEP: scaling up the replication controller
May 18 05:45:41.263: INFO: scanned /root for discovery docs: <nil>
May 18 05:45:41.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 18 05:45:42.355: INFO: stderr: ""
May 18 05:45:42.355: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 18 05:45:42.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 18 05:45:42.472: INFO: stderr: ""
May 18 05:45:42.472: INFO: stdout: "update-demo-nautilus-2ks4t update-demo-nautilus-trccg "
May 18 05:45:42.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods update-demo-nautilus-2ks4t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 18 05:45:42.555: INFO: stderr: ""
May 18 05:45:42.555: INFO: stdout: ""
May 18 05:45:42.555: INFO: update-demo-nautilus-2ks4t is created but not running
May 18 05:45:47.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 18 05:45:47.625: INFO: stderr: ""
May 18 05:45:47.625: INFO: stdout: "update-demo-nautilus-2ks4t update-demo-nautilus-trccg "
May 18 05:45:47.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods update-demo-nautilus-2ks4t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 18 05:45:47.697: INFO: stderr: ""
May 18 05:45:47.697: INFO: stdout: "true"
May 18 05:45:47.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods update-demo-nautilus-2ks4t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 18 05:45:47.758: INFO: stderr: ""
May 18 05:45:47.758: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 18 05:45:47.758: INFO: validating pod update-demo-nautilus-2ks4t
May 18 05:45:47.762: INFO: got data: {
  "image": "nautilus.jpg"
}

May 18 05:45:47.762: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 18 05:45:47.762: INFO: update-demo-nautilus-2ks4t is verified up and running
May 18 05:45:47.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods update-demo-nautilus-trccg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 18 05:45:47.831: INFO: stderr: ""
May 18 05:45:47.831: INFO: stdout: "true"
May 18 05:45:47.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods update-demo-nautilus-trccg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 18 05:45:47.899: INFO: stderr: ""
May 18 05:45:47.899: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 18 05:45:47.899: INFO: validating pod update-demo-nautilus-trccg
May 18 05:45:47.901: INFO: got data: {
  "image": "nautilus.jpg"
}

May 18 05:45:47.901: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 18 05:45:47.901: INFO: update-demo-nautilus-trccg is verified up and running
STEP: using delete to clean up resources
May 18 05:45:47.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 delete --grace-period=0 --force -f -'
May 18 05:45:47.983: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 18 05:45:47.983: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 18 05:45:47.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get rc,svc -l name=update-demo --no-headers'
May 18 05:45:48.100: INFO: stderr: "No resources found in kubectl-8727 namespace.\n"
May 18 05:45:48.100: INFO: stdout: ""
May 18 05:45:48.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8727 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 18 05:45:48.207: INFO: stderr: ""
May 18 05:45:48.207: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:45:48.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8727" for this suite.

• [SLOW TEST:19.397 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":337,"completed":208,"skipped":3245,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:45:48.228: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:45:49.303: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:45:52.340: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:45:52.344: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-412-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:45:55.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9767" for this suite.
STEP: Destroying namespace "webhook-9767-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.341 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":337,"completed":209,"skipped":3258,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:45:55.569: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:46:11.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8039" for this suite.

• [SLOW TEST:16.214 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":337,"completed":210,"skipped":3259,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:46:11.784: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1281
STEP: creating service affinity-clusterip in namespace services-1281
STEP: creating replication controller affinity-clusterip in namespace services-1281
I0518 05:46:11.832195      19 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-1281, replica count: 3
I0518 05:46:14.883117      19 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 18 05:46:14.892: INFO: Creating new exec pod
May 18 05:46:17.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-1281 exec execpod-affinitynbz55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
May 18 05:46:18.073: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 18 05:46:18.073: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:46:18.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-1281 exec execpod-affinitynbz55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.24.4.245 80'
May 18 05:46:18.238: INFO: stderr: "+ nc -v -t -w 2 172.24.4.245 80\n+ echo hostName\nConnection to 172.24.4.245 80 port [tcp/http] succeeded!\n"
May 18 05:46:18.238: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 05:46:18.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-1281 exec execpod-affinitynbz55 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.4.245:80/ ; done'
May 18 05:46:18.474: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.4.245:80/\n"
May 18 05:46:18.474: INFO: stdout: "\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp\naffinity-clusterip-84ldp"
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Received response from host: affinity-clusterip-84ldp
May 18 05:46:18.474: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1281, will wait for the garbage collector to delete the pods
May 18 05:46:18.550: INFO: Deleting ReplicationController affinity-clusterip took: 3.846082ms
May 18 05:46:18.651: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.516944ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:46:34.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1281" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:22.919 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":337,"completed":211,"skipped":3327,"failed":0}
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:46:34.703: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 05:46:34.747: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aace20b8-a771-428d-95ef-e232f9d6ccea" in namespace "downward-api-8639" to be "Succeeded or Failed"
May 18 05:46:34.753: INFO: Pod "downwardapi-volume-aace20b8-a771-428d-95ef-e232f9d6ccea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.319671ms
May 18 05:46:36.761: INFO: Pod "downwardapi-volume-aace20b8-a771-428d-95ef-e232f9d6ccea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014463667s
STEP: Saw pod success
May 18 05:46:36.761: INFO: Pod "downwardapi-volume-aace20b8-a771-428d-95ef-e232f9d6ccea" satisfied condition "Succeeded or Failed"
May 18 05:46:36.765: INFO: Trying to get logs from node node2 pod downwardapi-volume-aace20b8-a771-428d-95ef-e232f9d6ccea container client-container: <nil>
STEP: delete the pod
May 18 05:46:36.809: INFO: Waiting for pod downwardapi-volume-aace20b8-a771-428d-95ef-e232f9d6ccea to disappear
May 18 05:46:36.812: INFO: Pod downwardapi-volume-aace20b8-a771-428d-95ef-e232f9d6ccea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:46:36.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8639" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":337,"completed":212,"skipped":3327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:46:36.824: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:46:37.655: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:46:40.681: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:46:50.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-716" for this suite.
STEP: Destroying namespace "webhook-716-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.027 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":337,"completed":213,"skipped":3367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:46:50.852: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-b898fa03-f801-4550-952e-31516e786482
STEP: Creating a pod to test consume configMaps
May 18 05:46:50.910: INFO: Waiting up to 5m0s for pod "pod-configmaps-ae5ca38e-538d-4ef3-95b5-a8db876e1e2e" in namespace "configmap-140" to be "Succeeded or Failed"
May 18 05:46:50.926: INFO: Pod "pod-configmaps-ae5ca38e-538d-4ef3-95b5-a8db876e1e2e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.799328ms
May 18 05:46:52.935: INFO: Pod "pod-configmaps-ae5ca38e-538d-4ef3-95b5-a8db876e1e2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024975219s
STEP: Saw pod success
May 18 05:46:52.935: INFO: Pod "pod-configmaps-ae5ca38e-538d-4ef3-95b5-a8db876e1e2e" satisfied condition "Succeeded or Failed"
May 18 05:46:52.939: INFO: Trying to get logs from node node2 pod pod-configmaps-ae5ca38e-538d-4ef3-95b5-a8db876e1e2e container configmap-volume-test: <nil>
STEP: delete the pod
May 18 05:46:52.973: INFO: Waiting for pod pod-configmaps-ae5ca38e-538d-4ef3-95b5-a8db876e1e2e to disappear
May 18 05:46:52.979: INFO: Pod pod-configmaps-ae5ca38e-538d-4ef3-95b5-a8db876e1e2e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:46:52.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-140" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":337,"completed":214,"skipped":3392,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:46:52.991: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 18 05:46:53.075: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:46:53.083: INFO: Number of nodes with available pods: 0
May 18 05:46:53.083: INFO: Node node1 is running more than one daemon pod
May 18 05:46:54.095: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:46:54.098: INFO: Number of nodes with available pods: 0
May 18 05:46:54.098: INFO: Node node1 is running more than one daemon pod
May 18 05:46:55.090: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:46:55.094: INFO: Number of nodes with available pods: 2
May 18 05:46:55.094: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 18 05:46:55.116: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:46:55.123: INFO: Number of nodes with available pods: 1
May 18 05:46:55.123: INFO: Node node1 is running more than one daemon pod
May 18 05:46:56.127: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:46:56.130: INFO: Number of nodes with available pods: 1
May 18 05:46:56.130: INFO: Node node1 is running more than one daemon pod
May 18 05:46:57.127: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:46:57.129: INFO: Number of nodes with available pods: 1
May 18 05:46:57.129: INFO: Node node1 is running more than one daemon pod
May 18 05:46:58.128: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:46:58.132: INFO: Number of nodes with available pods: 1
May 18 05:46:58.132: INFO: Node node1 is running more than one daemon pod
May 18 05:46:59.134: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:46:59.137: INFO: Number of nodes with available pods: 1
May 18 05:46:59.137: INFO: Node node1 is running more than one daemon pod
May 18 05:47:00.129: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:00.131: INFO: Number of nodes with available pods: 1
May 18 05:47:00.131: INFO: Node node1 is running more than one daemon pod
May 18 05:47:01.129: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:01.131: INFO: Number of nodes with available pods: 1
May 18 05:47:01.131: INFO: Node node1 is running more than one daemon pod
May 18 05:47:02.128: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:02.130: INFO: Number of nodes with available pods: 1
May 18 05:47:02.130: INFO: Node node1 is running more than one daemon pod
May 18 05:47:03.130: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:03.132: INFO: Number of nodes with available pods: 1
May 18 05:47:03.132: INFO: Node node1 is running more than one daemon pod
May 18 05:47:04.128: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:04.130: INFO: Number of nodes with available pods: 1
May 18 05:47:04.130: INFO: Node node1 is running more than one daemon pod
May 18 05:47:05.129: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:05.132: INFO: Number of nodes with available pods: 1
May 18 05:47:05.132: INFO: Node node1 is running more than one daemon pod
May 18 05:47:06.127: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:06.130: INFO: Number of nodes with available pods: 1
May 18 05:47:06.130: INFO: Node node1 is running more than one daemon pod
May 18 05:47:07.129: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:07.131: INFO: Number of nodes with available pods: 1
May 18 05:47:07.131: INFO: Node node1 is running more than one daemon pod
May 18 05:47:08.128: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:08.130: INFO: Number of nodes with available pods: 1
May 18 05:47:08.130: INFO: Node node1 is running more than one daemon pod
May 18 05:47:09.129: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:09.135: INFO: Number of nodes with available pods: 1
May 18 05:47:09.135: INFO: Node node1 is running more than one daemon pod
May 18 05:47:10.127: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:10.129: INFO: Number of nodes with available pods: 1
May 18 05:47:10.129: INFO: Node node1 is running more than one daemon pod
May 18 05:47:11.128: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:11.131: INFO: Number of nodes with available pods: 1
May 18 05:47:11.131: INFO: Node node1 is running more than one daemon pod
May 18 05:47:12.128: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:12.130: INFO: Number of nodes with available pods: 1
May 18 05:47:12.130: INFO: Node node1 is running more than one daemon pod
May 18 05:47:13.128: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 05:47:13.130: INFO: Number of nodes with available pods: 2
May 18 05:47:13.130: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7954, will wait for the garbage collector to delete the pods
May 18 05:47:13.189: INFO: Deleting DaemonSet.extensions daemon-set took: 4.50668ms
May 18 05:47:13.290: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.806743ms
May 18 05:47:21.004: INFO: Number of nodes with available pods: 0
May 18 05:47:21.004: INFO: Number of running nodes: 0, number of available pods: 0
May 18 05:47:21.007: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39761"},"items":null}

May 18 05:47:21.010: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39761"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:47:21.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7954" for this suite.

• [SLOW TEST:28.035 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":337,"completed":215,"skipped":3410,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:47:21.027: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-4bcd2ce8-55f6-48b4-8e3d-f9368263b761
STEP: Creating a pod to test consume configMaps
May 18 05:47:21.062: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bc7dce8a-3b26-404c-b2bf-11663db19e4f" in namespace "projected-6890" to be "Succeeded or Failed"
May 18 05:47:21.070: INFO: Pod "pod-projected-configmaps-bc7dce8a-3b26-404c-b2bf-11663db19e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.796965ms
May 18 05:47:23.076: INFO: Pod "pod-projected-configmaps-bc7dce8a-3b26-404c-b2bf-11663db19e4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01389987s
STEP: Saw pod success
May 18 05:47:23.076: INFO: Pod "pod-projected-configmaps-bc7dce8a-3b26-404c-b2bf-11663db19e4f" satisfied condition "Succeeded or Failed"
May 18 05:47:23.078: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-bc7dce8a-3b26-404c-b2bf-11663db19e4f container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 18 05:47:23.094: INFO: Waiting for pod pod-projected-configmaps-bc7dce8a-3b26-404c-b2bf-11663db19e4f to disappear
May 18 05:47:23.096: INFO: Pod pod-projected-configmaps-bc7dce8a-3b26-404c-b2bf-11663db19e4f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:47:23.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6890" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":337,"completed":216,"skipped":3427,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:47:23.107: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:47:29.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4204" for this suite.

• [SLOW TEST:6.112 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":337,"completed":217,"skipped":3449,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:47:29.219: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
W0518 05:47:29.259687      19 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 18 05:47:29.270: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 18 05:47:29.274: INFO: starting watch
STEP: patching
STEP: updating
May 18 05:47:29.292: INFO: waiting for watch events with expected annotations
May 18 05:47:29.292: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:47:29.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8702" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":337,"completed":218,"skipped":3457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:47:29.337: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:47:29.372: INFO: created pod
May 18 05:47:29.372: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-170" to be "Succeeded or Failed"
May 18 05:47:29.375: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.396789ms
May 18 05:47:31.379: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006249005s
STEP: Saw pod success
May 18 05:47:31.379: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May 18 05:48:01.380: INFO: polling logs
May 18 05:48:01.393: INFO: Pod logs: 
2021/05/18 05:47:30 OK: Got token
2021/05/18 05:47:30 validating with in-cluster discovery
2021/05/18 05:47:30 OK: got issuer https://kubernetes.default.svc.cluster.local
2021/05/18 05:47:30 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-170:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1621317449, NotBefore:1621316849, IssuedAt:1621316849, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-170", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"da53aa5b-a89c-49b8-a4a3-e10e4914eedb"}}}
2021/05/18 05:47:30 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2021/05/18 05:47:30 OK: Validated signature on JWT
2021/05/18 05:47:30 OK: Got valid claims from token!
2021/05/18 05:47:30 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-170:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1621317449, NotBefore:1621316849, IssuedAt:1621316849, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-170", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"da53aa5b-a89c-49b8-a4a3-e10e4914eedb"}}}

May 18 05:48:01.393: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:48:01.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-170" for this suite.

• [SLOW TEST:32.074 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":337,"completed":219,"skipped":3485,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:48:01.411: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:48:05.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1970" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":337,"completed":220,"skipped":3497,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:48:05.594: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:48:05.627: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c4f22793-fa9c-4bfd-b852-acf634decc29" in namespace "security-context-test-8231" to be "Succeeded or Failed"
May 18 05:48:05.638: INFO: Pod "alpine-nnp-false-c4f22793-fa9c-4bfd-b852-acf634decc29": Phase="Pending", Reason="", readiness=false. Elapsed: 11.281148ms
May 18 05:48:07.643: INFO: Pod "alpine-nnp-false-c4f22793-fa9c-4bfd-b852-acf634decc29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016172959s
May 18 05:48:07.643: INFO: Pod "alpine-nnp-false-c4f22793-fa9c-4bfd-b852-acf634decc29" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:48:07.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8231" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":221,"skipped":3506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:48:07.656: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:48:07.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6433" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":337,"completed":222,"skipped":3530,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:48:07.725: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:48:07.766: INFO: The status of Pod server-envvars-0ed1c65a-4af1-47b6-bc77-b27adf107376 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:48:09.772: INFO: The status of Pod server-envvars-0ed1c65a-4af1-47b6-bc77-b27adf107376 is Running (Ready = true)
May 18 05:48:09.792: INFO: Waiting up to 5m0s for pod "client-envvars-bc93f252-efd4-495d-9a6f-70b91c346022" in namespace "pods-8557" to be "Succeeded or Failed"
May 18 05:48:09.799: INFO: Pod "client-envvars-bc93f252-efd4-495d-9a6f-70b91c346022": Phase="Pending", Reason="", readiness=false. Elapsed: 7.819965ms
May 18 05:48:11.805: INFO: Pod "client-envvars-bc93f252-efd4-495d-9a6f-70b91c346022": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013212173s
STEP: Saw pod success
May 18 05:48:11.805: INFO: Pod "client-envvars-bc93f252-efd4-495d-9a6f-70b91c346022" satisfied condition "Succeeded or Failed"
May 18 05:48:11.808: INFO: Trying to get logs from node node1 pod client-envvars-bc93f252-efd4-495d-9a6f-70b91c346022 container env3cont: <nil>
STEP: delete the pod
May 18 05:48:11.836: INFO: Waiting for pod client-envvars-bc93f252-efd4-495d-9a6f-70b91c346022 to disappear
May 18 05:48:11.839: INFO: Pod client-envvars-bc93f252-efd4-495d-9a6f-70b91c346022 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:48:11.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8557" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":337,"completed":223,"skipped":3537,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:48:11.849: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:48:11.897: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 18 05:48:16.905: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 18 05:48:16.905: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 18 05:48:18.916: INFO: Creating deployment "test-rollover-deployment"
May 18 05:48:18.932: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 18 05:48:20.940: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 18 05:48:20.944: INFO: Ensure that both replica sets have 1 created replica
May 18 05:48:20.948: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 18 05:48:20.953: INFO: Updating deployment test-rollover-deployment
May 18 05:48:20.953: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 18 05:48:22.963: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 18 05:48:22.968: INFO: Make sure deployment "test-rollover-deployment" is complete
May 18 05:48:22.973: INFO: all replica sets need to contain the pod-template-hash label
May 18 05:48:22.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913699, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913699, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913702, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913698, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 18 05:48:24.989: INFO: all replica sets need to contain the pod-template-hash label
May 18 05:48:24.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913699, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913699, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913702, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913698, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 18 05:48:26.986: INFO: all replica sets need to contain the pod-template-hash label
May 18 05:48:26.986: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913699, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913699, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913702, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913698, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 18 05:48:28.980: INFO: all replica sets need to contain the pod-template-hash label
May 18 05:48:28.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913699, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913699, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913702, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913698, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 18 05:48:30.985: INFO: all replica sets need to contain the pod-template-hash label
May 18 05:48:30.985: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913699, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913699, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913702, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913698, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 18 05:48:32.987: INFO: 
May 18 05:48:32.987: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 18 05:48:33.000: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9443  93574048-4e0c-4a7b-8088-5913f52f3642 40286 2 2021-05-18 05:48:18 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-18 05:48:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-18 05:48:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0005fe268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-18 05:48:19 +0000 UTC,LastTransitionTime:2021-05-18 05:48:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-05-18 05:48:32 +0000 UTC,LastTransitionTime:2021-05-18 05:48:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 18 05:48:33.006: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-9443  51914c35-142f-4162-a3fb-09ab0554c47c 40275 2 2021-05-18 05:48:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 93574048-4e0c-4a7b-8088-5913f52f3642 0xc00121be60 0xc00121be61}] []  [{kube-controller-manager Update apps/v1 2021-05-18 05:48:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93574048-4e0c-4a7b-8088-5913f52f3642\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0002e4138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 18 05:48:33.006: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 18 05:48:33.006: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9443  fca7cf33-94fc-4e35-9170-3061c4977c5b 40284 2 2021-05-18 05:48:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 93574048-4e0c-4a7b-8088-5913f52f3642 0xc00121b487 0xc00121b488}] []  [{e2e.test Update apps/v1 2021-05-18 05:48:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-18 05:48:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93574048-4e0c-4a7b-8088-5913f52f3642\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00121b908 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 18 05:48:33.006: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-9443  b4b553ea-ac94-4fc8-a2ff-18549dc2d729 40234 2 2021-05-18 05:48:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 93574048-4e0c-4a7b-8088-5913f52f3642 0xc00121bc07 0xc00121bc08}] []  [{kube-controller-manager Update apps/v1 2021-05-18 05:48:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93574048-4e0c-4a7b-8088-5913f52f3642\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00121bd88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 18 05:48:33.012: INFO: Pod "test-rollover-deployment-98c5f4599-fcfrr" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-fcfrr test-rollover-deployment-98c5f4599- deployment-9443  ff1ffb4f-984b-4706-a97b-932f6bb9f58c 40256 0 2021-05-18 05:48:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/podIP:172.30.166.165/32 cni.projectcalico.org/podIPs:172.30.166.165/32] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 51914c35-142f-4162-a3fb-09ab0554c47c 0xc0005fe6a0 0xc0005fe6a1}] []  [{kube-controller-manager Update v1 2021-05-18 05:48:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"51914c35-142f-4162-a3fb-09ab0554c47c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 05:48:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 05:48:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.166.165\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dn89x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dn89x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:48:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:48:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:48:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:48:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.166.165,StartTime:2021-05-18 05:48:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:48:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://ccfbcfc6bc32b9f213726a17059343c2e18a8750ba7589831304d6eb07c2525a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.166.165,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:48:33.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9443" for this suite.

• [SLOW TEST:21.181 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":337,"completed":224,"skipped":3624,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:48:33.031: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 18 05:48:33.095: INFO: Waiting up to 5m0s for pod "pod-207afb35-b0d3-4864-b2de-7e2e5cf75433" in namespace "emptydir-9561" to be "Succeeded or Failed"
May 18 05:48:33.100: INFO: Pod "pod-207afb35-b0d3-4864-b2de-7e2e5cf75433": Phase="Pending", Reason="", readiness=false. Elapsed: 4.894777ms
May 18 05:48:35.109: INFO: Pod "pod-207afb35-b0d3-4864-b2de-7e2e5cf75433": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01375767s
STEP: Saw pod success
May 18 05:48:35.109: INFO: Pod "pod-207afb35-b0d3-4864-b2de-7e2e5cf75433" satisfied condition "Succeeded or Failed"
May 18 05:48:35.113: INFO: Trying to get logs from node node2 pod pod-207afb35-b0d3-4864-b2de-7e2e5cf75433 container test-container: <nil>
STEP: delete the pod
May 18 05:48:35.139: INFO: Waiting for pod pod-207afb35-b0d3-4864-b2de-7e2e5cf75433 to disappear
May 18 05:48:35.146: INFO: Pod pod-207afb35-b0d3-4864-b2de-7e2e5cf75433 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:48:35.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9561" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":225,"skipped":3636,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:48:35.156: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0518 05:48:35.900437      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 18 05:49:37.931: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:49:37.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6384" for this suite.

• [SLOW TEST:62.785 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":337,"completed":226,"skipped":3637,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:49:37.941: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
May 18 05:49:37.978: INFO: Waiting up to 5m0s for pod "client-containers-73c05525-18d0-45b0-830f-27d3e253102d" in namespace "containers-6567" to be "Succeeded or Failed"
May 18 05:49:37.987: INFO: Pod "client-containers-73c05525-18d0-45b0-830f-27d3e253102d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.359158ms
May 18 05:49:40.007: INFO: Pod "client-containers-73c05525-18d0-45b0-830f-27d3e253102d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0292493s
STEP: Saw pod success
May 18 05:49:40.007: INFO: Pod "client-containers-73c05525-18d0-45b0-830f-27d3e253102d" satisfied condition "Succeeded or Failed"
May 18 05:49:40.012: INFO: Trying to get logs from node node1 pod client-containers-73c05525-18d0-45b0-830f-27d3e253102d container agnhost-container: <nil>
STEP: delete the pod
May 18 05:49:40.041: INFO: Waiting for pod client-containers-73c05525-18d0-45b0-830f-27d3e253102d to disappear
May 18 05:49:40.043: INFO: Pod client-containers-73c05525-18d0-45b0-830f-27d3e253102d no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:49:40.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6567" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":337,"completed":227,"skipped":3654,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:49:40.051: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-887
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-887 to expose endpoints map[]
May 18 05:49:40.103: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May 18 05:49:41.119: INFO: successfully validated that service multi-endpoint-test in namespace services-887 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-887
May 18 05:49:41.128: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:49:43.134: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-887 to expose endpoints map[pod1:[100]]
May 18 05:49:43.146: INFO: successfully validated that service multi-endpoint-test in namespace services-887 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-887
May 18 05:49:43.156: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:49:45.162: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-887 to expose endpoints map[pod1:[100] pod2:[101]]
May 18 05:49:45.174: INFO: successfully validated that service multi-endpoint-test in namespace services-887 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-887
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-887 to expose endpoints map[pod2:[101]]
May 18 05:49:45.208: INFO: successfully validated that service multi-endpoint-test in namespace services-887 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-887
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-887 to expose endpoints map[]
May 18 05:49:45.242: INFO: successfully validated that service multi-endpoint-test in namespace services-887 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:49:45.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-887" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:5.253 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":337,"completed":228,"skipped":3657,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:49:45.304: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:49:45.758: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 18 05:49:47.773: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913785, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913785, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913785, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756913785, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:49:50.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:49:50.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5255" for this suite.
STEP: Destroying namespace "webhook-5255-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.756 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":337,"completed":229,"skipped":3661,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:49:51.060: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
May 18 05:49:51.117: INFO: namespace kubectl-5461
May 18 05:49:51.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-5461 create -f -'
May 18 05:49:51.560: INFO: stderr: ""
May 18 05:49:51.560: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 18 05:49:52.567: INFO: Selector matched 1 pods for map[app:agnhost]
May 18 05:49:52.567: INFO: Found 0 / 1
May 18 05:49:53.567: INFO: Selector matched 1 pods for map[app:agnhost]
May 18 05:49:53.567: INFO: Found 1 / 1
May 18 05:49:53.567: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 18 05:49:53.570: INFO: Selector matched 1 pods for map[app:agnhost]
May 18 05:49:53.570: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 18 05:49:53.570: INFO: wait on agnhost-primary startup in kubectl-5461 
May 18 05:49:53.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-5461 logs agnhost-primary-rkt8s agnhost-primary'
May 18 05:49:53.707: INFO: stderr: ""
May 18 05:49:53.707: INFO: stdout: "Paused\n"
STEP: exposing RC
May 18 05:49:53.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-5461 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 18 05:49:53.863: INFO: stderr: ""
May 18 05:49:53.863: INFO: stdout: "service/rm2 exposed\n"
May 18 05:49:53.870: INFO: Service rm2 in namespace kubectl-5461 found.
STEP: exposing service
May 18 05:49:55.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-5461 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 18 05:49:55.994: INFO: stderr: ""
May 18 05:49:55.994: INFO: stdout: "service/rm3 exposed\n"
May 18 05:49:56.003: INFO: Service rm3 in namespace kubectl-5461 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:49:58.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5461" for this suite.

• [SLOW TEST:6.958 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1223
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":337,"completed":230,"skipped":3668,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:49:58.018: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:49:58.052: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-71dbe01f-721c-4c9c-a708-5e0fe82187b1" in namespace "security-context-test-4692" to be "Succeeded or Failed"
May 18 05:49:58.062: INFO: Pod "busybox-readonly-false-71dbe01f-721c-4c9c-a708-5e0fe82187b1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.467052ms
May 18 05:50:00.068: INFO: Pod "busybox-readonly-false-71dbe01f-721c-4c9c-a708-5e0fe82187b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015778961s
May 18 05:50:00.068: INFO: Pod "busybox-readonly-false-71dbe01f-721c-4c9c-a708-5e0fe82187b1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:50:00.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4692" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":337,"completed":231,"skipped":3673,"failed":0}

------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:50:00.073: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-5428
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-5428
May 18 05:50:00.122: INFO: Found 0 stateful pods, waiting for 1
May 18 05:50:10.129: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
May 18 05:50:10.164: INFO: Deleting all statefulset in ns statefulset-5428
May 18 05:50:10.169: INFO: Scaling statefulset ss to 0
May 18 05:50:40.190: INFO: Waiting for statefulset status.replicas updated to 0
May 18 05:50:40.192: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:50:40.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5428" for this suite.

• [SLOW TEST:40.149 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":337,"completed":232,"skipped":3673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:50:40.223: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-5309aadb-ad62-4c10-9994-8acc8e1aa6f2
STEP: Creating a pod to test consume configMaps
May 18 05:50:40.257: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8688445d-80a6-489f-aca2-95533fb03ab0" in namespace "projected-4756" to be "Succeeded or Failed"
May 18 05:50:40.261: INFO: Pod "pod-projected-configmaps-8688445d-80a6-489f-aca2-95533fb03ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039282ms
May 18 05:50:42.266: INFO: Pod "pod-projected-configmaps-8688445d-80a6-489f-aca2-95533fb03ab0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009525689s
STEP: Saw pod success
May 18 05:50:42.266: INFO: Pod "pod-projected-configmaps-8688445d-80a6-489f-aca2-95533fb03ab0" satisfied condition "Succeeded or Failed"
May 18 05:50:42.268: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-8688445d-80a6-489f-aca2-95533fb03ab0 container agnhost-container: <nil>
STEP: delete the pod
May 18 05:50:42.285: INFO: Waiting for pod pod-projected-configmaps-8688445d-80a6-489f-aca2-95533fb03ab0 to disappear
May 18 05:50:42.287: INFO: Pod pod-projected-configmaps-8688445d-80a6-489f-aca2-95533fb03ab0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:50:42.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4756" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":337,"completed":233,"skipped":3709,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:50:42.293: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:50:42.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6462" for this suite.
STEP: Destroying namespace "nspatchtest-6f09ae7c-c12a-42e3-bcc1-4fe863379bdc-3411" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":337,"completed":234,"skipped":3714,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:50:42.364: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0518 05:51:22.437388      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 18 05:52:24.458: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 18 05:52:24.458: INFO: Deleting pod "simpletest.rc-bkgs9" in namespace "gc-2593"
May 18 05:52:24.484: INFO: Deleting pod "simpletest.rc-f8nq8" in namespace "gc-2593"
May 18 05:52:24.507: INFO: Deleting pod "simpletest.rc-f9wzq" in namespace "gc-2593"
May 18 05:52:24.518: INFO: Deleting pod "simpletest.rc-jr4c4" in namespace "gc-2593"
May 18 05:52:24.533: INFO: Deleting pod "simpletest.rc-lbvxp" in namespace "gc-2593"
May 18 05:52:24.554: INFO: Deleting pod "simpletest.rc-ps28n" in namespace "gc-2593"
May 18 05:52:24.569: INFO: Deleting pod "simpletest.rc-pw9tc" in namespace "gc-2593"
May 18 05:52:24.584: INFO: Deleting pod "simpletest.rc-tsffn" in namespace "gc-2593"
May 18 05:52:24.598: INFO: Deleting pod "simpletest.rc-vffzb" in namespace "gc-2593"
May 18 05:52:24.610: INFO: Deleting pod "simpletest.rc-zqqdc" in namespace "gc-2593"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:52:24.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2593" for this suite.

• [SLOW TEST:102.273 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":337,"completed":235,"skipped":3755,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:52:24.637: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
May 18 05:52:24.687: INFO: Waiting up to 5m0s for pod "pod-de037989-03ce-44cb-bda5-3bcd009c5e67" in namespace "emptydir-2247" to be "Succeeded or Failed"
May 18 05:52:24.693: INFO: Pod "pod-de037989-03ce-44cb-bda5-3bcd009c5e67": Phase="Pending", Reason="", readiness=false. Elapsed: 5.892973ms
May 18 05:52:26.704: INFO: Pod "pod-de037989-03ce-44cb-bda5-3bcd009c5e67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017400654s
May 18 05:52:28.712: INFO: Pod "pod-de037989-03ce-44cb-bda5-3bcd009c5e67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02548915s
STEP: Saw pod success
May 18 05:52:28.712: INFO: Pod "pod-de037989-03ce-44cb-bda5-3bcd009c5e67" satisfied condition "Succeeded or Failed"
May 18 05:52:28.715: INFO: Trying to get logs from node node2 pod pod-de037989-03ce-44cb-bda5-3bcd009c5e67 container test-container: <nil>
STEP: delete the pod
May 18 05:52:28.754: INFO: Waiting for pod pod-de037989-03ce-44cb-bda5-3bcd009c5e67 to disappear
May 18 05:52:28.758: INFO: Pod pod-de037989-03ce-44cb-bda5-3bcd009c5e67 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:52:28.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2247" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":236,"skipped":3770,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:52:28.768: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0518 05:52:28.827191      19 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:54:00.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5954" for this suite.

• [SLOW TEST:92.099 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":337,"completed":237,"skipped":3785,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:54:00.867: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
May 18 05:54:00.922: INFO: Waiting up to 5m0s for pod "security-context-92dfc75f-f18c-44a9-8f28-56276ff72404" in namespace "security-context-2178" to be "Succeeded or Failed"
May 18 05:54:00.936: INFO: Pod "security-context-92dfc75f-f18c-44a9-8f28-56276ff72404": Phase="Pending", Reason="", readiness=false. Elapsed: 14.058236ms
May 18 05:54:02.950: INFO: Pod "security-context-92dfc75f-f18c-44a9-8f28-56276ff72404": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027264509s
May 18 05:54:04.953: INFO: Pod "security-context-92dfc75f-f18c-44a9-8f28-56276ff72404": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030946225s
STEP: Saw pod success
May 18 05:54:04.953: INFO: Pod "security-context-92dfc75f-f18c-44a9-8f28-56276ff72404" satisfied condition "Succeeded or Failed"
May 18 05:54:04.957: INFO: Trying to get logs from node node1 pod security-context-92dfc75f-f18c-44a9-8f28-56276ff72404 container test-container: <nil>
STEP: delete the pod
May 18 05:54:04.991: INFO: Waiting for pod security-context-92dfc75f-f18c-44a9-8f28-56276ff72404 to disappear
May 18 05:54:05.000: INFO: Pod security-context-92dfc75f-f18c-44a9-8f28-56276ff72404 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:54:05.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2178" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":337,"completed":238,"skipped":3798,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:54:05.012: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:54:05.990: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:54:09.007: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:54:09.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5007" for this suite.
STEP: Destroying namespace "webhook-5007-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":337,"completed":239,"skipped":3813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:54:09.226: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-bnl6
STEP: Creating a pod to test atomic-volume-subpath
May 18 05:54:09.303: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-bnl6" in namespace "subpath-356" to be "Succeeded or Failed"
May 18 05:54:09.317: INFO: Pod "pod-subpath-test-downwardapi-bnl6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.678038ms
May 18 05:54:11.324: INFO: Pod "pod-subpath-test-downwardapi-bnl6": Phase="Running", Reason="", readiness=true. Elapsed: 2.021105337s
May 18 05:54:13.330: INFO: Pod "pod-subpath-test-downwardapi-bnl6": Phase="Running", Reason="", readiness=true. Elapsed: 4.027036143s
May 18 05:54:15.334: INFO: Pod "pod-subpath-test-downwardapi-bnl6": Phase="Running", Reason="", readiness=true. Elapsed: 6.030933458s
May 18 05:54:17.341: INFO: Pod "pod-subpath-test-downwardapi-bnl6": Phase="Running", Reason="", readiness=true. Elapsed: 8.037977159s
May 18 05:54:19.348: INFO: Pod "pod-subpath-test-downwardapi-bnl6": Phase="Running", Reason="", readiness=true. Elapsed: 10.044774061s
May 18 05:54:21.355: INFO: Pod "pod-subpath-test-downwardapi-bnl6": Phase="Running", Reason="", readiness=true. Elapsed: 12.051422264s
May 18 05:54:23.361: INFO: Pod "pod-subpath-test-downwardapi-bnl6": Phase="Running", Reason="", readiness=true. Elapsed: 14.057979067s
May 18 05:54:25.368: INFO: Pod "pod-subpath-test-downwardapi-bnl6": Phase="Running", Reason="", readiness=true. Elapsed: 16.06449947s
May 18 05:54:27.373: INFO: Pod "pod-subpath-test-downwardapi-bnl6": Phase="Running", Reason="", readiness=true. Elapsed: 18.069813678s
May 18 05:54:29.379: INFO: Pod "pod-subpath-test-downwardapi-bnl6": Phase="Running", Reason="", readiness=true. Elapsed: 20.075403086s
May 18 05:54:31.385: INFO: Pod "pod-subpath-test-downwardapi-bnl6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.082107188s
STEP: Saw pod success
May 18 05:54:31.385: INFO: Pod "pod-subpath-test-downwardapi-bnl6" satisfied condition "Succeeded or Failed"
May 18 05:54:31.393: INFO: Trying to get logs from node node1 pod pod-subpath-test-downwardapi-bnl6 container test-container-subpath-downwardapi-bnl6: <nil>
STEP: delete the pod
May 18 05:54:31.422: INFO: Waiting for pod pod-subpath-test-downwardapi-bnl6 to disappear
May 18 05:54:31.428: INFO: Pod pod-subpath-test-downwardapi-bnl6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-bnl6
May 18 05:54:31.428: INFO: Deleting pod "pod-subpath-test-downwardapi-bnl6" in namespace "subpath-356"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:54:31.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-356" for this suite.

• [SLOW TEST:22.228 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":337,"completed":240,"skipped":3852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:54:31.455: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-a2b66a3e-bea4-46d1-8d8f-54623bf45455
STEP: Creating configMap with name cm-test-opt-upd-8577aeb5-1920-4e93-8d4e-903b3a0d7ed8
STEP: Creating the pod
May 18 05:54:31.539: INFO: The status of Pod pod-configmaps-db814324-5f0b-49b9-9ced-6929d23ab29e is Pending, waiting for it to be Running (with Ready = true)
May 18 05:54:33.542: INFO: The status of Pod pod-configmaps-db814324-5f0b-49b9-9ced-6929d23ab29e is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-a2b66a3e-bea4-46d1-8d8f-54623bf45455
STEP: Updating configmap cm-test-opt-upd-8577aeb5-1920-4e93-8d4e-903b3a0d7ed8
STEP: Creating configMap with name cm-test-opt-create-5af3f1e9-3e3f-4f23-bc3d-5c0b9216fdea
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:54:35.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1512" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":337,"completed":241,"skipped":3899,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:54:35.616: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-1f616a70-a59f-41c8-a771-de2ad4a765b0
STEP: Creating a pod to test consume secrets
May 18 05:54:35.657: INFO: Waiting up to 5m0s for pod "pod-secrets-d95a7812-847d-4c21-b0f2-beddaf8c75a0" in namespace "secrets-6485" to be "Succeeded or Failed"
May 18 05:54:35.662: INFO: Pod "pod-secrets-d95a7812-847d-4c21-b0f2-beddaf8c75a0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295076ms
May 18 05:54:37.669: INFO: Pod "pod-secrets-d95a7812-847d-4c21-b0f2-beddaf8c75a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012234477s
STEP: Saw pod success
May 18 05:54:37.669: INFO: Pod "pod-secrets-d95a7812-847d-4c21-b0f2-beddaf8c75a0" satisfied condition "Succeeded or Failed"
May 18 05:54:37.672: INFO: Trying to get logs from node node2 pod pod-secrets-d95a7812-847d-4c21-b0f2-beddaf8c75a0 container secret-volume-test: <nil>
STEP: delete the pod
May 18 05:54:37.706: INFO: Waiting for pod pod-secrets-d95a7812-847d-4c21-b0f2-beddaf8c75a0 to disappear
May 18 05:54:37.708: INFO: Pod pod-secrets-d95a7812-847d-4c21-b0f2-beddaf8c75a0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:54:37.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6485" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":242,"skipped":3922,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:54:37.717: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7946.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7946.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7946.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7946.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 151.17.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.17.151_udp@PTR;check="$$(dig +tcp +noall +answer +search 151.17.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.17.151_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7946.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7946.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7946.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7946.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 151.17.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.17.151_udp@PTR;check="$$(dig +tcp +noall +answer +search 151.17.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.17.151_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 18 05:54:39.822: INFO: Unable to read wheezy_udp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:39.825: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:39.828: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:39.830: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:39.848: INFO: Unable to read jessie_udp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:39.850: INFO: Unable to read jessie_tcp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:39.853: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:39.856: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:39.870: INFO: Lookups using dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f failed for: [wheezy_udp@dns-test-service.dns-7946.svc.cluster.local wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local jessie_udp@dns-test-service.dns-7946.svc.cluster.local jessie_tcp@dns-test-service.dns-7946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local]

May 18 05:54:44.873: INFO: Unable to read wheezy_udp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:44.876: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:44.878: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:44.881: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:44.898: INFO: Unable to read jessie_udp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:44.901: INFO: Unable to read jessie_tcp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:44.903: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:44.906: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:44.920: INFO: Lookups using dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f failed for: [wheezy_udp@dns-test-service.dns-7946.svc.cluster.local wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local jessie_udp@dns-test-service.dns-7946.svc.cluster.local jessie_tcp@dns-test-service.dns-7946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local]

May 18 05:54:49.875: INFO: Unable to read wheezy_udp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:49.877: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:49.880: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:49.883: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:49.900: INFO: Unable to read jessie_udp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:49.903: INFO: Unable to read jessie_tcp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:49.906: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:49.909: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:49.926: INFO: Lookups using dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f failed for: [wheezy_udp@dns-test-service.dns-7946.svc.cluster.local wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local jessie_udp@dns-test-service.dns-7946.svc.cluster.local jessie_tcp@dns-test-service.dns-7946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local]

May 18 05:54:54.877: INFO: Unable to read wheezy_udp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:54.882: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:54.886: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:54.891: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:54.911: INFO: Unable to read jessie_udp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:54.914: INFO: Unable to read jessie_tcp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:54.916: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:54.919: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:54.934: INFO: Lookups using dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f failed for: [wheezy_udp@dns-test-service.dns-7946.svc.cluster.local wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local jessie_udp@dns-test-service.dns-7946.svc.cluster.local jessie_tcp@dns-test-service.dns-7946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local]

May 18 05:54:59.874: INFO: Unable to read wheezy_udp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:59.878: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:59.881: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:59.884: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:59.902: INFO: Unable to read jessie_udp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:59.904: INFO: Unable to read jessie_tcp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:59.907: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:59.909: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:54:59.926: INFO: Lookups using dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f failed for: [wheezy_udp@dns-test-service.dns-7946.svc.cluster.local wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local jessie_udp@dns-test-service.dns-7946.svc.cluster.local jessie_tcp@dns-test-service.dns-7946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local]

May 18 05:55:04.874: INFO: Unable to read wheezy_udp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:55:04.876: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:55:04.879: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:55:04.881: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:55:04.901: INFO: Unable to read jessie_udp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:55:04.903: INFO: Unable to read jessie_tcp@dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:55:04.906: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:55:04.909: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local from pod dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f: the server could not find the requested resource (get pods dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f)
May 18 05:55:04.925: INFO: Lookups using dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f failed for: [wheezy_udp@dns-test-service.dns-7946.svc.cluster.local wheezy_tcp@dns-test-service.dns-7946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local jessie_udp@dns-test-service.dns-7946.svc.cluster.local jessie_tcp@dns-test-service.dns-7946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7946.svc.cluster.local]

May 18 05:55:09.936: INFO: DNS probes using dns-7946/dns-test-0f4db7cc-0067-4d9b-9377-26d120d02d8f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:55:10.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7946" for this suite.

• [SLOW TEST:32.316 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":337,"completed":243,"skipped":3938,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:55:10.034: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
May 18 05:55:10.117: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 18 05:55:12.122: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
May 18 05:55:12.135: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 18 05:55:14.140: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
May 18 05:55:14.147: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 18 05:55:14.152: INFO: Pod pod-with-prestop-http-hook still exists
May 18 05:55:16.152: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 18 05:55:16.157: INFO: Pod pod-with-prestop-http-hook still exists
May 18 05:55:18.152: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 18 05:55:18.154: INFO: Pod pod-with-prestop-http-hook still exists
May 18 05:55:20.152: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 18 05:55:20.157: INFO: Pod pod-with-prestop-http-hook still exists
May 18 05:55:22.152: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 18 05:55:22.157: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:55:22.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3789" for this suite.

• [SLOW TEST:12.146 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":337,"completed":244,"skipped":3955,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:55:22.180: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 05:55:22.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a96339c-181d-4f03-9999-d10c22d1e218" in namespace "downward-api-2023" to be "Succeeded or Failed"
May 18 05:55:22.230: INFO: Pod "downwardapi-volume-3a96339c-181d-4f03-9999-d10c22d1e218": Phase="Pending", Reason="", readiness=false. Elapsed: 8.732161ms
May 18 05:55:24.241: INFO: Pod "downwardapi-volume-3a96339c-181d-4f03-9999-d10c22d1e218": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019869043s
STEP: Saw pod success
May 18 05:55:24.241: INFO: Pod "downwardapi-volume-3a96339c-181d-4f03-9999-d10c22d1e218" satisfied condition "Succeeded or Failed"
May 18 05:55:24.244: INFO: Trying to get logs from node node1 pod downwardapi-volume-3a96339c-181d-4f03-9999-d10c22d1e218 container client-container: <nil>
STEP: delete the pod
May 18 05:55:24.260: INFO: Waiting for pod downwardapi-volume-3a96339c-181d-4f03-9999-d10c22d1e218 to disappear
May 18 05:55:24.264: INFO: Pod downwardapi-volume-3a96339c-181d-4f03-9999-d10c22d1e218 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:55:24.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2023" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":337,"completed":245,"skipped":3976,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:55:24.270: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:55:41.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3698" for this suite.

• [SLOW TEST:17.079 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":337,"completed":246,"skipped":4001,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:55:41.349: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1514
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
May 18 05:55:41.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-2752 run e2e-test-httpd-pod --restart=Never --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
May 18 05:55:41.781: INFO: stderr: ""
May 18 05:55:41.781: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1518
May 18 05:55:41.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-2752 delete pods e2e-test-httpd-pod'
May 18 05:55:44.534: INFO: stderr: ""
May 18 05:55:44.534: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:55:44.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2752" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":337,"completed":247,"skipped":4007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:55:44.544: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 18 05:55:46.603: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:55:46.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2599" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":337,"completed":248,"skipped":4106,"failed":0}
S
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:55:46.641: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:55:46.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7101" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":337,"completed":249,"skipped":4107,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:55:46.768: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:55:48.828: INFO: Deleting pod "var-expansion-74d08b7f-ae86-405d-9dd8-266fbb8097b7" in namespace "var-expansion-8561"
May 18 05:55:48.834: INFO: Wait up to 5m0s for pod "var-expansion-74d08b7f-ae86-405d-9dd8-266fbb8097b7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:55:54.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8561" for this suite.

• [SLOW TEST:8.083 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":337,"completed":250,"skipped":4114,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:55:54.851: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
May 18 05:55:54.895: INFO: The status of Pod labelsupdate118fc71a-a28a-4a69-a7a0-6f945cb83e17 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:55:56.901: INFO: The status of Pod labelsupdate118fc71a-a28a-4a69-a7a0-6f945cb83e17 is Running (Ready = true)
May 18 05:55:57.425: INFO: Successfully updated pod "labelsupdate118fc71a-a28a-4a69-a7a0-6f945cb83e17"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:01.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4618" for this suite.

• [SLOW TEST:6.619 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":337,"completed":251,"skipped":4117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:01.471: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-2803bf51-256e-4d50-91be-fa7da4160ae4
STEP: Creating a pod to test consume secrets
May 18 05:56:01.508: INFO: Waiting up to 5m0s for pod "pod-secrets-c031358c-e74c-40b7-93ce-9df3f3746ccd" in namespace "secrets-2350" to be "Succeeded or Failed"
May 18 05:56:01.515: INFO: Pod "pod-secrets-c031358c-e74c-40b7-93ce-9df3f3746ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.572566ms
May 18 05:56:03.521: INFO: Pod "pod-secrets-c031358c-e74c-40b7-93ce-9df3f3746ccd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012801175s
STEP: Saw pod success
May 18 05:56:03.521: INFO: Pod "pod-secrets-c031358c-e74c-40b7-93ce-9df3f3746ccd" satisfied condition "Succeeded or Failed"
May 18 05:56:03.523: INFO: Trying to get logs from node node2 pod pod-secrets-c031358c-e74c-40b7-93ce-9df3f3746ccd container secret-volume-test: <nil>
STEP: delete the pod
May 18 05:56:03.542: INFO: Waiting for pod pod-secrets-c031358c-e74c-40b7-93ce-9df3f3746ccd to disappear
May 18 05:56:03.548: INFO: Pod pod-secrets-c031358c-e74c-40b7-93ce-9df3f3746ccd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:03.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2350" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":337,"completed":252,"skipped":4162,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:03.558: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 18 05:56:03.585: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 18 05:56:03.592: INFO: Waiting for terminating namespaces to be deleted...
May 18 05:56:03.598: INFO: 
Logging pods the apiserver thinks is on node node1 before test
May 18 05:56:03.604: INFO: labelsupdate118fc71a-a28a-4a69-a7a0-6f945cb83e17 from downward-api-4618 started at 2021-05-18 05:55:54 +0000 UTC (1 container statuses recorded)
May 18 05:56:03.604: INFO: 	Container client-container ready: true, restart count 0
May 18 05:56:03.604: INFO: calico-node-bzct9 from kube-system started at 2021-05-18 02:09:57 +0000 UTC (1 container statuses recorded)
May 18 05:56:03.604: INFO: 	Container calico-node ready: true, restart count 0
May 18 05:56:03.604: INFO: kube-proxy-vxmpl from kube-system started at 2021-05-18 02:09:57 +0000 UTC (1 container statuses recorded)
May 18 05:56:03.604: INFO: 	Container kube-proxy ready: true, restart count 0
May 18 05:56:03.604: INFO: sonobuoy-e2e-job-d53ed4c4301a4fe4 from sonobuoy started at 2021-05-18 04:30:03 +0000 UTC (2 container statuses recorded)
May 18 05:56:03.604: INFO: 	Container e2e ready: true, restart count 0
May 18 05:56:03.604: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 18 05:56:03.604: INFO: sonobuoy-systemd-logs-daemon-set-0e6496dadced4aae-5wgz6 from sonobuoy started at 2021-05-18 04:30:04 +0000 UTC (2 container statuses recorded)
May 18 05:56:03.604: INFO: 	Container sonobuoy-worker ready: false, restart count 9
May 18 05:56:03.604: INFO: 	Container systemd-logs ready: true, restart count 0
May 18 05:56:03.604: INFO: 
Logging pods the apiserver thinks is on node node2 before test
May 18 05:56:03.609: INFO: calico-node-ddzct from kube-system started at 2021-05-18 04:29:32 +0000 UTC (1 container statuses recorded)
May 18 05:56:03.609: INFO: 	Container calico-node ready: true, restart count 0
May 18 05:56:03.609: INFO: kube-proxy-d6j4t from kube-system started at 2021-05-18 02:10:08 +0000 UTC (1 container statuses recorded)
May 18 05:56:03.609: INFO: 	Container kube-proxy ready: true, restart count 0
May 18 05:56:03.609: INFO: metrics-server-78ff8bd4fd-zg6vw from kube-system started at 2021-05-18 05:53:47 +0000 UTC (1 container statuses recorded)
May 18 05:56:03.609: INFO: 	Container metrics-server ready: true, restart count 0
May 18 05:56:03.609: INFO: sonobuoy from sonobuoy started at 2021-05-18 04:30:02 +0000 UTC (1 container statuses recorded)
May 18 05:56:03.609: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 18 05:56:03.609: INFO: sonobuoy-systemd-logs-daemon-set-0e6496dadced4aae-7bgms from sonobuoy started at 2021-05-18 04:30:03 +0000 UTC (2 container statuses recorded)
May 18 05:56:03.609: INFO: 	Container sonobuoy-worker ready: false, restart count 9
May 18 05:56:03.609: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1680138d55677ac7], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:04.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3394" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":337,"completed":253,"skipped":4174,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:04.640: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:10.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1830" for this suite.
STEP: Destroying namespace "nsdeletetest-1191" for this suite.
May 18 05:56:10.799: INFO: Namespace nsdeletetest-1191 was already deleted
STEP: Destroying namespace "nsdeletetest-5773" for this suite.

• [SLOW TEST:6.162 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":337,"completed":254,"skipped":4187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:10.803: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 05:56:10.829: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5c5b1be-c9f5-4776-a54c-ad72795f6b93" in namespace "projected-7938" to be "Succeeded or Failed"
May 18 05:56:10.834: INFO: Pod "downwardapi-volume-d5c5b1be-c9f5-4776-a54c-ad72795f6b93": Phase="Pending", Reason="", readiness=false. Elapsed: 5.401676ms
May 18 05:56:12.840: INFO: Pod "downwardapi-volume-d5c5b1be-c9f5-4776-a54c-ad72795f6b93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010911384s
May 18 05:56:14.846: INFO: Pod "downwardapi-volume-d5c5b1be-c9f5-4776-a54c-ad72795f6b93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01670419s
STEP: Saw pod success
May 18 05:56:14.846: INFO: Pod "downwardapi-volume-d5c5b1be-c9f5-4776-a54c-ad72795f6b93" satisfied condition "Succeeded or Failed"
May 18 05:56:14.848: INFO: Trying to get logs from node node2 pod downwardapi-volume-d5c5b1be-c9f5-4776-a54c-ad72795f6b93 container client-container: <nil>
STEP: delete the pod
May 18 05:56:14.860: INFO: Waiting for pod downwardapi-volume-d5c5b1be-c9f5-4776-a54c-ad72795f6b93 to disappear
May 18 05:56:14.864: INFO: Pod downwardapi-volume-d5c5b1be-c9f5-4776-a54c-ad72795f6b93 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:14.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7938" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":255,"skipped":4227,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:14.870: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-c83e0440-fe0f-4054-afc2-debf4a4ac05a
STEP: Creating a pod to test consume configMaps
May 18 05:56:14.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-389298c5-8c87-4d1b-b55c-47092726325c" in namespace "configmap-2978" to be "Succeeded or Failed"
May 18 05:56:14.904: INFO: Pod "pod-configmaps-389298c5-8c87-4d1b-b55c-47092726325c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.736674ms
May 18 05:56:16.915: INFO: Pod "pod-configmaps-389298c5-8c87-4d1b-b55c-47092726325c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016796657s
STEP: Saw pod success
May 18 05:56:16.915: INFO: Pod "pod-configmaps-389298c5-8c87-4d1b-b55c-47092726325c" satisfied condition "Succeeded or Failed"
May 18 05:56:16.918: INFO: Trying to get logs from node node1 pod pod-configmaps-389298c5-8c87-4d1b-b55c-47092726325c container agnhost-container: <nil>
STEP: delete the pod
May 18 05:56:16.933: INFO: Waiting for pod pod-configmaps-389298c5-8c87-4d1b-b55c-47092726325c to disappear
May 18 05:56:16.935: INFO: Pod pod-configmaps-389298c5-8c87-4d1b-b55c-47092726325c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:16.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2978" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":337,"completed":256,"skipped":4233,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:16.941: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-3c396c32-e9d0-4b8b-a5c2-3e11e5d9b99a
STEP: Creating a pod to test consume secrets
May 18 05:56:16.971: INFO: Waiting up to 5m0s for pod "pod-secrets-0d1f8ad9-4abf-4aeb-85fc-0bc9e4648fac" in namespace "secrets-1728" to be "Succeeded or Failed"
May 18 05:56:16.979: INFO: Pod "pod-secrets-0d1f8ad9-4abf-4aeb-85fc-0bc9e4648fac": Phase="Pending", Reason="", readiness=false. Elapsed: 7.537966ms
May 18 05:56:18.983: INFO: Pod "pod-secrets-0d1f8ad9-4abf-4aeb-85fc-0bc9e4648fac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01164608s
STEP: Saw pod success
May 18 05:56:18.983: INFO: Pod "pod-secrets-0d1f8ad9-4abf-4aeb-85fc-0bc9e4648fac" satisfied condition "Succeeded or Failed"
May 18 05:56:18.985: INFO: Trying to get logs from node node1 pod pod-secrets-0d1f8ad9-4abf-4aeb-85fc-0bc9e4648fac container secret-env-test: <nil>
STEP: delete the pod
May 18 05:56:19.000: INFO: Waiting for pod pod-secrets-0d1f8ad9-4abf-4aeb-85fc-0bc9e4648fac to disappear
May 18 05:56:19.003: INFO: Pod pod-secrets-0d1f8ad9-4abf-4aeb-85fc-0bc9e4648fac no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:19.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1728" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":337,"completed":257,"skipped":4242,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:19.009: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:21.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8356" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":337,"completed":258,"skipped":4260,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:21.065: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-c3f025f8-1e5a-4d14-a4d4-6c6c63830b95
STEP: Creating a pod to test consume configMaps
May 18 05:56:21.103: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8ded852f-cf0d-4877-8da8-660787a7af7b" in namespace "projected-2000" to be "Succeeded or Failed"
May 18 05:56:21.106: INFO: Pod "pod-projected-configmaps-8ded852f-cf0d-4877-8da8-660787a7af7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.529389ms
May 18 05:56:23.109: INFO: Pod "pod-projected-configmaps-8ded852f-cf0d-4877-8da8-660787a7af7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005689007s
STEP: Saw pod success
May 18 05:56:23.109: INFO: Pod "pod-projected-configmaps-8ded852f-cf0d-4877-8da8-660787a7af7b" satisfied condition "Succeeded or Failed"
May 18 05:56:23.111: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-8ded852f-cf0d-4877-8da8-660787a7af7b container agnhost-container: <nil>
STEP: delete the pod
May 18 05:56:23.130: INFO: Waiting for pod pod-projected-configmaps-8ded852f-cf0d-4877-8da8-660787a7af7b to disappear
May 18 05:56:23.132: INFO: Pod pod-projected-configmaps-8ded852f-cf0d-4877-8da8-660787a7af7b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:23.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2000" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":259,"skipped":4265,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:23.143: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
May 18 05:56:23.177: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:26.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5271" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":337,"completed":260,"skipped":4275,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:26.133: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:26.176: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-3769
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:30.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-7396" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:30.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3769" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":337,"completed":261,"skipped":4288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:30.304: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 18 05:56:32.365: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:32.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7372" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":337,"completed":262,"skipped":4322,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:32.388: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 18 05:56:32.424: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:40.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3539" for this suite.

• [SLOW TEST:8.525 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":337,"completed":263,"skipped":4330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:40.913: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
May 18 05:56:40.940: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-9639 proxy --unix-socket=/tmp/kubectl-proxy-unix620194659/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:56:41.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9639" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":337,"completed":264,"skipped":4353,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:56:41.012: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 18 05:56:41.052: INFO: Waiting up to 1m0s for all nodes to be ready
May 18 05:57:41.079: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:57:41.081: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:57:41.124: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
May 18 05:57:41.126: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:57:41.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1451" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:57:41.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3964" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.214 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":337,"completed":265,"skipped":4381,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:57:41.227: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:57:41.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-15" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":337,"completed":266,"skipped":4384,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:57:41.256: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5554
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5554
I0518 05:57:41.302953      19 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5554, replica count: 2
May 18 05:57:44.354: INFO: Creating new exec pod
I0518 05:57:44.354374      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 18 05:57:49.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5554 exec execpod7fmvv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 18 05:57:49.535: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 18 05:57:49.535: INFO: stdout: ""
May 18 05:57:50.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5554 exec execpod7fmvv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 18 05:57:50.674: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 18 05:57:50.674: INFO: stdout: ""
May 18 05:57:51.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5554 exec execpod7fmvv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 18 05:57:51.671: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 18 05:57:51.671: INFO: stdout: ""
May 18 05:57:52.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5554 exec execpod7fmvv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 18 05:57:52.666: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 18 05:57:52.666: INFO: stdout: ""
May 18 05:57:53.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5554 exec execpod7fmvv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 18 05:57:53.675: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 18 05:57:53.675: INFO: stdout: ""
May 18 05:57:54.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5554 exec execpod7fmvv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 18 05:57:54.672: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 18 05:57:54.672: INFO: stdout: ""
May 18 05:57:55.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5554 exec execpod7fmvv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 18 05:57:55.667: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 18 05:57:55.667: INFO: stdout: ""
May 18 05:57:56.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5554 exec execpod7fmvv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 18 05:57:56.674: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 18 05:57:56.674: INFO: stdout: ""
May 18 05:57:57.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5554 exec execpod7fmvv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 18 05:57:57.679: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 18 05:57:57.679: INFO: stdout: "externalname-service-x2jmz"
May 18 05:57:57.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5554 exec execpod7fmvv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.24.143.250 80'
May 18 05:57:57.822: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.24.143.250 80\nConnection to 172.24.143.250 80 port [tcp/http] succeeded!\n"
May 18 05:57:57.822: INFO: stdout: "externalname-service-k2gq5"
May 18 05:57:57.822: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:57:57.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5554" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:16.599 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":337,"completed":267,"skipped":4391,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:57:57.855: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 05:57:57.902: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aee96053-85a2-49cc-aaf2-20ee560ec85a" in namespace "downward-api-9185" to be "Succeeded or Failed"
May 18 05:57:57.910: INFO: Pod "downwardapi-volume-aee96053-85a2-49cc-aaf2-20ee560ec85a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.077964ms
May 18 05:57:59.916: INFO: Pod "downwardapi-volume-aee96053-85a2-49cc-aaf2-20ee560ec85a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01389837s
STEP: Saw pod success
May 18 05:57:59.916: INFO: Pod "downwardapi-volume-aee96053-85a2-49cc-aaf2-20ee560ec85a" satisfied condition "Succeeded or Failed"
May 18 05:57:59.920: INFO: Trying to get logs from node node2 pod downwardapi-volume-aee96053-85a2-49cc-aaf2-20ee560ec85a container client-container: <nil>
STEP: delete the pod
May 18 05:57:59.942: INFO: Waiting for pod downwardapi-volume-aee96053-85a2-49cc-aaf2-20ee560ec85a to disappear
May 18 05:57:59.945: INFO: Pod downwardapi-volume-aee96053-85a2-49cc-aaf2-20ee560ec85a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:57:59.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9185" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":268,"skipped":4393,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:57:59.953: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 05:57:59.988: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1e20988-5d77-42cf-8805-c45adb7a772d" in namespace "projected-1928" to be "Succeeded or Failed"
May 18 05:57:59.994: INFO: Pod "downwardapi-volume-a1e20988-5d77-42cf-8805-c45adb7a772d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.154472ms
May 18 05:58:02.008: INFO: Pod "downwardapi-volume-a1e20988-5d77-42cf-8805-c45adb7a772d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019845342s
STEP: Saw pod success
May 18 05:58:02.008: INFO: Pod "downwardapi-volume-a1e20988-5d77-42cf-8805-c45adb7a772d" satisfied condition "Succeeded or Failed"
May 18 05:58:02.012: INFO: Trying to get logs from node node2 pod downwardapi-volume-a1e20988-5d77-42cf-8805-c45adb7a772d container client-container: <nil>
STEP: delete the pod
May 18 05:58:02.056: INFO: Waiting for pod downwardapi-volume-a1e20988-5d77-42cf-8805-c45adb7a772d to disappear
May 18 05:58:02.060: INFO: Pod downwardapi-volume-a1e20988-5d77-42cf-8805-c45adb7a772d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:58:02.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1928" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":337,"completed":269,"skipped":4418,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:58:02.076: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:58:02.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6678" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":337,"completed":270,"skipped":4444,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:58:02.219: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 18 05:58:06.783: INFO: Successfully updated pod "adopt-release-78ttt"
STEP: Checking that the Job readopts the Pod
May 18 05:58:06.783: INFO: Waiting up to 15m0s for pod "adopt-release-78ttt" in namespace "job-340" to be "adopted"
May 18 05:58:06.799: INFO: Pod "adopt-release-78ttt": Phase="Running", Reason="", readiness=true. Elapsed: 15.63653ms
May 18 05:58:08.802: INFO: Pod "adopt-release-78ttt": Phase="Running", Reason="", readiness=true. Elapsed: 2.018827348s
May 18 05:58:08.802: INFO: Pod "adopt-release-78ttt" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 18 05:58:09.312: INFO: Successfully updated pod "adopt-release-78ttt"
STEP: Checking that the Job releases the Pod
May 18 05:58:09.312: INFO: Waiting up to 15m0s for pod "adopt-release-78ttt" in namespace "job-340" to be "released"
May 18 05:58:09.322: INFO: Pod "adopt-release-78ttt": Phase="Running", Reason="", readiness=true. Elapsed: 9.882656ms
May 18 05:58:11.327: INFO: Pod "adopt-release-78ttt": Phase="Running", Reason="", readiness=true. Elapsed: 2.015239664s
May 18 05:58:11.328: INFO: Pod "adopt-release-78ttt" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:58:11.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-340" for this suite.

• [SLOW TEST:9.116 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":337,"completed":271,"skipped":4449,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:58:11.335: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-1806
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 18 05:58:11.369: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 18 05:58:11.407: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 18 05:58:13.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:58:15.412: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:58:17.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:58:19.412: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:58:21.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:58:23.412: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:58:25.417: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:58:27.412: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:58:29.411: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 05:58:31.414: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 18 05:58:31.423: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 18 05:58:33.441: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 18 05:58:33.441: INFO: Breadth first check of 172.30.166.186 on host 172.28.128.12...
May 18 05:58:33.443: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.166.187:9080/dial?request=hostname&protocol=udp&host=172.30.166.186&port=8081&tries=1'] Namespace:pod-network-test-1806 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:58:33.443: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:58:33.512: INFO: Waiting for responses: map[]
May 18 05:58:33.512: INFO: reached 172.30.166.186 after 0/1 tries
May 18 05:58:33.512: INFO: Breadth first check of 172.30.104.62 on host 172.28.128.13...
May 18 05:58:33.514: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.166.187:9080/dial?request=hostname&protocol=udp&host=172.30.104.62&port=8081&tries=1'] Namespace:pod-network-test-1806 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 05:58:33.514: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 05:58:33.578: INFO: Waiting for responses: map[]
May 18 05:58:33.578: INFO: reached 172.30.104.62 after 0/1 tries
May 18 05:58:33.578: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:58:33.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1806" for this suite.

• [SLOW TEST:22.250 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":337,"completed":272,"skipped":4461,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:58:33.585: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:58:33.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8066" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":337,"completed":273,"skipped":4467,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:58:33.640: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 05:58:34.062: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:58:37.080: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:58:37.084: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8037-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:58:40.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9859" for this suite.
STEP: Destroying namespace "webhook-9859-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.575 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":337,"completed":274,"skipped":4467,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:58:40.215: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1386
STEP: creating an pod
May 18 05:58:40.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1178 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
May 18 05:58:40.464: INFO: stderr: ""
May 18 05:58:40.464: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
May 18 05:58:40.465: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 18 05:58:40.465: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1178" to be "running and ready, or succeeded"
May 18 05:58:40.469: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.230981ms
May 18 05:58:42.476: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.011273982s
May 18 05:58:42.476: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 18 05:58:42.476: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 18 05:58:42.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1178 logs logs-generator logs-generator'
May 18 05:58:42.603: INFO: stderr: ""
May 18 05:58:42.603: INFO: stdout: "I0518 05:58:41.938998       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/hx4 354\nI0518 05:58:42.136170       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/555 422\nI0518 05:58:42.337219       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/99z 470\nI0518 05:58:42.536638       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/8d94 446\n"
STEP: limiting log lines
May 18 05:58:42.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1178 logs logs-generator logs-generator --tail=1'
May 18 05:58:42.751: INFO: stderr: ""
May 18 05:58:42.751: INFO: stdout: "I0518 05:58:42.735935       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/sqql 236\n"
May 18 05:58:42.751: INFO: got output "I0518 05:58:42.735935       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/sqql 236\n"
STEP: limiting log bytes
May 18 05:58:42.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1178 logs logs-generator logs-generator --limit-bytes=1'
May 18 05:58:42.898: INFO: stderr: ""
May 18 05:58:42.898: INFO: stdout: "I"
May 18 05:58:42.898: INFO: got output "I"
STEP: exposing timestamps
May 18 05:58:42.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1178 logs logs-generator logs-generator --tail=1 --timestamps'
May 18 05:58:43.080: INFO: stderr: ""
May 18 05:58:43.080: INFO: stdout: "2021-05-18T05:58:42.937175123Z I0518 05:58:42.936953       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/tqw 448\n"
May 18 05:58:43.080: INFO: got output "2021-05-18T05:58:42.937175123Z I0518 05:58:42.936953       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/tqw 448\n"
STEP: restricting to a time range
May 18 05:58:45.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1178 logs logs-generator logs-generator --since=1s'
May 18 05:58:45.667: INFO: stderr: ""
May 18 05:58:45.667: INFO: stdout: "I0518 05:58:44.736852       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/fzd 426\nI0518 05:58:44.936279       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/99vc 476\nI0518 05:58:45.135875       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/9qk 229\nI0518 05:58:45.336226       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/n5r 456\nI0518 05:58:45.536643       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/tmc 443\n"
May 18 05:58:45.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1178 logs logs-generator logs-generator --since=24h'
May 18 05:58:45.737: INFO: stderr: ""
May 18 05:58:45.737: INFO: stdout: "I0518 05:58:41.938998       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/hx4 354\nI0518 05:58:42.136170       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/555 422\nI0518 05:58:42.337219       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/99z 470\nI0518 05:58:42.536638       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/8d94 446\nI0518 05:58:42.735935       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/sqql 236\nI0518 05:58:42.936953       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/tqw 448\nI0518 05:58:43.136673       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/c58h 203\nI0518 05:58:43.335863       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/gzg 311\nI0518 05:58:43.536362       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/v6x 452\nI0518 05:58:43.736723       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/fszf 505\nI0518 05:58:43.936073       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/td8t 371\nI0518 05:58:44.136548       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/xd2 347\nI0518 05:58:44.335886       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/kph 301\nI0518 05:58:44.536311       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/2vg 282\nI0518 05:58:44.736852       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/fzd 426\nI0518 05:58:44.936279       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/99vc 476\nI0518 05:58:45.135875       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/9qk 229\nI0518 05:58:45.336226       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/n5r 456\nI0518 05:58:45.536643       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/tmc 443\nI0518 05:58:45.735943       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/llh6 391\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1391
May 18 05:58:45.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-1178 delete pod logs-generator'
May 18 05:58:47.376: INFO: stderr: ""
May 18 05:58:47.376: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:58:47.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1178" for this suite.

• [SLOW TEST:7.174 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1383
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":337,"completed":275,"skipped":4469,"failed":0}
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:58:47.389: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:58:47.422: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 18 05:58:47.428: INFO: Pod name sample-pod: Found 0 pods out of 1
May 18 05:58:52.437: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 18 05:58:52.437: INFO: Creating deployment "test-rolling-update-deployment"
May 18 05:58:52.442: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 18 05:58:52.457: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 18 05:58:54.467: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 18 05:58:54.470: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 18 05:58:54.479: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4511  a7193a55-e357-46fa-9114-a30f2c5b9eef 43879 1 2021-05-18 05:58:52 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-05-18 05:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-18 05:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0064f06f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-18 05:58:52 +0000 UTC,LastTransitionTime:2021-05-18 05:58:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-05-18 05:58:54 +0000 UTC,LastTransitionTime:2021-05-18 05:58:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 18 05:58:54.482: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-4511  4755c9d4-c96d-4872-87c4-05ab340ad459 43868 1 2021-05-18 05:58:52 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a7193a55-e357-46fa-9114-a30f2c5b9eef 0xc0064f0be7 0xc0064f0be8}] []  [{kube-controller-manager Update apps/v1 2021-05-18 05:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7193a55-e357-46fa-9114-a30f2c5b9eef\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0064f0c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 18 05:58:54.482: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 18 05:58:54.482: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4511  61919eb7-8a43-4bb6-9e68-56b720abf03c 43878 2 2021-05-18 05:58:47 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a7193a55-e357-46fa-9114-a30f2c5b9eef 0xc0064f0ab7 0xc0064f0ab8}] []  [{e2e.test Update apps/v1 2021-05-18 05:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-18 05:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7193a55-e357-46fa-9114-a30f2c5b9eef\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0064f0b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 18 05:58:54.485: INFO: Pod "test-rolling-update-deployment-585b757574-24db7" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-24db7 test-rolling-update-deployment-585b757574- deployment-4511  4da6a653-fdbe-44ad-b078-e2c16094c5d7 43867 0 2021-05-18 05:58:52 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/podIP:172.30.166.191/32 cni.projectcalico.org/podIPs:172.30.166.191/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 4755c9d4-c96d-4872-87c4-05ab340ad459 0xc0064f10b7 0xc0064f10b8}] []  [{kube-controller-manager Update v1 2021-05-18 05:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4755c9d4-c96d-4872-87c4-05ab340ad459\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-18 05:58:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-18 05:58:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.166.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hxwdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxwdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*60,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-18 05:58:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.28.128.12,PodIP:172.30.166.191,StartTime:2021-05-18 05:58:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-18 05:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://60f71a1f7548fe69b824f95705e76c05edfa1f7aee523771d2bb78229a5f66ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.166.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:58:54.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4511" for this suite.

• [SLOW TEST:7.105 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":337,"completed":276,"skipped":4469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:58:54.494: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:58:54.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-9903 version'
May 18 05:58:54.625: INFO: stderr: ""
May 18 05:58:54.625: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.1\", GitCommit:\"5e58841cce77d4bc13713ad2b91fa0d961e69192\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T14:18:45Z\", GoVersion:\"go1.16.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.1\", GitCommit:\"5e58841cce77d4bc13713ad2b91fa0d961e69192\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T14:12:29Z\", GoVersion:\"go1.16.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:58:54.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9903" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":337,"completed":277,"skipped":4494,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:58:54.633: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 18 05:58:55.064: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 05:58:58.087: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 05:58:58.090: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:59:01.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5709" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.581 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":337,"completed":278,"skipped":4495,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:59:01.215: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 05:59:01.269: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9364fb74-354a-4134-8e68-11fb35851b7e" in namespace "downward-api-2814" to be "Succeeded or Failed"
May 18 05:59:01.275: INFO: Pod "downwardapi-volume-9364fb74-354a-4134-8e68-11fb35851b7e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079572ms
May 18 05:59:03.288: INFO: Pod "downwardapi-volume-9364fb74-354a-4134-8e68-11fb35851b7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019326045s
STEP: Saw pod success
May 18 05:59:03.288: INFO: Pod "downwardapi-volume-9364fb74-354a-4134-8e68-11fb35851b7e" satisfied condition "Succeeded or Failed"
May 18 05:59:03.294: INFO: Trying to get logs from node node2 pod downwardapi-volume-9364fb74-354a-4134-8e68-11fb35851b7e container client-container: <nil>
STEP: delete the pod
May 18 05:59:03.339: INFO: Waiting for pod downwardapi-volume-9364fb74-354a-4134-8e68-11fb35851b7e to disappear
May 18 05:59:03.349: INFO: Pod downwardapi-volume-9364fb74-354a-4134-8e68-11fb35851b7e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 05:59:03.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2814" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":337,"completed":279,"skipped":4508,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 05:59:03.365: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-6123
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
May 18 05:59:03.421: INFO: Found 0 stateful pods, waiting for 3
May 18 05:59:13.430: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 18 05:59:13.430: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 18 05:59:13.430: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
May 18 05:59:13.454: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 18 05:59:23.507: INFO: Updating stateful set ss2
May 18 05:59:23.534: INFO: Waiting for Pod statefulset-6123/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
May 18 05:59:33.592: INFO: Found 2 stateful pods, waiting for 3
May 18 05:59:43.621: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 18 05:59:43.621: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 18 05:59:43.621: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 18 05:59:43.644: INFO: Updating stateful set ss2
May 18 05:59:43.663: INFO: Waiting for Pod statefulset-6123/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
May 18 05:59:53.688: INFO: Updating stateful set ss2
May 18 05:59:53.697: INFO: Waiting for StatefulSet statefulset-6123/ss2 to complete update
May 18 05:59:53.697: INFO: Waiting for Pod statefulset-6123/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
May 18 06:00:03.708: INFO: Deleting all statefulset in ns statefulset-6123
May 18 06:00:03.710: INFO: Scaling statefulset ss2 to 0
May 18 06:00:33.727: INFO: Waiting for statefulset status.replicas updated to 0
May 18 06:00:33.732: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:00:33.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6123" for this suite.

• [SLOW TEST:90.454 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":337,"completed":280,"skipped":4514,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:00:33.819: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 18 06:00:38.004: INFO: DNS probes using dns-test-6a836221-84c0-4dc4-9ead-bb3e9c98d946 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 18 06:00:42.051: INFO: File wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 18 06:00:42.054: INFO: File jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 18 06:00:42.054: INFO: Lookups using dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 failed for: [wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local]

May 18 06:00:47.058: INFO: File wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 18 06:00:47.060: INFO: File jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 18 06:00:47.060: INFO: Lookups using dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 failed for: [wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local]

May 18 06:00:52.058: INFO: File wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 18 06:00:52.060: INFO: File jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 18 06:00:52.060: INFO: Lookups using dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 failed for: [wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local]

May 18 06:00:57.058: INFO: File wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 18 06:00:57.060: INFO: File jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 18 06:00:57.060: INFO: Lookups using dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 failed for: [wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local]

May 18 06:01:02.058: INFO: File wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 18 06:01:02.061: INFO: File jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 18 06:01:02.061: INFO: Lookups using dns-6694/dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 failed for: [wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local]

May 18 06:01:07.063: INFO: DNS probes using dns-test-263da00e-c2bb-4d92-8bb1-1fc7050ed426 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 18 06:01:11.146: INFO: DNS probes using dns-test-78911a46-5456-4ac0-858c-f46d1c8d41b3 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:01:11.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6694" for this suite.

• [SLOW TEST:37.384 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":337,"completed":281,"skipped":4517,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:01:11.203: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 18 06:01:13.264: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:01:13.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1391" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":337,"completed":282,"skipped":4550,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:01:13.292: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
May 18 06:01:13.343: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 18 06:01:15.350: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.28.128.13 on the node which pod1 resides and expect scheduled
May 18 06:01:15.358: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 18 06:01:17.362: INFO: The status of Pod pod2 is Running (Ready = false)
May 18 06:01:19.361: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.28.128.13 but use UDP protocol on the node which pod2 resides
May 18 06:01:19.370: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May 18 06:01:21.375: INFO: The status of Pod pod3 is Running (Ready = true)
May 18 06:01:21.392: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May 18 06:01:23.395: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
May 18 06:01:23.398: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.28.128.13 http://127.0.0.1:54323/hostname] Namespace:hostport-4228 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 06:01:23.398: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.28.128.13, port: 54323
May 18 06:01:23.503: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.28.128.13:54323/hostname] Namespace:hostport-4228 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 06:01:23.503: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.28.128.13, port: 54323 UDP
May 18 06:01:23.601: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.28.128.13 54323] Namespace:hostport-4228 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 06:01:23.601: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:01:28.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-4228" for this suite.

• [SLOW TEST:15.415 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":337,"completed":283,"skipped":4575,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:01:28.708: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 06:01:28.754: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 18 06:01:30.789: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:01:30.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5755" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":337,"completed":284,"skipped":4589,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:01:30.809: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May 18 06:01:30.854: INFO: Waiting up to 5m0s for pod "downward-api-eae32668-436c-4a9f-8179-c9c6f50db86d" in namespace "downward-api-9046" to be "Succeeded or Failed"
May 18 06:01:30.861: INFO: Pod "downward-api-eae32668-436c-4a9f-8179-c9c6f50db86d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.990568ms
May 18 06:01:32.872: INFO: Pod "downward-api-eae32668-436c-4a9f-8179-c9c6f50db86d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01825315s
May 18 06:01:34.879: INFO: Pod "downward-api-eae32668-436c-4a9f-8179-c9c6f50db86d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025664649s
STEP: Saw pod success
May 18 06:01:34.879: INFO: Pod "downward-api-eae32668-436c-4a9f-8179-c9c6f50db86d" satisfied condition "Succeeded or Failed"
May 18 06:01:34.884: INFO: Trying to get logs from node node1 pod downward-api-eae32668-436c-4a9f-8179-c9c6f50db86d container dapi-container: <nil>
STEP: delete the pod
May 18 06:01:34.937: INFO: Waiting for pod downward-api-eae32668-436c-4a9f-8179-c9c6f50db86d to disappear
May 18 06:01:34.944: INFO: Pod downward-api-eae32668-436c-4a9f-8179-c9c6f50db86d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:01:34.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9046" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":337,"completed":285,"skipped":4615,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:01:34.959: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-6699/configmap-test-dd8b4b21-49be-40bd-b892-9bac9a6b42dd
STEP: Creating a pod to test consume configMaps
May 18 06:01:35.039: INFO: Waiting up to 5m0s for pod "pod-configmaps-370d7dbf-35cb-44b3-ac6a-85008381c6ff" in namespace "configmap-6699" to be "Succeeded or Failed"
May 18 06:01:35.055: INFO: Pod "pod-configmaps-370d7dbf-35cb-44b3-ac6a-85008381c6ff": Phase="Pending", Reason="", readiness=false. Elapsed: 16.319126ms
May 18 06:01:37.061: INFO: Pod "pod-configmaps-370d7dbf-35cb-44b3-ac6a-85008381c6ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021704635s
May 18 06:01:39.066: INFO: Pod "pod-configmaps-370d7dbf-35cb-44b3-ac6a-85008381c6ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027230942s
STEP: Saw pod success
May 18 06:01:39.066: INFO: Pod "pod-configmaps-370d7dbf-35cb-44b3-ac6a-85008381c6ff" satisfied condition "Succeeded or Failed"
May 18 06:01:39.068: INFO: Trying to get logs from node node1 pod pod-configmaps-370d7dbf-35cb-44b3-ac6a-85008381c6ff container env-test: <nil>
STEP: delete the pod
May 18 06:01:39.082: INFO: Waiting for pod pod-configmaps-370d7dbf-35cb-44b3-ac6a-85008381c6ff to disappear
May 18 06:01:39.084: INFO: Pod pod-configmaps-370d7dbf-35cb-44b3-ac6a-85008381c6ff no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:01:39.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6699" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":337,"completed":286,"skipped":4687,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:01:39.094: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
May 18 06:01:39.141: INFO: Waiting up to 5m0s for pod "pod-87a4fb01-4765-42b2-9746-dd717790a0ab" in namespace "emptydir-1966" to be "Succeeded or Failed"
May 18 06:01:39.154: INFO: Pod "pod-87a4fb01-4765-42b2-9746-dd717790a0ab": Phase="Pending", Reason="", readiness=false. Elapsed: 12.694442ms
May 18 06:01:41.160: INFO: Pod "pod-87a4fb01-4765-42b2-9746-dd717790a0ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018468849s
May 18 06:01:43.171: INFO: Pod "pod-87a4fb01-4765-42b2-9746-dd717790a0ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02976733s
STEP: Saw pod success
May 18 06:01:43.171: INFO: Pod "pod-87a4fb01-4765-42b2-9746-dd717790a0ab" satisfied condition "Succeeded or Failed"
May 18 06:01:43.174: INFO: Trying to get logs from node node1 pod pod-87a4fb01-4765-42b2-9746-dd717790a0ab container test-container: <nil>
STEP: delete the pod
May 18 06:01:43.204: INFO: Waiting for pod pod-87a4fb01-4765-42b2-9746-dd717790a0ab to disappear
May 18 06:01:43.207: INFO: Pod pod-87a4fb01-4765-42b2-9746-dd717790a0ab no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:01:43.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1966" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":287,"skipped":4691,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:01:43.217: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 18 06:01:43.271: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 18 06:01:43.274: INFO: starting watch
STEP: patching
STEP: updating
May 18 06:01:43.284: INFO: waiting for watch events with expected annotations
May 18 06:01:43.284: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:01:43.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3012" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":337,"completed":288,"skipped":4701,"failed":0}
S
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:01:43.310: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0518 06:01:43.338913      19 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:03:01.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6781" for this suite.

• [SLOW TEST:78.079 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":337,"completed":289,"skipped":4702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:03:01.389: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 18 06:03:01.800: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 18 06:03:03.809: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756914581, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756914581, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756914581, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756914581, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 06:03:06.825: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 06:03:06.829: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:03:09.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1543" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.672 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":337,"completed":290,"skipped":4724,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:03:10.061: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
May 18 06:03:10.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-8598 cluster-info'
May 18 06:03:10.238: INFO: stderr: ""
May 18 06:03:10.239: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.24.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:03:10.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8598" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":337,"completed":291,"skipped":4735,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:03:10.247: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
May 18 06:03:10.289: INFO: Waiting up to 5m0s for pod "test-pod-873878fb-e309-4c67-8433-cf2ce7e0c0d4" in namespace "svcaccounts-2427" to be "Succeeded or Failed"
May 18 06:03:10.294: INFO: Pod "test-pod-873878fb-e309-4c67-8433-cf2ce7e0c0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.161376ms
May 18 06:03:12.299: INFO: Pod "test-pod-873878fb-e309-4c67-8433-cf2ce7e0c0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010225386s
STEP: Saw pod success
May 18 06:03:12.299: INFO: Pod "test-pod-873878fb-e309-4c67-8433-cf2ce7e0c0d4" satisfied condition "Succeeded or Failed"
May 18 06:03:12.308: INFO: Trying to get logs from node node2 pod test-pod-873878fb-e309-4c67-8433-cf2ce7e0c0d4 container agnhost-container: <nil>
STEP: delete the pod
May 18 06:03:12.349: INFO: Waiting for pod test-pod-873878fb-e309-4c67-8433-cf2ce7e0c0d4 to disappear
May 18 06:03:12.352: INFO: Pod test-pod-873878fb-e309-4c67-8433-cf2ce7e0c0d4 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:03:12.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2427" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":337,"completed":292,"skipped":4740,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:03:12.365: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 06:03:12.407: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 18 06:03:12.423: INFO: The status of Pod pod-exec-websocket-c8a50b9d-7c76-4122-ab1b-b8a744965852 is Pending, waiting for it to be Running (with Ready = true)
May 18 06:03:14.431: INFO: The status of Pod pod-exec-websocket-c8a50b9d-7c76-4122-ab1b-b8a744965852 is Pending, waiting for it to be Running (with Ready = true)
May 18 06:03:16.429: INFO: The status of Pod pod-exec-websocket-c8a50b9d-7c76-4122-ab1b-b8a744965852 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:03:16.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-638" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":337,"completed":293,"skipped":4746,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:03:16.495: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1450
May 18 06:03:16.535: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 18 06:03:18.540: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
May 18 06:03:18.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-1450 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 18 06:03:18.706: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 18 06:03:18.706: INFO: stdout: "iptables"
May 18 06:03:18.706: INFO: proxyMode: iptables
May 18 06:03:18.723: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 18 06:03:18.726: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1450
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1450
I0518 06:03:18.747376      19 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1450, replica count: 3
I0518 06:03:21.811063      19 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 18 06:03:21.814: INFO: Creating new exec pod
May 18 06:03:24.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-1450 exec execpod-affinityjn6xn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
May 18 06:03:24.977: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout 80\n+ echo hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 18 06:03:24.977: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 06:03:24.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-1450 exec execpod-affinityjn6xn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.24.87.9 80'
May 18 06:03:25.139: INFO: stderr: "+ nc -v -t -w 2 172.24.87.9 80\n+ echo hostName\nConnection to 172.24.87.9 80 port [tcp/http] succeeded!\n"
May 18 06:03:25.139: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 18 06:03:25.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-1450 exec execpod-affinityjn6xn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.87.9:80/ ; done'
May 18 06:03:25.358: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n"
May 18 06:03:25.358: INFO: stdout: "\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc\naffinity-clusterip-timeout-8t7dc"
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Received response from host: affinity-clusterip-timeout-8t7dc
May 18 06:03:25.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-1450 exec execpod-affinityjn6xn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.24.87.9:80/'
May 18 06:03:25.509: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n"
May 18 06:03:25.509: INFO: stdout: "affinity-clusterip-timeout-8t7dc"
May 18 06:03:45.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-1450 exec execpod-affinityjn6xn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.24.87.9:80/'
May 18 06:03:45.786: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.24.87.9:80/\n"
May 18 06:03:45.786: INFO: stdout: "affinity-clusterip-timeout-cp872"
May 18 06:03:45.786: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1450, will wait for the garbage collector to delete the pods
May 18 06:03:45.904: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 35.151541ms
May 18 06:03:46.005: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.218046ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:04:01.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1450" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:44.608 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":337,"completed":294,"skipped":4758,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:04:01.103: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 06:04:01.164: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:04:04.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1557" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":337,"completed":295,"skipped":4765,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:04:04.317: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
May 18 06:04:04.359: INFO: Waiting up to 5m0s for pod "pod-e7cb188d-f92d-44a2-9096-70a6d506baa5" in namespace "emptydir-3110" to be "Succeeded or Failed"
May 18 06:04:04.362: INFO: Pod "pod-e7cb188d-f92d-44a2-9096-70a6d506baa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.670488ms
May 18 06:04:06.370: INFO: Pod "pod-e7cb188d-f92d-44a2-9096-70a6d506baa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010869984s
STEP: Saw pod success
May 18 06:04:06.370: INFO: Pod "pod-e7cb188d-f92d-44a2-9096-70a6d506baa5" satisfied condition "Succeeded or Failed"
May 18 06:04:06.385: INFO: Trying to get logs from node node1 pod pod-e7cb188d-f92d-44a2-9096-70a6d506baa5 container test-container: <nil>
STEP: delete the pod
May 18 06:04:06.419: INFO: Waiting for pod pod-e7cb188d-f92d-44a2-9096-70a6d506baa5 to disappear
May 18 06:04:06.424: INFO: Pod pod-e7cb188d-f92d-44a2-9096-70a6d506baa5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:04:06.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3110" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":296,"skipped":4784,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:04:06.437: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
May 18 06:04:06.481: INFO: Waiting up to 5m0s for pod "pod-88679b2d-87e5-4d48-a427-4ebcd624407e" in namespace "emptydir-9998" to be "Succeeded or Failed"
May 18 06:04:06.485: INFO: Pod "pod-88679b2d-87e5-4d48-a427-4ebcd624407e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.640683ms
May 18 06:04:08.494: INFO: Pod "pod-88679b2d-87e5-4d48-a427-4ebcd624407e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012925974s
STEP: Saw pod success
May 18 06:04:08.494: INFO: Pod "pod-88679b2d-87e5-4d48-a427-4ebcd624407e" satisfied condition "Succeeded or Failed"
May 18 06:04:08.498: INFO: Trying to get logs from node node1 pod pod-88679b2d-87e5-4d48-a427-4ebcd624407e container test-container: <nil>
STEP: delete the pod
May 18 06:04:08.525: INFO: Waiting for pod pod-88679b2d-87e5-4d48-a427-4ebcd624407e to disappear
May 18 06:04:08.530: INFO: Pod pod-88679b2d-87e5-4d48-a427-4ebcd624407e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:04:08.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9998" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":297,"skipped":4805,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:04:08.540: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-e801ea53-355d-4cc6-8213-d661dbeac41b
STEP: Creating a pod to test consume configMaps
May 18 06:04:08.605: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98f62fa1-dc9f-4db2-8f88-e28cfe5cc306" in namespace "projected-1338" to be "Succeeded or Failed"
May 18 06:04:08.615: INFO: Pod "pod-projected-configmaps-98f62fa1-dc9f-4db2-8f88-e28cfe5cc306": Phase="Pending", Reason="", readiness=false. Elapsed: 9.729256ms
May 18 06:04:10.623: INFO: Pod "pod-projected-configmaps-98f62fa1-dc9f-4db2-8f88-e28cfe5cc306": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017221755s
STEP: Saw pod success
May 18 06:04:10.623: INFO: Pod "pod-projected-configmaps-98f62fa1-dc9f-4db2-8f88-e28cfe5cc306" satisfied condition "Succeeded or Failed"
May 18 06:04:10.628: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-98f62fa1-dc9f-4db2-8f88-e28cfe5cc306 container agnhost-container: <nil>
STEP: delete the pod
May 18 06:04:10.664: INFO: Waiting for pod pod-projected-configmaps-98f62fa1-dc9f-4db2-8f88-e28cfe5cc306 to disappear
May 18 06:04:10.669: INFO: Pod pod-projected-configmaps-98f62fa1-dc9f-4db2-8f88-e28cfe5cc306 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:04:10.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1338" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":337,"completed":298,"skipped":4825,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:04:10.682: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0518 06:04:11.784583      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 18 06:05:13.809: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:05:13.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2240" for this suite.

• [SLOW TEST:63.136 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":337,"completed":299,"skipped":4839,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:05:13.818: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-0dcf2833-a60a-4066-a2ab-292d58f7674c
STEP: Creating secret with name s-test-opt-upd-ec31c206-6744-464a-b835-d6629a4545a5
STEP: Creating the pod
May 18 06:05:13.875: INFO: The status of Pod pod-secrets-b00da678-415f-4c25-aa9c-0a364be48761 is Pending, waiting for it to be Running (with Ready = true)
May 18 06:05:15.887: INFO: The status of Pod pod-secrets-b00da678-415f-4c25-aa9c-0a364be48761 is Pending, waiting for it to be Running (with Ready = true)
May 18 06:05:17.887: INFO: The status of Pod pod-secrets-b00da678-415f-4c25-aa9c-0a364be48761 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-0dcf2833-a60a-4066-a2ab-292d58f7674c
STEP: Updating secret s-test-opt-upd-ec31c206-6744-464a-b835-d6629a4545a5
STEP: Creating secret with name s-test-opt-create-ac18c200-cd44-4753-8b07-6ff8c7f3e6dc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:06:24.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4851" for this suite.

• [SLOW TEST:70.574 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":337,"completed":300,"skipped":4844,"failed":0}
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:06:24.392: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 06:06:24.522: INFO: The status of Pod busybox-readonly-fs2b0f72a0-bf54-4086-bd1e-7964e1ff2fb9 is Pending, waiting for it to be Running (with Ready = true)
May 18 06:06:26.530: INFO: The status of Pod busybox-readonly-fs2b0f72a0-bf54-4086-bd1e-7964e1ff2fb9 is Pending, waiting for it to be Running (with Ready = true)
May 18 06:06:28.546: INFO: The status of Pod busybox-readonly-fs2b0f72a0-bf54-4086-bd1e-7964e1ff2fb9 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:06:28.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3749" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":301,"skipped":4844,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:06:28.604: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May 18 06:06:28.748: INFO: Waiting up to 5m0s for pod "downward-api-869269b9-54a0-4d5b-b7e9-c11dadcdd9ae" in namespace "downward-api-7562" to be "Succeeded or Failed"
May 18 06:06:28.776: INFO: Pod "downward-api-869269b9-54a0-4d5b-b7e9-c11dadcdd9ae": Phase="Pending", Reason="", readiness=false. Elapsed: 27.348476ms
May 18 06:06:30.810: INFO: Pod "downward-api-869269b9-54a0-4d5b-b7e9-c11dadcdd9ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062033251s
May 18 06:06:32.834: INFO: Pod "downward-api-869269b9-54a0-4d5b-b7e9-c11dadcdd9ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085782976s
STEP: Saw pod success
May 18 06:06:32.834: INFO: Pod "downward-api-869269b9-54a0-4d5b-b7e9-c11dadcdd9ae" satisfied condition "Succeeded or Failed"
May 18 06:06:32.840: INFO: Trying to get logs from node node1 pod downward-api-869269b9-54a0-4d5b-b7e9-c11dadcdd9ae container dapi-container: <nil>
STEP: delete the pod
May 18 06:06:32.872: INFO: Waiting for pod downward-api-869269b9-54a0-4d5b-b7e9-c11dadcdd9ae to disappear
May 18 06:06:32.887: INFO: Pod downward-api-869269b9-54a0-4d5b-b7e9-c11dadcdd9ae no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:06:32.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7562" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":337,"completed":302,"skipped":4853,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:06:32.905: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 06:06:32.993: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 18 06:06:33.010: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:33.025: INFO: Number of nodes with available pods: 0
May 18 06:06:33.025: INFO: Node node1 is running more than one daemon pod
May 18 06:06:34.105: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:34.129: INFO: Number of nodes with available pods: 0
May 18 06:06:34.164: INFO: Node node1 is running more than one daemon pod
May 18 06:06:35.038: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:35.046: INFO: Number of nodes with available pods: 0
May 18 06:06:35.046: INFO: Node node1 is running more than one daemon pod
May 18 06:06:36.035: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:36.039: INFO: Number of nodes with available pods: 2
May 18 06:06:36.039: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 18 06:06:36.076: INFO: Wrong image for pod: daemon-set-k7kvg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 18 06:06:36.076: INFO: Wrong image for pod: daemon-set-wqbpj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 18 06:06:36.095: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:37.108: INFO: Wrong image for pod: daemon-set-k7kvg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 18 06:06:37.113: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:38.102: INFO: Wrong image for pod: daemon-set-k7kvg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 18 06:06:38.107: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:39.101: INFO: Wrong image for pod: daemon-set-k7kvg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 18 06:06:39.107: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:40.102: INFO: Pod daemon-set-jl56c is not available
May 18 06:06:40.102: INFO: Wrong image for pod: daemon-set-k7kvg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 18 06:06:40.106: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:41.098: INFO: Pod daemon-set-jl56c is not available
May 18 06:06:41.098: INFO: Wrong image for pod: daemon-set-k7kvg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 18 06:06:41.101: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:42.103: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:43.113: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:44.106: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:45.108: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:46.106: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:47.102: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:48.102: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:49.103: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:50.103: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:51.181: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:52.109: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:53.113: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:54.139: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:55.107: INFO: Pod daemon-set-7vwtt is not available
May 18 06:06:55.122: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 18 06:06:55.133: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:55.147: INFO: Number of nodes with available pods: 1
May 18 06:06:55.147: INFO: Node node2 is running more than one daemon pod
May 18 06:06:56.161: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:56.166: INFO: Number of nodes with available pods: 1
May 18 06:06:56.166: INFO: Node node2 is running more than one daemon pod
May 18 06:06:57.157: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 18 06:06:57.162: INFO: Number of nodes with available pods: 2
May 18 06:06:57.162: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-89, will wait for the garbage collector to delete the pods
May 18 06:06:57.247: INFO: Deleting DaemonSet.extensions daemon-set took: 7.672065ms
May 18 06:06:57.348: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.165346ms
May 18 06:07:10.968: INFO: Number of nodes with available pods: 0
May 18 06:07:10.968: INFO: Number of running nodes: 0, number of available pods: 0
May 18 06:07:10.971: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46377"},"items":null}

May 18 06:07:10.974: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46377"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:07:10.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-89" for this suite.

• [SLOW TEST:38.092 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":337,"completed":303,"skipped":4860,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:07:10.998: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 06:07:11.076: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 18 06:07:15.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-987 --namespace=crd-publish-openapi-987 create -f -'
May 18 06:07:16.914: INFO: stderr: ""
May 18 06:07:16.914: INFO: stdout: "e2e-test-crd-publish-openapi-5183-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 18 06:07:16.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-987 --namespace=crd-publish-openapi-987 delete e2e-test-crd-publish-openapi-5183-crds test-cr'
May 18 06:07:17.042: INFO: stderr: ""
May 18 06:07:17.042: INFO: stdout: "e2e-test-crd-publish-openapi-5183-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 18 06:07:17.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-987 --namespace=crd-publish-openapi-987 apply -f -'
May 18 06:07:17.434: INFO: stderr: ""
May 18 06:07:17.434: INFO: stdout: "e2e-test-crd-publish-openapi-5183-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 18 06:07:17.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-987 --namespace=crd-publish-openapi-987 delete e2e-test-crd-publish-openapi-5183-crds test-cr'
May 18 06:07:17.597: INFO: stderr: ""
May 18 06:07:17.597: INFO: stdout: "e2e-test-crd-publish-openapi-5183-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 18 06:07:17.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-987 explain e2e-test-crd-publish-openapi-5183-crds'
May 18 06:07:18.015: INFO: stderr: ""
May 18 06:07:18.015: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5183-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:07:22.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-987" for this suite.

• [SLOW TEST:11.477 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":337,"completed":304,"skipped":4865,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:07:22.475: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 18 06:07:22.513: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5750  c52c6b9b-1a91-454e-823b-1de9a5bb657a 46462 0 2021-05-18 06:07:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-18 06:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 06:07:22.513: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5750  c52c6b9b-1a91-454e-823b-1de9a5bb657a 46462 0 2021-05-18 06:07:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-18 06:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 18 06:07:32.521: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5750  c52c6b9b-1a91-454e-823b-1de9a5bb657a 46486 0 2021-05-18 06:07:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-18 06:07:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 06:07:32.521: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5750  c52c6b9b-1a91-454e-823b-1de9a5bb657a 46486 0 2021-05-18 06:07:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-18 06:07:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 18 06:07:42.529: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5750  c52c6b9b-1a91-454e-823b-1de9a5bb657a 46501 0 2021-05-18 06:07:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-18 06:07:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 06:07:42.529: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5750  c52c6b9b-1a91-454e-823b-1de9a5bb657a 46501 0 2021-05-18 06:07:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-18 06:07:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 18 06:07:52.534: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5750  c52c6b9b-1a91-454e-823b-1de9a5bb657a 46517 0 2021-05-18 06:07:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-18 06:07:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 06:07:52.534: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5750  c52c6b9b-1a91-454e-823b-1de9a5bb657a 46517 0 2021-05-18 06:07:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-18 06:07:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 18 06:08:02.540: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5750  9eadef08-c6ef-403d-af51-812e2cf8bcc2 46532 0 2021-05-18 06:08:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-18 06:08:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 06:08:02.540: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5750  9eadef08-c6ef-403d-af51-812e2cf8bcc2 46532 0 2021-05-18 06:08:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-18 06:08:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 18 06:08:12.545: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5750  9eadef08-c6ef-403d-af51-812e2cf8bcc2 46547 0 2021-05-18 06:08:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-18 06:08:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 06:08:12.545: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5750  9eadef08-c6ef-403d-af51-812e2cf8bcc2 46547 0 2021-05-18 06:08:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-18 06:08:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:08:22.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5750" for this suite.

• [SLOW TEST:60.086 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":337,"completed":305,"skipped":4905,"failed":0}
S
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:08:22.561: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:08:22.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9646" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":337,"completed":306,"skipped":4906,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:08:22.623: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-66149cc8-afbe-4ff4-bc97-fba87c5512d7
STEP: Creating a pod to test consume secrets
May 18 06:08:22.650: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-449e7487-cbc6-48e0-bbf6-d3d5d0e013ba" in namespace "projected-5444" to be "Succeeded or Failed"
May 18 06:08:22.653: INFO: Pod "pod-projected-secrets-449e7487-cbc6-48e0-bbf6-d3d5d0e013ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.684383ms
May 18 06:08:24.662: INFO: Pod "pod-projected-secrets-449e7487-cbc6-48e0-bbf6-d3d5d0e013ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011928078s
STEP: Saw pod success
May 18 06:08:24.662: INFO: Pod "pod-projected-secrets-449e7487-cbc6-48e0-bbf6-d3d5d0e013ba" satisfied condition "Succeeded or Failed"
May 18 06:08:24.665: INFO: Trying to get logs from node node2 pod pod-projected-secrets-449e7487-cbc6-48e0-bbf6-d3d5d0e013ba container projected-secret-volume-test: <nil>
STEP: delete the pod
May 18 06:08:24.700: INFO: Waiting for pod pod-projected-secrets-449e7487-cbc6-48e0-bbf6-d3d5d0e013ba to disappear
May 18 06:08:24.703: INFO: Pod pod-projected-secrets-449e7487-cbc6-48e0-bbf6-d3d5d0e013ba no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:08:24.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5444" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":337,"completed":307,"skipped":4919,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:08:24.713: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May 18 06:08:24.756: INFO: Waiting up to 5m0s for pod "downward-api-e553ab00-1f77-4a97-b691-afc7ca63def9" in namespace "downward-api-4640" to be "Succeeded or Failed"
May 18 06:08:24.763: INFO: Pod "downward-api-e553ab00-1f77-4a97-b691-afc7ca63def9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.420167ms
May 18 06:08:26.772: INFO: Pod "downward-api-e553ab00-1f77-4a97-b691-afc7ca63def9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015990461s
STEP: Saw pod success
May 18 06:08:26.772: INFO: Pod "downward-api-e553ab00-1f77-4a97-b691-afc7ca63def9" satisfied condition "Succeeded or Failed"
May 18 06:08:26.779: INFO: Trying to get logs from node node2 pod downward-api-e553ab00-1f77-4a97-b691-afc7ca63def9 container dapi-container: <nil>
STEP: delete the pod
May 18 06:08:26.807: INFO: Waiting for pod downward-api-e553ab00-1f77-4a97-b691-afc7ca63def9 to disappear
May 18 06:08:26.812: INFO: Pod downward-api-e553ab00-1f77-4a97-b691-afc7ca63def9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:08:26.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4640" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":337,"completed":308,"skipped":4924,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:08:26.822: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:08:28.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8955" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":337,"completed":309,"skipped":4990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:08:28.978: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
May 18 06:08:29.076: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:08:47.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3367" for this suite.

• [SLOW TEST:18.680 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":337,"completed":310,"skipped":5052,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:08:47.658: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 18 06:08:47.705: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-929  954c33ce-6e2a-49d3-aba5-06381d1e8533 46731 0 2021-05-18 06:08:47 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-18 06:08:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 18 06:08:47.705: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-929  954c33ce-6e2a-49d3-aba5-06381d1e8533 46732 0 2021-05-18 06:08:47 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-18 06:08:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:08:47.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-929" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":337,"completed":311,"skipped":5058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:08:47.712: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-kbbz8 in namespace proxy-8809
I0518 06:08:47.755684      19 runners.go:190] Created replication controller with name: proxy-service-kbbz8, namespace: proxy-8809, replica count: 1
I0518 06:08:48.807132      19 runners.go:190] proxy-service-kbbz8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0518 06:08:49.809193      19 runners.go:190] proxy-service-kbbz8 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 18 06:08:49.814: INFO: setup took 2.078575976s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 18 06:08:49.822: INFO: (0) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 8.192863ms)
May 18 06:08:49.822: INFO: (0) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 8.404962ms)
May 18 06:08:49.822: INFO: (0) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 7.483866ms)
May 18 06:08:49.823: INFO: (0) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 8.79936ms)
May 18 06:08:49.823: INFO: (0) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 8.482861ms)
May 18 06:08:49.823: INFO: (0) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 8.78606ms)
May 18 06:08:49.823: INFO: (0) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 8.942159ms)
May 18 06:08:49.823: INFO: (0) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 8.376262ms)
May 18 06:08:49.826: INFO: (0) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 12.417043ms)
May 18 06:08:49.830: INFO: (0) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 15.46453ms)
May 18 06:08:49.830: INFO: (0) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 15.43753ms)
May 18 06:08:49.830: INFO: (0) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 15.675829ms)
May 18 06:08:49.830: INFO: (0) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 16.142427ms)
May 18 06:08:49.830: INFO: (0) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 15.951228ms)
May 18 06:08:49.832: INFO: (0) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 17.78142ms)
May 18 06:08:49.832: INFO: (0) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 17.69232ms)
May 18 06:08:49.835: INFO: (1) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 3.498084ms)
May 18 06:08:49.836: INFO: (1) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 3.672084ms)
May 18 06:08:49.836: INFO: (1) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 3.590083ms)
May 18 06:08:49.837: INFO: (1) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 5.139277ms)
May 18 06:08:49.837: INFO: (1) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 5.342276ms)
May 18 06:08:49.837: INFO: (1) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 5.420475ms)
May 18 06:08:49.838: INFO: (1) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 5.419875ms)
May 18 06:08:49.838: INFO: (1) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 5.296976ms)
May 18 06:08:49.838: INFO: (1) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 5.298276ms)
May 18 06:08:49.838: INFO: (1) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 6.218872ms)
May 18 06:08:49.840: INFO: (1) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 7.622165ms)
May 18 06:08:49.840: INFO: (1) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 7.761165ms)
May 18 06:08:49.840: INFO: (1) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 7.768365ms)
May 18 06:08:49.840: INFO: (1) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 7.913264ms)
May 18 06:08:49.840: INFO: (1) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 7.723165ms)
May 18 06:08:49.840: INFO: (1) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 7.818865ms)
May 18 06:08:49.845: INFO: (2) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 4.36708ms)
May 18 06:08:49.845: INFO: (2) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 4.566379ms)
May 18 06:08:49.845: INFO: (2) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 4.660579ms)
May 18 06:08:49.845: INFO: (2) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 4.440379ms)
May 18 06:08:49.845: INFO: (2) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 4.546279ms)
May 18 06:08:49.845: INFO: (2) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 4.670279ms)
May 18 06:08:49.845: INFO: (2) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 4.927678ms)
May 18 06:08:49.848: INFO: (2) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 7.915664ms)
May 18 06:08:49.848: INFO: (2) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 7.757065ms)
May 18 06:08:49.848: INFO: (2) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 7.799365ms)
May 18 06:08:49.848: INFO: (2) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 7.759365ms)
May 18 06:08:49.848: INFO: (2) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 8.013564ms)
May 18 06:08:49.848: INFO: (2) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 7.944364ms)
May 18 06:08:49.848: INFO: (2) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 7.960964ms)
May 18 06:08:49.848: INFO: (2) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 8.107263ms)
May 18 06:08:49.848: INFO: (2) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 8.255463ms)
May 18 06:08:49.855: INFO: (3) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 5.330576ms)
May 18 06:08:49.855: INFO: (3) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 6.247672ms)
May 18 06:08:49.855: INFO: (3) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 6.61167ms)
May 18 06:08:49.855: INFO: (3) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 6.142372ms)
May 18 06:08:49.855: INFO: (3) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 6.996768ms)
May 18 06:08:49.855: INFO: (3) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 6.025173ms)
May 18 06:08:49.856: INFO: (3) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 7.074368ms)
May 18 06:08:49.856: INFO: (3) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 7.389967ms)
May 18 06:08:49.856: INFO: (3) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 6.970768ms)
May 18 06:08:49.856: INFO: (3) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 7.051868ms)
May 18 06:08:49.857: INFO: (3) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 8.017664ms)
May 18 06:08:49.857: INFO: (3) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 8.88006ms)
May 18 06:08:49.857: INFO: (3) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 7.975564ms)
May 18 06:08:49.857: INFO: (3) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 8.583761ms)
May 18 06:08:49.858: INFO: (3) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 9.317157ms)
May 18 06:08:49.858: INFO: (3) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 9.758955ms)
May 18 06:08:49.862: INFO: (4) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 3.053386ms)
May 18 06:08:49.862: INFO: (4) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 2.987487ms)
May 18 06:08:49.863: INFO: (4) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 4.130181ms)
May 18 06:08:49.863: INFO: (4) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 4.278881ms)
May 18 06:08:49.863: INFO: (4) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 4.47968ms)
May 18 06:08:49.863: INFO: (4) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 4.543579ms)
May 18 06:08:49.863: INFO: (4) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 4.35368ms)
May 18 06:08:49.863: INFO: (4) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 4.120782ms)
May 18 06:08:49.865: INFO: (4) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 6.955569ms)
May 18 06:08:49.865: INFO: (4) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 6.891869ms)
May 18 06:08:49.865: INFO: (4) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 6.45587ms)
May 18 06:08:49.869: INFO: (4) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 9.675956ms)
May 18 06:08:49.869: INFO: (4) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 9.467757ms)
May 18 06:08:49.869: INFO: (4) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 9.768356ms)
May 18 06:08:49.869: INFO: (4) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 9.450157ms)
May 18 06:08:49.869: INFO: (4) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 9.574057ms)
May 18 06:08:49.874: INFO: (5) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 4.137182ms)
May 18 06:08:49.874: INFO: (5) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 4.756678ms)
May 18 06:08:49.874: INFO: (5) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 4.480279ms)
May 18 06:08:49.874: INFO: (5) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 5.005377ms)
May 18 06:08:49.874: INFO: (5) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 4.780678ms)
May 18 06:08:49.874: INFO: (5) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 4.915978ms)
May 18 06:08:49.874: INFO: (5) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 5.151477ms)
May 18 06:08:49.874: INFO: (5) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 5.527875ms)
May 18 06:08:49.874: INFO: (5) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 4.859878ms)
May 18 06:08:49.875: INFO: (5) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 5.502675ms)
May 18 06:08:49.875: INFO: (5) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 6.176372ms)
May 18 06:08:49.876: INFO: (5) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 6.275072ms)
May 18 06:08:49.876: INFO: (5) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 6.327572ms)
May 18 06:08:49.876: INFO: (5) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 6.78127ms)
May 18 06:08:49.876: INFO: (5) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 6.581171ms)
May 18 06:08:49.876: INFO: (5) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 6.272571ms)
May 18 06:08:49.884: INFO: (6) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 8.168563ms)
May 18 06:08:49.884: INFO: (6) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 7.615866ms)
May 18 06:08:49.884: INFO: (6) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 7.909064ms)
May 18 06:08:49.885: INFO: (6) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 8.373662ms)
May 18 06:08:49.886: INFO: (6) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 9.562957ms)
May 18 06:08:49.886: INFO: (6) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 9.519757ms)
May 18 06:08:49.886: INFO: (6) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 9.662956ms)
May 18 06:08:49.886: INFO: (6) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 9.907455ms)
May 18 06:08:49.886: INFO: (6) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 10.117055ms)
May 18 06:08:49.886: INFO: (6) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 10.384053ms)
May 18 06:08:49.886: INFO: (6) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 10.217254ms)
May 18 06:08:49.886: INFO: (6) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 10.132654ms)
May 18 06:08:49.886: INFO: (6) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 10.106955ms)
May 18 06:08:49.886: INFO: (6) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 9.919055ms)
May 18 06:08:49.886: INFO: (6) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 10.135854ms)
May 18 06:08:49.886: INFO: (6) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 10.615552ms)
May 18 06:08:49.891: INFO: (7) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 3.966282ms)
May 18 06:08:49.892: INFO: (7) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 5.501775ms)
May 18 06:08:49.892: INFO: (7) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 5.524375ms)
May 18 06:08:49.892: INFO: (7) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 5.607374ms)
May 18 06:08:49.892: INFO: (7) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 5.886873ms)
May 18 06:08:49.892: INFO: (7) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 5.697574ms)
May 18 06:08:49.892: INFO: (7) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 5.264576ms)
May 18 06:08:49.892: INFO: (7) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 5.424976ms)
May 18 06:08:49.892: INFO: (7) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 5.477875ms)
May 18 06:08:49.893: INFO: (7) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 5.556975ms)
May 18 06:08:49.894: INFO: (7) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 6.383871ms)
May 18 06:08:49.894: INFO: (7) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 6.996069ms)
May 18 06:08:49.894: INFO: (7) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 6.915769ms)
May 18 06:08:49.894: INFO: (7) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 7.603866ms)
May 18 06:08:49.894: INFO: (7) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 7.292967ms)
May 18 06:08:49.894: INFO: (7) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 7.402367ms)
May 18 06:08:49.898: INFO: (8) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 3.749983ms)
May 18 06:08:49.898: INFO: (8) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 3.964082ms)
May 18 06:08:49.898: INFO: (8) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 3.764283ms)
May 18 06:08:49.898: INFO: (8) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 3.730583ms)
May 18 06:08:49.898: INFO: (8) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 3.640383ms)
May 18 06:08:49.899: INFO: (8) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 4.47388ms)
May 18 06:08:49.902: INFO: (8) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 6.968168ms)
May 18 06:08:49.902: INFO: (8) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 7.209267ms)
May 18 06:08:49.902: INFO: (8) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 7.046268ms)
May 18 06:08:49.902: INFO: (8) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 7.139868ms)
May 18 06:08:49.902: INFO: (8) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 7.206567ms)
May 18 06:08:49.902: INFO: (8) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 7.158367ms)
May 18 06:08:49.902: INFO: (8) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 7.190167ms)
May 18 06:08:49.904: INFO: (8) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 8.952259ms)
May 18 06:08:49.904: INFO: (8) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 8.997259ms)
May 18 06:08:49.904: INFO: (8) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 9.014459ms)
May 18 06:08:49.909: INFO: (9) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 5.216577ms)
May 18 06:08:49.909: INFO: (9) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 5.620275ms)
May 18 06:08:49.910: INFO: (9) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 5.341776ms)
May 18 06:08:49.910: INFO: (9) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 5.729974ms)
May 18 06:08:49.910: INFO: (9) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 5.574875ms)
May 18 06:08:49.910: INFO: (9) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 5.659375ms)
May 18 06:08:49.910: INFO: (9) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 5.799574ms)
May 18 06:08:49.910: INFO: (9) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 5.848473ms)
May 18 06:08:49.910: INFO: (9) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 5.903373ms)
May 18 06:08:49.910: INFO: (9) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 5.958973ms)
May 18 06:08:49.914: INFO: (9) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 9.517257ms)
May 18 06:08:49.914: INFO: (9) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 10.397152ms)
May 18 06:08:49.914: INFO: (9) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 10.050654ms)
May 18 06:08:49.914: INFO: (9) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 10.045754ms)
May 18 06:08:49.914: INFO: (9) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 10.429753ms)
May 18 06:08:49.915: INFO: (9) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 10.489953ms)
May 18 06:08:49.928: INFO: (10) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 12.735142ms)
May 18 06:08:49.928: INFO: (10) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 12.684843ms)
May 18 06:08:49.928: INFO: (10) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 12.736743ms)
May 18 06:08:49.928: INFO: (10) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 13.424139ms)
May 18 06:08:49.928: INFO: (10) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 13.503639ms)
May 18 06:08:49.928: INFO: (10) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 13.32914ms)
May 18 06:08:49.928: INFO: (10) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 13.595238ms)
May 18 06:08:49.928: INFO: (10) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 13.034141ms)
May 18 06:08:49.928: INFO: (10) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 13.31864ms)
May 18 06:08:49.929: INFO: (10) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 13.544338ms)
May 18 06:08:49.929: INFO: (10) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 14.808533ms)
May 18 06:08:49.930: INFO: (10) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 14.362935ms)
May 18 06:08:49.932: INFO: (10) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 17.406821ms)
May 18 06:08:49.932: INFO: (10) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 17.148122ms)
May 18 06:08:49.932: INFO: (10) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 17.203522ms)
May 18 06:08:49.932: INFO: (10) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 17.019023ms)
May 18 06:08:49.937: INFO: (11) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 4.538279ms)
May 18 06:08:49.940: INFO: (11) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 7.733165ms)
May 18 06:08:49.940: INFO: (11) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 7.893964ms)
May 18 06:08:49.941: INFO: (11) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 8.178762ms)
May 18 06:08:49.941: INFO: (11) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 8.261862ms)
May 18 06:08:49.941: INFO: (11) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 8.164363ms)
May 18 06:08:49.941: INFO: (11) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 8.407462ms)
May 18 06:08:49.941: INFO: (11) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 8.85296ms)
May 18 06:08:49.941: INFO: (11) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 8.626461ms)
May 18 06:08:49.941: INFO: (11) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 8.343062ms)
May 18 06:08:49.941: INFO: (11) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 8.320562ms)
May 18 06:08:49.941: INFO: (11) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 8.86786ms)
May 18 06:08:49.943: INFO: (11) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 10.139254ms)
May 18 06:08:49.943: INFO: (11) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 10.151354ms)
May 18 06:08:49.943: INFO: (11) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 10.205253ms)
May 18 06:08:49.943: INFO: (11) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 9.823155ms)
May 18 06:08:49.950: INFO: (12) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 6.60977ms)
May 18 06:08:49.950: INFO: (12) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 6.743669ms)
May 18 06:08:49.950: INFO: (12) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 7.024468ms)
May 18 06:08:49.956: INFO: (12) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 12.901642ms)
May 18 06:08:49.956: INFO: (12) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 12.932041ms)
May 18 06:08:49.957: INFO: (12) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 13.565338ms)
May 18 06:08:49.957: INFO: (12) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 14.262335ms)
May 18 06:08:49.957: INFO: (12) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 14.219836ms)
May 18 06:08:49.957: INFO: (12) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 13.835737ms)
May 18 06:08:49.957: INFO: (12) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 14.107436ms)
May 18 06:08:49.957: INFO: (12) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 14.075036ms)
May 18 06:08:49.957: INFO: (12) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 13.968937ms)
May 18 06:08:49.957: INFO: (12) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 13.799838ms)
May 18 06:08:49.959: INFO: (12) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 16.009027ms)
May 18 06:08:49.959: INFO: (12) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 15.991227ms)
May 18 06:08:49.959: INFO: (12) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 16.393326ms)
May 18 06:08:49.966: INFO: (13) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 6.102272ms)
May 18 06:08:49.967: INFO: (13) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 6.67867ms)
May 18 06:08:49.967: INFO: (13) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 7.510466ms)
May 18 06:08:49.967: INFO: (13) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 6.955268ms)
May 18 06:08:49.967: INFO: (13) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 7.078268ms)
May 18 06:08:49.967: INFO: (13) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 7.707965ms)
May 18 06:08:49.967: INFO: (13) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 7.400766ms)
May 18 06:08:49.967: INFO: (13) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 7.652665ms)
May 18 06:08:49.967: INFO: (13) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 7.391567ms)
May 18 06:08:49.967: INFO: (13) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 7.229767ms)
May 18 06:08:49.968: INFO: (13) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 7.963564ms)
May 18 06:08:49.968: INFO: (13) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 7.454266ms)
May 18 06:08:49.968: INFO: (13) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 7.796465ms)
May 18 06:08:49.968: INFO: (13) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 8.369462ms)
May 18 06:08:49.968: INFO: (13) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 8.026763ms)
May 18 06:08:49.968: INFO: (13) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 8.088863ms)
May 18 06:08:49.974: INFO: (14) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 4.960877ms)
May 18 06:08:49.974: INFO: (14) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 4.37728ms)
May 18 06:08:49.974: INFO: (14) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 4.51398ms)
May 18 06:08:49.975: INFO: (14) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 6.187072ms)
May 18 06:08:49.975: INFO: (14) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 7.428466ms)
May 18 06:08:49.976: INFO: (14) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 6.767269ms)
May 18 06:08:49.976: INFO: (14) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 6.348071ms)
May 18 06:08:49.976: INFO: (14) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 7.268367ms)
May 18 06:08:49.976: INFO: (14) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 7.080768ms)
May 18 06:08:49.976: INFO: (14) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 7.499166ms)
May 18 06:08:49.978: INFO: (14) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 9.510957ms)
May 18 06:08:49.978: INFO: (14) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 8.69476ms)
May 18 06:08:49.978: INFO: (14) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 8.885059ms)
May 18 06:08:49.978: INFO: (14) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 8.76936ms)
May 18 06:08:49.978: INFO: (14) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 9.818156ms)
May 18 06:08:49.978: INFO: (14) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 9.919855ms)
May 18 06:08:49.983: INFO: (15) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 4.26328ms)
May 18 06:08:49.983: INFO: (15) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 4.172281ms)
May 18 06:08:49.985: INFO: (15) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 6.52867ms)
May 18 06:08:49.985: INFO: (15) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 6.421871ms)
May 18 06:08:49.985: INFO: (15) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 6.61117ms)
May 18 06:08:49.985: INFO: (15) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 6.489171ms)
May 18 06:08:49.985: INFO: (15) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 6.65467ms)
May 18 06:08:49.985: INFO: (15) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 6.53037ms)
May 18 06:08:49.988: INFO: (15) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 10.031454ms)
May 18 06:08:49.988: INFO: (15) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 9.979255ms)
May 18 06:08:49.988: INFO: (15) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 10.140154ms)
May 18 06:08:49.989: INFO: (15) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 10.307354ms)
May 18 06:08:49.989: INFO: (15) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 10.199153ms)
May 18 06:08:49.989: INFO: (15) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 10.289553ms)
May 18 06:08:49.989: INFO: (15) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 10.690852ms)
May 18 06:08:49.989: INFO: (15) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 10.854251ms)
May 18 06:08:49.995: INFO: (16) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 5.487275ms)
May 18 06:08:49.995: INFO: (16) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 5.146577ms)
May 18 06:08:49.995: INFO: (16) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 5.422776ms)
May 18 06:08:49.996: INFO: (16) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 5.694574ms)
May 18 06:08:49.996: INFO: (16) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 5.679175ms)
May 18 06:08:49.996: INFO: (16) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 5.728774ms)
May 18 06:08:49.996: INFO: (16) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 6.271472ms)
May 18 06:08:49.996: INFO: (16) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 5.961573ms)
May 18 06:08:49.996: INFO: (16) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 6.366571ms)
May 18 06:08:49.996: INFO: (16) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 6.088773ms)
May 18 06:08:49.997: INFO: (16) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 7.105968ms)
May 18 06:08:49.997: INFO: (16) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 7.202568ms)
May 18 06:08:49.997: INFO: (16) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 6.970269ms)
May 18 06:08:49.997: INFO: (16) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 7.223568ms)
May 18 06:08:49.997: INFO: (16) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 6.904368ms)
May 18 06:08:49.997: INFO: (16) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 6.936768ms)
May 18 06:08:50.002: INFO: (17) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 4.873278ms)
May 18 06:08:50.002: INFO: (17) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 5.107077ms)
May 18 06:08:50.003: INFO: (17) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 6.296171ms)
May 18 06:08:50.003: INFO: (17) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 6.225272ms)
May 18 06:08:50.003: INFO: (17) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 6.198572ms)
May 18 06:08:50.003: INFO: (17) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 6.337071ms)
May 18 06:08:50.004: INFO: (17) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 6.463571ms)
May 18 06:08:50.004: INFO: (17) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 6.473671ms)
May 18 06:08:50.004: INFO: (17) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 6.57987ms)
May 18 06:08:50.004: INFO: (17) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 6.48167ms)
May 18 06:08:50.004: INFO: (17) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 6.793269ms)
May 18 06:08:50.004: INFO: (17) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 6.756269ms)
May 18 06:08:50.004: INFO: (17) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 6.52787ms)
May 18 06:08:50.004: INFO: (17) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 6.55497ms)
May 18 06:08:50.004: INFO: (17) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 6.929469ms)
May 18 06:08:50.005: INFO: (17) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 8.005664ms)
May 18 06:08:50.011: INFO: (18) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 4.250381ms)
May 18 06:08:50.011: INFO: (18) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 4.985777ms)
May 18 06:08:50.012: INFO: (18) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 5.644574ms)
May 18 06:08:50.012: INFO: (18) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 5.368276ms)
May 18 06:08:50.012: INFO: (18) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 6.133873ms)
May 18 06:08:50.012: INFO: (18) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 6.54097ms)
May 18 06:08:50.012: INFO: (18) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 6.262472ms)
May 18 06:08:50.012: INFO: (18) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 5.877273ms)
May 18 06:08:50.012: INFO: (18) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 6.334971ms)
May 18 06:08:50.012: INFO: (18) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 6.61257ms)
May 18 06:08:50.012: INFO: (18) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 6.265072ms)
May 18 06:08:50.013: INFO: (18) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 7.305467ms)
May 18 06:08:50.013: INFO: (18) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 6.62887ms)
May 18 06:08:50.013: INFO: (18) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 7.333567ms)
May 18 06:08:50.013: INFO: (18) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 6.61807ms)
May 18 06:08:50.013: INFO: (18) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 7.792165ms)
May 18 06:08:50.017: INFO: (19) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">... (200; 4.062981ms)
May 18 06:08:50.017: INFO: (19) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:462/proxy/: tls qux (200; 4.066281ms)
May 18 06:08:50.019: INFO: (19) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks/proxy/rewriteme">test</a> (200; 5.452376ms)
May 18 06:08:50.019: INFO: (19) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 5.382676ms)
May 18 06:08:50.019: INFO: (19) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:460/proxy/: tls baz (200; 5.579974ms)
May 18 06:08:50.019: INFO: (19) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 5.551475ms)
May 18 06:08:50.019: INFO: (19) /api/v1/namespaces/proxy-8809/pods/http:proxy-service-kbbz8-xr6ks:162/proxy/: bar (200; 5.908873ms)
May 18 06:08:50.019: INFO: (19) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:160/proxy/: foo (200; 5.787374ms)
May 18 06:08:50.019: INFO: (19) /api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/https:proxy-service-kbbz8-xr6ks:443/proxy/tlsrewritem... (200; 5.855074ms)
May 18 06:08:50.019: INFO: (19) /api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/: <a href="/api/v1/namespaces/proxy-8809/pods/proxy-service-kbbz8-xr6ks:1080/proxy/rewriteme">test<... (200; 6.100372ms)
May 18 06:08:50.019: INFO: (19) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname1/proxy/: foo (200; 6.240872ms)
May 18 06:08:50.019: INFO: (19) /api/v1/namespaces/proxy-8809/services/http:proxy-service-kbbz8:portname2/proxy/: bar (200; 6.212672ms)
May 18 06:08:50.020: INFO: (19) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname1/proxy/: foo (200; 6.72207ms)
May 18 06:08:50.020: INFO: (19) /api/v1/namespaces/proxy-8809/services/proxy-service-kbbz8:portname2/proxy/: bar (200; 6.862968ms)
May 18 06:08:50.020: INFO: (19) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname1/proxy/: tls baz (200; 6.962968ms)
May 18 06:08:50.020: INFO: (19) /api/v1/namespaces/proxy-8809/services/https:proxy-service-kbbz8:tlsportname2/proxy/: tls qux (200; 6.966768ms)
STEP: deleting ReplicationController proxy-service-kbbz8 in namespace proxy-8809, will wait for the garbage collector to delete the pods
May 18 06:08:50.078: INFO: Deleting ReplicationController proxy-service-kbbz8 took: 3.836382ms
May 18 06:08:50.178: INFO: Terminating ReplicationController proxy-service-kbbz8 pods took: 100.170646ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:04.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8809" for this suite.

• [SLOW TEST:16.876 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":337,"completed":312,"skipped":5084,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:04.588: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-d6397713-4639-44fa-a64e-2465972d6c44
STEP: Creating a pod to test consume secrets
May 18 06:09:04.657: INFO: Waiting up to 5m0s for pod "pod-secrets-fe9e9e3c-7cfd-4564-94d8-3637b3f93d8a" in namespace "secrets-4867" to be "Succeeded or Failed"
May 18 06:09:04.663: INFO: Pod "pod-secrets-fe9e9e3c-7cfd-4564-94d8-3637b3f93d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299671ms
May 18 06:09:06.672: INFO: Pod "pod-secrets-fe9e9e3c-7cfd-4564-94d8-3637b3f93d8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015680461s
STEP: Saw pod success
May 18 06:09:06.672: INFO: Pod "pod-secrets-fe9e9e3c-7cfd-4564-94d8-3637b3f93d8a" satisfied condition "Succeeded or Failed"
May 18 06:09:06.677: INFO: Trying to get logs from node node1 pod pod-secrets-fe9e9e3c-7cfd-4564-94d8-3637b3f93d8a container secret-volume-test: <nil>
STEP: delete the pod
May 18 06:09:06.715: INFO: Waiting for pod pod-secrets-fe9e9e3c-7cfd-4564-94d8-3637b3f93d8a to disappear
May 18 06:09:06.722: INFO: Pod pod-secrets-fe9e9e3c-7cfd-4564-94d8-3637b3f93d8a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:06.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4867" for this suite.
STEP: Destroying namespace "secret-namespace-2691" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":337,"completed":313,"skipped":5092,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:06.739: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 18 06:09:09.819: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:09.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9988" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":337,"completed":314,"skipped":5102,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:09.839: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8811.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8811.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 18 06:09:13.925: INFO: DNS probes using dns-8811/dns-test-9a52cb22-2b80-4ccc-9a89-0d1920105f6e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:13.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8811" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":337,"completed":315,"skipped":5103,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:13.974: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
May 18 06:09:14.027: INFO: Waiting up to 5m0s for pod "pod-a44f3ee8-5df5-4227-9418-745c2e7afd6c" in namespace "emptydir-2830" to be "Succeeded or Failed"
May 18 06:09:14.031: INFO: Pod "pod-a44f3ee8-5df5-4227-9418-745c2e7afd6c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.902583ms
May 18 06:09:16.036: INFO: Pod "pod-a44f3ee8-5df5-4227-9418-745c2e7afd6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009263691s
May 18 06:09:18.043: INFO: Pod "pod-a44f3ee8-5df5-4227-9418-745c2e7afd6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016000893s
STEP: Saw pod success
May 18 06:09:18.043: INFO: Pod "pod-a44f3ee8-5df5-4227-9418-745c2e7afd6c" satisfied condition "Succeeded or Failed"
May 18 06:09:18.045: INFO: Trying to get logs from node node1 pod pod-a44f3ee8-5df5-4227-9418-745c2e7afd6c container test-container: <nil>
STEP: delete the pod
May 18 06:09:18.070: INFO: Waiting for pod pod-a44f3ee8-5df5-4227-9418-745c2e7afd6c to disappear
May 18 06:09:18.072: INFO: Pod pod-a44f3ee8-5df5-4227-9418-745c2e7afd6c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:18.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2830" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":316,"skipped":5103,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:18.079: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 06:09:18.114: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27414be6-1942-4baa-a6cf-8226404474d3" in namespace "downward-api-3691" to be "Succeeded or Failed"
May 18 06:09:18.121: INFO: Pod "downwardapi-volume-27414be6-1942-4baa-a6cf-8226404474d3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.420667ms
May 18 06:09:20.127: INFO: Pod "downwardapi-volume-27414be6-1942-4baa-a6cf-8226404474d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013535172s
May 18 06:09:22.131: INFO: Pod "downwardapi-volume-27414be6-1942-4baa-a6cf-8226404474d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017674786s
STEP: Saw pod success
May 18 06:09:22.131: INFO: Pod "downwardapi-volume-27414be6-1942-4baa-a6cf-8226404474d3" satisfied condition "Succeeded or Failed"
May 18 06:09:22.134: INFO: Trying to get logs from node node1 pod downwardapi-volume-27414be6-1942-4baa-a6cf-8226404474d3 container client-container: <nil>
STEP: delete the pod
May 18 06:09:22.148: INFO: Waiting for pod downwardapi-volume-27414be6-1942-4baa-a6cf-8226404474d3 to disappear
May 18 06:09:22.150: INFO: Pod downwardapi-volume-27414be6-1942-4baa-a6cf-8226404474d3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:22.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3691" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":337,"completed":317,"skipped":5104,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:22.156: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 06:09:22.186: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e24f9ed-fe60-4f9d-8656-9501c845c82c" in namespace "projected-404" to be "Succeeded or Failed"
May 18 06:09:22.188: INFO: Pod "downwardapi-volume-1e24f9ed-fe60-4f9d-8656-9501c845c82c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012191ms
May 18 06:09:24.193: INFO: Pod "downwardapi-volume-1e24f9ed-fe60-4f9d-8656-9501c845c82c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007063201s
STEP: Saw pod success
May 18 06:09:24.193: INFO: Pod "downwardapi-volume-1e24f9ed-fe60-4f9d-8656-9501c845c82c" satisfied condition "Succeeded or Failed"
May 18 06:09:24.195: INFO: Trying to get logs from node node2 pod downwardapi-volume-1e24f9ed-fe60-4f9d-8656-9501c845c82c container client-container: <nil>
STEP: delete the pod
May 18 06:09:24.214: INFO: Waiting for pod downwardapi-volume-1e24f9ed-fe60-4f9d-8656-9501c845c82c to disappear
May 18 06:09:24.220: INFO: Pod downwardapi-volume-1e24f9ed-fe60-4f9d-8656-9501c845c82c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:24.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-404" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":337,"completed":318,"skipped":5124,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:24.228: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
May 18 06:09:24.262: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-6667 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:24.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6667" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":337,"completed":319,"skipped":5158,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:24.331: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 18 06:09:24.363: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f163cf6-384e-4ecc-9536-25a302e494a2" in namespace "projected-7294" to be "Succeeded or Failed"
May 18 06:09:24.370: INFO: Pod "downwardapi-volume-1f163cf6-384e-4ecc-9536-25a302e494a2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.892569ms
May 18 06:09:26.375: INFO: Pod "downwardapi-volume-1f163cf6-384e-4ecc-9536-25a302e494a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012336777s
STEP: Saw pod success
May 18 06:09:26.375: INFO: Pod "downwardapi-volume-1f163cf6-384e-4ecc-9536-25a302e494a2" satisfied condition "Succeeded or Failed"
May 18 06:09:26.378: INFO: Trying to get logs from node node2 pod downwardapi-volume-1f163cf6-384e-4ecc-9536-25a302e494a2 container client-container: <nil>
STEP: delete the pod
May 18 06:09:26.394: INFO: Waiting for pod downwardapi-volume-1f163cf6-384e-4ecc-9536-25a302e494a2 to disappear
May 18 06:09:26.401: INFO: Pod downwardapi-volume-1f163cf6-384e-4ecc-9536-25a302e494a2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:26.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7294" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":337,"completed":320,"skipped":5164,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:26.410: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
May 18 06:09:28.512: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:30.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4493" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":337,"completed":321,"skipped":5185,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:30.565: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
May 18 06:09:30.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=kubectl-5338 api-versions'
May 18 06:09:30.804: INFO: stderr: ""
May 18 06:09:30.804: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:30.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5338" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":337,"completed":322,"skipped":5194,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:30.817: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-43d6ec04-dfa9-4678-9c7c-c9153ca0f3cb
STEP: Creating a pod to test consume secrets
May 18 06:09:30.873: INFO: Waiting up to 5m0s for pod "pod-secrets-cc28d8bd-ac92-434b-ac5d-fa2b7aaf4b7c" in namespace "secrets-3647" to be "Succeeded or Failed"
May 18 06:09:30.881: INFO: Pod "pod-secrets-cc28d8bd-ac92-434b-ac5d-fa2b7aaf4b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.219363ms
May 18 06:09:32.891: INFO: Pod "pod-secrets-cc28d8bd-ac92-434b-ac5d-fa2b7aaf4b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017778652s
May 18 06:09:34.907: INFO: Pod "pod-secrets-cc28d8bd-ac92-434b-ac5d-fa2b7aaf4b7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034060011s
STEP: Saw pod success
May 18 06:09:34.907: INFO: Pod "pod-secrets-cc28d8bd-ac92-434b-ac5d-fa2b7aaf4b7c" satisfied condition "Succeeded or Failed"
May 18 06:09:34.912: INFO: Trying to get logs from node node1 pod pod-secrets-cc28d8bd-ac92-434b-ac5d-fa2b7aaf4b7c container secret-volume-test: <nil>
STEP: delete the pod
May 18 06:09:34.949: INFO: Waiting for pod pod-secrets-cc28d8bd-ac92-434b-ac5d-fa2b7aaf4b7c to disappear
May 18 06:09:34.953: INFO: Pod pod-secrets-cc28d8bd-ac92-434b-ac5d-fa2b7aaf4b7c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:34.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3647" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":337,"completed":323,"skipped":5195,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:34.965: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 06:09:36.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 18 06:09:38.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756914976, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756914976, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756914977, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756914976, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 06:09:41.971: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 06:09:41.975: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8627-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:45.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-538" for this suite.
STEP: Destroying namespace "webhook-538-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.227 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":337,"completed":324,"skipped":5196,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:45.192: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:45.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5485" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":337,"completed":325,"skipped":5229,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:45.345: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 06:09:45.396: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 18 06:09:50.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 create -f -'
May 18 06:09:51.738: INFO: stderr: ""
May 18 06:09:51.738: INFO: stdout: "e2e-test-crd-publish-openapi-8360-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 18 06:09:51.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 delete e2e-test-crd-publish-openapi-8360-crds test-foo'
May 18 06:09:51.873: INFO: stderr: ""
May 18 06:09:51.873: INFO: stdout: "e2e-test-crd-publish-openapi-8360-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 18 06:09:51.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 apply -f -'
May 18 06:09:52.252: INFO: stderr: ""
May 18 06:09:52.252: INFO: stdout: "e2e-test-crd-publish-openapi-8360-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 18 06:09:52.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 delete e2e-test-crd-publish-openapi-8360-crds test-foo'
May 18 06:09:52.418: INFO: stderr: ""
May 18 06:09:52.418: INFO: stdout: "e2e-test-crd-publish-openapi-8360-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 18 06:09:52.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 create -f -'
May 18 06:09:52.715: INFO: rc: 1
May 18 06:09:52.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 apply -f -'
May 18 06:09:53.087: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 18 06:09:53.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 create -f -'
May 18 06:09:53.434: INFO: rc: 1
May 18 06:09:53.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 --namespace=crd-publish-openapi-2054 apply -f -'
May 18 06:09:53.686: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 18 06:09:53.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 explain e2e-test-crd-publish-openapi-8360-crds'
May 18 06:09:53.891: INFO: stderr: ""
May 18 06:09:53.891: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8360-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 18 06:09:53.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 explain e2e-test-crd-publish-openapi-8360-crds.metadata'
May 18 06:09:54.146: INFO: stderr: ""
May 18 06:09:54.146: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8360-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 18 06:09:54.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 explain e2e-test-crd-publish-openapi-8360-crds.spec'
May 18 06:09:54.341: INFO: stderr: ""
May 18 06:09:54.341: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8360-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 18 06:09:54.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 explain e2e-test-crd-publish-openapi-8360-crds.spec.bars'
May 18 06:09:54.554: INFO: stderr: ""
May 18 06:09:54.554: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8360-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 18 06:09:54.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-2054 explain e2e-test-crd-publish-openapi-8360-crds.spec.bars2'
May 18 06:09:54.750: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:09:56.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2054" for this suite.

• [SLOW TEST:11.350 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":337,"completed":326,"skipped":5244,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:09:56.695: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 18 06:09:56.736: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 18 06:09:56.741: INFO: Waiting for terminating namespaces to be deleted...
May 18 06:09:56.743: INFO: 
Logging pods the apiserver thinks is on node node1 before test
May 18 06:09:56.751: INFO: calico-node-bzct9 from kube-system started at 2021-05-18 02:09:57 +0000 UTC (1 container statuses recorded)
May 18 06:09:56.751: INFO: 	Container calico-node ready: true, restart count 0
May 18 06:09:56.751: INFO: kube-proxy-vxmpl from kube-system started at 2021-05-18 02:09:57 +0000 UTC (1 container statuses recorded)
May 18 06:09:56.751: INFO: 	Container kube-proxy ready: true, restart count 0
May 18 06:09:56.751: INFO: sonobuoy-e2e-job-d53ed4c4301a4fe4 from sonobuoy started at 2021-05-18 04:30:03 +0000 UTC (2 container statuses recorded)
May 18 06:09:56.751: INFO: 	Container e2e ready: true, restart count 0
May 18 06:09:56.751: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 18 06:09:56.751: INFO: sonobuoy-systemd-logs-daemon-set-0e6496dadced4aae-5wgz6 from sonobuoy started at 2021-05-18 04:30:04 +0000 UTC (2 container statuses recorded)
May 18 06:09:56.751: INFO: 	Container sonobuoy-worker ready: false, restart count 12
May 18 06:09:56.751: INFO: 	Container systemd-logs ready: true, restart count 0
May 18 06:09:56.751: INFO: 
Logging pods the apiserver thinks is on node node2 before test
May 18 06:09:56.755: INFO: calico-node-ddzct from kube-system started at 2021-05-18 04:29:32 +0000 UTC (1 container statuses recorded)
May 18 06:09:56.755: INFO: 	Container calico-node ready: true, restart count 0
May 18 06:09:56.755: INFO: kube-proxy-d6j4t from kube-system started at 2021-05-18 02:10:08 +0000 UTC (1 container statuses recorded)
May 18 06:09:56.755: INFO: 	Container kube-proxy ready: true, restart count 0
May 18 06:09:56.755: INFO: metrics-server-78ff8bd4fd-zg6vw from kube-system started at 2021-05-18 05:53:47 +0000 UTC (1 container statuses recorded)
May 18 06:09:56.755: INFO: 	Container metrics-server ready: true, restart count 0
May 18 06:09:56.755: INFO: sonobuoy from sonobuoy started at 2021-05-18 04:30:02 +0000 UTC (1 container statuses recorded)
May 18 06:09:56.755: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 18 06:09:56.755: INFO: sonobuoy-systemd-logs-daemon-set-0e6496dadced4aae-7bgms from sonobuoy started at 2021-05-18 04:30:03 +0000 UTC (2 container statuses recorded)
May 18 06:09:56.755: INFO: 	Container sonobuoy-worker ready: false, restart count 12
May 18 06:09:56.755: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-916eb91b-278d-4376-9c2c-e44b3d229b55 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-916eb91b-278d-4376-9c2c-e44b3d229b55 off the node node1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-916eb91b-278d-4376-9c2c-e44b3d229b55
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:10:02.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2262" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.192 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":337,"completed":327,"skipped":5248,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:10:02.889: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-1158
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 18 06:10:02.919: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 18 06:10:02.946: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 18 06:10:04.950: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 18 06:10:06.951: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 06:10:08.954: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 06:10:10.957: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 06:10:12.950: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 06:10:14.951: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 06:10:16.951: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 06:10:18.951: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 06:10:20.950: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 06:10:22.963: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 18 06:10:24.952: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 18 06:10:24.959: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 18 06:10:27.011: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 18 06:10:27.011: INFO: Going to poll 172.30.166.162 on port 8080 at least 0 times, with a maximum of 34 tries before failing
May 18 06:10:27.014: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.166.162:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1158 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 06:10:27.015: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 06:10:27.162: INFO: Found all 1 expected endpoints: [netserver-0]
May 18 06:10:27.162: INFO: Going to poll 172.30.104.28 on port 8080 at least 0 times, with a maximum of 34 tries before failing
May 18 06:10:27.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.104.28:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1158 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 18 06:10:27.168: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
May 18 06:10:27.290: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:10:27.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1158" for this suite.

• [SLOW TEST:24.416 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":337,"completed":328,"skipped":5325,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:10:27.306: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-acc41534-e4ca-4f14-96af-2b0bd73b7ac5
STEP: Creating the pod
May 18 06:10:27.405: INFO: The status of Pod pod-projected-configmaps-ab9d2e1f-abe3-4862-afe4-994a255338c0 is Pending, waiting for it to be Running (with Ready = true)
May 18 06:10:29.412: INFO: The status of Pod pod-projected-configmaps-ab9d2e1f-abe3-4862-afe4-994a255338c0 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-acc41534-e4ca-4f14-96af-2b0bd73b7ac5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:10:31.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4798" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":337,"completed":329,"skipped":5328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:10:31.459: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-fjhq
STEP: Creating a pod to test atomic-volume-subpath
May 18 06:10:31.510: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fjhq" in namespace "subpath-5798" to be "Succeeded or Failed"
May 18 06:10:31.523: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Pending", Reason="", readiness=false. Elapsed: 12.268845ms
May 18 06:10:33.532: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021416236s
May 18 06:10:35.541: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Running", Reason="", readiness=true. Elapsed: 4.030460528s
May 18 06:10:37.556: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Running", Reason="", readiness=true. Elapsed: 6.045632292s
May 18 06:10:39.562: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Running", Reason="", readiness=true. Elapsed: 8.0510608s
May 18 06:10:41.567: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Running", Reason="", readiness=true. Elapsed: 10.056682907s
May 18 06:10:43.573: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Running", Reason="", readiness=true. Elapsed: 12.062449314s
May 18 06:10:45.578: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Running", Reason="", readiness=true. Elapsed: 14.067244325s
May 18 06:10:47.583: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Running", Reason="", readiness=true. Elapsed: 16.072547533s
May 18 06:10:49.589: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Running", Reason="", readiness=true. Elapsed: 18.078039541s
May 18 06:10:51.592: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Running", Reason="", readiness=true. Elapsed: 20.081933656s
May 18 06:10:53.600: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Running", Reason="", readiness=true. Elapsed: 22.089094657s
May 18 06:10:55.605: INFO: Pod "pod-subpath-test-configmap-fjhq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.094123067s
STEP: Saw pod success
May 18 06:10:55.605: INFO: Pod "pod-subpath-test-configmap-fjhq" satisfied condition "Succeeded or Failed"
May 18 06:10:55.608: INFO: Trying to get logs from node node2 pod pod-subpath-test-configmap-fjhq container test-container-subpath-configmap-fjhq: <nil>
STEP: delete the pod
May 18 06:10:55.630: INFO: Waiting for pod pod-subpath-test-configmap-fjhq to disappear
May 18 06:10:55.634: INFO: Pod pod-subpath-test-configmap-fjhq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fjhq
May 18 06:10:55.634: INFO: Deleting pod "pod-subpath-test-configmap-fjhq" in namespace "subpath-5798"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:10:55.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5798" for this suite.

• [SLOW TEST:24.184 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":337,"completed":330,"skipped":5365,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:10:55.643: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-5305
STEP: creating replication controller nodeport-test in namespace services-5305
I0518 06:10:55.703401      19 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-5305, replica count: 2
May 18 06:10:58.754: INFO: Creating new exec pod
I0518 06:10:58.754194      19 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 18 06:11:01.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:01.930: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:01.930: INFO: stdout: ""
May 18 06:11:02.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:03.082: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:03.082: INFO: stdout: ""
May 18 06:11:03.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:04.115: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:04.115: INFO: stdout: ""
May 18 06:11:04.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:05.078: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:05.078: INFO: stdout: ""
May 18 06:11:05.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:06.109: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:06.109: INFO: stdout: ""
May 18 06:11:06.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:07.085: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:07.085: INFO: stdout: ""
May 18 06:11:07.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:08.061: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:08.061: INFO: stdout: ""
May 18 06:11:08.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:09.075: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:09.075: INFO: stdout: ""
May 18 06:11:09.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:10.092: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:10.092: INFO: stdout: ""
May 18 06:11:10.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:11.087: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:11.087: INFO: stdout: ""
May 18 06:11:11.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:12.080: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:12.080: INFO: stdout: ""
May 18 06:11:12.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:13.104: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:13.104: INFO: stdout: ""
May 18 06:11:13.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:14.103: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:14.103: INFO: stdout: ""
May 18 06:11:14.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:15.077: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:15.077: INFO: stdout: ""
May 18 06:11:15.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 18 06:11:16.091: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 18 06:11:16.091: INFO: stdout: "nodeport-test-rgq8s"
May 18 06:11:16.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.24.106.207 80'
May 18 06:11:16.249: INFO: stderr: "+ nc -v -t -w 2 172.24.106.207 80\n+ echo hostName\nConnection to 172.24.106.207 80 port [tcp/http] succeeded!\n"
May 18 06:11:16.249: INFO: stdout: "nodeport-test-rgq8s"
May 18 06:11:16.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 31415'
May 18 06:11:16.407: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.12 31415\nConnection to 172.28.128.12 31415 port [tcp/*] succeeded!\n"
May 18 06:11:16.407: INFO: stdout: ""
May 18 06:11:17.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 31415'
May 18 06:11:17.690: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.12 31415\nConnection to 172.28.128.12 31415 port [tcp/*] succeeded!\n"
May 18 06:11:17.690: INFO: stdout: ""
May 18 06:11:18.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 31415'
May 18 06:11:18.691: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.12 31415\nConnection to 172.28.128.12 31415 port [tcp/*] succeeded!\n"
May 18 06:11:18.691: INFO: stdout: ""
May 18 06:11:19.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.12 31415'
May 18 06:11:19.676: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.128.12 31415\nConnection to 172.28.128.12 31415 port [tcp/*] succeeded!\n"
May 18 06:11:19.676: INFO: stdout: "nodeport-test-wcbh2"
May 18 06:11:19.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=services-5305 exec execpod8wdw8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.128.13 31415'
May 18 06:11:19.924: INFO: stderr: "+ nc -v -t -w 2 172.28.128.13 31415\n+ echo hostName\nConnection to 172.28.128.13 31415 port [tcp/*] succeeded!\n"
May 18 06:11:19.924: INFO: stdout: "nodeport-test-rgq8s"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:11:19.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5305" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:24.295 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":337,"completed":331,"skipped":5372,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:11:19.939: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 06:11:20.008: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 18 06:11:25.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-5123 --namespace=crd-publish-openapi-5123 create -f -'
May 18 06:11:26.747: INFO: stderr: ""
May 18 06:11:26.747: INFO: stdout: "e2e-test-crd-publish-openapi-1550-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 18 06:11:26.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-5123 --namespace=crd-publish-openapi-5123 delete e2e-test-crd-publish-openapi-1550-crds test-cr'
May 18 06:11:26.912: INFO: stderr: ""
May 18 06:11:26.912: INFO: stdout: "e2e-test-crd-publish-openapi-1550-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 18 06:11:26.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-5123 --namespace=crd-publish-openapi-5123 apply -f -'
May 18 06:11:27.459: INFO: stderr: ""
May 18 06:11:27.459: INFO: stdout: "e2e-test-crd-publish-openapi-1550-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 18 06:11:27.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-5123 --namespace=crd-publish-openapi-5123 delete e2e-test-crd-publish-openapi-1550-crds test-cr'
May 18 06:11:27.732: INFO: stderr: ""
May 18 06:11:27.732: INFO: stdout: "e2e-test-crd-publish-openapi-1550-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 18 06:11:27.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-258366932 --namespace=crd-publish-openapi-5123 explain e2e-test-crd-publish-openapi-1550-crds'
May 18 06:11:28.357: INFO: stderr: ""
May 18 06:11:28.357: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1550-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:11:32.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5123" for this suite.

• [SLOW TEST:12.403 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":337,"completed":332,"skipped":5397,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:11:32.342: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
May 18 06:11:32.951: INFO: created pod pod-service-account-defaultsa
May 18 06:11:32.951: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 18 06:11:32.954: INFO: created pod pod-service-account-mountsa
May 18 06:11:32.954: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 18 06:11:32.965: INFO: created pod pod-service-account-nomountsa
May 18 06:11:32.965: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 18 06:11:32.973: INFO: created pod pod-service-account-defaultsa-mountspec
May 18 06:11:32.973: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 18 06:11:32.986: INFO: created pod pod-service-account-mountsa-mountspec
May 18 06:11:32.986: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 18 06:11:32.992: INFO: created pod pod-service-account-nomountsa-mountspec
May 18 06:11:32.992: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 18 06:11:33.002: INFO: created pod pod-service-account-defaultsa-nomountspec
May 18 06:11:33.002: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 18 06:11:33.030: INFO: created pod pod-service-account-mountsa-nomountspec
May 18 06:11:33.030: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 18 06:11:33.043: INFO: created pod pod-service-account-nomountsa-nomountspec
May 18 06:11:33.043: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:11:33.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1718" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":337,"completed":333,"skipped":5410,"failed":0}

------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:11:33.074: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 18 06:11:33.145: INFO: Waiting up to 1m0s for all nodes to be ready
May 18 06:12:33.198: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:12:33.203: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 18 06:12:35.309: INFO: found a healthy node: node1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 18 06:12:45.438: INFO: pods created so far: [1 1 1]
May 18 06:12:45.438: INFO: length of pods created so far: 3
May 18 06:12:53.448: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:13:00.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5322" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:13:00.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2251" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:87.524 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":337,"completed":334,"skipped":5410,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:13:00.598: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 18 06:13:01.163: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 18 06:13:04.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
May 18 06:13:04.214: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:13:04.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3973" for this suite.
STEP: Destroying namespace "webhook-3973-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":337,"completed":335,"skipped":5415,"failed":0}
SS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:13:04.456: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:13:04.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7811" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":337,"completed":336,"skipped":5417,"failed":0}
SSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 18 06:13:04.546: INFO: >>> kubeConfig: /tmp/kubeconfig-258366932
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0518 06:13:04.600503      19 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 18 06:18:04.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6295" for this suite.

• [SLOW TEST:300.088 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":337,"completed":337,"skipped":5423,"failed":0}
SSSSSSSSSSSMay 18 06:18:04.634: INFO: Running AfterSuite actions on all nodes
May 18 06:18:04.634: INFO: Running AfterSuite actions on node 1
May 18 06:18:04.634: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":337,"completed":337,"skipped":5434,"failed":0}

Ran 337 of 5771 Specs in 6473.670 seconds
SUCCESS! -- 337 Passed | 0 Failed | 0 Pending | 5434 Skipped
PASS

Ginkgo ran 1 suite in 1h47m55.311041956s
Test Suite Passed
