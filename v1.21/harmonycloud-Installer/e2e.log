I0525 02:27:06.286676      23 e2e.go:129] Starting e2e run "d8ee609c-c938-4bd2-ac40-ed3abc4c9371" on Ginkgo node 1
{"msg":"Test Suite starting","total":339,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1621909622 - Will randomize all specs
Will run 339 of 5771 specs

May 25 02:27:06.319: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 02:27:06.325: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 25 02:27:06.376: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 25 02:27:06.449: INFO: 9 / 9 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 25 02:27:06.449: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
May 25 02:27:06.449: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 25 02:27:06.465: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 25 02:27:06.465: INFO: e2e test version: v1.21.1
May 25 02:27:06.467: INFO: kube-apiserver version: v1.21.1
May 25 02:27:06.467: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 02:27:06.477: INFO: Cluster IP family: ipv4
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:27:06.477: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename statefulset
W0525 02:27:06.551795      23 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
May 25 02:27:06.552: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
May 25 02:27:06.564: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-6299
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6299
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6299
May 25 02:27:06.592: INFO: Found 0 stateful pods, waiting for 1
May 25 02:27:16.599: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 25 02:27:16.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-6299 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 02:27:17.119: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 02:27:17.119: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 02:27:17.119: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 02:27:17.126: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 25 02:27:27.136: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 25 02:27:27.136: INFO: Waiting for statefulset status.replicas updated to 0
May 25 02:27:27.187: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999154s
May 25 02:27:28.202: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.966499068s
May 25 02:27:29.211: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.952372451s
May 25 02:27:30.224: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.943568932s
May 25 02:27:31.233: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.930952382s
May 25 02:27:32.247: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.920886408s
May 25 02:27:33.257: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.907591468s
May 25 02:27:34.271: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.897693668s
May 25 02:27:35.280: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.884099754s
May 25 02:27:36.292: INFO: Verifying statefulset ss doesn't scale past 1 for another 874.608468ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6299
May 25 02:27:37.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-6299 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:27:37.677: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 02:27:37.677: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 02:27:37.677: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 02:27:37.683: INFO: Found 1 stateful pods, waiting for 3
May 25 02:27:47.696: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 25 02:27:47.696: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 25 02:27:47.696: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 25 02:27:47.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-6299 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 02:27:48.078: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 02:27:48.079: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 02:27:48.079: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 02:27:48.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-6299 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 02:27:48.486: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 02:27:48.486: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 02:27:48.486: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 02:27:48.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-6299 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 02:27:48.873: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 02:27:48.873: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 02:27:48.873: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 02:27:48.873: INFO: Waiting for statefulset status.replicas updated to 0
May 25 02:27:48.879: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 25 02:27:58.895: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 25 02:27:58.895: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 25 02:27:58.895: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 25 02:27:58.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999347s
May 25 02:27:59.971: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.946919107s
May 25 02:28:00.978: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.935886635s
May 25 02:28:01.992: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.92731373s
May 25 02:28:03.013: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.914928578s
May 25 02:28:04.025: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.893978253s
May 25 02:28:05.049: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.880991512s
May 25 02:28:06.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.857759274s
May 25 02:28:07.076: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.845013128s
May 25 02:28:08.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 830.086358ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6299
May 25 02:28:09.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-6299 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:28:09.488: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 02:28:09.488: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 02:28:09.488: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 02:28:09.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-6299 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:28:09.857: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 02:28:09.857: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 02:28:09.857: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 02:28:09.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-6299 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:28:10.235: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 02:28:10.235: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 02:28:10.235: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 02:28:10.235: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
May 25 02:28:20.266: INFO: Deleting all statefulset in ns statefulset-6299
May 25 02:28:20.272: INFO: Scaling statefulset ss to 0
May 25 02:28:20.290: INFO: Waiting for statefulset status.replicas updated to 0
May 25 02:28:20.293: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:28:20.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6299" for this suite.

• [SLOW TEST:73.863 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":339,"completed":1,"skipped":7,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:28:20.341: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1318
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1318
STEP: creating replication controller externalsvc in namespace services-1318
I0525 02:28:20.461193      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1318, replica count: 2
I0525 02:28:23.512940      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 25 02:28:23.558: INFO: Creating new exec pod
May 25 02:28:27.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-1318 exec execpodwkczc -- /bin/sh -x -c nslookup nodeport-service.services-1318.svc.cluster.local'
May 25 02:28:28.603: INFO: stderr: "+ nslookup nodeport-service.services-1318.svc.cluster.local\n"
May 25 02:28:28.603: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-1318.svc.cluster.local\tcanonical name = externalsvc.services-1318.svc.cluster.local.\nName:\texternalsvc.services-1318.svc.cluster.local\nAddress: 10.106.65.106\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1318, will wait for the garbage collector to delete the pods
May 25 02:28:28.677: INFO: Deleting ReplicationController externalsvc took: 10.940147ms
May 25 02:28:28.778: INFO: Terminating ReplicationController externalsvc pods took: 100.877545ms
May 25 02:28:33.012: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:28:33.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1318" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:12.704 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":339,"completed":2,"skipped":54,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:28:33.046: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:28:33.109: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-7a784408-8d00-49e8-8e7f-9c1ba63f57b4" in namespace "security-context-test-6339" to be "Succeeded or Failed"
May 25 02:28:33.114: INFO: Pod "alpine-nnp-false-7a784408-8d00-49e8-8e7f-9c1ba63f57b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215609ms
May 25 02:28:35.120: INFO: Pod "alpine-nnp-false-7a784408-8d00-49e8-8e7f-9c1ba63f57b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010563525s
May 25 02:28:35.120: INFO: Pod "alpine-nnp-false-7a784408-8d00-49e8-8e7f-9c1ba63f57b4" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:28:35.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6339" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":3,"skipped":68,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:28:35.180: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
May 25 02:28:35.253: INFO: Waiting up to 5m0s for pod "pod-24a448ba-0f1b-4f4b-aeb6-2a80d1ed1c5e" in namespace "emptydir-6914" to be "Succeeded or Failed"
May 25 02:28:35.257: INFO: Pod "pod-24a448ba-0f1b-4f4b-aeb6-2a80d1ed1c5e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.126651ms
May 25 02:28:37.266: INFO: Pod "pod-24a448ba-0f1b-4f4b-aeb6-2a80d1ed1c5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012879538s
STEP: Saw pod success
May 25 02:28:37.266: INFO: Pod "pod-24a448ba-0f1b-4f4b-aeb6-2a80d1ed1c5e" satisfied condition "Succeeded or Failed"
May 25 02:28:37.270: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-24a448ba-0f1b-4f4b-aeb6-2a80d1ed1c5e container test-container: <nil>
STEP: delete the pod
May 25 02:28:37.297: INFO: Waiting for pod pod-24a448ba-0f1b-4f4b-aeb6-2a80d1ed1c5e to disappear
May 25 02:28:37.301: INFO: Pod pod-24a448ba-0f1b-4f4b-aeb6-2a80d1ed1c5e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:28:37.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6914" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":4,"skipped":71,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:28:37.314: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:28:37.386: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e803dbae-95cf-4c53-bc7b-ec2cf3e47988" in namespace "security-context-test-9617" to be "Succeeded or Failed"
May 25 02:28:37.390: INFO: Pod "busybox-privileged-false-e803dbae-95cf-4c53-bc7b-ec2cf3e47988": Phase="Pending", Reason="", readiness=false. Elapsed: 4.492615ms
May 25 02:28:39.397: INFO: Pod "busybox-privileged-false-e803dbae-95cf-4c53-bc7b-ec2cf3e47988": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011458029s
May 25 02:28:39.397: INFO: Pod "busybox-privileged-false-e803dbae-95cf-4c53-bc7b-ec2cf3e47988" satisfied condition "Succeeded or Failed"
May 25 02:28:39.425: INFO: Got logs for pod "busybox-privileged-false-e803dbae-95cf-4c53-bc7b-ec2cf3e47988": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:28:39.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9617" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":5,"skipped":78,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:28:39.438: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:28:39.499: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 25 02:28:46.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-154 --namespace=crd-publish-openapi-154 create -f -'
May 25 02:28:47.074: INFO: stderr: ""
May 25 02:28:47.074: INFO: stdout: "e2e-test-crd-publish-openapi-5345-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 25 02:28:47.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-154 --namespace=crd-publish-openapi-154 delete e2e-test-crd-publish-openapi-5345-crds test-cr'
May 25 02:28:47.292: INFO: stderr: ""
May 25 02:28:47.292: INFO: stdout: "e2e-test-crd-publish-openapi-5345-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 25 02:28:47.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-154 --namespace=crd-publish-openapi-154 apply -f -'
May 25 02:28:47.789: INFO: stderr: ""
May 25 02:28:47.789: INFO: stdout: "e2e-test-crd-publish-openapi-5345-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 25 02:28:47.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-154 --namespace=crd-publish-openapi-154 delete e2e-test-crd-publish-openapi-5345-crds test-cr'
May 25 02:28:47.993: INFO: stderr: ""
May 25 02:28:47.993: INFO: stdout: "e2e-test-crd-publish-openapi-5345-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 25 02:28:47.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-154 explain e2e-test-crd-publish-openapi-5345-crds'
May 25 02:28:48.481: INFO: stderr: ""
May 25 02:28:48.481: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5345-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:28:54.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-154" for this suite.

• [SLOW TEST:15.504 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":339,"completed":6,"skipped":78,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:28:54.943: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-d962998a-1b25-4b08-8926-62b468339243
STEP: Creating a pod to test consume secrets
May 25 02:28:55.018: INFO: Waiting up to 5m0s for pod "pod-secrets-637f86e6-919a-4d08-9664-c90594d5bbdb" in namespace "secrets-7713" to be "Succeeded or Failed"
May 25 02:28:55.035: INFO: Pod "pod-secrets-637f86e6-919a-4d08-9664-c90594d5bbdb": Phase="Pending", Reason="", readiness=false. Elapsed: 17.089235ms
May 25 02:28:57.045: INFO: Pod "pod-secrets-637f86e6-919a-4d08-9664-c90594d5bbdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026395633s
STEP: Saw pod success
May 25 02:28:57.045: INFO: Pod "pod-secrets-637f86e6-919a-4d08-9664-c90594d5bbdb" satisfied condition "Succeeded or Failed"
May 25 02:28:57.049: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-secrets-637f86e6-919a-4d08-9664-c90594d5bbdb container secret-volume-test: <nil>
STEP: delete the pod
May 25 02:28:57.078: INFO: Waiting for pod pod-secrets-637f86e6-919a-4d08-9664-c90594d5bbdb to disappear
May 25 02:28:57.082: INFO: Pod pod-secrets-637f86e6-919a-4d08-9664-c90594d5bbdb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:28:57.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7713" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":7,"skipped":121,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:28:57.097: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-f85d8608-68ad-45de-a7e2-4c4c19342957
STEP: Creating a pod to test consume configMaps
May 25 02:28:57.170: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2ef9ec39-2f17-423c-aa1c-e131583f9b26" in namespace "projected-1190" to be "Succeeded or Failed"
May 25 02:28:57.177: INFO: Pod "pod-projected-configmaps-2ef9ec39-2f17-423c-aa1c-e131583f9b26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.185204ms
May 25 02:28:59.187: INFO: Pod "pod-projected-configmaps-2ef9ec39-2f17-423c-aa1c-e131583f9b26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016433374s
STEP: Saw pod success
May 25 02:28:59.187: INFO: Pod "pod-projected-configmaps-2ef9ec39-2f17-423c-aa1c-e131583f9b26" satisfied condition "Succeeded or Failed"
May 25 02:28:59.191: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-projected-configmaps-2ef9ec39-2f17-423c-aa1c-e131583f9b26 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 25 02:28:59.218: INFO: Waiting for pod pod-projected-configmaps-2ef9ec39-2f17-423c-aa1c-e131583f9b26 to disappear
May 25 02:28:59.222: INFO: Pod pod-projected-configmaps-2ef9ec39-2f17-423c-aa1c-e131583f9b26 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:28:59.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1190" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":8,"skipped":127,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:28:59.237: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 25 02:29:01.333: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:29:01.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8796" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":9,"skipped":136,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:29:01.457: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6226.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6226.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6226.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6226.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6226.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6226.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 02:29:05.627: INFO: DNS probes using dns-6226/dns-test-e7ab0091-af11-487d-b917-03271a5fefd1 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:29:05.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6226" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":339,"completed":10,"skipped":151,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:29:05.695: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8025
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8025
STEP: creating replication controller externalsvc in namespace services-8025
I0525 02:29:05.798719      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8025, replica count: 2
I0525 02:29:08.850332      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 25 02:29:08.888: INFO: Creating new exec pod
May 25 02:29:10.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8025 exec execpodsxx8c -- /bin/sh -x -c nslookup clusterip-service.services-8025.svc.cluster.local'
May 25 02:29:11.473: INFO: stderr: "+ nslookup clusterip-service.services-8025.svc.cluster.local\n"
May 25 02:29:11.473: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-8025.svc.cluster.local\tcanonical name = externalsvc.services-8025.svc.cluster.local.\nName:\texternalsvc.services-8025.svc.cluster.local\nAddress: 10.100.222.143\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8025, will wait for the garbage collector to delete the pods
May 25 02:29:11.624: INFO: Deleting ReplicationController externalsvc took: 8.879713ms
May 25 02:29:11.725: INFO: Terminating ReplicationController externalsvc pods took: 100.95662ms
May 25 02:29:16.960: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:29:16.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8025" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:11.299 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":339,"completed":11,"skipped":151,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:29:16.994: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-c0201676-1a73-41dc-81ac-e27729e1decf
STEP: Creating a pod to test consume secrets
May 25 02:29:17.065: INFO: Waiting up to 5m0s for pod "pod-secrets-592c0579-1506-4762-9fff-5a0241ae6aa0" in namespace "secrets-5424" to be "Succeeded or Failed"
May 25 02:29:17.072: INFO: Pod "pod-secrets-592c0579-1506-4762-9fff-5a0241ae6aa0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.39262ms
May 25 02:29:19.081: INFO: Pod "pod-secrets-592c0579-1506-4762-9fff-5a0241ae6aa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016602595s
STEP: Saw pod success
May 25 02:29:19.081: INFO: Pod "pod-secrets-592c0579-1506-4762-9fff-5a0241ae6aa0" satisfied condition "Succeeded or Failed"
May 25 02:29:19.086: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-secrets-592c0579-1506-4762-9fff-5a0241ae6aa0 container secret-volume-test: <nil>
STEP: delete the pod
May 25 02:29:19.118: INFO: Waiting for pod pod-secrets-592c0579-1506-4762-9fff-5a0241ae6aa0 to disappear
May 25 02:29:19.123: INFO: Pod pod-secrets-592c0579-1506-4762-9fff-5a0241ae6aa0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:29:19.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5424" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":12,"skipped":157,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:29:19.141: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:29:25.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3844" for this suite.
STEP: Destroying namespace "nsdeletetest-759" for this suite.
May 25 02:29:25.370: INFO: Namespace nsdeletetest-759 was already deleted
STEP: Destroying namespace "nsdeletetest-2065" for this suite.

• [SLOW TEST:6.236 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":339,"completed":13,"skipped":165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:29:25.378: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:29:25.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7949" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":339,"completed":14,"skipped":225,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:29:25.438: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 02:29:26.789: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 25 02:29:28.807: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757506566, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757506566, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757506566, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757506566, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 02:29:31.827: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:29:31.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9995" for this suite.
STEP: Destroying namespace "webhook-9995-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.542 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":339,"completed":15,"skipped":242,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:29:31.981: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:29:32.049: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 25 02:29:38.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 --namespace=crd-publish-openapi-3460 create -f -'
May 25 02:29:39.191: INFO: stderr: ""
May 25 02:29:39.191: INFO: stdout: "e2e-test-crd-publish-openapi-2474-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 25 02:29:39.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 --namespace=crd-publish-openapi-3460 delete e2e-test-crd-publish-openapi-2474-crds test-foo'
May 25 02:29:39.406: INFO: stderr: ""
May 25 02:29:39.406: INFO: stdout: "e2e-test-crd-publish-openapi-2474-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 25 02:29:39.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 --namespace=crd-publish-openapi-3460 apply -f -'
May 25 02:29:39.904: INFO: stderr: ""
May 25 02:29:39.904: INFO: stdout: "e2e-test-crd-publish-openapi-2474-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 25 02:29:39.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 --namespace=crd-publish-openapi-3460 delete e2e-test-crd-publish-openapi-2474-crds test-foo'
May 25 02:29:40.104: INFO: stderr: ""
May 25 02:29:40.104: INFO: stdout: "e2e-test-crd-publish-openapi-2474-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 25 02:29:40.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 --namespace=crd-publish-openapi-3460 create -f -'
May 25 02:29:40.558: INFO: rc: 1
May 25 02:29:40.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 --namespace=crd-publish-openapi-3460 apply -f -'
May 25 02:29:41.026: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 25 02:29:41.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 --namespace=crd-publish-openapi-3460 create -f -'
May 25 02:29:41.503: INFO: rc: 1
May 25 02:29:41.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 --namespace=crd-publish-openapi-3460 apply -f -'
May 25 02:29:41.975: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 25 02:29:41.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 explain e2e-test-crd-publish-openapi-2474-crds'
May 25 02:29:42.461: INFO: stderr: ""
May 25 02:29:42.461: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2474-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 25 02:29:42.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 explain e2e-test-crd-publish-openapi-2474-crds.metadata'
May 25 02:29:42.956: INFO: stderr: ""
May 25 02:29:42.956: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2474-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 25 02:29:42.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 explain e2e-test-crd-publish-openapi-2474-crds.spec'
May 25 02:29:43.439: INFO: stderr: ""
May 25 02:29:43.440: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2474-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 25 02:29:43.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 explain e2e-test-crd-publish-openapi-2474-crds.spec.bars'
May 25 02:29:43.924: INFO: stderr: ""
May 25 02:29:43.924: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2474-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 25 02:29:43.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3460 explain e2e-test-crd-publish-openapi-2474-crds.spec.bars2'
May 25 02:29:44.404: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:29:50.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3460" for this suite.

• [SLOW TEST:18.462 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":339,"completed":16,"skipped":248,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:29:50.443: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:29:50.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2394" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":17,"skipped":337,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:29:50.621: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
May 25 02:29:50.665: INFO: PodSpec: initContainers in spec.initContainers
May 25 02:30:38.094: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a3ecbe42-6aab-4d36-b122-b3ad254a1b67", GenerateName:"", Namespace:"init-container-1810", SelfLink:"", UID:"a3ededd4-bbdc-4d1a-b2ef-81dfa8e63e7f", ResourceVersion:"827836", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63757506590, loc:(*time.Location)(0x9dc0820)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"665677761"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.31.126/32", "cni.projectcalico.org/podIPs":"192.168.31.126/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004022d20), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004022d38)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004022d50), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004022d68)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004022d80), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004022d98)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-vs4kc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc002da41a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-vs4kc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-vs4kc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.4.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-vs4kc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005ca72a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.10.103.202-worker", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003299570), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005ca7370)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005ca7390)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005ca7398), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005ca739c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0045218b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757506590, loc:(*time.Location)(0x9dc0820)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757506590, loc:(*time.Location)(0x9dc0820)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757506590, loc:(*time.Location)(0x9dc0820)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757506590, loc:(*time.Location)(0x9dc0820)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.103.202", PodIP:"192.168.31.126", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.31.126"}}, StartTime:(*v1.Time)(0xc004022dc8), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003299650)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0032996c0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"docker://sha256:6b8f5a05efd5a16a4e1706a73ed4f32be739c951a26e32ebd5975f2ffe862eee", ContainerID:"docker://1a815500a73de942161d39bae5c66a4c886e853defe6d1f4ab41afa49b452250", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002da4220), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002da4200), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.4.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc005ca7444)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:30:38.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1810" for this suite.

• [SLOW TEST:47.491 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":339,"completed":18,"skipped":350,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:30:38.113: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:30:38.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-9259 version'
May 25 02:30:38.384: INFO: stderr: ""
May 25 02:30:38.384: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.1\", GitCommit:\"5e58841cce77d4bc13713ad2b91fa0d961e69192\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T14:18:45Z\", GoVersion:\"go1.16.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.1\", GitCommit:\"5e58841cce77d4bc13713ad2b91fa0d961e69192\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T14:12:29Z\", GoVersion:\"go1.16.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:30:38.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9259" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":339,"completed":19,"skipped":353,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:30:38.399: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0525 02:30:48.499094      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 25 02:31:50.528: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:31:50.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-183" for this suite.

• [SLOW TEST:72.154 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":339,"completed":20,"skipped":358,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:31:50.554: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
May 25 02:31:50.647: INFO: The status of Pod labelsupdatee3b6559d-9a5d-4c66-9675-43cb6421917d is Pending, waiting for it to be Running (with Ready = true)
May 25 02:31:52.662: INFO: The status of Pod labelsupdatee3b6559d-9a5d-4c66-9675-43cb6421917d is Pending, waiting for it to be Running (with Ready = true)
May 25 02:31:54.659: INFO: The status of Pod labelsupdatee3b6559d-9a5d-4c66-9675-43cb6421917d is Running (Ready = true)
May 25 02:31:55.212: INFO: Successfully updated pod "labelsupdatee3b6559d-9a5d-4c66-9675-43cb6421917d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:31:57.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6397" for this suite.

• [SLOW TEST:6.700 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":21,"skipped":375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:31:57.255: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0525 02:32:07.465538      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 25 02:33:09.491: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 25 02:33:09.491: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gpms" in namespace "gc-5071"
May 25 02:33:09.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qqfk" in namespace "gc-5071"
May 25 02:33:09.534: INFO: Deleting pod "simpletest-rc-to-be-deleted-959lj" in namespace "gc-5071"
May 25 02:33:09.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs686" in namespace "gc-5071"
May 25 02:33:09.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-nc6km" in namespace "gc-5071"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:33:09.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5071" for this suite.

• [SLOW TEST:72.354 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":339,"completed":22,"skipped":404,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:33:09.609: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
May 25 02:33:09.741: INFO: Waiting up to 5m0s for pod "pod-ae2d8a6e-de1e-40f8-baea-5f93f678435a" in namespace "emptydir-7118" to be "Succeeded or Failed"
May 25 02:33:09.748: INFO: Pod "pod-ae2d8a6e-de1e-40f8-baea-5f93f678435a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.91476ms
May 25 02:33:11.758: INFO: Pod "pod-ae2d8a6e-de1e-40f8-baea-5f93f678435a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016926696s
May 25 02:33:13.769: INFO: Pod "pod-ae2d8a6e-de1e-40f8-baea-5f93f678435a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028229302s
STEP: Saw pod success
May 25 02:33:13.769: INFO: Pod "pod-ae2d8a6e-de1e-40f8-baea-5f93f678435a" satisfied condition "Succeeded or Failed"
May 25 02:33:13.774: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-ae2d8a6e-de1e-40f8-baea-5f93f678435a container test-container: <nil>
STEP: delete the pod
May 25 02:33:13.808: INFO: Waiting for pod pod-ae2d8a6e-de1e-40f8-baea-5f93f678435a to disappear
May 25 02:33:13.812: INFO: Pod pod-ae2d8a6e-de1e-40f8-baea-5f93f678435a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:33:13.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7118" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":23,"skipped":405,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:33:13.826: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 02:33:15.163: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 02:33:17.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757506795, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757506795, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757506795, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757506795, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 02:33:20.209: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:33:30.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4336" for this suite.
STEP: Destroying namespace "webhook-4336-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.698 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":339,"completed":24,"skipped":408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:33:30.525: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:33:30.604: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 25 02:33:30.620: INFO: The status of Pod pod-logs-websocket-0c975c19-6fa6-47c0-b35a-4eed63ec935e is Pending, waiting for it to be Running (with Ready = true)
May 25 02:33:32.626: INFO: The status of Pod pod-logs-websocket-0c975c19-6fa6-47c0-b35a-4eed63ec935e is Pending, waiting for it to be Running (with Ready = true)
May 25 02:33:34.632: INFO: The status of Pod pod-logs-websocket-0c975c19-6fa6-47c0-b35a-4eed63ec935e is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:33:34.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5564" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":339,"completed":25,"skipped":459,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:33:34.676: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 25 02:33:34.754: INFO: Waiting up to 5m0s for pod "pod-3ab0a8c6-b9f2-48a7-aaa9-d3053621460c" in namespace "emptydir-9472" to be "Succeeded or Failed"
May 25 02:33:34.759: INFO: Pod "pod-3ab0a8c6-b9f2-48a7-aaa9-d3053621460c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.410666ms
May 25 02:33:36.768: INFO: Pod "pod-3ab0a8c6-b9f2-48a7-aaa9-d3053621460c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013794608s
STEP: Saw pod success
May 25 02:33:36.768: INFO: Pod "pod-3ab0a8c6-b9f2-48a7-aaa9-d3053621460c" satisfied condition "Succeeded or Failed"
May 25 02:33:36.772: INFO: Trying to get logs from node 10.10.103.201-worker pod pod-3ab0a8c6-b9f2-48a7-aaa9-d3053621460c container test-container: <nil>
STEP: delete the pod
May 25 02:33:36.821: INFO: Waiting for pod pod-3ab0a8c6-b9f2-48a7-aaa9-d3053621460c to disappear
May 25 02:33:36.825: INFO: Pod pod-3ab0a8c6-b9f2-48a7-aaa9-d3053621460c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:33:36.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9472" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":26,"skipped":469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:33:36.841: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 25 02:33:36.900: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3478  462f3fa4-775f-4768-8dae-32b814b3277e 828852 0 2021-05-25 02:33:36 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-05-25 02:33:36 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kmp4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kmp4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 02:33:36.905: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 25 02:33:38.914: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 25 02:33:38.914: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3478 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 02:33:38.914: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Verifying customized DNS server is configured on pod...
May 25 02:33:39.120: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3478 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 02:33:39.120: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 02:33:39.330: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:33:39.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3478" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":339,"completed":27,"skipped":500,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:33:39.360: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:33:44.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8232" for this suite.

• [SLOW TEST:5.062 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":339,"completed":28,"skipped":513,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:33:44.423: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 02:33:46.041: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 02:33:49.075: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:33:49.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2040" for this suite.
STEP: Destroying namespace "webhook-2040-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":339,"completed":29,"skipped":515,"failed":0}
SSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:33:49.305: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:33:49.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2902" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":339,"completed":30,"skipped":522,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:33:49.452: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:33:49.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4601" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":339,"completed":31,"skipped":530,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:33:49.565: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-9fa67b12-5ee4-4d52-bc02-88854a4cf827
STEP: Creating a pod to test consume secrets
May 25 02:33:49.641: INFO: Waiting up to 5m0s for pod "pod-secrets-c040986c-de3d-41bd-b176-1edfb3c8275f" in namespace "secrets-3288" to be "Succeeded or Failed"
May 25 02:33:49.653: INFO: Pod "pod-secrets-c040986c-de3d-41bd-b176-1edfb3c8275f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.699489ms
May 25 02:33:51.663: INFO: Pod "pod-secrets-c040986c-de3d-41bd-b176-1edfb3c8275f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021884714s
May 25 02:33:53.671: INFO: Pod "pod-secrets-c040986c-de3d-41bd-b176-1edfb3c8275f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030106342s
STEP: Saw pod success
May 25 02:33:53.671: INFO: Pod "pod-secrets-c040986c-de3d-41bd-b176-1edfb3c8275f" satisfied condition "Succeeded or Failed"
May 25 02:33:53.675: INFO: Trying to get logs from node 10.10.103.201-worker pod pod-secrets-c040986c-de3d-41bd-b176-1edfb3c8275f container secret-volume-test: <nil>
STEP: delete the pod
May 25 02:33:53.705: INFO: Waiting for pod pod-secrets-c040986c-de3d-41bd-b176-1edfb3c8275f to disappear
May 25 02:33:53.709: INFO: Pod pod-secrets-c040986c-de3d-41bd-b176-1edfb3c8275f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:33:53.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3288" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":32,"skipped":530,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:33:53.722: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-d0b640d5-e01e-4869-b4bf-70e91e29e7f4 in namespace container-probe-5500
May 25 02:33:57.821: INFO: Started pod liveness-d0b640d5-e01e-4869-b4bf-70e91e29e7f4 in namespace container-probe-5500
STEP: checking the pod's current state and verifying that restartCount is present
May 25 02:33:57.825: INFO: Initial restart count of pod liveness-d0b640d5-e01e-4869-b4bf-70e91e29e7f4 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:37:59.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5500" for this suite.

• [SLOW TEST:246.120 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":339,"completed":33,"skipped":539,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:37:59.843: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-c8372e1e-c374-4b35-b769-88db2df01833
STEP: Creating a pod to test consume configMaps
May 25 02:37:59.921: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4579bf99-8da0-46d7-9a7f-d26a1c689084" in namespace "projected-564" to be "Succeeded or Failed"
May 25 02:37:59.929: INFO: Pod "pod-projected-configmaps-4579bf99-8da0-46d7-9a7f-d26a1c689084": Phase="Pending", Reason="", readiness=false. Elapsed: 7.461977ms
May 25 02:38:01.936: INFO: Pod "pod-projected-configmaps-4579bf99-8da0-46d7-9a7f-d26a1c689084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014401903s
May 25 02:38:03.942: INFO: Pod "pod-projected-configmaps-4579bf99-8da0-46d7-9a7f-d26a1c689084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020642838s
STEP: Saw pod success
May 25 02:38:03.942: INFO: Pod "pod-projected-configmaps-4579bf99-8da0-46d7-9a7f-d26a1c689084" satisfied condition "Succeeded or Failed"
May 25 02:38:03.946: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-projected-configmaps-4579bf99-8da0-46d7-9a7f-d26a1c689084 container agnhost-container: <nil>
STEP: delete the pod
May 25 02:38:04.017: INFO: Waiting for pod pod-projected-configmaps-4579bf99-8da0-46d7-9a7f-d26a1c689084 to disappear
May 25 02:38:04.021: INFO: Pod pod-projected-configmaps-4579bf99-8da0-46d7-9a7f-d26a1c689084 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:38:04.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-564" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":34,"skipped":557,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:38:04.034: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4176
STEP: creating service affinity-clusterip-transition in namespace services-4176
STEP: creating replication controller affinity-clusterip-transition in namespace services-4176
I0525 02:38:04.124852      23 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-4176, replica count: 3
I0525 02:38:07.177313      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 02:38:07.195: INFO: Creating new exec pod
May 25 02:38:10.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-4176 exec execpod-affinityzmkfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
May 25 02:38:10.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 25 02:38:10.619: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 02:38:10.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-4176 exec execpod-affinityzmkfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.1.99 80'
May 25 02:38:10.990: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.1.99 80\nConnection to 10.109.1.99 80 port [tcp/http] succeeded!\n"
May 25 02:38:10.990: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 02:38:11.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-4176 exec execpod-affinityzmkfj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.109.1.99:80/ ; done'
May 25 02:38:11.524: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n"
May 25 02:38:11.524: INFO: stdout: "\naffinity-clusterip-transition-9m2sx\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-9m2sx\naffinity-clusterip-transition-9m2sx\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-9m2sx\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-9m2sx\naffinity-clusterip-transition-sqrfp\naffinity-clusterip-transition-9m2sx\naffinity-clusterip-transition-sqrfp\naffinity-clusterip-transition-9m2sx\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-sqrfp\naffinity-clusterip-transition-sqrfp"
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-9m2sx
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-9m2sx
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-9m2sx
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-9m2sx
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-9m2sx
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-sqrfp
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-9m2sx
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-sqrfp
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-9m2sx
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-sqrfp
May 25 02:38:11.524: INFO: Received response from host: affinity-clusterip-transition-sqrfp
May 25 02:38:11.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-4176 exec execpod-affinityzmkfj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.109.1.99:80/ ; done'
May 25 02:38:12.037: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.1.99:80/\n"
May 25 02:38:12.037: INFO: stdout: "\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz\naffinity-clusterip-transition-tlbzz"
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Received response from host: affinity-clusterip-transition-tlbzz
May 25 02:38:12.037: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4176, will wait for the garbage collector to delete the pods
May 25 02:38:12.114: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.568928ms
May 25 02:38:12.215: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.298946ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:38:28.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4176" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:24.424 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":35,"skipped":561,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:38:28.459: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 02:38:28.521: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2480076d-5eef-4abf-9ffb-2d31eecf2707" in namespace "downward-api-6539" to be "Succeeded or Failed"
May 25 02:38:28.527: INFO: Pod "downwardapi-volume-2480076d-5eef-4abf-9ffb-2d31eecf2707": Phase="Pending", Reason="", readiness=false. Elapsed: 6.207262ms
May 25 02:38:30.535: INFO: Pod "downwardapi-volume-2480076d-5eef-4abf-9ffb-2d31eecf2707": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014324978s
May 25 02:38:32.544: INFO: Pod "downwardapi-volume-2480076d-5eef-4abf-9ffb-2d31eecf2707": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023156758s
STEP: Saw pod success
May 25 02:38:32.544: INFO: Pod "downwardapi-volume-2480076d-5eef-4abf-9ffb-2d31eecf2707" satisfied condition "Succeeded or Failed"
May 25 02:38:32.548: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-2480076d-5eef-4abf-9ffb-2d31eecf2707 container client-container: <nil>
STEP: delete the pod
May 25 02:38:32.573: INFO: Waiting for pod downwardapi-volume-2480076d-5eef-4abf-9ffb-2d31eecf2707 to disappear
May 25 02:38:32.586: INFO: Pod downwardapi-volume-2480076d-5eef-4abf-9ffb-2d31eecf2707 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:38:32.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6539" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":36,"skipped":599,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:38:32.599: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
May 25 02:38:32.678: INFO: Found Service test-service-dhl58 in namespace services-116 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May 25 02:38:32.678: INFO: Service test-service-dhl58 created
STEP: Getting /status
May 25 02:38:32.684: INFO: Service test-service-dhl58 has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
May 25 02:38:32.695: INFO: observed Service test-service-dhl58 in namespace services-116 with annotations: map[] & LoadBalancer: {[]}
May 25 02:38:32.695: INFO: Found Service test-service-dhl58 in namespace services-116 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May 25 02:38:32.695: INFO: Service test-service-dhl58 has service status patched
STEP: updating the ServiceStatus
May 25 02:38:32.708: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
May 25 02:38:32.711: INFO: Observed Service test-service-dhl58 in namespace services-116 with annotations: map[] & Conditions: {[]}
May 25 02:38:32.711: INFO: Observed event: &Service{ObjectMeta:{test-service-dhl58  services-116  384863fc-5764-4c72-9c8f-bc3ec92b6a07 830105 0 2021-05-25 02:38:32 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2021-05-25 02:38:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}},"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}}}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.99.228.0,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,TopologyKeys:[],IPFamilyPolicy:*SingleStack,ClusterIPs:[10.99.228.0],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:nil,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May 25 02:38:32.712: INFO: Found Service test-service-dhl58 in namespace services-116 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 25 02:38:32.712: INFO: Service test-service-dhl58 has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
May 25 02:38:32.724: INFO: observed Service test-service-dhl58 in namespace services-116 with labels: map[test-service-static:true]
May 25 02:38:32.724: INFO: observed Service test-service-dhl58 in namespace services-116 with labels: map[test-service-static:true]
May 25 02:38:32.724: INFO: observed Service test-service-dhl58 in namespace services-116 with labels: map[test-service-static:true]
May 25 02:38:32.724: INFO: Found Service test-service-dhl58 in namespace services-116 with labels: map[test-service:patched test-service-static:true]
May 25 02:38:32.724: INFO: Service test-service-dhl58 patched
STEP: deleting the service
STEP: watching for the Service to be deleted
May 25 02:38:32.746: INFO: Observed event: ADDED
May 25 02:38:32.746: INFO: Observed event: MODIFIED
May 25 02:38:32.747: INFO: Observed event: MODIFIED
May 25 02:38:32.747: INFO: Observed event: MODIFIED
May 25 02:38:32.747: INFO: Found Service test-service-dhl58 in namespace services-116 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May 25 02:38:32.747: INFO: Service test-service-dhl58 deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:38:32.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-116" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":339,"completed":37,"skipped":612,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:38:32.764: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0525 02:39:12.877633      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 25 02:40:14.904: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 25 02:40:14.904: INFO: Deleting pod "simpletest.rc-24pdr" in namespace "gc-3959"
May 25 02:40:14.924: INFO: Deleting pod "simpletest.rc-2vk67" in namespace "gc-3959"
May 25 02:40:14.953: INFO: Deleting pod "simpletest.rc-jvkxb" in namespace "gc-3959"
May 25 02:40:14.968: INFO: Deleting pod "simpletest.rc-kj8rx" in namespace "gc-3959"
May 25 02:40:14.989: INFO: Deleting pod "simpletest.rc-lvjt4" in namespace "gc-3959"
May 25 02:40:15.003: INFO: Deleting pod "simpletest.rc-n75wj" in namespace "gc-3959"
May 25 02:40:15.018: INFO: Deleting pod "simpletest.rc-ntc4w" in namespace "gc-3959"
May 25 02:40:15.032: INFO: Deleting pod "simpletest.rc-q6q4n" in namespace "gc-3959"
May 25 02:40:15.046: INFO: Deleting pod "simpletest.rc-vl8db" in namespace "gc-3959"
May 25 02:40:15.062: INFO: Deleting pod "simpletest.rc-z82d2" in namespace "gc-3959"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:40:15.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3959" for this suite.

• [SLOW TEST:102.331 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":339,"completed":38,"skipped":653,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:40:15.095: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
May 25 02:40:15.182: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:40:52.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7801" for this suite.

• [SLOW TEST:37.882 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":339,"completed":39,"skipped":653,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:40:52.977: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 25 02:40:55.110: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:40:55.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3818" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":339,"completed":40,"skipped":688,"failed":0}
SSSSSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:40:55.140: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 25 02:40:57.230: INFO: &Pod{ObjectMeta:{send-events-84a9f8f9-9a58-4e48-bd54-9c4d7206e9bf  events-5430  7c32201c-75f6-4475-9832-5b407d12dc57 830839 0 2021-05-25 02:40:55 +0000 UTC <nil> <nil> map[name:foo time:198230947] map[cni.projectcalico.org/podIP:192.168.31.90/32 cni.projectcalico.org/podIPs:192.168.31.90/32] [] []  [{e2e.test Update v1 2021-05-25 02:40:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 02:40:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 02:40:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.31.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94xmv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94xmv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 02:40:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 02:40:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 02:40:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 02:40:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:192.168.31.90,StartTime:2021-05-25 02:40:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 02:40:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker://sha256:a51db9feaeb94e79df8275b6c4921047efdd40b23de5153754aedde5fd42a900,ContainerID:docker://4bf9f9de03db58f5e7afd51477748c8f980c82c6d991ba344f6efeab4a4b7ef3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.31.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 25 02:40:59.243: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 25 02:41:01.251: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:41:01.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5430" for this suite.

• [SLOW TEST:6.158 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":339,"completed":41,"skipped":695,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:41:01.298: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1514
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
May 25 02:41:01.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-8995 run e2e-test-httpd-pod --restart=Never --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
May 25 02:41:01.652: INFO: stderr: ""
May 25 02:41:01.652: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1518
May 25 02:41:01.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-8995 delete pods e2e-test-httpd-pod'
May 25 02:41:05.485: INFO: stderr: ""
May 25 02:41:05.485: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:41:05.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8995" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":339,"completed":42,"skipped":708,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:41:05.504: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-a8eef7a6-2ee9-4d38-9151-bc65c3810543
STEP: Creating a pod to test consume secrets
May 25 02:41:05.578: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4a041942-a782-46a8-bb1d-b77df61d84cb" in namespace "projected-8736" to be "Succeeded or Failed"
May 25 02:41:05.586: INFO: Pod "pod-projected-secrets-4a041942-a782-46a8-bb1d-b77df61d84cb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.123617ms
May 25 02:41:07.595: INFO: Pod "pod-projected-secrets-4a041942-a782-46a8-bb1d-b77df61d84cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016396517s
STEP: Saw pod success
May 25 02:41:07.595: INFO: Pod "pod-projected-secrets-4a041942-a782-46a8-bb1d-b77df61d84cb" satisfied condition "Succeeded or Failed"
May 25 02:41:07.599: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-projected-secrets-4a041942-a782-46a8-bb1d-b77df61d84cb container projected-secret-volume-test: <nil>
STEP: delete the pod
May 25 02:41:07.662: INFO: Waiting for pod pod-projected-secrets-4a041942-a782-46a8-bb1d-b77df61d84cb to disappear
May 25 02:41:07.667: INFO: Pod pod-projected-secrets-4a041942-a782-46a8-bb1d-b77df61d84cb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:41:07.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8736" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":43,"skipped":710,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:41:07.681: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:41:23.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9623" for this suite.

• [SLOW TEST:16.289 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":339,"completed":44,"skipped":712,"failed":0}
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:41:23.970: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
May 25 02:41:24.053: INFO: observed Pod pod-test in namespace pods-1564 in phase Pending with labels: map[test-pod-static:true] & conditions []
May 25 02:41:24.060: INFO: observed Pod pod-test in namespace pods-1564 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:24 +0000 UTC  }]
May 25 02:41:24.081: INFO: observed Pod pod-test in namespace pods-1564 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:24 +0000 UTC  }]
May 25 02:41:25.371: INFO: observed Pod pod-test in namespace pods-1564 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:24 +0000 UTC  }]
May 25 02:41:26.043: INFO: Found Pod pod-test in namespace pods-1564 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:41:24 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
May 25 02:41:26.066: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
May 25 02:41:26.111: INFO: observed event type ADDED
May 25 02:41:26.111: INFO: observed event type MODIFIED
May 25 02:41:26.111: INFO: observed event type MODIFIED
May 25 02:41:26.112: INFO: observed event type MODIFIED
May 25 02:41:26.112: INFO: observed event type MODIFIED
May 25 02:41:26.112: INFO: observed event type MODIFIED
May 25 02:41:26.112: INFO: observed event type MODIFIED
May 25 02:41:26.112: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:41:26.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1564" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":339,"completed":45,"skipped":712,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:41:26.126: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 25 02:41:26.203: INFO: Waiting up to 1m0s for all nodes to be ready
May 25 02:42:26.261: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:42:26.265: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 25 02:42:28.360: INFO: found a healthy node: 10.10.103.202-worker
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:42:42.476: INFO: pods created so far: [1 1 1]
May 25 02:42:42.476: INFO: length of pods created so far: 3
May 25 02:42:52.497: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:42:59.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3655" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:42:59.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1212" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:93.495 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":339,"completed":46,"skipped":726,"failed":0}
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:42:59.621: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 02:42:59.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-50987f89-a8d3-42fd-b66e-004089f31b7f" in namespace "downward-api-9895" to be "Succeeded or Failed"
May 25 02:42:59.713: INFO: Pod "downwardapi-volume-50987f89-a8d3-42fd-b66e-004089f31b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.322648ms
May 25 02:43:01.721: INFO: Pod "downwardapi-volume-50987f89-a8d3-42fd-b66e-004089f31b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012439726s
STEP: Saw pod success
May 25 02:43:01.721: INFO: Pod "downwardapi-volume-50987f89-a8d3-42fd-b66e-004089f31b7f" satisfied condition "Succeeded or Failed"
May 25 02:43:01.725: INFO: Trying to get logs from node 10.10.103.201-worker pod downwardapi-volume-50987f89-a8d3-42fd-b66e-004089f31b7f container client-container: <nil>
STEP: delete the pod
May 25 02:43:01.774: INFO: Waiting for pod downwardapi-volume-50987f89-a8d3-42fd-b66e-004089f31b7f to disappear
May 25 02:43:01.782: INFO: Pod downwardapi-volume-50987f89-a8d3-42fd-b66e-004089f31b7f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:43:01.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9895" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":47,"skipped":726,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:43:01.794: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-09faa82c-be2b-4c1b-b25e-ae7b5aa74d8a
STEP: Creating a pod to test consume configMaps
May 25 02:43:01.867: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4f36bfe7-c946-4f8e-96ca-0b1c4aa733d2" in namespace "projected-9607" to be "Succeeded or Failed"
May 25 02:43:01.872: INFO: Pod "pod-projected-configmaps-4f36bfe7-c946-4f8e-96ca-0b1c4aa733d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.641106ms
May 25 02:43:03.881: INFO: Pod "pod-projected-configmaps-4f36bfe7-c946-4f8e-96ca-0b1c4aa733d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014047013s
STEP: Saw pod success
May 25 02:43:03.881: INFO: Pod "pod-projected-configmaps-4f36bfe7-c946-4f8e-96ca-0b1c4aa733d2" satisfied condition "Succeeded or Failed"
May 25 02:43:03.887: INFO: Trying to get logs from node 10.10.103.201-worker pod pod-projected-configmaps-4f36bfe7-c946-4f8e-96ca-0b1c4aa733d2 container agnhost-container: <nil>
STEP: delete the pod
May 25 02:43:03.917: INFO: Waiting for pod pod-projected-configmaps-4f36bfe7-c946-4f8e-96ca-0b1c4aa733d2 to disappear
May 25 02:43:03.927: INFO: Pod pod-projected-configmaps-4f36bfe7-c946-4f8e-96ca-0b1c4aa733d2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:43:03.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9607" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":48,"skipped":732,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:43:03.943: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0525 02:43:10.050300      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 25 02:44:12.080: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:44:12.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1717" for this suite.

• [SLOW TEST:68.156 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":339,"completed":49,"skipped":742,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:44:12.100: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7322
STEP: creating service affinity-clusterip in namespace services-7322
STEP: creating replication controller affinity-clusterip in namespace services-7322
I0525 02:44:12.200425      23 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-7322, replica count: 3
I0525 02:44:15.251211      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 02:44:15.324: INFO: Creating new exec pod
May 25 02:44:20.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7322 exec execpod-affinityv6gns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
May 25 02:44:20.759: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 25 02:44:20.760: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 02:44:20.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7322 exec execpod-affinityv6gns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.120.227 80'
May 25 02:44:21.123: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.120.227 80\nConnection to 10.103.120.227 80 port [tcp/http] succeeded!\n"
May 25 02:44:21.123: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 02:44:21.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7322 exec execpod-affinityv6gns -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.120.227:80/ ; done'
May 25 02:44:21.641: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.120.227:80/\n"
May 25 02:44:21.642: INFO: stdout: "\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v\naffinity-clusterip-zlw9v"
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Received response from host: affinity-clusterip-zlw9v
May 25 02:44:21.642: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7322, will wait for the garbage collector to delete the pods
May 25 02:44:21.729: INFO: Deleting ReplicationController affinity-clusterip took: 6.771914ms
May 25 02:44:21.829: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.518045ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:44:35.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7322" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:23.581 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":50,"skipped":755,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:44:35.681: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:44:35.763: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-aa7769aa-3e08-4760-9a1e-47eff92e9e06" in namespace "security-context-test-4147" to be "Succeeded or Failed"
May 25 02:44:35.770: INFO: Pod "busybox-readonly-false-aa7769aa-3e08-4760-9a1e-47eff92e9e06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.282577ms
May 25 02:44:37.778: INFO: Pod "busybox-readonly-false-aa7769aa-3e08-4760-9a1e-47eff92e9e06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014670004s
May 25 02:44:39.789: INFO: Pod "busybox-readonly-false-aa7769aa-3e08-4760-9a1e-47eff92e9e06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026120133s
May 25 02:44:39.790: INFO: Pod "busybox-readonly-false-aa7769aa-3e08-4760-9a1e-47eff92e9e06" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:44:39.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4147" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":339,"completed":51,"skipped":786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:44:39.804: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 25 02:44:39.874: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:44:48.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8996" for this suite.

• [SLOW TEST:8.609 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":339,"completed":52,"skipped":809,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:44:48.413: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:45:01.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7809" for this suite.
STEP: Destroying namespace "nsdeletetest-3132" for this suite.
May 25 02:45:01.622: INFO: Namespace nsdeletetest-3132 was already deleted
STEP: Destroying namespace "nsdeletetest-2400" for this suite.

• [SLOW TEST:13.216 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":339,"completed":53,"skipped":825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:45:01.630: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
May 25 02:45:01.696: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May 25 02:45:03.706: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May 25 02:45:05.710: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 25 02:45:06.746: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:45:07.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-329" for this suite.

• [SLOW TEST:6.155 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":339,"completed":54,"skipped":902,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:45:07.786: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 02:45:07.859: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fec896f-4781-4f76-be09-6f696e3a5023" in namespace "projected-6297" to be "Succeeded or Failed"
May 25 02:45:07.868: INFO: Pod "downwardapi-volume-0fec896f-4781-4f76-be09-6f696e3a5023": Phase="Pending", Reason="", readiness=false. Elapsed: 8.820744ms
May 25 02:45:09.879: INFO: Pod "downwardapi-volume-0fec896f-4781-4f76-be09-6f696e3a5023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019608989s
May 25 02:45:11.892: INFO: Pod "downwardapi-volume-0fec896f-4781-4f76-be09-6f696e3a5023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032772514s
STEP: Saw pod success
May 25 02:45:11.892: INFO: Pod "downwardapi-volume-0fec896f-4781-4f76-be09-6f696e3a5023" satisfied condition "Succeeded or Failed"
May 25 02:45:11.896: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-0fec896f-4781-4f76-be09-6f696e3a5023 container client-container: <nil>
STEP: delete the pod
May 25 02:45:11.965: INFO: Waiting for pod downwardapi-volume-0fec896f-4781-4f76-be09-6f696e3a5023 to disappear
May 25 02:45:11.969: INFO: Pod downwardapi-volume-0fec896f-4781-4f76-be09-6f696e3a5023 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:45:11.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6297" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":55,"skipped":911,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:45:11.982: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 25 02:45:12.059: INFO: Waiting up to 1m0s for all nodes to be ready
May 25 02:46:12.122: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:46:12.126: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:46:12.222: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
May 25 02:46:12.227: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:46:12.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7256" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:46:12.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7279" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.352 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":339,"completed":56,"skipped":918,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:46:12.335: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-1594
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1594 to expose endpoints map[]
May 25 02:46:12.408: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May 25 02:46:13.423: INFO: successfully validated that service endpoint-test2 in namespace services-1594 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1594
May 25 02:46:13.438: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 25 02:46:15.446: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 25 02:46:17.444: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1594 to expose endpoints map[pod1:[80]]
May 25 02:46:17.465: INFO: successfully validated that service endpoint-test2 in namespace services-1594 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-1594
May 25 02:46:17.477: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 25 02:46:19.483: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 25 02:46:21.482: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1594 to expose endpoints map[pod1:[80] pod2:[80]]
May 25 02:46:21.503: INFO: successfully validated that service endpoint-test2 in namespace services-1594 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-1594
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1594 to expose endpoints map[pod2:[80]]
May 25 02:46:21.563: INFO: successfully validated that service endpoint-test2 in namespace services-1594 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-1594
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1594 to expose endpoints map[]
May 25 02:46:22.599: INFO: successfully validated that service endpoint-test2 in namespace services-1594 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:46:22.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1594" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:10.320 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":339,"completed":57,"skipped":949,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:46:22.655: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
May 25 02:46:22.713: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-5240 proxy --unix-socket=/tmp/kubectl-proxy-unix825036672/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:46:22.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5240" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":339,"completed":58,"skipped":952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:46:22.893: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 25 02:46:22.966: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5780  858846e0-8b73-489e-b5cf-3ceb40acb45f 832835 0 2021-05-25 02:46:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 02:46:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 02:46:22.966: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5780  858846e0-8b73-489e-b5cf-3ceb40acb45f 832835 0 2021-05-25 02:46:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 02:46:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 25 02:46:32.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5780  858846e0-8b73-489e-b5cf-3ceb40acb45f 832882 0 2021-05-25 02:46:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 02:46:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 02:46:32.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5780  858846e0-8b73-489e-b5cf-3ceb40acb45f 832882 0 2021-05-25 02:46:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 02:46:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 25 02:46:43.013: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5780  858846e0-8b73-489e-b5cf-3ceb40acb45f 832907 0 2021-05-25 02:46:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 02:46:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 02:46:43.013: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5780  858846e0-8b73-489e-b5cf-3ceb40acb45f 832907 0 2021-05-25 02:46:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 02:46:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 25 02:46:53.031: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5780  858846e0-8b73-489e-b5cf-3ceb40acb45f 832932 0 2021-05-25 02:46:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 02:46:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 02:46:53.031: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5780  858846e0-8b73-489e-b5cf-3ceb40acb45f 832932 0 2021-05-25 02:46:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-25 02:46:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 25 02:47:03.051: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5780  a77cf8dc-1c46-4475-b92c-c2f9296e65e8 832957 0 2021-05-25 02:47:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-25 02:47:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 02:47:03.051: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5780  a77cf8dc-1c46-4475-b92c-c2f9296e65e8 832957 0 2021-05-25 02:47:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-25 02:47:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 25 02:47:13.065: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5780  a77cf8dc-1c46-4475-b92c-c2f9296e65e8 832982 0 2021-05-25 02:47:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-25 02:47:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 02:47:13.066: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5780  a77cf8dc-1c46-4475-b92c-c2f9296e65e8 832982 0 2021-05-25 02:47:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-25 02:47:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:47:23.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5780" for this suite.

• [SLOW TEST:60.198 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":339,"completed":59,"skipped":974,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:47:23.091: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
May 25 02:47:23.155: INFO: created test-pod-1
May 25 02:47:23.164: INFO: created test-pod-2
May 25 02:47:23.190: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:47:23.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9211" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":339,"completed":60,"skipped":979,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:47:23.284: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:47:51.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2890" for this suite.

• [SLOW TEST:28.138 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":339,"completed":61,"skipped":990,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:47:51.423: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:47:51.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-504" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":339,"completed":62,"skipped":993,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:47:51.526: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-c5523a3b-1ada-4ae6-9a2f-c9f7458c58c9
STEP: Creating a pod to test consume configMaps
May 25 02:47:51.583: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-411b5875-8258-4b95-94a0-bffa7ee14f0f" in namespace "projected-2202" to be "Succeeded or Failed"
May 25 02:47:51.587: INFO: Pod "pod-projected-configmaps-411b5875-8258-4b95-94a0-bffa7ee14f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125297ms
May 25 02:47:53.598: INFO: Pod "pod-projected-configmaps-411b5875-8258-4b95-94a0-bffa7ee14f0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014614975s
STEP: Saw pod success
May 25 02:47:53.598: INFO: Pod "pod-projected-configmaps-411b5875-8258-4b95-94a0-bffa7ee14f0f" satisfied condition "Succeeded or Failed"
May 25 02:47:53.606: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-projected-configmaps-411b5875-8258-4b95-94a0-bffa7ee14f0f container agnhost-container: <nil>
STEP: delete the pod
May 25 02:47:53.657: INFO: Waiting for pod pod-projected-configmaps-411b5875-8258-4b95-94a0-bffa7ee14f0f to disappear
May 25 02:47:53.661: INFO: Pod pod-projected-configmaps-411b5875-8258-4b95-94a0-bffa7ee14f0f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:47:53.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2202" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":63,"skipped":1008,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:47:53.675: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
May 25 02:47:57.762: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4081 PodName:pod-sharedvolume-388de8d0-f4a1-4dca-968f-cb803c4357c5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 02:47:57.762: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 02:47:57.945: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:47:57.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4081" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":339,"completed":64,"skipped":1033,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:47:57.963: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:47:58.016: INFO: Creating pod...
May 25 02:47:58.029: INFO: Pod Quantity: 1 Status: Pending
May 25 02:47:59.036: INFO: Pod Quantity: 1 Status: Pending
May 25 02:48:00.038: INFO: Pod Status: Running
May 25 02:48:00.038: INFO: Creating service...
May 25 02:48:00.053: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/pods/agnhost/proxy/some/path/with/DELETE
May 25 02:48:00.067: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 25 02:48:00.067: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/pods/agnhost/proxy/some/path/with/GET
May 25 02:48:00.074: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 25 02:48:00.074: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/pods/agnhost/proxy/some/path/with/HEAD
May 25 02:48:00.082: INFO: http.Client request:HEAD | StatusCode:200
May 25 02:48:00.082: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/pods/agnhost/proxy/some/path/with/OPTIONS
May 25 02:48:00.087: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 25 02:48:00.087: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/pods/agnhost/proxy/some/path/with/PATCH
May 25 02:48:00.091: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 25 02:48:00.091: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/pods/agnhost/proxy/some/path/with/POST
May 25 02:48:00.096: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 25 02:48:00.096: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/pods/agnhost/proxy/some/path/with/PUT
May 25 02:48:00.100: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 25 02:48:00.100: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/services/test-service/proxy/some/path/with/DELETE
May 25 02:48:00.106: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 25 02:48:00.106: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/services/test-service/proxy/some/path/with/GET
May 25 02:48:00.112: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 25 02:48:00.113: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/services/test-service/proxy/some/path/with/HEAD
May 25 02:48:00.118: INFO: http.Client request:HEAD | StatusCode:200
May 25 02:48:00.118: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/services/test-service/proxy/some/path/with/OPTIONS
May 25 02:48:00.124: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 25 02:48:00.124: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/services/test-service/proxy/some/path/with/PATCH
May 25 02:48:00.130: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 25 02:48:00.130: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/services/test-service/proxy/some/path/with/POST
May 25 02:48:00.136: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 25 02:48:00.137: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6231/services/test-service/proxy/some/path/with/PUT
May 25 02:48:00.143: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:48:00.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6231" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":339,"completed":65,"skipped":1035,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:48:00.159: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:48:00.210: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-7296
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:48:06.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-2417" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:48:06.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7296" for this suite.

• [SLOW TEST:6.236 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":339,"completed":66,"skipped":1053,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:48:06.395: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 02:48:06.461: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57238c65-1b8f-4d37-9b2b-bf3dedf1471f" in namespace "projected-7290" to be "Succeeded or Failed"
May 25 02:48:06.465: INFO: Pod "downwardapi-volume-57238c65-1b8f-4d37-9b2b-bf3dedf1471f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.170615ms
May 25 02:48:08.572: INFO: Pod "downwardapi-volume-57238c65-1b8f-4d37-9b2b-bf3dedf1471f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110656619s
May 25 02:48:10.589: INFO: Pod "downwardapi-volume-57238c65-1b8f-4d37-9b2b-bf3dedf1471f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.12775545s
STEP: Saw pod success
May 25 02:48:10.589: INFO: Pod "downwardapi-volume-57238c65-1b8f-4d37-9b2b-bf3dedf1471f" satisfied condition "Succeeded or Failed"
May 25 02:48:10.604: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-57238c65-1b8f-4d37-9b2b-bf3dedf1471f container client-container: <nil>
STEP: delete the pod
May 25 02:48:10.630: INFO: Waiting for pod downwardapi-volume-57238c65-1b8f-4d37-9b2b-bf3dedf1471f to disappear
May 25 02:48:10.643: INFO: Pod downwardapi-volume-57238c65-1b8f-4d37-9b2b-bf3dedf1471f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:48:10.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7290" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":67,"skipped":1062,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:48:10.658: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
May 25 02:48:10.717: INFO: Waiting up to 5m0s for pod "pod-850f8ee4-b7a4-4d6b-9ff6-72388212393a" in namespace "emptydir-1479" to be "Succeeded or Failed"
May 25 02:48:10.723: INFO: Pod "pod-850f8ee4-b7a4-4d6b-9ff6-72388212393a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.367538ms
May 25 02:48:12.732: INFO: Pod "pod-850f8ee4-b7a4-4d6b-9ff6-72388212393a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014045271s
STEP: Saw pod success
May 25 02:48:12.732: INFO: Pod "pod-850f8ee4-b7a4-4d6b-9ff6-72388212393a" satisfied condition "Succeeded or Failed"
May 25 02:48:12.736: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-850f8ee4-b7a4-4d6b-9ff6-72388212393a container test-container: <nil>
STEP: delete the pod
May 25 02:48:12.774: INFO: Waiting for pod pod-850f8ee4-b7a4-4d6b-9ff6-72388212393a to disappear
May 25 02:48:12.779: INFO: Pod pod-850f8ee4-b7a4-4d6b-9ff6-72388212393a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:48:12.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1479" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":68,"skipped":1089,"failed":0}
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:48:12.793: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
May 25 02:48:12.852: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:48:18.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-975" for this suite.

• [SLOW TEST:6.145 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":339,"completed":69,"skipped":1094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:48:18.939: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 02:48:18.998: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a21e899a-49ba-4831-bf17-7d502ea7a040" in namespace "projected-4514" to be "Succeeded or Failed"
May 25 02:48:19.003: INFO: Pod "downwardapi-volume-a21e899a-49ba-4831-bf17-7d502ea7a040": Phase="Pending", Reason="", readiness=false. Elapsed: 5.751217ms
May 25 02:48:21.014: INFO: Pod "downwardapi-volume-a21e899a-49ba-4831-bf17-7d502ea7a040": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016116809s
May 25 02:48:23.025: INFO: Pod "downwardapi-volume-a21e899a-49ba-4831-bf17-7d502ea7a040": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027008897s
STEP: Saw pod success
May 25 02:48:23.025: INFO: Pod "downwardapi-volume-a21e899a-49ba-4831-bf17-7d502ea7a040" satisfied condition "Succeeded or Failed"
May 25 02:48:23.029: INFO: Trying to get logs from node 10.10.103.201-worker pod downwardapi-volume-a21e899a-49ba-4831-bf17-7d502ea7a040 container client-container: <nil>
STEP: delete the pod
May 25 02:48:23.087: INFO: Waiting for pod downwardapi-volume-a21e899a-49ba-4831-bf17-7d502ea7a040 to disappear
May 25 02:48:23.091: INFO: Pod downwardapi-volume-a21e899a-49ba-4831-bf17-7d502ea7a040 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:48:23.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4514" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":339,"completed":70,"skipped":1125,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:48:23.103: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-4427
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 25 02:48:23.167: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 25 02:48:23.198: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 25 02:48:25.204: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 25 02:48:27.204: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 02:48:29.209: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 02:48:31.212: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 02:48:33.213: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 02:48:35.213: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 02:48:37.210: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 02:48:39.207: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 02:48:41.206: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 02:48:43.225: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 02:48:45.210: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 25 02:48:45.218: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 25 02:48:47.273: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 25 02:48:47.273: INFO: Breadth first check of 192.168.171.152 on host 10.10.103.201...
May 25 02:48:47.277: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.31.115:9080/dial?request=hostname&protocol=udp&host=192.168.171.152&port=8081&tries=1'] Namespace:pod-network-test-4427 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 02:48:47.277: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 02:48:47.448: INFO: Waiting for responses: map[]
May 25 02:48:47.448: INFO: reached 192.168.171.152 after 0/1 tries
May 25 02:48:47.448: INFO: Breadth first check of 192.168.31.116 on host 10.10.103.202...
May 25 02:48:47.455: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.31.115:9080/dial?request=hostname&protocol=udp&host=192.168.31.116&port=8081&tries=1'] Namespace:pod-network-test-4427 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 02:48:47.455: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 02:48:47.620: INFO: Waiting for responses: map[]
May 25 02:48:47.620: INFO: reached 192.168.31.116 after 0/1 tries
May 25 02:48:47.620: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:48:47.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4427" for this suite.

• [SLOW TEST:24.533 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":339,"completed":71,"skipped":1142,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:48:47.636: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 25 02:48:47.705: INFO: Pod name pod-release: Found 0 pods out of 1
May 25 02:48:52.715: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:48:53.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8770" for this suite.

• [SLOW TEST:6.119 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":339,"completed":72,"skipped":1153,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:48:53.756: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-2995
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-2995
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2995
May 25 02:48:53.859: INFO: Found 0 stateful pods, waiting for 1
May 25 02:49:03.877: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 25 02:49:03.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 02:49:04.267: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 02:49:04.267: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 02:49:04.267: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 02:49:04.275: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 25 02:49:14.283: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 25 02:49:14.283: INFO: Waiting for statefulset status.replicas updated to 0
May 25 02:49:14.328: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
May 25 02:49:14.328: INFO: ss-0  10.10.103.202-worker  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:48:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:48:53 +0000 UTC  }]
May 25 02:49:14.328: INFO: 
May 25 02:49:14.328: INFO: StatefulSet ss has not reached scale 3, at 1
May 25 02:49:15.346: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.972208251s
May 25 02:49:16.357: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.954373811s
May 25 02:49:17.373: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.942111238s
May 25 02:49:18.382: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.926596017s
May 25 02:49:19.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.918357191s
May 25 02:49:20.397: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.913109981s
May 25 02:49:21.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.903359697s
May 25 02:49:22.414: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.891693192s
May 25 02:49:23.427: INFO: Verifying statefulset ss doesn't scale past 3 for another 885.63963ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2995
May 25 02:49:24.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:49:24.831: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 02:49:24.831: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 02:49:24.831: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 02:49:24.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:49:25.206: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 25 02:49:25.206: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 02:49:25.206: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 02:49:25.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:49:25.625: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 25 02:49:25.625: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 02:49:25.625: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 02:49:25.633: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 25 02:49:25.633: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 25 02:49:25.633: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 25 02:49:25.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 02:49:26.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 02:49:26.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 02:49:26.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 02:49:26.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 02:49:26.386: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 02:49:26.386: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 02:49:26.386: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 02:49:26.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 02:49:26.773: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 02:49:26.773: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 02:49:26.773: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 02:49:26.773: INFO: Waiting for statefulset status.replicas updated to 0
May 25 02:49:26.779: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 25 02:49:36.794: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 25 02:49:36.794: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 25 02:49:36.794: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 25 02:49:36.812: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
May 25 02:49:36.812: INFO: ss-0  10.10.103.202-worker  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:48:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:48:53 +0000 UTC  }]
May 25 02:49:36.812: INFO: ss-1  10.10.103.201-worker  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:36.812: INFO: ss-2  10.10.103.202-worker  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:36.812: INFO: 
May 25 02:49:36.812: INFO: StatefulSet ss has not reached scale 0, at 3
May 25 02:49:37.820: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
May 25 02:49:37.820: INFO: ss-0  10.10.103.202-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:48:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:48:53 +0000 UTC  }]
May 25 02:49:37.820: INFO: ss-1  10.10.103.201-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:37.820: INFO: ss-2  10.10.103.202-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:37.820: INFO: 
May 25 02:49:37.820: INFO: StatefulSet ss has not reached scale 0, at 3
May 25 02:49:38.829: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
May 25 02:49:38.829: INFO: ss-0  10.10.103.202-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:48:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:48:53 +0000 UTC  }]
May 25 02:49:38.829: INFO: ss-1  10.10.103.201-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:38.829: INFO: ss-2  10.10.103.202-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:38.829: INFO: 
May 25 02:49:38.829: INFO: StatefulSet ss has not reached scale 0, at 3
May 25 02:49:39.836: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
May 25 02:49:39.836: INFO: ss-2  10.10.103.202-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:39.836: INFO: 
May 25 02:49:39.836: INFO: StatefulSet ss has not reached scale 0, at 1
May 25 02:49:40.844: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
May 25 02:49:40.844: INFO: ss-2  10.10.103.202-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:40.844: INFO: 
May 25 02:49:40.844: INFO: StatefulSet ss has not reached scale 0, at 1
May 25 02:49:41.856: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
May 25 02:49:41.856: INFO: ss-2  10.10.103.202-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:41.856: INFO: 
May 25 02:49:41.856: INFO: StatefulSet ss has not reached scale 0, at 1
May 25 02:49:42.863: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
May 25 02:49:42.863: INFO: ss-2  10.10.103.202-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:42.863: INFO: 
May 25 02:49:42.863: INFO: StatefulSet ss has not reached scale 0, at 1
May 25 02:49:43.875: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
May 25 02:49:43.875: INFO: ss-2  10.10.103.202-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:43.875: INFO: 
May 25 02:49:43.875: INFO: StatefulSet ss has not reached scale 0, at 1
May 25 02:49:44.883: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
May 25 02:49:44.883: INFO: ss-2  10.10.103.202-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:44.883: INFO: 
May 25 02:49:44.883: INFO: StatefulSet ss has not reached scale 0, at 1
May 25 02:49:45.894: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
May 25 02:49:45.894: INFO: ss-2  10.10.103.202-worker  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-25 02:49:14 +0000 UTC  }]
May 25 02:49:45.894: INFO: 
May 25 02:49:45.894: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2995
May 25 02:49:46.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:49:47.176: INFO: rc: 1
May 25 02:49:47.176: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 25 02:49:57.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:49:57.384: INFO: rc: 1
May 25 02:49:57.384: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:50:07.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:50:07.595: INFO: rc: 1
May 25 02:50:07.595: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:50:17.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:50:17.797: INFO: rc: 1
May 25 02:50:17.797: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:50:27.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:50:28.005: INFO: rc: 1
May 25 02:50:28.005: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:50:38.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:50:38.218: INFO: rc: 1
May 25 02:50:38.218: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:50:48.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:50:48.424: INFO: rc: 1
May 25 02:50:48.424: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:50:58.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:50:58.625: INFO: rc: 1
May 25 02:50:58.625: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:51:08.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:51:08.922: INFO: rc: 1
May 25 02:51:08.922: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:51:18.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:51:19.145: INFO: rc: 1
May 25 02:51:19.145: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:51:29.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:51:29.359: INFO: rc: 1
May 25 02:51:29.359: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:51:39.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:51:39.561: INFO: rc: 1
May 25 02:51:39.561: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:51:49.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:51:49.761: INFO: rc: 1
May 25 02:51:49.762: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:51:59.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:51:59.960: INFO: rc: 1
May 25 02:51:59.960: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:52:09.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:52:10.162: INFO: rc: 1
May 25 02:52:10.162: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:52:20.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:52:20.372: INFO: rc: 1
May 25 02:52:20.372: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:52:30.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:52:30.582: INFO: rc: 1
May 25 02:52:30.583: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:52:40.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:52:40.797: INFO: rc: 1
May 25 02:52:40.797: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:52:50.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:52:51.006: INFO: rc: 1
May 25 02:52:51.006: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:53:01.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:53:01.206: INFO: rc: 1
May 25 02:53:01.206: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:53:11.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:53:11.414: INFO: rc: 1
May 25 02:53:11.414: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:53:21.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:53:21.625: INFO: rc: 1
May 25 02:53:21.625: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:53:31.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:53:31.827: INFO: rc: 1
May 25 02:53:31.827: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:53:41.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:53:42.038: INFO: rc: 1
May 25 02:53:42.038: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:53:52.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:53:52.246: INFO: rc: 1
May 25 02:53:52.246: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:54:02.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:54:02.451: INFO: rc: 1
May 25 02:54:02.451: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:54:12.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:54:12.655: INFO: rc: 1
May 25 02:54:12.655: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:54:22.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:54:22.859: INFO: rc: 1
May 25 02:54:22.860: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:54:32.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:54:33.060: INFO: rc: 1
May 25 02:54:33.060: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:54:43.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:54:43.269: INFO: rc: 1
May 25 02:54:43.269: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 25 02:54:53.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-2995 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 02:54:53.472: INFO: rc: 1
May 25 02:54:53.472: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
May 25 02:54:53.472: INFO: Scaling statefulset ss to 0
May 25 02:54:53.492: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
May 25 02:54:53.495: INFO: Deleting all statefulset in ns statefulset-2995
May 25 02:54:53.499: INFO: Scaling statefulset ss to 0
May 25 02:54:53.513: INFO: Waiting for statefulset status.replicas updated to 0
May 25 02:54:53.517: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:54:53.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2995" for this suite.

• [SLOW TEST:359.835 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":339,"completed":73,"skipped":1163,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:54:53.591: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 02:54:56.133: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 25 02:54:58.149: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508096, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508096, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508096, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508096, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 02:55:01.179: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:55:01.188: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9161-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:55:04.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7506" for this suite.
STEP: Destroying namespace "webhook-7506-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.219 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":339,"completed":74,"skipped":1177,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:55:04.811: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7267.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7267.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7267.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7267.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7267.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7267.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 106.174.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.174.106_udp@PTR;check="$$(dig +tcp +noall +answer +search 106.174.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.174.106_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7267.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7267.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7267.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7267.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7267.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7267.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7267.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 106.174.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.174.106_udp@PTR;check="$$(dig +tcp +noall +answer +search 106.174.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.174.106_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 02:55:08.934: INFO: Unable to read wheezy_udp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:08.939: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:08.944: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:08.948: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:08.975: INFO: Unable to read jessie_udp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:08.980: INFO: Unable to read jessie_tcp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:08.984: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:08.988: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:09.013: INFO: Lookups using dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8 failed for: [wheezy_udp@dns-test-service.dns-7267.svc.cluster.local wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local jessie_udp@dns-test-service.dns-7267.svc.cluster.local jessie_tcp@dns-test-service.dns-7267.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local]

May 25 02:55:14.023: INFO: Unable to read wheezy_udp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:14.028: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:14.032: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:14.037: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:14.068: INFO: Unable to read jessie_udp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:14.073: INFO: Unable to read jessie_tcp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:14.078: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:14.084: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:14.112: INFO: Lookups using dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8 failed for: [wheezy_udp@dns-test-service.dns-7267.svc.cluster.local wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local jessie_udp@dns-test-service.dns-7267.svc.cluster.local jessie_tcp@dns-test-service.dns-7267.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local]

May 25 02:55:19.020: INFO: Unable to read wheezy_udp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:19.027: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:19.035: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:19.040: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:19.079: INFO: Unable to read jessie_udp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:19.083: INFO: Unable to read jessie_tcp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:19.089: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:19.094: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:19.132: INFO: Lookups using dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8 failed for: [wheezy_udp@dns-test-service.dns-7267.svc.cluster.local wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local jessie_udp@dns-test-service.dns-7267.svc.cluster.local jessie_tcp@dns-test-service.dns-7267.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local]

May 25 02:55:24.021: INFO: Unable to read wheezy_udp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:24.026: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:24.031: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:24.035: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:24.066: INFO: Unable to read jessie_udp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:24.070: INFO: Unable to read jessie_tcp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:24.075: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:24.080: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:24.106: INFO: Lookups using dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8 failed for: [wheezy_udp@dns-test-service.dns-7267.svc.cluster.local wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local jessie_udp@dns-test-service.dns-7267.svc.cluster.local jessie_tcp@dns-test-service.dns-7267.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local]

May 25 02:55:29.020: INFO: Unable to read wheezy_udp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:29.024: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:29.029: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:29.033: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:29.065: INFO: Unable to read jessie_udp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:29.070: INFO: Unable to read jessie_tcp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:29.075: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:29.080: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:29.113: INFO: Lookups using dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8 failed for: [wheezy_udp@dns-test-service.dns-7267.svc.cluster.local wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local jessie_udp@dns-test-service.dns-7267.svc.cluster.local jessie_tcp@dns-test-service.dns-7267.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local]

May 25 02:55:34.021: INFO: Unable to read wheezy_udp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:34.026: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:34.031: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:34.035: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:34.066: INFO: Unable to read jessie_udp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:34.070: INFO: Unable to read jessie_tcp@dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:34.075: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:34.080: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local from pod dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8: the server could not find the requested resource (get pods dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8)
May 25 02:55:34.107: INFO: Lookups using dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8 failed for: [wheezy_udp@dns-test-service.dns-7267.svc.cluster.local wheezy_tcp@dns-test-service.dns-7267.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local jessie_udp@dns-test-service.dns-7267.svc.cluster.local jessie_tcp@dns-test-service.dns-7267.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7267.svc.cluster.local]

May 25 02:55:39.120: INFO: DNS probes using dns-7267/dns-test-88acfe7c-a2a6-49a4-98d7-aa1c3463f9d8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:55:39.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7267" for this suite.

• [SLOW TEST:34.483 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":339,"completed":75,"skipped":1182,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:55:39.294: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:55:39.352: INFO: Creating deployment "test-recreate-deployment"
May 25 02:55:39.358: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 25 02:55:39.380: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 25 02:55:41.390: INFO: Waiting deployment "test-recreate-deployment" to complete
May 25 02:55:41.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508139, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508139, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508139, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508139, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6cb8b65c46\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 02:55:43.412: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 25 02:55:43.424: INFO: Updating deployment test-recreate-deployment
May 25 02:55:43.424: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 25 02:55:43.542: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4166  8004baaa-81e1-4020-ba3f-f6964fd00fe4 835134 2 2021-05-25 02:55:39 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-25 02:55:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-25 02:55:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00404e5a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-25 02:55:43 +0000 UTC,LastTransitionTime:2021-05-25 02:55:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-05-25 02:55:43 +0000 UTC,LastTransitionTime:2021-05-25 02:55:39 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 25 02:55:43.547: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-4166  6ff6fe64-8055-423d-aa78-29c95b9838c6 835133 1 2021-05-25 02:55:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8004baaa-81e1-4020-ba3f-f6964fd00fe4 0xc00404ea30 0xc00404ea31}] []  [{kube-controller-manager Update apps/v1 2021-05-25 02:55:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8004baaa-81e1-4020-ba3f-f6964fd00fe4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00404eaa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 25 02:55:43.547: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 25 02:55:43.548: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-4166  0de61f73-a139-4232-895e-6e5f183b2c41 835120 2 2021-05-25 02:55:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8004baaa-81e1-4020-ba3f-f6964fd00fe4 0xc00404e937 0xc00404e938}] []  [{kube-controller-manager Update apps/v1 2021-05-25 02:55:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8004baaa-81e1-4020-ba3f-f6964fd00fe4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00404e9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 25 02:55:43.553: INFO: Pod "test-recreate-deployment-85d47dcb4-9wj4b" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-9wj4b test-recreate-deployment-85d47dcb4- deployment-4166  eae77d2b-b3e0-4321-975a-4af91b71713e 835135 0 2021-05-25 02:55:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 6ff6fe64-8055-423d-aa78-29c95b9838c6 0xc00404ef00 0xc00404ef01}] []  [{kube-controller-manager Update v1 2021-05-25 02:55:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ff6fe64-8055-423d-aa78-29c95b9838c6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-25 02:55:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h7k4t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h7k4t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 02:55:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 02:55:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 02:55:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 02:55:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:,StartTime:2021-05-25 02:55:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:55:43.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4166" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":76,"skipped":1190,"failed":0}

------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:55:43.566: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:55:43.624: INFO: Waiting up to 5m0s for pod "busybox-user-65534-09e7c415-a35e-42af-b209-0dc2e76c715f" in namespace "security-context-test-9988" to be "Succeeded or Failed"
May 25 02:55:43.629: INFO: Pod "busybox-user-65534-09e7c415-a35e-42af-b209-0dc2e76c715f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.78112ms
May 25 02:55:45.636: INFO: Pod "busybox-user-65534-09e7c415-a35e-42af-b209-0dc2e76c715f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011412063s
May 25 02:55:47.641: INFO: Pod "busybox-user-65534-09e7c415-a35e-42af-b209-0dc2e76c715f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01671903s
May 25 02:55:47.641: INFO: Pod "busybox-user-65534-09e7c415-a35e-42af-b209-0dc2e76c715f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:55:47.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9988" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":77,"skipped":1190,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:55:47.654: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 25 02:55:48.947: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 25 02:55:50.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508148, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508148, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508149, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508148, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 02:55:53.996: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:55:54.007: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:55:57.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8825" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.638 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":339,"completed":78,"skipped":1199,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:55:57.292: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 02:55:57.381: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7bd2ce26-7a4a-4746-8e15-32826827b15c" in namespace "downward-api-9010" to be "Succeeded or Failed"
May 25 02:55:57.385: INFO: Pod "downwardapi-volume-7bd2ce26-7a4a-4746-8e15-32826827b15c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.97878ms
May 25 02:55:59.395: INFO: Pod "downwardapi-volume-7bd2ce26-7a4a-4746-8e15-32826827b15c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013905197s
May 25 02:56:01.408: INFO: Pod "downwardapi-volume-7bd2ce26-7a4a-4746-8e15-32826827b15c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026364435s
STEP: Saw pod success
May 25 02:56:01.408: INFO: Pod "downwardapi-volume-7bd2ce26-7a4a-4746-8e15-32826827b15c" satisfied condition "Succeeded or Failed"
May 25 02:56:01.412: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-7bd2ce26-7a4a-4746-8e15-32826827b15c container client-container: <nil>
STEP: delete the pod
May 25 02:56:01.469: INFO: Waiting for pod downwardapi-volume-7bd2ce26-7a4a-4746-8e15-32826827b15c to disappear
May 25 02:56:01.475: INFO: Pod downwardapi-volume-7bd2ce26-7a4a-4746-8e15-32826827b15c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:56:01.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9010" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":79,"skipped":1205,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:56:01.489: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:56:01.606: INFO: Pod name sample-pod: Found 0 pods out of 1
May 25 02:56:06.615: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
May 25 02:56:06.630: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
May 25 02:56:06.640: INFO: observed ReplicaSet test-rs in namespace replicaset-3475 with ReadyReplicas 1, AvailableReplicas 1
May 25 02:56:06.666: INFO: observed ReplicaSet test-rs in namespace replicaset-3475 with ReadyReplicas 1, AvailableReplicas 1
May 25 02:56:06.703: INFO: observed ReplicaSet test-rs in namespace replicaset-3475 with ReadyReplicas 1, AvailableReplicas 1
May 25 02:56:06.712: INFO: observed ReplicaSet test-rs in namespace replicaset-3475 with ReadyReplicas 1, AvailableReplicas 1
May 25 02:56:08.579: INFO: observed ReplicaSet test-rs in namespace replicaset-3475 with ReadyReplicas 2, AvailableReplicas 2
May 25 02:56:08.701: INFO: observed Replicaset test-rs in namespace replicaset-3475 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:56:08.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3475" for this suite.

• [SLOW TEST:7.229 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":339,"completed":80,"skipped":1205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:56:08.719: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
May 25 02:56:08.777: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:56:12.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-979" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":339,"completed":81,"skipped":1231,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:56:12.904: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-8bb32d95-7674-4247-822d-ab44022aa327
STEP: Creating a pod to test consume secrets
May 25 02:56:12.988: INFO: Waiting up to 5m0s for pod "pod-secrets-bf1d3bf8-fc3b-431e-9d8b-16e5706be0c0" in namespace "secrets-2985" to be "Succeeded or Failed"
May 25 02:56:12.992: INFO: Pod "pod-secrets-bf1d3bf8-fc3b-431e-9d8b-16e5706be0c0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.795236ms
May 25 02:56:15.006: INFO: Pod "pod-secrets-bf1d3bf8-fc3b-431e-9d8b-16e5706be0c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018218502s
STEP: Saw pod success
May 25 02:56:15.007: INFO: Pod "pod-secrets-bf1d3bf8-fc3b-431e-9d8b-16e5706be0c0" satisfied condition "Succeeded or Failed"
May 25 02:56:15.012: INFO: Trying to get logs from node 10.10.103.201-worker pod pod-secrets-bf1d3bf8-fc3b-431e-9d8b-16e5706be0c0 container secret-volume-test: <nil>
STEP: delete the pod
May 25 02:56:15.064: INFO: Waiting for pod pod-secrets-bf1d3bf8-fc3b-431e-9d8b-16e5706be0c0 to disappear
May 25 02:56:15.067: INFO: Pod pod-secrets-bf1d3bf8-fc3b-431e-9d8b-16e5706be0c0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:56:15.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2985" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":82,"skipped":1231,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:56:15.080: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 02:56:16.612: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 25 02:56:18.629: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508176, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508176, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508176, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757508176, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 02:56:21.660: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:56:21.671: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3754-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:56:24.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5143" for this suite.
STEP: Destroying namespace "webhook-5143-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.807 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":339,"completed":83,"skipped":1236,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:56:24.887: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-13a384fd-c838-4a21-83be-024944738ed6 in namespace container-probe-6958
May 25 02:56:28.958: INFO: Started pod liveness-13a384fd-c838-4a21-83be-024944738ed6 in namespace container-probe-6958
STEP: checking the pod's current state and verifying that restartCount is present
May 25 02:56:28.962: INFO: Initial restart count of pod liveness-13a384fd-c838-4a21-83be-024944738ed6 is 0
May 25 02:56:47.067: INFO: Restart count of pod container-probe-6958/liveness-13a384fd-c838-4a21-83be-024944738ed6 is now 1 (18.104477759s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:56:47.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6958" for this suite.

• [SLOW TEST:22.237 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":84,"skipped":1242,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:56:47.124: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 02:56:47.191: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 25 02:56:53.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3563 --namespace=crd-publish-openapi-3563 create -f -'
May 25 02:56:53.778: INFO: stderr: ""
May 25 02:56:53.778: INFO: stdout: "e2e-test-crd-publish-openapi-3374-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 25 02:56:53.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3563 --namespace=crd-publish-openapi-3563 delete e2e-test-crd-publish-openapi-3374-crds test-cr'
May 25 02:56:53.977: INFO: stderr: ""
May 25 02:56:53.977: INFO: stdout: "e2e-test-crd-publish-openapi-3374-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 25 02:56:53.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3563 --namespace=crd-publish-openapi-3563 apply -f -'
May 25 02:56:54.476: INFO: stderr: ""
May 25 02:56:54.476: INFO: stdout: "e2e-test-crd-publish-openapi-3374-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 25 02:56:54.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3563 --namespace=crd-publish-openapi-3563 delete e2e-test-crd-publish-openapi-3374-crds test-cr'
May 25 02:56:54.680: INFO: stderr: ""
May 25 02:56:54.680: INFO: stdout: "e2e-test-crd-publish-openapi-3374-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 25 02:56:54.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-3563 explain e2e-test-crd-publish-openapi-3374-crds'
May 25 02:56:55.156: INFO: stderr: ""
May 25 02:56:55.156: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3374-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:57:01.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3563" for this suite.

• [SLOW TEST:14.333 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":339,"completed":85,"skipped":1251,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:57:01.458: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 25 02:57:01.533: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 25 02:57:01.543: INFO: starting watch
STEP: patching
STEP: updating
May 25 02:57:01.566: INFO: waiting for watch events with expected annotations
May 25 02:57:01.566: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:57:01.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4780" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":339,"completed":86,"skipped":1299,"failed":0}
SS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:57:01.627: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 25 02:57:01.711: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 25 02:57:01.716: INFO: starting watch
STEP: patching
STEP: updating
May 25 02:57:01.734: INFO: waiting for watch events with expected annotations
May 25 02:57:01.734: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:57:01.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1198" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":339,"completed":87,"skipped":1301,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:57:01.782: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:58:01.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5434" for this suite.

• [SLOW TEST:60.103 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":339,"completed":88,"skipped":1320,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:58:01.885: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:58:15.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6367" for this suite.

• [SLOW TEST:13.229 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":339,"completed":89,"skipped":1326,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:58:15.115: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
May 25 02:58:15.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4039 create -f -'
May 25 02:58:15.836: INFO: stderr: ""
May 25 02:58:15.836: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 25 02:58:15.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4039 diff -f -'
May 25 02:58:16.342: INFO: rc: 1
May 25 02:58:16.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4039 delete -f -'
May 25 02:58:16.539: INFO: stderr: ""
May 25 02:58:16.539: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:58:16.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4039" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":339,"completed":90,"skipped":1356,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:58:16.567: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 25 02:58:16.911: INFO: Pod name wrapped-volume-race-3d4807fb-8fce-4d75-99a6-4aeb3507c8dc: Found 0 pods out of 5
May 25 02:58:21.927: INFO: Pod name wrapped-volume-race-3d4807fb-8fce-4d75-99a6-4aeb3507c8dc: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3d4807fb-8fce-4d75-99a6-4aeb3507c8dc in namespace emptydir-wrapper-221, will wait for the garbage collector to delete the pods
May 25 02:58:32.025: INFO: Deleting ReplicationController wrapped-volume-race-3d4807fb-8fce-4d75-99a6-4aeb3507c8dc took: 8.01049ms
May 25 02:58:32.126: INFO: Terminating ReplicationController wrapped-volume-race-3d4807fb-8fce-4d75-99a6-4aeb3507c8dc pods took: 100.935196ms
STEP: Creating RC which spawns configmap-volume pods
May 25 02:58:48.563: INFO: Pod name wrapped-volume-race-a7079656-2a89-4983-ab75-261686dec4d9: Found 0 pods out of 5
May 25 02:58:53.581: INFO: Pod name wrapped-volume-race-a7079656-2a89-4983-ab75-261686dec4d9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a7079656-2a89-4983-ab75-261686dec4d9 in namespace emptydir-wrapper-221, will wait for the garbage collector to delete the pods
May 25 02:59:05.698: INFO: Deleting ReplicationController wrapped-volume-race-a7079656-2a89-4983-ab75-261686dec4d9 took: 9.143859ms
May 25 02:59:05.798: INFO: Terminating ReplicationController wrapped-volume-race-a7079656-2a89-4983-ab75-261686dec4d9 pods took: 100.80817ms
STEP: Creating RC which spawns configmap-volume pods
May 25 02:59:15.764: INFO: Pod name wrapped-volume-race-893d6d63-09df-4ddf-aa04-0998fca0a2cf: Found 0 pods out of 5
May 25 02:59:20.778: INFO: Pod name wrapped-volume-race-893d6d63-09df-4ddf-aa04-0998fca0a2cf: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-893d6d63-09df-4ddf-aa04-0998fca0a2cf in namespace emptydir-wrapper-221, will wait for the garbage collector to delete the pods
May 25 02:59:30.886: INFO: Deleting ReplicationController wrapped-volume-race-893d6d63-09df-4ddf-aa04-0998fca0a2cf took: 8.804168ms
May 25 02:59:30.986: INFO: Terminating ReplicationController wrapped-volume-race-893d6d63-09df-4ddf-aa04-0998fca0a2cf pods took: 100.43903ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 02:59:38.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-221" for this suite.

• [SLOW TEST:82.351 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":339,"completed":91,"skipped":1369,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 02:59:38.918: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-0472b78c-5891-47aa-87f6-411a61a45575 in namespace container-probe-3905
May 25 02:59:43.002: INFO: Started pod busybox-0472b78c-5891-47aa-87f6-411a61a45575 in namespace container-probe-3905
STEP: checking the pod's current state and verifying that restartCount is present
May 25 02:59:43.006: INFO: Initial restart count of pod busybox-0472b78c-5891-47aa-87f6-411a61a45575 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:03:44.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3905" for this suite.

• [SLOW TEST:245.521 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":92,"skipped":1383,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:03:44.439: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
May 25 03:03:44.519: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 03:03:44.519: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 03:03:44.534: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 03:03:44.534: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 03:03:44.563: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 03:03:44.563: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 03:03:44.597: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 03:03:44.597: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 25 03:03:47.646: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 25 03:03:47.646: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 25 03:03:47.665: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
May 25 03:03:47.677: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
May 25 03:03:47.679: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0
May 25 03:03:47.679: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0
May 25 03:03:47.679: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0
May 25 03:03:47.679: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0
May 25 03:03:47.680: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0
May 25 03:03:47.680: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0
May 25 03:03:47.680: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0
May 25 03:03:47.680: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 0
May 25 03:03:47.680: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
May 25 03:03:47.680: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
May 25 03:03:47.680: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:47.680: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:47.680: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:47.680: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:47.692: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:47.692: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:47.716: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:47.716: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:47.750: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
May 25 03:03:47.750: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
May 25 03:03:47.771: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
May 25 03:03:47.771: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
May 25 03:03:49.727: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:49.727: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:49.781: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
STEP: listing Deployments
May 25 03:03:49.790: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
May 25 03:03:49.807: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
May 25 03:03:49.822: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 25 03:03:49.827: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 25 03:03:49.861: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 25 03:03:49.898: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 25 03:03:49.907: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 25 03:03:51.968: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 25 03:03:51.991: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 25 03:03:52.007: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 25 03:03:52.025: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 25 03:03:53.734: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
May 25 03:03:53.814: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
May 25 03:03:53.814: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
May 25 03:03:53.814: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
May 25 03:03:53.815: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
May 25 03:03:53.815: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 1
May 25 03:03:53.815: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:53.815: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:53.816: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:53.816: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 2
May 25 03:03:53.816: INFO: observed Deployment test-deployment in namespace deployment-1 with ReadyReplicas 3
STEP: deleting the Deployment
May 25 03:03:53.829: INFO: observed event type MODIFIED
May 25 03:03:53.829: INFO: observed event type MODIFIED
May 25 03:03:53.829: INFO: observed event type MODIFIED
May 25 03:03:53.830: INFO: observed event type MODIFIED
May 25 03:03:53.830: INFO: observed event type MODIFIED
May 25 03:03:53.830: INFO: observed event type MODIFIED
May 25 03:03:53.830: INFO: observed event type MODIFIED
May 25 03:03:53.830: INFO: observed event type MODIFIED
May 25 03:03:53.830: INFO: observed event type MODIFIED
May 25 03:03:53.830: INFO: observed event type MODIFIED
May 25 03:03:53.831: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 25 03:03:53.840: INFO: Log out all the ReplicaSets if there is no deployment created
May 25 03:03:53.847: INFO: ReplicaSet "test-deployment-748588b7cd":
&ReplicaSet{ObjectMeta:{test-deployment-748588b7cd  deployment-1  6fbf6203-17d5-4010-aaaf-10914cf649f2 838121 4 2021-05-25 03:03:47 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 185ac844-0416-44e9-bded-9816346bda44 0xc003fe4357 0xc003fe4358}] []  [{kube-controller-manager Update apps/v1 2021-05-25 03:03:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"185ac844-0416-44e9-bded-9816346bda44\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 748588b7cd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.4.1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003fe43c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 25 03:03:53.853: INFO: ReplicaSet "test-deployment-7b4c744884":
&ReplicaSet{ObjectMeta:{test-deployment-7b4c744884  deployment-1  aaefb6dc-1abc-49d6-baec-2aa37df27736 838024 3 2021-05-25 03:03:44 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 185ac844-0416-44e9-bded-9816346bda44 0xc003fe4447 0xc003fe4448}] []  [{kube-controller-manager Update apps/v1 2021-05-25 03:03:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"185ac844-0416-44e9-bded-9816346bda44\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b4c744884,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003fe44b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 25 03:03:53.864: INFO: ReplicaSet "test-deployment-85d87c6f4b":
&ReplicaSet{ObjectMeta:{test-deployment-85d87c6f4b  deployment-1  54211a15-5481-42d9-8f47-4181854989e6 838111 2 2021-05-25 03:03:49 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 185ac844-0416-44e9-bded-9816346bda44 0xc003fe4947 0xc003fe4948}] []  [{kube-controller-manager Update apps/v1 2021-05-25 03:03:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"185ac844-0416-44e9-bded-9816346bda44\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 85d87c6f4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003fe49b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

May 25 03:03:53.871: INFO: pod: "test-deployment-85d87c6f4b-dns6c":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-dns6c test-deployment-85d87c6f4b- deployment-1  69f97599-1094-4ac8-9fe3-e282ffbe5595 838072 0 2021-05-25 03:03:49 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[cni.projectcalico.org/podIP:192.168.31.81/32 cni.projectcalico.org/podIPs:192.168.31.81/32] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b 54211a15-5481-42d9-8f47-4181854989e6 0xc00676c607 0xc00676c608}] []  [{kube-controller-manager Update v1 2021-05-25 03:03:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54211a15-5481-42d9-8f47-4181854989e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 03:03:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 03:03:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.31.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fcgtj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fcgtj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:03:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:03:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:03:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:03:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:192.168.31.81,StartTime:2021-05-25 03:03:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 03:03:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://9c479d4d46f7f34adf831159da43778dfb7f97a10ea5d995939706d2f05d8fdb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.31.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 25 03:03:53.872: INFO: pod: "test-deployment-85d87c6f4b-rj8p6":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-rj8p6 test-deployment-85d87c6f4b- deployment-1  b07dcce7-7c1f-4853-8328-232776d79b90 838110 0 2021-05-25 03:03:51 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[cni.projectcalico.org/podIP:192.168.171.178/32 cni.projectcalico.org/podIPs:192.168.171.178/32] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b 54211a15-5481-42d9-8f47-4181854989e6 0xc00676c817 0xc00676c818}] []  [{kube-controller-manager Update v1 2021-05-25 03:03:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54211a15-5481-42d9-8f47-4181854989e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 03:03:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 03:03:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.171.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6xvrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6xvrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.201-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:03:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:03:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:03:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:03:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.201,PodIP:192.168.171.178,StartTime:2021-05-25 03:03:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 03:03:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://7fb44e6f4ca76b71ff04863088d79797e7be3938c06c10e674347173920ba692,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.171.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:03:53.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1" for this suite.

• [SLOW TEST:9.449 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":339,"completed":93,"skipped":1387,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:03:53.889: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:03:53.987: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 25 03:03:54.001: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:03:54.015: INFO: Number of nodes with available pods: 0
May 25 03:03:54.015: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:03:55.021: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:03:55.026: INFO: Number of nodes with available pods: 0
May 25 03:03:55.026: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:03:56.025: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:03:56.031: INFO: Number of nodes with available pods: 0
May 25 03:03:56.031: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:03:57.028: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:03:57.032: INFO: Number of nodes with available pods: 2
May 25 03:03:57.033: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 25 03:03:57.072: INFO: Wrong image for pod: daemon-set-hw62x. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:03:57.072: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:03:57.077: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:03:58.090: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:03:58.095: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:03:59.083: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:03:59.090: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:00.088: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:04:00.094: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:01.090: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:04:01.095: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:02.088: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:04:02.093: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:03.088: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:04:03.094: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:04.091: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:04:04.096: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:05.090: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:04:05.096: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:06.090: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:04:06.096: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:07.089: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:04:07.095: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:08.087: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:04:08.092: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:09.087: INFO: Pod daemon-set-sfrb7 is not available
May 25 03:04:09.088: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:04:09.095: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:10.086: INFO: Pod daemon-set-sfrb7 is not available
May 25 03:04:10.086: INFO: Wrong image for pod: daemon-set-zfwts. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
May 25 03:04:10.091: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:11.089: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:12.094: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:13.086: INFO: Pod daemon-set-xr5gv is not available
May 25 03:04:13.092: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 25 03:04:13.099: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:13.107: INFO: Number of nodes with available pods: 1
May 25 03:04:13.107: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:04:14.116: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:14.121: INFO: Number of nodes with available pods: 1
May 25 03:04:14.121: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:04:15.120: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:04:15.125: INFO: Number of nodes with available pods: 2
May 25 03:04:15.125: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3300, will wait for the garbage collector to delete the pods
May 25 03:04:15.209: INFO: Deleting DaemonSet.extensions daemon-set took: 9.654892ms
May 25 03:04:15.310: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.175809ms
May 25 03:04:28.419: INFO: Number of nodes with available pods: 0
May 25 03:04:28.419: INFO: Number of running nodes: 0, number of available pods: 0
May 25 03:04:28.422: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"838385"},"items":null}

May 25 03:04:28.426: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"838385"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:04:28.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3300" for this suite.

• [SLOW TEST:34.563 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":339,"completed":94,"skipped":1395,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:04:28.453: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:04:28.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3034 create -f -'
May 25 03:04:29.024: INFO: stderr: ""
May 25 03:04:29.024: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 25 03:04:29.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3034 create -f -'
May 25 03:04:29.534: INFO: stderr: ""
May 25 03:04:29.534: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 25 03:04:30.544: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 03:04:30.544: INFO: Found 0 / 1
May 25 03:04:31.543: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 03:04:31.543: INFO: Found 1 / 1
May 25 03:04:31.543: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 25 03:04:31.548: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 03:04:31.548: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 25 03:04:31.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3034 describe pod agnhost-primary-hdqqx'
May 25 03:04:31.778: INFO: stderr: ""
May 25 03:04:31.778: INFO: stdout: "Name:         agnhost-primary-hdqqx\nNamespace:    kubectl-3034\nPriority:     0\nNode:         10.10.103.202-worker/10.10.103.202\nStart Time:   Tue, 25 May 2021 03:04:29 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 192.168.31.85/32\n              cni.projectcalico.org/podIPs: 192.168.31.85/32\nStatus:       Running\nIP:           192.168.31.85\nIPs:\n  IP:           192.168.31.85\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://786993380dba53da3cd9122a84ea7f6e0404a5d145be09e87b7a431c10d8adb5\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       docker://sha256:a51db9feaeb94e79df8275b6c4921047efdd40b23de5153754aedde5fd42a900\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 25 May 2021 03:04:30 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5tktj (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5tktj:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-3034/agnhost-primary-hdqqx to 10.10.103.202-worker\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
May 25 03:04:31.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3034 describe rc agnhost-primary'
May 25 03:04:32.026: INFO: stderr: ""
May 25 03:04:32.026: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3034\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-hdqqx\n"
May 25 03:04:32.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3034 describe service agnhost-primary'
May 25 03:04:32.239: INFO: stderr: ""
May 25 03:04:32.239: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3034\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.98.246.135\nIPs:               10.98.246.135\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.31.85:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 25 03:04:32.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3034 describe node 10.10.103.200-control-plane'
May 25 03:04:32.510: INFO: stderr: ""
May 25 03:04:32.510: INFO: stdout: "Name:               10.10.103.200-control-plane\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.10.103.200-control-plane\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.10.103.200/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 192.168.88.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 21 May 2021 05:13:48 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.10.103.200-control-plane\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 25 May 2021 03:04:22 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 21 May 2021 05:17:42 +0000   Fri, 21 May 2021 05:17:42 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 25 May 2021 02:59:48 +0000   Fri, 21 May 2021 05:13:42 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 25 May 2021 02:59:48 +0000   Fri, 21 May 2021 05:13:42 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 25 May 2021 02:59:48 +0000   Fri, 21 May 2021 05:13:42 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 25 May 2021 02:59:48 +0000   Fri, 21 May 2021 05:17:31 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.10.103.200\n  Hostname:    10.10.103.200-control-plane\nCapacity:\n  cpu:                4\n  ephemeral-storage:  46110724Ki\n  hugepages-2Mi:      0\n  memory:             8010508Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  42495643169\n  hugepages-2Mi:      0\n  memory:             7908108Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 684474595eb34a7a8fad52d82d2d29da\n  System UUID:                E8DA1242-F684-77FF-D53D-9F9EB27C0561\n  Boot ID:                    566a9ad7-d6df-4e86-a4d0-3c0130a9de47\n  Kernel Version:             3.10.0-693.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.6\n  Kubelet Version:            v1.21.1\n  Kube-Proxy Version:         v1.21.1\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system               calico-node-b2xq8                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d21h\n  calico-system               calico-typha-584858976f-b6cwm                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d21h\n  kube-system                 etcd-10.10.103.200-control-plane                           100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         3d21h\n  kube-system                 kube-apiserver-10.10.103.200-control-plane                 250m (6%)     0 (0%)      0 (0%)           0 (0%)         3d21h\n  kube-system                 kube-controller-manager-10.10.103.200-control-plane        200m (5%)     0 (0%)      0 (0%)           0 (0%)         3d21h\n  kube-system                 kube-proxy-v245t                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d21h\n  kube-system                 kube-scheduler-10.10.103.200-control-plane                 100m (2%)     0 (0%)      0 (0%)           0 (0%)         3d21h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-ebbefc146b074e77-fc54w    0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                650m (16%)  0 (0%)\n  memory             100Mi (1%)  0 (0%)\n  ephemeral-storage  100Mi (0%)  0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
May 25 03:04:32.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3034 describe namespace kubectl-3034'
May 25 03:04:32.735: INFO: stderr: ""
May 25 03:04:32.735: INFO: stdout: "Name:         kubectl-3034\nLabels:       e2e-framework=kubectl\n              e2e-run=d8ee609c-c938-4bd2-ac40-ed3abc4c9371\n              kubernetes.io/metadata.name=kubectl-3034\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:04:32.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3034" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":339,"completed":95,"skipped":1404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:04:32.757: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May 25 03:04:32.815: INFO: Waiting up to 5m0s for pod "downward-api-89e42b99-8b14-4db2-aacd-60b49f097a96" in namespace "downward-api-254" to be "Succeeded or Failed"
May 25 03:04:32.821: INFO: Pod "downward-api-89e42b99-8b14-4db2-aacd-60b49f097a96": Phase="Pending", Reason="", readiness=false. Elapsed: 6.746226ms
May 25 03:04:34.831: INFO: Pod "downward-api-89e42b99-8b14-4db2-aacd-60b49f097a96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016183518s
STEP: Saw pod success
May 25 03:04:34.831: INFO: Pod "downward-api-89e42b99-8b14-4db2-aacd-60b49f097a96" satisfied condition "Succeeded or Failed"
May 25 03:04:34.835: INFO: Trying to get logs from node 10.10.103.201-worker pod downward-api-89e42b99-8b14-4db2-aacd-60b49f097a96 container dapi-container: <nil>
STEP: delete the pod
May 25 03:04:34.890: INFO: Waiting for pod downward-api-89e42b99-8b14-4db2-aacd-60b49f097a96 to disappear
May 25 03:04:34.894: INFO: Pod downward-api-89e42b99-8b14-4db2-aacd-60b49f097a96 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:04:34.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-254" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":339,"completed":96,"skipped":1432,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:04:34.906: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-5818
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
May 25 03:04:34.993: INFO: Found 0 stateful pods, waiting for 3
May 25 03:04:45.009: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 25 03:04:45.010: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 25 03:04:45.010: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
May 25 03:04:45.052: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 25 03:04:55.114: INFO: Updating stateful set ss2
May 25 03:04:55.138: INFO: Waiting for Pod statefulset-5818/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
May 25 03:05:05.239: INFO: Found 1 stateful pods, waiting for 3
May 25 03:05:15.254: INFO: Found 2 stateful pods, waiting for 3
May 25 03:05:25.255: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 25 03:05:25.255: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 25 03:05:25.255: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 25 03:05:25.290: INFO: Updating stateful set ss2
May 25 03:05:25.306: INFO: Waiting for Pod statefulset-5818/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
May 25 03:05:35.346: INFO: Updating stateful set ss2
May 25 03:05:35.365: INFO: Waiting for StatefulSet statefulset-5818/ss2 to complete update
May 25 03:05:35.365: INFO: Waiting for Pod statefulset-5818/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
May 25 03:05:45.387: INFO: Waiting for StatefulSet statefulset-5818/ss2 to complete update
May 25 03:05:45.387: INFO: Waiting for Pod statefulset-5818/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
May 25 03:05:55.380: INFO: Deleting all statefulset in ns statefulset-5818
May 25 03:05:55.385: INFO: Scaling statefulset ss2 to 0
May 25 03:06:35.421: INFO: Waiting for statefulset status.replicas updated to 0
May 25 03:06:35.427: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:06:35.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5818" for this suite.

• [SLOW TEST:120.568 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":339,"completed":97,"skipped":1433,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:06:35.474: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
May 25 03:06:35.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4535 create -f -'
May 25 03:06:36.010: INFO: stderr: ""
May 25 03:06:36.010: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 25 03:06:37.022: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 03:06:37.022: INFO: Found 0 / 1
May 25 03:06:38.016: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 03:06:38.016: INFO: Found 1 / 1
May 25 03:06:38.016: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 25 03:06:38.020: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 03:06:38.020: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 25 03:06:38.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4535 patch pod agnhost-primary-krgd4 -p {"metadata":{"annotations":{"x":"y"}}}'
May 25 03:06:38.229: INFO: stderr: ""
May 25 03:06:38.229: INFO: stdout: "pod/agnhost-primary-krgd4 patched\n"
STEP: checking annotations
May 25 03:06:38.234: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 03:06:38.234: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:06:38.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4535" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":339,"completed":98,"skipped":1442,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:06:38.245: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-9j2g
STEP: Creating a pod to test atomic-volume-subpath
May 25 03:06:38.316: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9j2g" in namespace "subpath-2920" to be "Succeeded or Failed"
May 25 03:06:38.320: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.698468ms
May 25 03:06:40.328: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Running", Reason="", readiness=true. Elapsed: 2.011638799s
May 25 03:06:42.338: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Running", Reason="", readiness=true. Elapsed: 4.021744655s
May 25 03:06:44.346: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Running", Reason="", readiness=true. Elapsed: 6.02999827s
May 25 03:06:46.356: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Running", Reason="", readiness=true. Elapsed: 8.040153062s
May 25 03:06:48.377: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Running", Reason="", readiness=true. Elapsed: 10.060622005s
May 25 03:06:50.386: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Running", Reason="", readiness=true. Elapsed: 12.069607225s
May 25 03:06:52.395: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Running", Reason="", readiness=true. Elapsed: 14.078828415s
May 25 03:06:54.404: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Running", Reason="", readiness=true. Elapsed: 16.088486446s
May 25 03:06:56.415: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Running", Reason="", readiness=true. Elapsed: 18.098983228s
May 25 03:06:58.424: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Running", Reason="", readiness=true. Elapsed: 20.107565388s
May 25 03:07:00.431: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Running", Reason="", readiness=true. Elapsed: 22.114877432s
May 25 03:07:02.440: INFO: Pod "pod-subpath-test-configmap-9j2g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.124314549s
STEP: Saw pod success
May 25 03:07:02.440: INFO: Pod "pod-subpath-test-configmap-9j2g" satisfied condition "Succeeded or Failed"
May 25 03:07:02.445: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-subpath-test-configmap-9j2g container test-container-subpath-configmap-9j2g: <nil>
STEP: delete the pod
May 25 03:07:02.505: INFO: Waiting for pod pod-subpath-test-configmap-9j2g to disappear
May 25 03:07:02.510: INFO: Pod pod-subpath-test-configmap-9j2g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9j2g
May 25 03:07:02.510: INFO: Deleting pod "pod-subpath-test-configmap-9j2g" in namespace "subpath-2920"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:07:02.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2920" for this suite.

• [SLOW TEST:24.280 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":339,"completed":99,"skipped":1451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:07:02.526: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-41c61f1e-9216-4d3b-a5ec-282bbb2caa58 in namespace container-probe-1356
May 25 03:07:06.636: INFO: Started pod test-webserver-41c61f1e-9216-4d3b-a5ec-282bbb2caa58 in namespace container-probe-1356
STEP: checking the pod's current state and verifying that restartCount is present
May 25 03:07:06.640: INFO: Initial restart count of pod test-webserver-41c61f1e-9216-4d3b-a5ec-282bbb2caa58 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:11:06.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1356" for this suite.

• [SLOW TEST:244.355 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":100,"skipped":1479,"failed":0}
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:11:06.881: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0525 03:11:06.946975      23 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:17:00.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5671" for this suite.

• [SLOW TEST:354.139 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":339,"completed":101,"skipped":1479,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:17:01.021: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 03:17:02.806: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 25 03:17:04.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509422, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509422, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509422, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509422, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 03:17:07.844: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:17:07.857: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:17:11.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2214" for this suite.
STEP: Destroying namespace "webhook-2214-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.181 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":339,"completed":102,"skipped":1488,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:17:11.202: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4581, will wait for the garbage collector to delete the pods
May 25 03:17:15.326: INFO: Deleting Job.batch foo took: 8.804501ms
May 25 03:17:15.426: INFO: Terminating Job.batch foo pods took: 100.950514ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:17:49.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4581" for this suite.

• [SLOW TEST:37.949 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":339,"completed":103,"skipped":1490,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:17:49.151: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
May 25 03:17:49.233: INFO: Waiting up to 5m0s for pod "security-context-e689b02c-c846-4162-aaf0-1d2ce1b9276b" in namespace "security-context-3462" to be "Succeeded or Failed"
May 25 03:17:49.243: INFO: Pod "security-context-e689b02c-c846-4162-aaf0-1d2ce1b9276b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.386543ms
May 25 03:17:51.252: INFO: Pod "security-context-e689b02c-c846-4162-aaf0-1d2ce1b9276b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019009132s
STEP: Saw pod success
May 25 03:17:51.252: INFO: Pod "security-context-e689b02c-c846-4162-aaf0-1d2ce1b9276b" satisfied condition "Succeeded or Failed"
May 25 03:17:51.256: INFO: Trying to get logs from node 10.10.103.202-worker pod security-context-e689b02c-c846-4162-aaf0-1d2ce1b9276b container test-container: <nil>
STEP: delete the pod
May 25 03:17:51.311: INFO: Waiting for pod security-context-e689b02c-c846-4162-aaf0-1d2ce1b9276b to disappear
May 25 03:17:51.316: INFO: Pod security-context-e689b02c-c846-4162-aaf0-1d2ce1b9276b no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:17:51.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3462" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":104,"skipped":1501,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:17:51.329: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 25 03:17:52.251: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
May 25 03:17:54.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509472, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509472, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509472, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509472, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 03:17:57.296: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:17:57.308: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:18:00.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4425" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.369 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":339,"completed":105,"skipped":1508,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:18:00.698: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
May 25 03:18:00.790: INFO: Waiting up to 5m0s for pod "pod-a2ec04c9-2779-4162-a4b5-d07d3bde7506" in namespace "emptydir-8129" to be "Succeeded or Failed"
May 25 03:18:00.794: INFO: Pod "pod-a2ec04c9-2779-4162-a4b5-d07d3bde7506": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024987ms
May 25 03:18:02.804: INFO: Pod "pod-a2ec04c9-2779-4162-a4b5-d07d3bde7506": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014238632s
May 25 03:18:04.816: INFO: Pod "pod-a2ec04c9-2779-4162-a4b5-d07d3bde7506": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02578919s
STEP: Saw pod success
May 25 03:18:04.816: INFO: Pod "pod-a2ec04c9-2779-4162-a4b5-d07d3bde7506" satisfied condition "Succeeded or Failed"
May 25 03:18:04.820: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-a2ec04c9-2779-4162-a4b5-d07d3bde7506 container test-container: <nil>
STEP: delete the pod
May 25 03:18:04.855: INFO: Waiting for pod pod-a2ec04c9-2779-4162-a4b5-d07d3bde7506 to disappear
May 25 03:18:04.858: INFO: Pod pod-a2ec04c9-2779-4162-a4b5-d07d3bde7506 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:18:04.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8129" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":106,"skipped":1513,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:18:04.870: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1928
STEP: creating service affinity-nodeport in namespace services-1928
STEP: creating replication controller affinity-nodeport in namespace services-1928
I0525 03:18:04.956520      23 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-1928, replica count: 3
I0525 03:18:08.007992      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 03:18:08.025: INFO: Creating new exec pod
May 25 03:18:13.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-1928 exec execpod-affinitys42ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
May 25 03:18:13.553: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 25 03:18:13.553: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:18:13.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-1928 exec execpod-affinitys42ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.14.97 80'
May 25 03:18:13.934: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.14.97 80\nConnection to 10.108.14.97 80 port [tcp/http] succeeded!\n"
May 25 03:18:13.934: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:18:13.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-1928 exec execpod-affinitys42ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.201 32640'
May 25 03:18:14.308: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.201 32640\nConnection to 10.10.103.201 32640 port [tcp/*] succeeded!\n"
May 25 03:18:14.309: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:18:14.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-1928 exec execpod-affinitys42ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.202 32640'
May 25 03:18:14.706: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.202 32640\nConnection to 10.10.103.202 32640 port [tcp/*] succeeded!\n"
May 25 03:18:14.706: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:18:14.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-1928 exec execpod-affinitys42ft -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.103.201:32640/ ; done'
May 25 03:18:15.262: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:32640/\n"
May 25 03:18:15.262: INFO: stdout: "\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w\naffinity-nodeport-5j85w"
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Received response from host: affinity-nodeport-5j85w
May 25 03:18:15.263: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1928, will wait for the garbage collector to delete the pods
May 25 03:18:15.354: INFO: Deleting ReplicationController affinity-nodeport took: 8.345109ms
May 25 03:18:15.455: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.207487ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:18:28.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1928" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:23.630 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":107,"skipped":1514,"failed":0}
SS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:18:28.500: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May 25 03:18:28.551: INFO: Waiting up to 1m0s for all nodes to be ready
May 25 03:19:28.614: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:19:28.619: INFO: Starting informer...
STEP: Starting pods...
May 25 03:19:28.861: INFO: Pod1 is running on 10.10.103.202-worker. Tainting Node
May 25 03:19:31.096: INFO: Pod2 is running on 10.10.103.202-worker. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 25 03:19:48.433: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 25 03:19:58.861: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:19:58.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-3159" for this suite.

• [SLOW TEST:90.404 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":339,"completed":108,"skipped":1516,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:19:58.905: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7490
STEP: creating service affinity-nodeport-transition in namespace services-7490
STEP: creating replication controller affinity-nodeport-transition in namespace services-7490
I0525 03:19:58.992144      23 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-7490, replica count: 3
I0525 03:20:02.043328      23 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 03:20:02.062: INFO: Creating new exec pod
May 25 03:20:05.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7490 exec execpod-affinityfszz5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
May 25 03:20:05.478: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 25 03:20:05.478: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:20:05.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7490 exec execpod-affinityfszz5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.221.151 80'
May 25 03:20:05.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.221.151 80\nConnection to 10.105.221.151 80 port [tcp/http] succeeded!\n"
May 25 03:20:05.851: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:20:05.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7490 exec execpod-affinityfszz5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.201 30739'
May 25 03:20:06.245: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.201 30739\nConnection to 10.10.103.201 30739 port [tcp/*] succeeded!\n"
May 25 03:20:06.245: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:20:06.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7490 exec execpod-affinityfszz5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.202 30739'
May 25 03:20:06.622: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.202 30739\nConnection to 10.10.103.202 30739 port [tcp/*] succeeded!\n"
May 25 03:20:06.622: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:20:06.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7490 exec execpod-affinityfszz5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.103.201:30739/ ; done'
May 25 03:20:07.181: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n"
May 25 03:20:07.181: INFO: stdout: "\naffinity-nodeport-transition-bsfdh\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-f6x89\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-f6x89\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-bsfdh\naffinity-nodeport-transition-bsfdh\naffinity-nodeport-transition-f6x89\naffinity-nodeport-transition-f6x89\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-bsfdh\naffinity-nodeport-transition-bsfdh\naffinity-nodeport-transition-bsfdh\naffinity-nodeport-transition-mv656"
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-bsfdh
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-f6x89
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-f6x89
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-bsfdh
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-bsfdh
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-f6x89
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-f6x89
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.181: INFO: Received response from host: affinity-nodeport-transition-bsfdh
May 25 03:20:07.182: INFO: Received response from host: affinity-nodeport-transition-bsfdh
May 25 03:20:07.182: INFO: Received response from host: affinity-nodeport-transition-bsfdh
May 25 03:20:07.182: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7490 exec execpod-affinityfszz5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.103.201:30739/ ; done'
May 25 03:20:07.783: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:30739/\n"
May 25 03:20:07.784: INFO: stdout: "\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656\naffinity-nodeport-transition-mv656"
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Received response from host: affinity-nodeport-transition-mv656
May 25 03:20:07.784: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7490, will wait for the garbage collector to delete the pods
May 25 03:20:07.868: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.713256ms
May 25 03:20:07.969: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.380313ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:20:18.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7490" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:19.620 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":109,"skipped":1528,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:20:18.527: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:20:24.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4506" for this suite.

• [SLOW TEST:6.202 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":339,"completed":110,"skipped":1569,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:20:24.730: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:20:24.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9797" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":339,"completed":111,"skipped":1581,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:20:24.849: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:20:24.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4249" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":339,"completed":112,"skipped":1588,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:20:24.972: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
May 25 03:20:25.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4671 create -f -'
May 25 03:20:25.671: INFO: stderr: ""
May 25 03:20:25.671: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 25 03:20:25.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4671 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 03:20:25.892: INFO: stderr: ""
May 25 03:20:25.893: INFO: stdout: "update-demo-nautilus-d2hg5 update-demo-nautilus-s9h64 "
May 25 03:20:25.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4671 get pods update-demo-nautilus-d2hg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 03:20:26.093: INFO: stderr: ""
May 25 03:20:26.093: INFO: stdout: ""
May 25 03:20:26.093: INFO: update-demo-nautilus-d2hg5 is created but not running
May 25 03:20:31.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4671 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 03:20:31.325: INFO: stderr: ""
May 25 03:20:31.325: INFO: stdout: "update-demo-nautilus-d2hg5 update-demo-nautilus-s9h64 "
May 25 03:20:31.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4671 get pods update-demo-nautilus-d2hg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 03:20:31.520: INFO: stderr: ""
May 25 03:20:31.520: INFO: stdout: "true"
May 25 03:20:31.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4671 get pods update-demo-nautilus-d2hg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 03:20:31.717: INFO: stderr: ""
May 25 03:20:31.717: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 25 03:20:31.717: INFO: validating pod update-demo-nautilus-d2hg5
May 25 03:20:31.725: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 03:20:31.725: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 03:20:31.725: INFO: update-demo-nautilus-d2hg5 is verified up and running
May 25 03:20:31.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4671 get pods update-demo-nautilus-s9h64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 03:20:31.913: INFO: stderr: ""
May 25 03:20:31.913: INFO: stdout: "true"
May 25 03:20:31.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4671 get pods update-demo-nautilus-s9h64 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 03:20:32.113: INFO: stderr: ""
May 25 03:20:32.113: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 25 03:20:32.113: INFO: validating pod update-demo-nautilus-s9h64
May 25 03:20:32.121: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 03:20:32.121: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 03:20:32.121: INFO: update-demo-nautilus-s9h64 is verified up and running
STEP: using delete to clean up resources
May 25 03:20:32.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4671 delete --grace-period=0 --force -f -'
May 25 03:20:32.319: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 03:20:32.320: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 25 03:20:32.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4671 get rc,svc -l name=update-demo --no-headers'
May 25 03:20:32.541: INFO: stderr: "No resources found in kubectl-4671 namespace.\n"
May 25 03:20:32.541: INFO: stdout: ""
May 25 03:20:32.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4671 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 25 03:20:32.756: INFO: stderr: ""
May 25 03:20:32.756: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:20:32.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4671" for this suite.

• [SLOW TEST:7.797 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":339,"completed":113,"skipped":1627,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:20:32.769: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
May 25 03:20:32.833: INFO: Waiting up to 5m0s for pod "pod-8f4a0e4a-db12-4a9e-8f20-138f27ac1ec0" in namespace "emptydir-5233" to be "Succeeded or Failed"
May 25 03:20:32.838: INFO: Pod "pod-8f4a0e4a-db12-4a9e-8f20-138f27ac1ec0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.742901ms
May 25 03:20:34.847: INFO: Pod "pod-8f4a0e4a-db12-4a9e-8f20-138f27ac1ec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014101029s
May 25 03:20:36.858: INFO: Pod "pod-8f4a0e4a-db12-4a9e-8f20-138f27ac1ec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025370736s
STEP: Saw pod success
May 25 03:20:36.858: INFO: Pod "pod-8f4a0e4a-db12-4a9e-8f20-138f27ac1ec0" satisfied condition "Succeeded or Failed"
May 25 03:20:36.862: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-8f4a0e4a-db12-4a9e-8f20-138f27ac1ec0 container test-container: <nil>
STEP: delete the pod
May 25 03:20:36.928: INFO: Waiting for pod pod-8f4a0e4a-db12-4a9e-8f20-138f27ac1ec0 to disappear
May 25 03:20:36.933: INFO: Pod pod-8f4a0e4a-db12-4a9e-8f20-138f27ac1ec0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:20:36.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5233" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":114,"skipped":1634,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:20:36.949: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:20:48.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9043" for this suite.

• [SLOW TEST:11.260 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":339,"completed":115,"skipped":1649,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:20:48.209: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
May 25 03:20:48.262: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 25 03:20:48.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6180 create -f -'
May 25 03:20:48.768: INFO: stderr: ""
May 25 03:20:48.768: INFO: stdout: "service/agnhost-replica created\n"
May 25 03:20:48.768: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 25 03:20:48.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6180 create -f -'
May 25 03:20:49.323: INFO: stderr: ""
May 25 03:20:49.323: INFO: stdout: "service/agnhost-primary created\n"
May 25 03:20:49.323: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 25 03:20:49.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6180 create -f -'
May 25 03:20:49.835: INFO: stderr: ""
May 25 03:20:49.836: INFO: stdout: "service/frontend created\n"
May 25 03:20:49.836: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 25 03:20:49.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6180 create -f -'
May 25 03:20:50.339: INFO: stderr: ""
May 25 03:20:50.339: INFO: stdout: "deployment.apps/frontend created\n"
May 25 03:20:50.339: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 25 03:20:50.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6180 create -f -'
May 25 03:20:50.854: INFO: stderr: ""
May 25 03:20:50.854: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 25 03:20:50.854: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 25 03:20:50.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6180 create -f -'
May 25 03:20:51.361: INFO: stderr: ""
May 25 03:20:51.361: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 25 03:20:51.361: INFO: Waiting for all frontend pods to be Running.
May 25 03:20:56.412: INFO: Waiting for frontend to serve content.
May 25 03:20:56.429: INFO: Trying to add a new entry to the guestbook.
May 25 03:20:56.443: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 25 03:20:56.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6180 delete --grace-period=0 --force -f -'
May 25 03:20:56.683: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 03:20:56.683: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 25 03:20:56.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6180 delete --grace-period=0 --force -f -'
May 25 03:20:56.899: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 03:20:56.899: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 25 03:20:56.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6180 delete --grace-period=0 --force -f -'
May 25 03:20:57.146: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 03:20:57.146: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 25 03:20:57.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6180 delete --grace-period=0 --force -f -'
May 25 03:20:57.357: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 03:20:57.357: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 25 03:20:57.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6180 delete --grace-period=0 --force -f -'
May 25 03:20:57.559: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 03:20:57.560: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 25 03:20:57.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6180 delete --grace-period=0 --force -f -'
May 25 03:20:57.802: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 03:20:57.802: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:20:57.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6180" for this suite.

• [SLOW TEST:9.609 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:336
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":339,"completed":116,"skipped":1658,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:20:57.819: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 25 03:20:57.909: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8777  0d38317d-d7fe-4bf9-9710-2ee3ec389de3 842525 0 2021-05-25 03:20:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-25 03:20:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 03:20:57.909: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8777  0d38317d-d7fe-4bf9-9710-2ee3ec389de3 842526 0 2021-05-25 03:20:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-25 03:20:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 25 03:20:57.929: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8777  0d38317d-d7fe-4bf9-9710-2ee3ec389de3 842527 0 2021-05-25 03:20:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-25 03:20:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 03:20:57.929: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8777  0d38317d-d7fe-4bf9-9710-2ee3ec389de3 842528 0 2021-05-25 03:20:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-25 03:20:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:20:57.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8777" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":339,"completed":117,"skipped":1667,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:20:57.941: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2955
May 25 03:20:58.033: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 25 03:21:00.044: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 25 03:21:02.046: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
May 25 03:21:02.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-2955 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 25 03:21:02.447: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 25 03:21:02.448: INFO: stdout: "iptables"
May 25 03:21:02.448: INFO: proxyMode: iptables
May 25 03:21:02.463: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 25 03:21:02.468: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-2955
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2955
I0525 03:21:02.510413      23 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2955, replica count: 3
I0525 03:21:05.562044      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0525 03:21:08.562975      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 03:21:08.572: INFO: Creating new exec pod
May 25 03:21:13.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-2955 exec execpod-affinityg9glp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
May 25 03:21:13.967: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 25 03:21:13.967: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:21:13.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-2955 exec execpod-affinityg9glp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.193.68 80'
May 25 03:21:14.346: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.193.68 80\nConnection to 10.106.193.68 80 port [tcp/http] succeeded!\n"
May 25 03:21:14.347: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:21:14.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-2955 exec execpod-affinityg9glp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.193.68:80/ ; done'
May 25 03:21:14.851: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n"
May 25 03:21:14.851: INFO: stdout: "\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs\naffinity-clusterip-timeout-8t9zs"
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Received response from host: affinity-clusterip-timeout-8t9zs
May 25 03:21:14.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-2955 exec execpod-affinityg9glp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.106.193.68:80/'
May 25 03:21:15.232: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n"
May 25 03:21:15.232: INFO: stdout: "affinity-clusterip-timeout-8t9zs"
May 25 03:21:35.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-2955 exec execpod-affinityg9glp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.106.193.68:80/'
May 25 03:21:35.624: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.106.193.68:80/\n"
May 25 03:21:35.624: INFO: stdout: "affinity-clusterip-timeout-dg78s"
May 25 03:21:35.624: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2955, will wait for the garbage collector to delete the pods
May 25 03:21:35.735: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.361301ms
May 25 03:21:35.835: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.259609ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:21:48.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2955" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:50.657 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":118,"skipped":1675,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:21:48.598: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
May 25 03:21:48.671: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 25 03:21:50.680: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 25 03:21:52.679: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
May 25 03:21:52.713: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 25 03:21:54.721: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
May 25 03:21:54.733: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 25 03:21:54.738: INFO: Pod pod-with-prestop-http-hook still exists
May 25 03:21:56.738: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 25 03:21:56.750: INFO: Pod pod-with-prestop-http-hook still exists
May 25 03:21:58.739: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 25 03:21:58.745: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:21:58.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7715" for this suite.

• [SLOW TEST:10.171 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":339,"completed":119,"skipped":1711,"failed":0}
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:21:58.769: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
May 25 03:21:58.836: INFO: The status of Pod labelsupdate204fc495-66be-4a2d-b7ef-8d633120bf7d is Pending, waiting for it to be Running (with Ready = true)
May 25 03:22:00.845: INFO: The status of Pod labelsupdate204fc495-66be-4a2d-b7ef-8d633120bf7d is Running (Ready = true)
May 25 03:22:01.385: INFO: Successfully updated pod "labelsupdate204fc495-66be-4a2d-b7ef-8d633120bf7d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:22:03.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-484" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":120,"skipped":1711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:22:03.440: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1548
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
May 25 03:22:03.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-2386 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
May 25 03:22:03.737: INFO: stderr: ""
May 25 03:22:03.737: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 25 03:22:08.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-2386 get pod e2e-test-httpd-pod -o json'
May 25 03:22:08.978: INFO: stderr: ""
May 25 03:22:08.978: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.31.126/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.31.126/32\"\n        },\n        \"creationTimestamp\": \"2021-05-25T03:22:03Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2386\",\n        \"resourceVersion\": \"843035\",\n        \"uid\": \"343bf518-56c5-40e3-9e37-e6500ab1cb36\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-mf4z4\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.10.103.202-worker\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-mf4z4\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-25T03:22:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-25T03:22:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-25T03:22:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-25T03:22:03Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://00e30393f7b7abd1bf49c3fab5884553338aa8e30799664b2e5841a9c633157b\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-05-25T03:22:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.103.202\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.31.126\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.31.126\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-05-25T03:22:03Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 25 03:22:08.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-2386 replace -f -'
May 25 03:22:09.465: INFO: stderr: ""
May 25 03:22:09.465: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1552
May 25 03:22:09.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-2386 delete pods e2e-test-httpd-pod'
May 25 03:22:18.425: INFO: stderr: ""
May 25 03:22:18.425: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:22:18.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2386" for this suite.

• [SLOW TEST:15.002 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":339,"completed":121,"skipped":1752,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:22:18.442: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0525 03:22:19.310266      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 25 03:23:21.351: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:23:21.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9352" for this suite.

• [SLOW TEST:62.929 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":339,"completed":122,"skipped":1753,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:23:21.372: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5157
May 25 03:23:21.459: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 25 03:23:23.471: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
May 25 03:23:23.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-5157 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 25 03:23:23.887: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 25 03:23:23.888: INFO: stdout: "iptables"
May 25 03:23:23.888: INFO: proxyMode: iptables
May 25 03:23:23.901: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 25 03:23:23.906: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5157
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5157
I0525 03:23:23.944347      23 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5157, replica count: 3
I0525 03:23:26.994751      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0525 03:23:29.995044      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 03:23:30.011: INFO: Creating new exec pod
May 25 03:23:33.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-5157 exec execpod-affinityszvkc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
May 25 03:23:33.407: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 25 03:23:33.407: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:23:33.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-5157 exec execpod-affinityszvkc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.94.12 80'
May 25 03:23:33.801: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.94.12 80\nConnection to 10.101.94.12 80 port [tcp/http] succeeded!\n"
May 25 03:23:33.802: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:23:33.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-5157 exec execpod-affinityszvkc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.201 31740'
May 25 03:23:34.183: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.201 31740\nConnection to 10.10.103.201 31740 port [tcp/*] succeeded!\n"
May 25 03:23:34.183: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:23:34.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-5157 exec execpod-affinityszvkc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.202 31740'
May 25 03:23:34.577: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.202 31740\nConnection to 10.10.103.202 31740 port [tcp/*] succeeded!\n"
May 25 03:23:34.577: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 25 03:23:34.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-5157 exec execpod-affinityszvkc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.103.201:31740/ ; done'
May 25 03:23:35.097: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n"
May 25 03:23:35.097: INFO: stdout: "\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62\naffinity-nodeport-timeout-jsw62"
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.097: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.098: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.098: INFO: Received response from host: affinity-nodeport-timeout-jsw62
May 25 03:23:35.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-5157 exec execpod-affinityszvkc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.103.201:31740/'
May 25 03:23:35.476: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n"
May 25 03:23:35.476: INFO: stdout: "affinity-nodeport-timeout-jsw62"
May 25 03:23:55.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-5157 exec execpod-affinityszvkc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.103.201:31740/'
May 25 03:23:55.843: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.103.201:31740/\n"
May 25 03:23:55.843: INFO: stdout: "affinity-nodeport-timeout-7hmdk"
May 25 03:23:55.843: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5157, will wait for the garbage collector to delete the pods
May 25 03:23:55.945: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 7.008879ms
May 25 03:23:56.046: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.630596ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:24:08.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5157" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:47.222 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":123,"skipped":1771,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:24:08.594: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 03:24:09.186: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 03:24:11.193: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509849, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509849, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509849, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509849, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 03:24:14.213: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 25 03:24:18.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=webhook-6505 attach --namespace=webhook-6505 to-be-attached-pod -i -c=container1'
May 25 03:24:18.559: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:24:18.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6505" for this suite.
STEP: Destroying namespace "webhook-6505-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.090 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":339,"completed":124,"skipped":1782,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:24:18.684: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:24:18.771: INFO: created pod
May 25 03:24:18.771: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8390" to be "Succeeded or Failed"
May 25 03:24:18.781: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.856492ms
May 25 03:24:20.906: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134517472s
May 25 03:24:22.910: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.1391177s
STEP: Saw pod success
May 25 03:24:22.910: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May 25 03:24:52.911: INFO: polling logs
May 25 03:24:52.957: INFO: Pod logs: 
2021/05/25 03:24:21 OK: Got token
2021/05/25 03:24:21 validating with in-cluster discovery
2021/05/25 03:24:21 OK: got issuer https://kubernetes.default.svc.cluster.local
2021/05/25 03:24:21 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8390:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1621913658, NotBefore:1621913058, IssuedAt:1621913058, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8390", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"5100e657-fb9b-462b-ac02-5ee532c49319"}}}
2021/05/25 03:24:21 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2021/05/25 03:24:21 OK: Validated signature on JWT
2021/05/25 03:24:21 OK: Got valid claims from token!
2021/05/25 03:24:21 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8390:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1621913658, NotBefore:1621913058, IssuedAt:1621913058, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8390", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"5100e657-fb9b-462b-ac02-5ee532c49319"}}}

May 25 03:24:52.957: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:24:52.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8390" for this suite.

• [SLOW TEST:34.293 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":339,"completed":125,"skipped":1797,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:24:52.978: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9529 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9529;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9529 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9529;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9529.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9529.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9529.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9529.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9529.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9529.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9529.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9529.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9529.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9529.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9529.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 7.13.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.13.7_udp@PTR;check="$$(dig +tcp +noall +answer +search 7.13.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.13.7_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9529 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9529;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9529 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9529;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9529.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9529.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9529.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9529.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9529.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9529.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9529.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9529.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9529.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9529.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9529.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9529.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 7.13.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.13.7_udp@PTR;check="$$(dig +tcp +noall +answer +search 7.13.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.13.7_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 03:24:57.102: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.116: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.121: INFO: Unable to read wheezy_udp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.126: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.134: INFO: Unable to read wheezy_udp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.138: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.142: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.147: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.176: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.180: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.184: INFO: Unable to read jessie_udp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.188: INFO: Unable to read jessie_tcp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.194: INFO: Unable to read jessie_udp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.200: INFO: Unable to read jessie_tcp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.208: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.212: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:24:57.241: INFO: Lookups using dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9529 wheezy_tcp@dns-test-service.dns-9529 wheezy_udp@dns-test-service.dns-9529.svc wheezy_tcp@dns-test-service.dns-9529.svc wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9529 jessie_tcp@dns-test-service.dns-9529 jessie_udp@dns-test-service.dns-9529.svc jessie_tcp@dns-test-service.dns-9529.svc jessie_udp@_http._tcp.dns-test-service.dns-9529.svc jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc]

May 25 03:25:02.248: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.253: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.257: INFO: Unable to read wheezy_udp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.262: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.266: INFO: Unable to read wheezy_udp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.270: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.274: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.278: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.309: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.313: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.318: INFO: Unable to read jessie_udp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.322: INFO: Unable to read jessie_tcp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.327: INFO: Unable to read jessie_udp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.332: INFO: Unable to read jessie_tcp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.336: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.345: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:02.378: INFO: Lookups using dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9529 wheezy_tcp@dns-test-service.dns-9529 wheezy_udp@dns-test-service.dns-9529.svc wheezy_tcp@dns-test-service.dns-9529.svc wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9529 jessie_tcp@dns-test-service.dns-9529 jessie_udp@dns-test-service.dns-9529.svc jessie_tcp@dns-test-service.dns-9529.svc jessie_udp@_http._tcp.dns-test-service.dns-9529.svc jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc]

May 25 03:25:07.248: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.253: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.257: INFO: Unable to read wheezy_udp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.261: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.265: INFO: Unable to read wheezy_udp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.269: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.273: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.277: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.308: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.312: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.316: INFO: Unable to read jessie_udp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.321: INFO: Unable to read jessie_tcp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.325: INFO: Unable to read jessie_udp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.329: INFO: Unable to read jessie_tcp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.333: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.337: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:07.366: INFO: Lookups using dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9529 wheezy_tcp@dns-test-service.dns-9529 wheezy_udp@dns-test-service.dns-9529.svc wheezy_tcp@dns-test-service.dns-9529.svc wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9529 jessie_tcp@dns-test-service.dns-9529 jessie_udp@dns-test-service.dns-9529.svc jessie_tcp@dns-test-service.dns-9529.svc jessie_udp@_http._tcp.dns-test-service.dns-9529.svc jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc]

May 25 03:25:12.248: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.253: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.258: INFO: Unable to read wheezy_udp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.263: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.267: INFO: Unable to read wheezy_udp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.272: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.276: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.281: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.313: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.318: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.322: INFO: Unable to read jessie_udp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.330: INFO: Unable to read jessie_tcp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.335: INFO: Unable to read jessie_udp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.339: INFO: Unable to read jessie_tcp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.344: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.352: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:12.378: INFO: Lookups using dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9529 wheezy_tcp@dns-test-service.dns-9529 wheezy_udp@dns-test-service.dns-9529.svc wheezy_tcp@dns-test-service.dns-9529.svc wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9529 jessie_tcp@dns-test-service.dns-9529 jessie_udp@dns-test-service.dns-9529.svc jessie_tcp@dns-test-service.dns-9529.svc jessie_udp@_http._tcp.dns-test-service.dns-9529.svc jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc]

May 25 03:25:17.249: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.254: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.259: INFO: Unable to read wheezy_udp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.265: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.270: INFO: Unable to read wheezy_udp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.274: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.279: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.284: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.318: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.322: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.331: INFO: Unable to read jessie_udp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.335: INFO: Unable to read jessie_tcp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.340: INFO: Unable to read jessie_udp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.344: INFO: Unable to read jessie_tcp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.349: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.354: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:17.397: INFO: Lookups using dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9529 wheezy_tcp@dns-test-service.dns-9529 wheezy_udp@dns-test-service.dns-9529.svc wheezy_tcp@dns-test-service.dns-9529.svc wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9529 jessie_tcp@dns-test-service.dns-9529 jessie_udp@dns-test-service.dns-9529.svc jessie_tcp@dns-test-service.dns-9529.svc jessie_udp@_http._tcp.dns-test-service.dns-9529.svc jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc]

May 25 03:25:22.249: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.254: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.259: INFO: Unable to read wheezy_udp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.263: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.267: INFO: Unable to read wheezy_udp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.271: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.275: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.280: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.312: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.316: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.321: INFO: Unable to read jessie_udp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.325: INFO: Unable to read jessie_tcp@dns-test-service.dns-9529 from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.329: INFO: Unable to read jessie_udp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.333: INFO: Unable to read jessie_tcp@dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.337: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.342: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc from pod dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018: the server could not find the requested resource (get pods dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018)
May 25 03:25:22.378: INFO: Lookups using dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9529 wheezy_tcp@dns-test-service.dns-9529 wheezy_udp@dns-test-service.dns-9529.svc wheezy_tcp@dns-test-service.dns-9529.svc wheezy_udp@_http._tcp.dns-test-service.dns-9529.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9529.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9529 jessie_tcp@dns-test-service.dns-9529 jessie_udp@dns-test-service.dns-9529.svc jessie_tcp@dns-test-service.dns-9529.svc jessie_udp@_http._tcp.dns-test-service.dns-9529.svc jessie_tcp@_http._tcp.dns-test-service.dns-9529.svc]

May 25 03:25:27.382: INFO: DNS probes using dns-9529/dns-test-9bd88094-1eee-44ea-875f-08bfc3f00018 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:25:27.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9529" for this suite.

• [SLOW TEST:34.606 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":339,"completed":126,"skipped":1820,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:25:27.585: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 03:25:28.751: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 25 03:25:30.768: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509928, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509928, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509928, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757509928, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 03:25:33.793: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:25:33.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4646" for this suite.
STEP: Destroying namespace "webhook-4646-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.406 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":339,"completed":127,"skipped":1822,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:25:33.991: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-c2f3ae46-d8eb-457a-919a-4981831ea198
STEP: Creating a pod to test consume secrets
May 25 03:25:34.103: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8ae102ce-2193-436d-9bac-aa83f56938f7" in namespace "projected-5754" to be "Succeeded or Failed"
May 25 03:25:34.117: INFO: Pod "pod-projected-secrets-8ae102ce-2193-436d-9bac-aa83f56938f7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.220819ms
May 25 03:25:36.126: INFO: Pod "pod-projected-secrets-8ae102ce-2193-436d-9bac-aa83f56938f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022223453s
May 25 03:25:38.134: INFO: Pod "pod-projected-secrets-8ae102ce-2193-436d-9bac-aa83f56938f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030712697s
STEP: Saw pod success
May 25 03:25:38.134: INFO: Pod "pod-projected-secrets-8ae102ce-2193-436d-9bac-aa83f56938f7" satisfied condition "Succeeded or Failed"
May 25 03:25:38.140: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-projected-secrets-8ae102ce-2193-436d-9bac-aa83f56938f7 container secret-volume-test: <nil>
STEP: delete the pod
May 25 03:25:38.180: INFO: Waiting for pod pod-projected-secrets-8ae102ce-2193-436d-9bac-aa83f56938f7 to disappear
May 25 03:25:38.184: INFO: Pod pod-projected-secrets-8ae102ce-2193-436d-9bac-aa83f56938f7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:25:38.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5754" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":128,"skipped":1843,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:25:38.198: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-njwv
STEP: Creating a pod to test atomic-volume-subpath
May 25 03:25:38.293: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-njwv" in namespace "subpath-3136" to be "Succeeded or Failed"
May 25 03:25:38.298: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Pending", Reason="", readiness=false. Elapsed: 5.346441ms
May 25 03:25:40.310: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017011942s
May 25 03:25:42.319: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Running", Reason="", readiness=true. Elapsed: 4.026346338s
May 25 03:25:44.330: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Running", Reason="", readiness=true. Elapsed: 6.037403268s
May 25 03:25:46.340: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Running", Reason="", readiness=true. Elapsed: 8.046728812s
May 25 03:25:48.353: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Running", Reason="", readiness=true. Elapsed: 10.059638758s
May 25 03:25:50.360: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Running", Reason="", readiness=true. Elapsed: 12.066681587s
May 25 03:25:52.367: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Running", Reason="", readiness=true. Elapsed: 14.074479487s
May 25 03:25:54.376: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Running", Reason="", readiness=true. Elapsed: 16.083139276s
May 25 03:25:56.386: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Running", Reason="", readiness=true. Elapsed: 18.09278898s
May 25 03:25:58.394: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Running", Reason="", readiness=true. Elapsed: 20.101506774s
May 25 03:26:00.403: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Running", Reason="", readiness=true. Elapsed: 22.109686164s
May 25 03:26:02.410: INFO: Pod "pod-subpath-test-secret-njwv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.116886447s
STEP: Saw pod success
May 25 03:26:02.410: INFO: Pod "pod-subpath-test-secret-njwv" satisfied condition "Succeeded or Failed"
May 25 03:26:02.414: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-subpath-test-secret-njwv container test-container-subpath-secret-njwv: <nil>
STEP: delete the pod
May 25 03:26:02.442: INFO: Waiting for pod pod-subpath-test-secret-njwv to disappear
May 25 03:26:02.447: INFO: Pod pod-subpath-test-secret-njwv no longer exists
STEP: Deleting pod pod-subpath-test-secret-njwv
May 25 03:26:02.447: INFO: Deleting pod "pod-subpath-test-secret-njwv" in namespace "subpath-3136"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:26:02.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3136" for this suite.

• [SLOW TEST:24.267 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":339,"completed":129,"skipped":1846,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:26:02.465: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
May 25 03:26:02.525: INFO: Waiting up to 5m0s for pod "var-expansion-64dfaf3b-b9ee-4e08-a7dd-f53b088664c0" in namespace "var-expansion-2185" to be "Succeeded or Failed"
May 25 03:26:02.529: INFO: Pod "var-expansion-64dfaf3b-b9ee-4e08-a7dd-f53b088664c0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.430996ms
May 25 03:26:04.537: INFO: Pod "var-expansion-64dfaf3b-b9ee-4e08-a7dd-f53b088664c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011756566s
May 25 03:26:06.545: INFO: Pod "var-expansion-64dfaf3b-b9ee-4e08-a7dd-f53b088664c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02023228s
STEP: Saw pod success
May 25 03:26:06.545: INFO: Pod "var-expansion-64dfaf3b-b9ee-4e08-a7dd-f53b088664c0" satisfied condition "Succeeded or Failed"
May 25 03:26:06.550: INFO: Trying to get logs from node 10.10.103.202-worker pod var-expansion-64dfaf3b-b9ee-4e08-a7dd-f53b088664c0 container dapi-container: <nil>
STEP: delete the pod
May 25 03:26:06.581: INFO: Waiting for pod var-expansion-64dfaf3b-b9ee-4e08-a7dd-f53b088664c0 to disappear
May 25 03:26:06.585: INFO: Pod var-expansion-64dfaf3b-b9ee-4e08-a7dd-f53b088664c0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:26:06.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2185" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":339,"completed":130,"skipped":1853,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:26:06.597: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-3470db21-0b79-4154-93b2-073ca2b80299
STEP: Creating configMap with name cm-test-opt-upd-a61be835-c984-46b0-9d35-6006c2584415
STEP: Creating the pod
May 25 03:26:06.687: INFO: The status of Pod pod-projected-configmaps-64e3dde1-7098-41bc-be54-cd145d15f60e is Pending, waiting for it to be Running (with Ready = true)
May 25 03:26:08.696: INFO: The status of Pod pod-projected-configmaps-64e3dde1-7098-41bc-be54-cd145d15f60e is Pending, waiting for it to be Running (with Ready = true)
May 25 03:26:10.693: INFO: The status of Pod pod-projected-configmaps-64e3dde1-7098-41bc-be54-cd145d15f60e is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-3470db21-0b79-4154-93b2-073ca2b80299
STEP: Updating configmap cm-test-opt-upd-a61be835-c984-46b0-9d35-6006c2584415
STEP: Creating configMap with name cm-test-opt-create-650c39d3-d264-47d2-96d4-435c1901195c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:27:29.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9876" for this suite.

• [SLOW TEST:82.848 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":131,"skipped":1861,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:27:29.446: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-21d6ad0e-7c13-4b6a-91dc-1502a27c3c2c
STEP: Creating a pod to test consume configMaps
May 25 03:27:29.534: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f025243-e616-427c-855a-8209b448dc4a" in namespace "configmap-2970" to be "Succeeded or Failed"
May 25 03:27:29.539: INFO: Pod "pod-configmaps-0f025243-e616-427c-855a-8209b448dc4a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.332193ms
May 25 03:27:31.547: INFO: Pod "pod-configmaps-0f025243-e616-427c-855a-8209b448dc4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013479086s
STEP: Saw pod success
May 25 03:27:31.547: INFO: Pod "pod-configmaps-0f025243-e616-427c-855a-8209b448dc4a" satisfied condition "Succeeded or Failed"
May 25 03:27:31.551: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-configmaps-0f025243-e616-427c-855a-8209b448dc4a container agnhost-container: <nil>
STEP: delete the pod
May 25 03:27:31.579: INFO: Waiting for pod pod-configmaps-0f025243-e616-427c-855a-8209b448dc4a to disappear
May 25 03:27:31.583: INFO: Pod pod-configmaps-0f025243-e616-427c-855a-8209b448dc4a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:27:31.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2970" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":132,"skipped":1878,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:27:31.596: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:27:31.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1489" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":339,"completed":133,"skipped":1879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:27:31.727: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 03:27:31.784: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f12a5a3-2088-41d5-a15f-2e2a9de37235" in namespace "downward-api-3189" to be "Succeeded or Failed"
May 25 03:27:31.789: INFO: Pod "downwardapi-volume-3f12a5a3-2088-41d5-a15f-2e2a9de37235": Phase="Pending", Reason="", readiness=false. Elapsed: 4.803798ms
May 25 03:27:33.795: INFO: Pod "downwardapi-volume-3f12a5a3-2088-41d5-a15f-2e2a9de37235": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010071578s
STEP: Saw pod success
May 25 03:27:33.795: INFO: Pod "downwardapi-volume-3f12a5a3-2088-41d5-a15f-2e2a9de37235" satisfied condition "Succeeded or Failed"
May 25 03:27:33.798: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-3f12a5a3-2088-41d5-a15f-2e2a9de37235 container client-container: <nil>
STEP: delete the pod
May 25 03:27:33.825: INFO: Waiting for pod downwardapi-volume-3f12a5a3-2088-41d5-a15f-2e2a9de37235 to disappear
May 25 03:27:33.831: INFO: Pod downwardapi-volume-3f12a5a3-2088-41d5-a15f-2e2a9de37235 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:27:33.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3189" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":339,"completed":134,"skipped":1901,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:27:33.843: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 25 03:27:33.907: INFO: Waiting up to 5m0s for pod "pod-fa22cb34-177f-45ef-9cbc-586a6a9365d5" in namespace "emptydir-7521" to be "Succeeded or Failed"
May 25 03:27:33.911: INFO: Pod "pod-fa22cb34-177f-45ef-9cbc-586a6a9365d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.15095ms
May 25 03:27:35.918: INFO: Pod "pod-fa22cb34-177f-45ef-9cbc-586a6a9365d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011065279s
May 25 03:27:37.925: INFO: Pod "pod-fa22cb34-177f-45ef-9cbc-586a6a9365d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017952087s
STEP: Saw pod success
May 25 03:27:37.925: INFO: Pod "pod-fa22cb34-177f-45ef-9cbc-586a6a9365d5" satisfied condition "Succeeded or Failed"
May 25 03:27:37.929: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-fa22cb34-177f-45ef-9cbc-586a6a9365d5 container test-container: <nil>
STEP: delete the pod
May 25 03:27:37.955: INFO: Waiting for pod pod-fa22cb34-177f-45ef-9cbc-586a6a9365d5 to disappear
May 25 03:27:37.960: INFO: Pod pod-fa22cb34-177f-45ef-9cbc-586a6a9365d5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:27:37.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7521" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":135,"skipped":1936,"failed":0}

------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:27:37.977: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
May 25 03:27:38.581: INFO: created pod pod-service-account-defaultsa
May 25 03:27:38.581: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 25 03:27:38.589: INFO: created pod pod-service-account-mountsa
May 25 03:27:38.589: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 25 03:27:38.598: INFO: created pod pod-service-account-nomountsa
May 25 03:27:38.598: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 25 03:27:38.607: INFO: created pod pod-service-account-defaultsa-mountspec
May 25 03:27:38.607: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 25 03:27:38.625: INFO: created pod pod-service-account-mountsa-mountspec
May 25 03:27:38.625: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 25 03:27:38.646: INFO: created pod pod-service-account-nomountsa-mountspec
May 25 03:27:38.647: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 25 03:27:38.666: INFO: created pod pod-service-account-defaultsa-nomountspec
May 25 03:27:38.666: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 25 03:27:38.676: INFO: created pod pod-service-account-mountsa-nomountspec
May 25 03:27:38.676: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 25 03:27:38.689: INFO: created pod pod-service-account-nomountsa-nomountspec
May 25 03:27:38.689: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:27:38.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2088" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":339,"completed":136,"skipped":1936,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:27:38.709: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 25 03:27:38.805: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9605  4bd7d1ba-aa76-4fe7-9e09-fd77a74667d6 844784 0 2021-05-25 03:27:38 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-25 03:27:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 03:27:38.805: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9605  4bd7d1ba-aa76-4fe7-9e09-fd77a74667d6 844786 0 2021-05-25 03:27:38 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-25 03:27:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:27:38.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9605" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":339,"completed":137,"skipped":1956,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:27:38.819: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 25 03:27:38.931: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:27:38.934: INFO: Number of nodes with available pods: 0
May 25 03:27:38.935: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:27:39.942: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:27:39.947: INFO: Number of nodes with available pods: 0
May 25 03:27:39.947: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:27:40.944: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:27:40.949: INFO: Number of nodes with available pods: 0
May 25 03:27:40.949: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:27:41.942: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:27:41.947: INFO: Number of nodes with available pods: 1
May 25 03:27:41.947: INFO: Node 10.10.103.202-worker is running more than one daemon pod
May 25 03:27:42.947: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:27:42.952: INFO: Number of nodes with available pods: 1
May 25 03:27:42.952: INFO: Node 10.10.103.202-worker is running more than one daemon pod
May 25 03:27:43.943: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:27:43.950: INFO: Number of nodes with available pods: 1
May 25 03:27:43.950: INFO: Node 10.10.103.202-worker is running more than one daemon pod
May 25 03:27:44.948: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:27:44.953: INFO: Number of nodes with available pods: 1
May 25 03:27:44.953: INFO: Node 10.10.103.202-worker is running more than one daemon pod
May 25 03:27:45.942: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:27:45.947: INFO: Number of nodes with available pods: 1
May 25 03:27:45.947: INFO: Node 10.10.103.202-worker is running more than one daemon pod
May 25 03:27:46.946: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:27:46.951: INFO: Number of nodes with available pods: 2
May 25 03:27:46.951: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 25 03:27:46.981: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:27:47.000: INFO: Number of nodes with available pods: 2
May 25 03:27:47.000: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5284, will wait for the garbage collector to delete the pods
May 25 03:27:47.082: INFO: Deleting DaemonSet.extensions daemon-set took: 8.574789ms
May 25 03:27:47.182: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.367872ms
May 25 03:27:55.493: INFO: Number of nodes with available pods: 0
May 25 03:27:55.493: INFO: Number of running nodes: 0, number of available pods: 0
May 25 03:27:55.496: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"845048"},"items":null}

May 25 03:27:55.500: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"845048"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:27:55.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5284" for this suite.

• [SLOW TEST:16.706 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":339,"completed":138,"skipped":1956,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:27:55.526: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:27:55.591: INFO: The status of Pod test-webserver-f87c8408-582d-4105-a148-28ed26794b84 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:27:57.599: INFO: The status of Pod test-webserver-f87c8408-582d-4105-a148-28ed26794b84 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:27:59.603: INFO: The status of Pod test-webserver-f87c8408-582d-4105-a148-28ed26794b84 is Running (Ready = false)
May 25 03:28:01.603: INFO: The status of Pod test-webserver-f87c8408-582d-4105-a148-28ed26794b84 is Running (Ready = false)
May 25 03:28:03.598: INFO: The status of Pod test-webserver-f87c8408-582d-4105-a148-28ed26794b84 is Running (Ready = false)
May 25 03:28:05.599: INFO: The status of Pod test-webserver-f87c8408-582d-4105-a148-28ed26794b84 is Running (Ready = false)
May 25 03:28:07.602: INFO: The status of Pod test-webserver-f87c8408-582d-4105-a148-28ed26794b84 is Running (Ready = false)
May 25 03:28:09.602: INFO: The status of Pod test-webserver-f87c8408-582d-4105-a148-28ed26794b84 is Running (Ready = false)
May 25 03:28:11.601: INFO: The status of Pod test-webserver-f87c8408-582d-4105-a148-28ed26794b84 is Running (Ready = false)
May 25 03:28:13.612: INFO: The status of Pod test-webserver-f87c8408-582d-4105-a148-28ed26794b84 is Running (Ready = false)
May 25 03:28:15.609: INFO: The status of Pod test-webserver-f87c8408-582d-4105-a148-28ed26794b84 is Running (Ready = false)
May 25 03:28:17.606: INFO: The status of Pod test-webserver-f87c8408-582d-4105-a148-28ed26794b84 is Running (Ready = true)
May 25 03:28:17.610: INFO: Container started at 2021-05-25 03:27:57 +0000 UTC, pod became ready at 2021-05-25 03:28:15 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:28:17.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3169" for this suite.

• [SLOW TEST:22.101 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":339,"completed":139,"skipped":1973,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:28:17.627: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 03:28:17.704: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ad6fad3-d32d-4791-9b97-7e958c8e033a" in namespace "projected-7011" to be "Succeeded or Failed"
May 25 03:28:17.712: INFO: Pod "downwardapi-volume-8ad6fad3-d32d-4791-9b97-7e958c8e033a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.859493ms
May 25 03:28:19.721: INFO: Pod "downwardapi-volume-8ad6fad3-d32d-4791-9b97-7e958c8e033a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017231288s
May 25 03:28:21.730: INFO: Pod "downwardapi-volume-8ad6fad3-d32d-4791-9b97-7e958c8e033a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026744454s
STEP: Saw pod success
May 25 03:28:21.730: INFO: Pod "downwardapi-volume-8ad6fad3-d32d-4791-9b97-7e958c8e033a" satisfied condition "Succeeded or Failed"
May 25 03:28:21.734: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-8ad6fad3-d32d-4791-9b97-7e958c8e033a container client-container: <nil>
STEP: delete the pod
May 25 03:28:21.759: INFO: Waiting for pod downwardapi-volume-8ad6fad3-d32d-4791-9b97-7e958c8e033a to disappear
May 25 03:28:21.763: INFO: Pod downwardapi-volume-8ad6fad3-d32d-4791-9b97-7e958c8e033a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:28:21.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7011" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":140,"skipped":1981,"failed":0}
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:28:21.775: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 25 03:28:23.821: INFO: starting watch
STEP: patching
STEP: updating
May 25 03:28:23.832: INFO: waiting for watch events with expected annotations
May 25 03:28:23.832: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:28:23.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9415" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":339,"completed":141,"skipped":1982,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:28:23.882: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-1c8389de-c71c-4ed7-ac1d-213d774cc227
STEP: Creating a pod to test consume secrets
May 25 03:28:23.945: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a2426f7e-6ae6-418c-a69d-97013574884e" in namespace "projected-6188" to be "Succeeded or Failed"
May 25 03:28:23.947: INFO: Pod "pod-projected-secrets-a2426f7e-6ae6-418c-a69d-97013574884e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.579238ms
May 25 03:28:25.957: INFO: Pod "pod-projected-secrets-a2426f7e-6ae6-418c-a69d-97013574884e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011619773s
STEP: Saw pod success
May 25 03:28:25.957: INFO: Pod "pod-projected-secrets-a2426f7e-6ae6-418c-a69d-97013574884e" satisfied condition "Succeeded or Failed"
May 25 03:28:25.961: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-projected-secrets-a2426f7e-6ae6-418c-a69d-97013574884e container projected-secret-volume-test: <nil>
STEP: delete the pod
May 25 03:28:25.991: INFO: Waiting for pod pod-projected-secrets-a2426f7e-6ae6-418c-a69d-97013574884e to disappear
May 25 03:28:25.994: INFO: Pod pod-projected-secrets-a2426f7e-6ae6-418c-a69d-97013574884e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:28:25.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6188" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":142,"skipped":1994,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:28:26.005: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
May 25 03:28:26.082: INFO: The status of Pod annotationupdate3ac9eed4-6af3-4899-8ff8-3e3395fbbc67 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:28:28.091: INFO: The status of Pod annotationupdate3ac9eed4-6af3-4899-8ff8-3e3395fbbc67 is Running (Ready = true)
May 25 03:28:28.624: INFO: Successfully updated pod "annotationupdate3ac9eed4-6af3-4899-8ff8-3e3395fbbc67"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:28:30.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-429" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":143,"skipped":1995,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:28:30.665: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-8n7b
STEP: Creating a pod to test atomic-volume-subpath
May 25 03:28:30.742: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8n7b" in namespace "subpath-9017" to be "Succeeded or Failed"
May 25 03:28:30.746: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.176985ms
May 25 03:28:32.752: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010639272s
May 25 03:28:34.759: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Running", Reason="", readiness=true. Elapsed: 4.017668771s
May 25 03:28:36.767: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Running", Reason="", readiness=true. Elapsed: 6.025674516s
May 25 03:28:38.773: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Running", Reason="", readiness=true. Elapsed: 8.031372961s
May 25 03:28:40.780: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Running", Reason="", readiness=true. Elapsed: 10.037846702s
May 25 03:28:42.785: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Running", Reason="", readiness=true. Elapsed: 12.043293431s
May 25 03:28:44.790: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Running", Reason="", readiness=true. Elapsed: 14.048717368s
May 25 03:28:46.797: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Running", Reason="", readiness=true. Elapsed: 16.055719599s
May 25 03:28:48.809: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Running", Reason="", readiness=true. Elapsed: 18.067182238s
May 25 03:28:50.823: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Running", Reason="", readiness=true. Elapsed: 20.081165354s
May 25 03:28:52.838: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Running", Reason="", readiness=true. Elapsed: 22.095898731s
May 25 03:28:54.847: INFO: Pod "pod-subpath-test-downwardapi-8n7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.105191514s
STEP: Saw pod success
May 25 03:28:54.847: INFO: Pod "pod-subpath-test-downwardapi-8n7b" satisfied condition "Succeeded or Failed"
May 25 03:28:54.851: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-subpath-test-downwardapi-8n7b container test-container-subpath-downwardapi-8n7b: <nil>
STEP: delete the pod
May 25 03:28:54.882: INFO: Waiting for pod pod-subpath-test-downwardapi-8n7b to disappear
May 25 03:28:54.886: INFO: Pod pod-subpath-test-downwardapi-8n7b no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8n7b
May 25 03:28:54.886: INFO: Deleting pod "pod-subpath-test-downwardapi-8n7b" in namespace "subpath-9017"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:28:54.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9017" for this suite.

• [SLOW TEST:24.240 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":339,"completed":144,"skipped":2009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:28:54.907: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
May 25 03:28:54.985: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:28:56.991: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.10.103.201 on the node which pod1 resides and expect scheduled
May 25 03:28:57.003: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:28:59.007: INFO: The status of Pod pod2 is Running (Ready = false)
May 25 03:29:01.007: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.10.103.201 but use UDP protocol on the node which pod2 resides
May 25 03:29:01.024: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:29:03.036: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:29:05.036: INFO: The status of Pod pod3 is Running (Ready = true)
May 25 03:29:05.056: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May 25 03:29:07.074: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
May 25 03:29:07.078: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.10.103.201 http://127.0.0.1:54323/hostname] Namespace:hostport-3282 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 03:29:07.078: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.10.103.201, port: 54323
May 25 03:29:07.257: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.10.103.201:54323/hostname] Namespace:hostport-3282 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 03:29:07.257: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.10.103.201, port: 54323 UDP
May 25 03:29:07.432: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.10.103.201 54323] Namespace:hostport-3282 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 03:29:07.432: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:29:12.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-3282" for this suite.

• [SLOW TEST:17.704 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":339,"completed":145,"skipped":2038,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:29:12.611: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 25 03:29:12.704: INFO: Waiting up to 5m0s for pod "pod-175b6157-c4f4-4d78-b691-5b6473825bed" in namespace "emptydir-9331" to be "Succeeded or Failed"
May 25 03:29:12.708: INFO: Pod "pod-175b6157-c4f4-4d78-b691-5b6473825bed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048606ms
May 25 03:29:14.715: INFO: Pod "pod-175b6157-c4f4-4d78-b691-5b6473825bed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011250832s
STEP: Saw pod success
May 25 03:29:14.715: INFO: Pod "pod-175b6157-c4f4-4d78-b691-5b6473825bed" satisfied condition "Succeeded or Failed"
May 25 03:29:14.719: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-175b6157-c4f4-4d78-b691-5b6473825bed container test-container: <nil>
STEP: delete the pod
May 25 03:29:14.742: INFO: Waiting for pod pod-175b6157-c4f4-4d78-b691-5b6473825bed to disappear
May 25 03:29:14.746: INFO: Pod pod-175b6157-c4f4-4d78-b691-5b6473825bed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:29:14.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9331" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":146,"skipped":2039,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:29:14.757: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 03:29:14.818: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a29c26e-fe51-468e-8411-973ffcd0a34d" in namespace "projected-6557" to be "Succeeded or Failed"
May 25 03:29:14.826: INFO: Pod "downwardapi-volume-3a29c26e-fe51-468e-8411-973ffcd0a34d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.489682ms
May 25 03:29:16.833: INFO: Pod "downwardapi-volume-3a29c26e-fe51-468e-8411-973ffcd0a34d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015138059s
May 25 03:29:18.840: INFO: Pod "downwardapi-volume-3a29c26e-fe51-468e-8411-973ffcd0a34d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021968225s
STEP: Saw pod success
May 25 03:29:18.840: INFO: Pod "downwardapi-volume-3a29c26e-fe51-468e-8411-973ffcd0a34d" satisfied condition "Succeeded or Failed"
May 25 03:29:18.844: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-3a29c26e-fe51-468e-8411-973ffcd0a34d container client-container: <nil>
STEP: delete the pod
May 25 03:29:18.876: INFO: Waiting for pod downwardapi-volume-3a29c26e-fe51-468e-8411-973ffcd0a34d to disappear
May 25 03:29:18.883: INFO: Pod downwardapi-volume-3a29c26e-fe51-468e-8411-973ffcd0a34d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:29:18.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6557" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":147,"skipped":2046,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:29:18.900: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-dfv6t in namespace proxy-3544
I0525 03:29:18.987216      23 runners.go:190] Created replication controller with name: proxy-service-dfv6t, namespace: proxy-3544, replica count: 1
I0525 03:29:20.038309      23 runners.go:190] proxy-service-dfv6t Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0525 03:29:21.039262      23 runners.go:190] proxy-service-dfv6t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 03:29:22.039657      23 runners.go:190] proxy-service-dfv6t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 03:29:23.040539      23 runners.go:190] proxy-service-dfv6t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 03:29:24.040854      23 runners.go:190] proxy-service-dfv6t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 03:29:25.041270      23 runners.go:190] proxy-service-dfv6t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 03:29:26.041511      23 runners.go:190] proxy-service-dfv6t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 03:29:27.041783      23 runners.go:190] proxy-service-dfv6t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 03:29:28.042017      23 runners.go:190] proxy-service-dfv6t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 03:29:29.042917      23 runners.go:190] proxy-service-dfv6t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0525 03:29:30.043923      23 runners.go:190] proxy-service-dfv6t Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 03:29:30.053: INFO: setup took 11.101906846s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 25 03:29:30.063: INFO: (0) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 8.943251ms)
May 25 03:29:30.063: INFO: (0) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 8.401333ms)
May 25 03:29:30.085: INFO: (0) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 30.612368ms)
May 25 03:29:30.085: INFO: (0) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 30.31906ms)
May 25 03:29:30.086: INFO: (0) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 31.262014ms)
May 25 03:29:30.086: INFO: (0) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 30.532695ms)
May 25 03:29:30.086: INFO: (0) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 31.219119ms)
May 25 03:29:30.086: INFO: (0) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 31.534947ms)
May 25 03:29:30.086: INFO: (0) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 31.159838ms)
May 25 03:29:30.086: INFO: (0) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 31.14854ms)
May 25 03:29:30.086: INFO: (0) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 31.367799ms)
May 25 03:29:30.127: INFO: (0) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 72.29284ms)
May 25 03:29:30.127: INFO: (0) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 73.53009ms)
May 25 03:29:30.128: INFO: (0) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 72.674476ms)
May 25 03:29:30.128: INFO: (0) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 72.738451ms)
May 25 03:29:30.128: INFO: (0) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 72.867384ms)
May 25 03:29:30.144: INFO: (1) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 16.075676ms)
May 25 03:29:30.157: INFO: (1) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 29.233089ms)
May 25 03:29:30.160: INFO: (1) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 32.07072ms)
May 25 03:29:30.172: INFO: (1) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 44.502226ms)
May 25 03:29:30.173: INFO: (1) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 44.747868ms)
May 25 03:29:30.173: INFO: (1) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 44.750711ms)
May 25 03:29:30.173: INFO: (1) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 44.398576ms)
May 25 03:29:30.173: INFO: (1) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 44.469015ms)
May 25 03:29:30.173: INFO: (1) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 45.243175ms)
May 25 03:29:30.173: INFO: (1) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 44.363159ms)
May 25 03:29:30.173: INFO: (1) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 44.390436ms)
May 25 03:29:30.173: INFO: (1) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 44.561447ms)
May 25 03:29:30.173: INFO: (1) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 44.692277ms)
May 25 03:29:30.173: INFO: (1) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 45.23668ms)
May 25 03:29:30.173: INFO: (1) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 44.631516ms)
May 25 03:29:30.179: INFO: (1) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 51.045646ms)
May 25 03:29:30.201: INFO: (2) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 21.113666ms)
May 25 03:29:30.201: INFO: (2) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 20.094333ms)
May 25 03:29:30.202: INFO: (2) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 18.724696ms)
May 25 03:29:30.202: INFO: (2) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 20.378308ms)
May 25 03:29:30.203: INFO: (2) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 20.537514ms)
May 25 03:29:30.203: INFO: (2) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 22.299021ms)
May 25 03:29:30.203: INFO: (2) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 21.522006ms)
May 25 03:29:30.205: INFO: (2) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 24.52424ms)
May 25 03:29:30.206: INFO: (2) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 22.952972ms)
May 25 03:29:30.207: INFO: (2) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 27.735329ms)
May 25 03:29:30.207: INFO: (2) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 27.141592ms)
May 25 03:29:30.213: INFO: (2) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 31.977808ms)
May 25 03:29:30.213: INFO: (2) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 30.167181ms)
May 25 03:29:30.225: INFO: (2) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 42.428133ms)
May 25 03:29:30.225: INFO: (2) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 42.231688ms)
May 25 03:29:30.225: INFO: (2) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 42.194251ms)
May 25 03:29:30.253: INFO: (3) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 27.238613ms)
May 25 03:29:30.253: INFO: (3) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 27.645867ms)
May 25 03:29:30.256: INFO: (3) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 30.505578ms)
May 25 03:29:30.256: INFO: (3) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 30.301096ms)
May 25 03:29:30.256: INFO: (3) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 30.908381ms)
May 25 03:29:30.258: INFO: (3) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 32.445173ms)
May 25 03:29:30.258: INFO: (3) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 32.86072ms)
May 25 03:29:30.259: INFO: (3) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 33.485519ms)
May 25 03:29:30.259: INFO: (3) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 34.057624ms)
May 25 03:29:30.260: INFO: (3) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 34.584877ms)
May 25 03:29:30.260: INFO: (3) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 34.457407ms)
May 25 03:29:30.260: INFO: (3) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 34.996374ms)
May 25 03:29:30.260: INFO: (3) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 34.922515ms)
May 25 03:29:30.260: INFO: (3) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 34.757988ms)
May 25 03:29:30.260: INFO: (3) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 35.025908ms)
May 25 03:29:30.262: INFO: (3) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 36.632452ms)
May 25 03:29:30.270: INFO: (4) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 7.127938ms)
May 25 03:29:30.270: INFO: (4) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 7.426117ms)
May 25 03:29:30.276: INFO: (4) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 12.366633ms)
May 25 03:29:30.276: INFO: (4) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 13.250517ms)
May 25 03:29:30.277: INFO: (4) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 13.126882ms)
May 25 03:29:30.282: INFO: (4) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 18.045426ms)
May 25 03:29:30.282: INFO: (4) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 18.880799ms)
May 25 03:29:30.282: INFO: (4) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 18.410142ms)
May 25 03:29:30.283: INFO: (4) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 18.59197ms)
May 25 03:29:30.283: INFO: (4) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 19.493836ms)
May 25 03:29:30.283: INFO: (4) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 18.921494ms)
May 25 03:29:30.283: INFO: (4) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 18.849615ms)
May 25 03:29:30.283: INFO: (4) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 19.101558ms)
May 25 03:29:30.283: INFO: (4) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 19.034797ms)
May 25 03:29:30.283: INFO: (4) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 19.413941ms)
May 25 03:29:30.284: INFO: (4) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 19.901272ms)
May 25 03:29:30.315: INFO: (5) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 30.32436ms)
May 25 03:29:30.315: INFO: (5) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 30.666385ms)
May 25 03:29:30.322: INFO: (5) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 37.816751ms)
May 25 03:29:30.323: INFO: (5) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 38.331447ms)
May 25 03:29:30.323: INFO: (5) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 39.317508ms)
May 25 03:29:30.324: INFO: (5) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 39.178476ms)
May 25 03:29:30.324: INFO: (5) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 38.912528ms)
May 25 03:29:30.324: INFO: (5) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 39.023368ms)
May 25 03:29:30.324: INFO: (5) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 39.582559ms)
May 25 03:29:30.324: INFO: (5) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 39.942557ms)
May 25 03:29:30.325: INFO: (5) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 40.460934ms)
May 25 03:29:30.325: INFO: (5) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 40.116286ms)
May 25 03:29:30.325: INFO: (5) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 41.001398ms)
May 25 03:29:30.326: INFO: (5) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 41.078222ms)
May 25 03:29:30.326: INFO: (5) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 41.319418ms)
May 25 03:29:30.327: INFO: (5) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 42.355015ms)
May 25 03:29:30.342: INFO: (6) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 14.22074ms)
May 25 03:29:30.342: INFO: (6) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 14.360774ms)
May 25 03:29:30.343: INFO: (6) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 15.449204ms)
May 25 03:29:30.344: INFO: (6) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 15.843177ms)
May 25 03:29:30.344: INFO: (6) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 16.127989ms)
May 25 03:29:30.344: INFO: (6) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 15.868278ms)
May 25 03:29:30.344: INFO: (6) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 16.32809ms)
May 25 03:29:30.344: INFO: (6) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 16.968176ms)
May 25 03:29:30.344: INFO: (6) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 16.806041ms)
May 25 03:29:30.344: INFO: (6) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 16.903483ms)
May 25 03:29:30.344: INFO: (6) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 16.485007ms)
May 25 03:29:30.350: INFO: (6) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 22.012549ms)
May 25 03:29:30.350: INFO: (6) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 22.265901ms)
May 25 03:29:30.350: INFO: (6) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 22.423868ms)
May 25 03:29:30.350: INFO: (6) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 22.254498ms)
May 25 03:29:30.351: INFO: (6) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 23.13587ms)
May 25 03:29:30.362: INFO: (7) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 10.850582ms)
May 25 03:29:30.363: INFO: (7) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 11.02112ms)
May 25 03:29:30.363: INFO: (7) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 11.347368ms)
May 25 03:29:30.363: INFO: (7) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 10.734542ms)
May 25 03:29:30.368: INFO: (7) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 15.159176ms)
May 25 03:29:30.368: INFO: (7) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 15.829083ms)
May 25 03:29:30.368: INFO: (7) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 16.07022ms)
May 25 03:29:30.368: INFO: (7) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 16.051016ms)
May 25 03:29:30.369: INFO: (7) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 16.270758ms)
May 25 03:29:30.376: INFO: (7) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 23.868565ms)
May 25 03:29:30.376: INFO: (7) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 23.974101ms)
May 25 03:29:30.376: INFO: (7) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 24.408712ms)
May 25 03:29:30.376: INFO: (7) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 24.287365ms)
May 25 03:29:30.377: INFO: (7) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 24.687926ms)
May 25 03:29:30.377: INFO: (7) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 25.489553ms)
May 25 03:29:30.378: INFO: (7) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 25.897273ms)
May 25 03:29:30.385: INFO: (8) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 6.964034ms)
May 25 03:29:30.386: INFO: (8) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 7.939629ms)
May 25 03:29:30.390: INFO: (8) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 11.823356ms)
May 25 03:29:30.391: INFO: (8) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 12.259311ms)
May 25 03:29:30.391: INFO: (8) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 13.161693ms)
May 25 03:29:30.391: INFO: (8) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 12.640747ms)
May 25 03:29:30.392: INFO: (8) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 12.785347ms)
May 25 03:29:30.392: INFO: (8) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 12.624173ms)
May 25 03:29:30.392: INFO: (8) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 12.370437ms)
May 25 03:29:30.392: INFO: (8) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 13.419067ms)
May 25 03:29:30.398: INFO: (8) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 19.516559ms)
May 25 03:29:30.398: INFO: (8) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 19.265726ms)
May 25 03:29:30.399: INFO: (8) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 19.568754ms)
May 25 03:29:30.399: INFO: (8) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 20.841729ms)
May 25 03:29:30.400: INFO: (8) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 20.429432ms)
May 25 03:29:30.400: INFO: (8) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 20.592576ms)
May 25 03:29:30.410: INFO: (9) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 9.801686ms)
May 25 03:29:30.410: INFO: (9) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 9.704574ms)
May 25 03:29:30.411: INFO: (9) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 10.410621ms)
May 25 03:29:30.413: INFO: (9) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 12.961195ms)
May 25 03:29:30.413: INFO: (9) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 13.312864ms)
May 25 03:29:30.413: INFO: (9) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 13.042441ms)
May 25 03:29:30.433: INFO: (9) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 33.217163ms)
May 25 03:29:30.435: INFO: (9) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 34.499794ms)
May 25 03:29:30.435: INFO: (9) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 34.34114ms)
May 25 03:29:30.437: INFO: (9) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 36.871134ms)
May 25 03:29:30.437: INFO: (9) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 36.786529ms)
May 25 03:29:30.438: INFO: (9) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 36.781391ms)
May 25 03:29:30.438: INFO: (9) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 37.119053ms)
May 25 03:29:30.438: INFO: (9) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 37.015105ms)
May 25 03:29:30.438: INFO: (9) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 37.151377ms)
May 25 03:29:30.438: INFO: (9) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 37.308118ms)
May 25 03:29:30.445: INFO: (10) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 6.625505ms)
May 25 03:29:30.449: INFO: (10) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 9.99048ms)
May 25 03:29:30.449: INFO: (10) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 9.944233ms)
May 25 03:29:30.449: INFO: (10) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 10.35256ms)
May 25 03:29:30.449: INFO: (10) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 10.708933ms)
May 25 03:29:30.449: INFO: (10) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 10.165698ms)
May 25 03:29:30.452: INFO: (10) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 12.817421ms)
May 25 03:29:30.452: INFO: (10) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 12.78858ms)
May 25 03:29:30.452: INFO: (10) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 13.165807ms)
May 25 03:29:30.453: INFO: (10) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 14.182343ms)
May 25 03:29:30.453: INFO: (10) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 14.320201ms)
May 25 03:29:30.453: INFO: (10) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 13.517992ms)
May 25 03:29:30.453: INFO: (10) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 14.219666ms)
May 25 03:29:30.453: INFO: (10) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 14.25025ms)
May 25 03:29:30.453: INFO: (10) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 14.159602ms)
May 25 03:29:30.454: INFO: (10) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 15.54603ms)
May 25 03:29:30.461: INFO: (11) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 6.635913ms)
May 25 03:29:30.465: INFO: (11) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 9.523542ms)
May 25 03:29:30.466: INFO: (11) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 8.90716ms)
May 25 03:29:30.466: INFO: (11) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 11.109602ms)
May 25 03:29:30.466: INFO: (11) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 9.660355ms)
May 25 03:29:30.467: INFO: (11) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 9.967313ms)
May 25 03:29:30.467: INFO: (11) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 10.515431ms)
May 25 03:29:30.467: INFO: (11) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 10.771467ms)
May 25 03:29:30.467: INFO: (11) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 10.98773ms)
May 25 03:29:30.467: INFO: (11) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 10.54769ms)
May 25 03:29:30.471: INFO: (11) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 15.849314ms)
May 25 03:29:30.471: INFO: (11) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 15.296866ms)
May 25 03:29:30.471: INFO: (11) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 14.566849ms)
May 25 03:29:30.472: INFO: (11) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 16.91039ms)
May 25 03:29:30.472: INFO: (11) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 16.378847ms)
May 25 03:29:30.472: INFO: (11) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 16.867212ms)
May 25 03:29:30.484: INFO: (12) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 11.674093ms)
May 25 03:29:30.484: INFO: (12) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 11.312657ms)
May 25 03:29:30.485: INFO: (12) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 11.847641ms)
May 25 03:29:30.485: INFO: (12) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 10.334506ms)
May 25 03:29:30.485: INFO: (12) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 10.016898ms)
May 25 03:29:30.485: INFO: (12) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 13.283941ms)
May 25 03:29:30.485: INFO: (12) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 12.874727ms)
May 25 03:29:30.485: INFO: (12) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 11.005517ms)
May 25 03:29:30.485: INFO: (12) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 11.233476ms)
May 25 03:29:30.485: INFO: (12) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 10.4448ms)
May 25 03:29:30.489: INFO: (12) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 15.014081ms)
May 25 03:29:30.490: INFO: (12) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 15.948963ms)
May 25 03:29:30.490: INFO: (12) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 16.623402ms)
May 25 03:29:30.490: INFO: (12) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 16.307506ms)
May 25 03:29:30.490: INFO: (12) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 15.115997ms)
May 25 03:29:30.490: INFO: (12) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 16.030902ms)
May 25 03:29:30.497: INFO: (13) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 7.19076ms)
May 25 03:29:30.497: INFO: (13) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 7.482214ms)
May 25 03:29:30.500: INFO: (13) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 9.849957ms)
May 25 03:29:30.500: INFO: (13) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 9.701173ms)
May 25 03:29:30.503: INFO: (13) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 12.620616ms)
May 25 03:29:30.503: INFO: (13) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 12.174495ms)
May 25 03:29:30.503: INFO: (13) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 12.618992ms)
May 25 03:29:30.503: INFO: (13) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 13.259182ms)
May 25 03:29:30.503: INFO: (13) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 13.16924ms)
May 25 03:29:30.504: INFO: (13) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 12.904294ms)
May 25 03:29:30.508: INFO: (13) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 17.29798ms)
May 25 03:29:30.508: INFO: (13) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 17.468207ms)
May 25 03:29:30.508: INFO: (13) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 17.425736ms)
May 25 03:29:30.508: INFO: (13) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 17.962172ms)
May 25 03:29:30.509: INFO: (13) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 18.030104ms)
May 25 03:29:30.509: INFO: (13) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 18.464149ms)
May 25 03:29:30.519: INFO: (14) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 9.142363ms)
May 25 03:29:30.520: INFO: (14) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 10.969302ms)
May 25 03:29:30.520: INFO: (14) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 11.472097ms)
May 25 03:29:30.520: INFO: (14) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 10.836301ms)
May 25 03:29:30.520: INFO: (14) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 10.367536ms)
May 25 03:29:30.522: INFO: (14) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 10.924074ms)
May 25 03:29:30.522: INFO: (14) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 12.146577ms)
May 25 03:29:30.522: INFO: (14) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 12.032967ms)
May 25 03:29:30.522: INFO: (14) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 11.912225ms)
May 25 03:29:30.523: INFO: (14) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 13.862404ms)
May 25 03:29:30.528: INFO: (14) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 18.038821ms)
May 25 03:29:30.529: INFO: (14) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 17.365495ms)
May 25 03:29:30.529: INFO: (14) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 17.255606ms)
May 25 03:29:30.529: INFO: (14) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 18.157991ms)
May 25 03:29:30.529: INFO: (14) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 17.417043ms)
May 25 03:29:30.529: INFO: (14) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 18.030955ms)
May 25 03:29:30.541: INFO: (15) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 11.526056ms)
May 25 03:29:30.542: INFO: (15) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 12.434978ms)
May 25 03:29:30.542: INFO: (15) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 12.286562ms)
May 25 03:29:30.542: INFO: (15) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 12.808344ms)
May 25 03:29:30.542: INFO: (15) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 13.174936ms)
May 25 03:29:30.542: INFO: (15) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 12.695424ms)
May 25 03:29:30.543: INFO: (15) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 13.145586ms)
May 25 03:29:30.543: INFO: (15) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 12.961889ms)
May 25 03:29:30.543: INFO: (15) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 13.047143ms)
May 25 03:29:30.543: INFO: (15) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 12.528093ms)
May 25 03:29:30.544: INFO: (15) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 15.049751ms)
May 25 03:29:30.545: INFO: (15) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 14.946466ms)
May 25 03:29:30.545: INFO: (15) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 15.006063ms)
May 25 03:29:30.545: INFO: (15) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 15.0241ms)
May 25 03:29:30.546: INFO: (15) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 16.084589ms)
May 25 03:29:30.546: INFO: (15) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 15.747947ms)
May 25 03:29:30.559: INFO: (16) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 12.175455ms)
May 25 03:29:30.559: INFO: (16) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 11.003869ms)
May 25 03:29:30.559: INFO: (16) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 10.936581ms)
May 25 03:29:30.559: INFO: (16) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 13.229451ms)
May 25 03:29:30.560: INFO: (16) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 10.455642ms)
May 25 03:29:30.560: INFO: (16) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 10.867352ms)
May 25 03:29:30.560: INFO: (16) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 10.763898ms)
May 25 03:29:30.560: INFO: (16) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 12.189967ms)
May 25 03:29:30.560: INFO: (16) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 11.031974ms)
May 25 03:29:30.560: INFO: (16) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 10.717767ms)
May 25 03:29:30.562: INFO: (16) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 14.0176ms)
May 25 03:29:30.563: INFO: (16) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 15.245064ms)
May 25 03:29:30.564: INFO: (16) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 15.64748ms)
May 25 03:29:30.564: INFO: (16) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 16.549316ms)
May 25 03:29:30.564: INFO: (16) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 15.599734ms)
May 25 03:29:30.564: INFO: (16) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 17.328652ms)
May 25 03:29:30.572: INFO: (17) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 7.852048ms)
May 25 03:29:30.573: INFO: (17) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 9.050091ms)
May 25 03:29:30.577: INFO: (17) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 12.984962ms)
May 25 03:29:30.577: INFO: (17) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 12.622895ms)
May 25 03:29:30.577: INFO: (17) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 12.675602ms)
May 25 03:29:30.577: INFO: (17) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 13.233536ms)
May 25 03:29:30.577: INFO: (17) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 13.170685ms)
May 25 03:29:30.578: INFO: (17) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 14.006166ms)
May 25 03:29:30.579: INFO: (17) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 14.789289ms)
May 25 03:29:30.579: INFO: (17) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 15.050238ms)
May 25 03:29:30.579: INFO: (17) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 15.233393ms)
May 25 03:29:30.580: INFO: (17) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 15.461055ms)
May 25 03:29:30.581: INFO: (17) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 16.248175ms)
May 25 03:29:30.581: INFO: (17) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 16.557972ms)
May 25 03:29:30.581: INFO: (17) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 16.582803ms)
May 25 03:29:30.581: INFO: (17) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 17.062719ms)
May 25 03:29:30.590: INFO: (18) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 8.358642ms)
May 25 03:29:30.590: INFO: (18) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 8.209716ms)
May 25 03:29:30.591: INFO: (18) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 9.211606ms)
May 25 03:29:30.593: INFO: (18) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 11.288492ms)
May 25 03:29:30.594: INFO: (18) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 12.225298ms)
May 25 03:29:30.594: INFO: (18) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 12.540524ms)
May 25 03:29:30.594: INFO: (18) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 12.239064ms)
May 25 03:29:30.594: INFO: (18) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 12.469734ms)
May 25 03:29:30.594: INFO: (18) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 12.846337ms)
May 25 03:29:30.595: INFO: (18) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 13.15017ms)
May 25 03:29:30.595: INFO: (18) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 13.056781ms)
May 25 03:29:30.595: INFO: (18) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 13.607151ms)
May 25 03:29:30.597: INFO: (18) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 15.65126ms)
May 25 03:29:30.597: INFO: (18) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 15.850247ms)
May 25 03:29:30.597: INFO: (18) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 15.394574ms)
May 25 03:29:30.598: INFO: (18) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 15.837794ms)
May 25 03:29:30.604: INFO: (19) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">test<... (200; 5.937046ms)
May 25 03:29:30.604: INFO: (19) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:460/proxy/: tls baz (200; 5.53281ms)
May 25 03:29:30.608: INFO: (19) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:1080/proxy/rewriteme">... (200; 9.518969ms)
May 25 03:29:30.609: INFO: (19) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:462/proxy/: tls qux (200; 10.4348ms)
May 25 03:29:30.609: INFO: (19) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24/proxy/rewriteme">test</a> (200; 10.144017ms)
May 25 03:29:30.610: INFO: (19) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 11.250645ms)
May 25 03:29:30.611: INFO: (19) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:162/proxy/: bar (200; 12.822749ms)
May 25 03:29:30.613: INFO: (19) /api/v1/namespaces/proxy-3544/pods/http:proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 14.362809ms)
May 25 03:29:30.613: INFO: (19) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname1/proxy/: foo (200; 15.04238ms)
May 25 03:29:30.614: INFO: (19) /api/v1/namespaces/proxy-3544/pods/proxy-service-dfv6t-7rg24:160/proxy/: foo (200; 14.788931ms)
May 25 03:29:30.614: INFO: (19) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname2/proxy/: bar (200; 15.837428ms)
May 25 03:29:30.614: INFO: (19) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname1/proxy/: tls baz (200; 15.477569ms)
May 25 03:29:30.615: INFO: (19) /api/v1/namespaces/proxy-3544/services/https:proxy-service-dfv6t:tlsportname2/proxy/: tls qux (200; 15.923192ms)
May 25 03:29:30.615: INFO: (19) /api/v1/namespaces/proxy-3544/services/http:proxy-service-dfv6t:portname1/proxy/: foo (200; 15.548822ms)
May 25 03:29:30.615: INFO: (19) /api/v1/namespaces/proxy-3544/services/proxy-service-dfv6t:portname2/proxy/: bar (200; 16.659952ms)
May 25 03:29:30.615: INFO: (19) /api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/: <a href="/api/v1/namespaces/proxy-3544/pods/https:proxy-service-dfv6t-7rg24:443/proxy/tlsrewritem... (200; 16.180668ms)
STEP: deleting ReplicationController proxy-service-dfv6t in namespace proxy-3544, will wait for the garbage collector to delete the pods
May 25 03:29:30.687: INFO: Deleting ReplicationController proxy-service-dfv6t took: 15.091848ms
May 25 03:29:30.789: INFO: Terminating ReplicationController proxy-service-dfv6t pods took: 101.160542ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:29:38.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3544" for this suite.

• [SLOW TEST:19.634 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":339,"completed":148,"skipped":2047,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:29:38.534: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
May 25 03:29:38.588: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:30:11.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5948" for this suite.

• [SLOW TEST:32.604 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":339,"completed":149,"skipped":2052,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:30:11.139: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:30:11.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7366" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":339,"completed":150,"skipped":2083,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:30:11.264: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 03:30:13.404: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 03:30:16.441: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:30:16.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8748" for this suite.
STEP: Destroying namespace "webhook-8748-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.488 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":339,"completed":151,"skipped":2096,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:30:16.753: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 25 03:30:16.809: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 25 03:30:16.820: INFO: Waiting for terminating namespaces to be deleted...
May 25 03:30:16.825: INFO: 
Logging pods the apiserver thinks is on node 10.10.103.201-worker before test
May 25 03:30:16.836: INFO: calico-kube-controllers-867fdc777c-xgnbl from calico-system started at 2021-05-25 03:19:31 +0000 UTC (1 container statuses recorded)
May 25 03:30:16.836: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 25 03:30:16.836: INFO: calico-node-gf4hs from calico-system started at 2021-05-21 05:16:19 +0000 UTC (1 container statuses recorded)
May 25 03:30:16.836: INFO: 	Container calico-node ready: true, restart count 0
May 25 03:30:16.836: INFO: calico-typha-584858976f-vj6gj from calico-system started at 2021-05-21 05:16:25 +0000 UTC (1 container statuses recorded)
May 25 03:30:16.836: INFO: 	Container calico-typha ready: true, restart count 0
May 25 03:30:16.836: INFO: coredns-768c7df75d-h7src from kube-system started at 2021-05-25 03:19:31 +0000 UTC (1 container statuses recorded)
May 25 03:30:16.836: INFO: 	Container coredns ready: true, restart count 0
May 25 03:30:16.836: INFO: coredns-768c7df75d-s7x9f from kube-system started at 2021-05-25 03:19:31 +0000 UTC (1 container statuses recorded)
May 25 03:30:16.836: INFO: 	Container coredns ready: true, restart count 0
May 25 03:30:16.836: INFO: kube-proxy-s6ql6 from kube-system started at 2021-05-21 05:14:25 +0000 UTC (1 container statuses recorded)
May 25 03:30:16.836: INFO: 	Container kube-proxy ready: true, restart count 1
May 25 03:30:16.836: INFO: sonobuoy from sonobuoy started at 2021-05-25 02:26:58 +0000 UTC (1 container statuses recorded)
May 25 03:30:16.836: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 25 03:30:16.836: INFO: sonobuoy-e2e-job-58d7b3bc039b4826 from sonobuoy started at 2021-05-25 02:27:00 +0000 UTC (2 container statuses recorded)
May 25 03:30:16.836: INFO: 	Container e2e ready: true, restart count 0
May 25 03:30:16.836: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 03:30:16.836: INFO: sonobuoy-systemd-logs-daemon-set-ebbefc146b074e77-p6vlh from sonobuoy started at 2021-05-25 02:27:00 +0000 UTC (2 container statuses recorded)
May 25 03:30:16.836: INFO: 	Container sonobuoy-worker ready: false, restart count 5
May 25 03:30:16.836: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 03:30:16.836: INFO: tigera-operator-5b47ff5dcc-25sz7 from tigera-operator started at 2021-05-21 05:15:37 +0000 UTC (1 container statuses recorded)
May 25 03:30:16.836: INFO: 	Container tigera-operator ready: true, restart count 2
May 25 03:30:16.836: INFO: 
Logging pods the apiserver thinks is on node 10.10.103.202-worker before test
May 25 03:30:16.847: INFO: calico-node-k6h5c from calico-system started at 2021-05-21 05:16:19 +0000 UTC (1 container statuses recorded)
May 25 03:30:16.847: INFO: 	Container calico-node ready: true, restart count 0
May 25 03:30:16.847: INFO: calico-typha-584858976f-x82v6 from calico-system started at 2021-05-21 05:16:18 +0000 UTC (1 container statuses recorded)
May 25 03:30:16.847: INFO: 	Container calico-typha ready: true, restart count 0
May 25 03:30:16.847: INFO: kube-proxy-fvq49 from kube-system started at 2021-05-21 05:14:38 +0000 UTC (1 container statuses recorded)
May 25 03:30:16.847: INFO: 	Container kube-proxy ready: true, restart count 0
May 25 03:30:16.847: INFO: pod-qos-class-567ac7e9-8e36-41c3-a6c4-0fc13c9e3d2c from pods-7366 started at 2021-05-25 03:30:11 +0000 UTC (1 container statuses recorded)
May 25 03:30:16.847: INFO: 	Container agnhost ready: false, restart count 0
May 25 03:30:16.847: INFO: sonobuoy-systemd-logs-daemon-set-ebbefc146b074e77-kj9h6 from sonobuoy started at 2021-05-25 02:27:00 +0000 UTC (2 container statuses recorded)
May 25 03:30:16.847: INFO: 	Container sonobuoy-worker ready: false, restart count 5
May 25 03:30:16.847: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node 10.10.103.201-worker
STEP: verifying the node has the label node 10.10.103.202-worker
May 25 03:30:16.932: INFO: Pod calico-kube-controllers-867fdc777c-xgnbl requesting resource cpu=0m on Node 10.10.103.201-worker
May 25 03:30:16.932: INFO: Pod calico-node-gf4hs requesting resource cpu=0m on Node 10.10.103.201-worker
May 25 03:30:16.932: INFO: Pod calico-node-k6h5c requesting resource cpu=0m on Node 10.10.103.202-worker
May 25 03:30:16.932: INFO: Pod calico-typha-584858976f-vj6gj requesting resource cpu=0m on Node 10.10.103.201-worker
May 25 03:30:16.932: INFO: Pod calico-typha-584858976f-x82v6 requesting resource cpu=0m on Node 10.10.103.202-worker
May 25 03:30:16.932: INFO: Pod coredns-768c7df75d-h7src requesting resource cpu=100m on Node 10.10.103.201-worker
May 25 03:30:16.932: INFO: Pod coredns-768c7df75d-s7x9f requesting resource cpu=100m on Node 10.10.103.201-worker
May 25 03:30:16.932: INFO: Pod kube-proxy-fvq49 requesting resource cpu=0m on Node 10.10.103.202-worker
May 25 03:30:16.932: INFO: Pod kube-proxy-s6ql6 requesting resource cpu=0m on Node 10.10.103.201-worker
May 25 03:30:16.932: INFO: Pod pod-qos-class-567ac7e9-8e36-41c3-a6c4-0fc13c9e3d2c requesting resource cpu=100m on Node 10.10.103.202-worker
May 25 03:30:16.932: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.10.103.201-worker
May 25 03:30:16.932: INFO: Pod sonobuoy-e2e-job-58d7b3bc039b4826 requesting resource cpu=0m on Node 10.10.103.201-worker
May 25 03:30:16.932: INFO: Pod sonobuoy-systemd-logs-daemon-set-ebbefc146b074e77-kj9h6 requesting resource cpu=0m on Node 10.10.103.202-worker
May 25 03:30:16.932: INFO: Pod sonobuoy-systemd-logs-daemon-set-ebbefc146b074e77-p6vlh requesting resource cpu=0m on Node 10.10.103.201-worker
May 25 03:30:16.932: INFO: Pod tigera-operator-5b47ff5dcc-25sz7 requesting resource cpu=0m on Node 10.10.103.201-worker
STEP: Starting Pods to consume most of the cluster CPU.
May 25 03:30:16.932: INFO: Creating a pod which consumes cpu=2660m on Node 10.10.103.201-worker
May 25 03:30:16.942: INFO: Creating a pod which consumes cpu=2730m on Node 10.10.103.202-worker
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9e5a0973-80f1-4b80-be73-44c4b45dc249.168231a8d28994fe], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5793/filler-pod-9e5a0973-80f1-4b80-be73-44c4b45dc249 to 10.10.103.202-worker]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9e5a0973-80f1-4b80-be73-44c4b45dc249.168231a9263b01f6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9e5a0973-80f1-4b80-be73-44c4b45dc249.168231a92929e2e1], Reason = [Created], Message = [Created container filler-pod-9e5a0973-80f1-4b80-be73-44c4b45dc249]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9e5a0973-80f1-4b80-be73-44c4b45dc249.168231a9355e223e], Reason = [Started], Message = [Started container filler-pod-9e5a0973-80f1-4b80-be73-44c4b45dc249]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dddb2fd2-8026-457e-95af-c3b5ac602461.168231a8d1d7fe0c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5793/filler-pod-dddb2fd2-8026-457e-95af-c3b5ac602461 to 10.10.103.201-worker]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dddb2fd2-8026-457e-95af-c3b5ac602461.168231a9256341de], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dddb2fd2-8026-457e-95af-c3b5ac602461.168231a927a32b0e], Reason = [Created], Message = [Created container filler-pod-dddb2fd2-8026-457e-95af-c3b5ac602461]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dddb2fd2-8026-457e-95af-c3b5ac602461.168231a932904faa], Reason = [Started], Message = [Started container filler-pod-dddb2fd2-8026-457e-95af-c3b5ac602461]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168231a9c27b5568], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node 10.10.103.201-worker
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.10.103.202-worker
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:30:22.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5793" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.315 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":339,"completed":152,"skipped":2115,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:30:22.068: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0525 03:30:22.124121      23 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
May 25 03:32:00.157: INFO: Warning: Found 0 jobs in namespace cronjob-5098
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:32:02.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5098" for this suite.

• [SLOW TEST:100.125 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":339,"completed":153,"skipped":2134,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:32:02.193: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-a6bd2cb7-88ac-49e7-8a1a-b8fa6ba3d829
STEP: Creating secret with name s-test-opt-upd-6b9ad77c-ed9e-42b1-9199-ecaadfd9233e
STEP: Creating the pod
May 25 03:32:02.274: INFO: The status of Pod pod-projected-secrets-382f2ee6-631b-4f6a-bf1d-e191b0c72bca is Pending, waiting for it to be Running (with Ready = true)
May 25 03:32:04.281: INFO: The status of Pod pod-projected-secrets-382f2ee6-631b-4f6a-bf1d-e191b0c72bca is Pending, waiting for it to be Running (with Ready = true)
May 25 03:32:06.281: INFO: The status of Pod pod-projected-secrets-382f2ee6-631b-4f6a-bf1d-e191b0c72bca is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-a6bd2cb7-88ac-49e7-8a1a-b8fa6ba3d829
STEP: Updating secret s-test-opt-upd-6b9ad77c-ed9e-42b1-9199-ecaadfd9233e
STEP: Creating secret with name s-test-opt-create-b7ce86aa-a66d-4347-87ad-d0f43bd99331
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:32:10.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3757" for this suite.

• [SLOW TEST:8.351 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":154,"skipped":2137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:32:10.545: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-a078d28c-17d2-4dea-8bd4-575182b6288d in namespace container-probe-8947
May 25 03:32:14.625: INFO: Started pod liveness-a078d28c-17d2-4dea-8bd4-575182b6288d in namespace container-probe-8947
STEP: checking the pod's current state and verifying that restartCount is present
May 25 03:32:14.629: INFO: Initial restart count of pod liveness-a078d28c-17d2-4dea-8bd4-575182b6288d is 0
May 25 03:32:32.695: INFO: Restart count of pod container-probe-8947/liveness-a078d28c-17d2-4dea-8bd4-575182b6288d is now 1 (18.065442165s elapsed)
May 25 03:32:52.954: INFO: Restart count of pod container-probe-8947/liveness-a078d28c-17d2-4dea-8bd4-575182b6288d is now 2 (38.324628473s elapsed)
May 25 03:33:13.261: INFO: Restart count of pod container-probe-8947/liveness-a078d28c-17d2-4dea-8bd4-575182b6288d is now 3 (58.63193985s elapsed)
May 25 03:33:31.570: INFO: Restart count of pod container-probe-8947/liveness-a078d28c-17d2-4dea-8bd4-575182b6288d is now 4 (1m16.94045192s elapsed)
May 25 03:34:44.739: INFO: Restart count of pod container-probe-8947/liveness-a078d28c-17d2-4dea-8bd4-575182b6288d is now 5 (2m30.109547672s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:34:44.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8947" for this suite.

• [SLOW TEST:154.235 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":339,"completed":155,"skipped":2161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:34:44.781: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
May 25 03:34:44.851: INFO: Waiting up to 5m0s for pod "client-containers-46233366-3340-4ae6-8899-84e73c320286" in namespace "containers-5786" to be "Succeeded or Failed"
May 25 03:34:44.855: INFO: Pod "client-containers-46233366-3340-4ae6-8899-84e73c320286": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024705ms
May 25 03:34:46.861: INFO: Pod "client-containers-46233366-3340-4ae6-8899-84e73c320286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00998154s
STEP: Saw pod success
May 25 03:34:46.861: INFO: Pod "client-containers-46233366-3340-4ae6-8899-84e73c320286" satisfied condition "Succeeded or Failed"
May 25 03:34:46.865: INFO: Trying to get logs from node 10.10.103.202-worker pod client-containers-46233366-3340-4ae6-8899-84e73c320286 container agnhost-container: <nil>
STEP: delete the pod
May 25 03:34:46.915: INFO: Waiting for pod client-containers-46233366-3340-4ae6-8899-84e73c320286 to disappear
May 25 03:34:46.918: INFO: Pod client-containers-46233366-3340-4ae6-8899-84e73c320286 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:34:46.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5786" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":339,"completed":156,"skipped":2183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:34:46.931: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:34:51.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4347" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":339,"completed":157,"skipped":2209,"failed":0}
SSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:34:51.032: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
May 25 03:34:51.091: INFO: created test-event-1
May 25 03:34:51.096: INFO: created test-event-2
May 25 03:34:51.101: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 25 03:34:51.105: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 25 03:34:51.125: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:34:51.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1522" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":339,"completed":158,"skipped":2216,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:34:51.141: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
May 25 03:34:51.187: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
May 25 03:34:52.052: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
May 25 03:34:54.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757510492, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757510492, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757510492, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757510492, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 03:34:57.875: INFO: Waited 1.729583122s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
May 25 03:34:58.101: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:34:58.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6394" for this suite.

• [SLOW TEST:7.841 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":339,"completed":159,"skipped":2221,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:34:58.982: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
May 25 03:35:01.080: INFO: running pods: 0 < 1
May 25 03:35:03.086: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:35:05.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1955" for this suite.

• [SLOW TEST:6.169 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":339,"completed":160,"skipped":2233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:35:05.152: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 25 03:35:05.213: INFO: Waiting up to 1m0s for all nodes to be ready
May 25 03:36:05.277: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
May 25 03:36:05.311: INFO: Created pod: pod0-sched-preemption-low-priority
May 25 03:36:05.347: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:36:29.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1794" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:84.377 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":339,"completed":161,"skipped":2264,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:36:29.529: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-27a147cc-46ac-4e38-9050-cbbd8ebc6bef
STEP: Creating a pod to test consume secrets
May 25 03:36:29.626: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c00ead95-b5e0-4435-b1bb-064a3f458811" in namespace "projected-6385" to be "Succeeded or Failed"
May 25 03:36:29.630: INFO: Pod "pod-projected-secrets-c00ead95-b5e0-4435-b1bb-064a3f458811": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016164ms
May 25 03:36:31.639: INFO: Pod "pod-projected-secrets-c00ead95-b5e0-4435-b1bb-064a3f458811": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012411986s
STEP: Saw pod success
May 25 03:36:31.639: INFO: Pod "pod-projected-secrets-c00ead95-b5e0-4435-b1bb-064a3f458811" satisfied condition "Succeeded or Failed"
May 25 03:36:31.643: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-projected-secrets-c00ead95-b5e0-4435-b1bb-064a3f458811 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 25 03:36:31.686: INFO: Waiting for pod pod-projected-secrets-c00ead95-b5e0-4435-b1bb-064a3f458811 to disappear
May 25 03:36:31.690: INFO: Pod pod-projected-secrets-c00ead95-b5e0-4435-b1bb-064a3f458811 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:36:31.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6385" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":162,"skipped":2264,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:36:31.704: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May 25 03:36:31.758: INFO: Waiting up to 1m0s for all nodes to be ready
May 25 03:37:31.816: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:37:31.823: INFO: Starting informer...
STEP: Starting pod...
May 25 03:37:32.052: INFO: Pod is running on 10.10.103.202-worker. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 25 03:37:32.077: INFO: Pod wasn't evicted. Proceeding
May 25 03:37:32.077: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 25 03:38:47.100: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:38:47.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8593" for this suite.

• [SLOW TEST:135.420 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":339,"completed":163,"skipped":2280,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:38:47.124: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:38:47.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2952" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":339,"completed":164,"skipped":2290,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:38:47.210: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 25 03:38:47.273: INFO: Waiting up to 1m0s for all nodes to be ready
May 25 03:39:47.330: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
May 25 03:39:47.364: INFO: Created pod: pod0-sched-preemption-low-priority
May 25 03:39:47.420: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:39:59.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1571" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:72.329 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":339,"completed":165,"skipped":2319,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:39:59.540: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:39:59.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-7709" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":339,"completed":166,"skipped":2338,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:39:59.686: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 03:40:01.490: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 25 03:40:03.505: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757510801, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757510801, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757510801, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757510801, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 03:40:06.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:40:06.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1918" for this suite.
STEP: Destroying namespace "webhook-1918-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.153 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":339,"completed":167,"skipped":2359,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:40:06.839: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 25 03:40:06.947: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:40:06.951: INFO: Number of nodes with available pods: 0
May 25 03:40:06.951: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:40:07.959: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:40:07.964: INFO: Number of nodes with available pods: 0
May 25 03:40:07.964: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:40:08.960: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:40:08.967: INFO: Number of nodes with available pods: 0
May 25 03:40:08.967: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:40:09.959: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:40:09.964: INFO: Number of nodes with available pods: 2
May 25 03:40:09.964: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 25 03:40:10.003: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:40:10.009: INFO: Number of nodes with available pods: 1
May 25 03:40:10.009: INFO: Node 10.10.103.202-worker is running more than one daemon pod
May 25 03:40:11.019: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:40:11.024: INFO: Number of nodes with available pods: 1
May 25 03:40:11.024: INFO: Node 10.10.103.202-worker is running more than one daemon pod
May 25 03:40:12.020: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:40:12.026: INFO: Number of nodes with available pods: 1
May 25 03:40:12.026: INFO: Node 10.10.103.202-worker is running more than one daemon pod
May 25 03:40:13.020: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:40:13.030: INFO: Number of nodes with available pods: 1
May 25 03:40:13.030: INFO: Node 10.10.103.202-worker is running more than one daemon pod
May 25 03:40:14.020: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:40:14.024: INFO: Number of nodes with available pods: 1
May 25 03:40:14.024: INFO: Node 10.10.103.202-worker is running more than one daemon pod
May 25 03:40:15.018: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:40:15.022: INFO: Number of nodes with available pods: 1
May 25 03:40:15.022: INFO: Node 10.10.103.202-worker is running more than one daemon pod
May 25 03:40:16.023: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:40:16.028: INFO: Number of nodes with available pods: 2
May 25 03:40:16.028: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5066, will wait for the garbage collector to delete the pods
May 25 03:40:16.097: INFO: Deleting DaemonSet.extensions daemon-set took: 8.6843ms
May 25 03:40:16.198: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.108005ms
May 25 03:40:28.507: INFO: Number of nodes with available pods: 0
May 25 03:40:28.507: INFO: Number of running nodes: 0, number of available pods: 0
May 25 03:40:28.511: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"848505"},"items":null}

May 25 03:40:28.515: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"848505"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:40:28.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5066" for this suite.

• [SLOW TEST:21.703 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":339,"completed":168,"skipped":2373,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:40:28.542: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:40:28.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3948" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":339,"completed":169,"skipped":2392,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:40:28.706: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7218.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7218.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 03:40:32.833: INFO: DNS probes using dns-7218/dns-test-47a2eb3e-c274-41d9-823d-ab0faf89184c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:40:32.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7218" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":339,"completed":170,"skipped":2409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:40:32.871: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
May 25 03:40:53.124: INFO: EndpointSlice for Service endpointslice-8841/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:41:03.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8841" for this suite.

• [SLOW TEST:30.295 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":339,"completed":171,"skipped":2440,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:41:03.167: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 03:41:03.261: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1772d694-4a5d-4206-936e-d08a1ca36298" in namespace "downward-api-2925" to be "Succeeded or Failed"
May 25 03:41:03.267: INFO: Pod "downwardapi-volume-1772d694-4a5d-4206-936e-d08a1ca36298": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025241ms
May 25 03:41:05.277: INFO: Pod "downwardapi-volume-1772d694-4a5d-4206-936e-d08a1ca36298": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016031307s
May 25 03:41:07.291: INFO: Pod "downwardapi-volume-1772d694-4a5d-4206-936e-d08a1ca36298": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029337064s
STEP: Saw pod success
May 25 03:41:07.291: INFO: Pod "downwardapi-volume-1772d694-4a5d-4206-936e-d08a1ca36298" satisfied condition "Succeeded or Failed"
May 25 03:41:07.295: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-1772d694-4a5d-4206-936e-d08a1ca36298 container client-container: <nil>
STEP: delete the pod
May 25 03:41:07.356: INFO: Waiting for pod downwardapi-volume-1772d694-4a5d-4206-936e-d08a1ca36298 to disappear
May 25 03:41:07.360: INFO: Pod downwardapi-volume-1772d694-4a5d-4206-936e-d08a1ca36298 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:41:07.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2925" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":172,"skipped":2460,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:41:07.372: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:41:07.423: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 25 03:41:07.437: INFO: The status of Pod pod-exec-websocket-0f162650-b522-4b72-9229-f66c926ae022 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:41:09.443: INFO: The status of Pod pod-exec-websocket-0f162650-b522-4b72-9229-f66c926ae022 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:41:11.449: INFO: The status of Pod pod-exec-websocket-0f162650-b522-4b72-9229-f66c926ae022 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:41:11.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4203" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":339,"completed":173,"skipped":2468,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:41:11.618: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-4186
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-4186
May 25 03:41:11.708: INFO: Found 0 stateful pods, waiting for 1
May 25 03:41:21.721: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
May 25 03:41:21.790: INFO: Deleting all statefulset in ns statefulset-4186
May 25 03:41:21.794: INFO: Scaling statefulset ss to 0
May 25 03:41:31.851: INFO: Waiting for statefulset status.replicas updated to 0
May 25 03:41:31.854: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:41:31.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4186" for this suite.

• [SLOW TEST:20.277 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":339,"completed":174,"skipped":2477,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:41:31.897: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:41:31.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4666" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":339,"completed":175,"skipped":2502,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:41:31.966: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-ca75a5d3-f2e5-4612-9eba-e937dcae8cc6
STEP: Creating a pod to test consume secrets
May 25 03:41:32.031: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9d72d23b-0b4a-481d-b9ce-4c36821627dc" in namespace "projected-4680" to be "Succeeded or Failed"
May 25 03:41:32.038: INFO: Pod "pod-projected-secrets-9d72d23b-0b4a-481d-b9ce-4c36821627dc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.390371ms
May 25 03:41:34.050: INFO: Pod "pod-projected-secrets-9d72d23b-0b4a-481d-b9ce-4c36821627dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018650646s
May 25 03:41:36.061: INFO: Pod "pod-projected-secrets-9d72d23b-0b4a-481d-b9ce-4c36821627dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029703509s
STEP: Saw pod success
May 25 03:41:36.061: INFO: Pod "pod-projected-secrets-9d72d23b-0b4a-481d-b9ce-4c36821627dc" satisfied condition "Succeeded or Failed"
May 25 03:41:36.065: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-projected-secrets-9d72d23b-0b4a-481d-b9ce-4c36821627dc container projected-secret-volume-test: <nil>
STEP: delete the pod
May 25 03:41:36.103: INFO: Waiting for pod pod-projected-secrets-9d72d23b-0b4a-481d-b9ce-4c36821627dc to disappear
May 25 03:41:36.107: INFO: Pod pod-projected-secrets-9d72d23b-0b4a-481d-b9ce-4c36821627dc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:41:36.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4680" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":176,"skipped":2507,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:41:36.121: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-1fc015a0-4e1d-4459-b31a-093fa581bda1
STEP: Creating a pod to test consume secrets
May 25 03:41:36.203: INFO: Waiting up to 5m0s for pod "pod-secrets-091b9f81-c2ad-4ceb-aed0-1a89c165663b" in namespace "secrets-9338" to be "Succeeded or Failed"
May 25 03:41:36.209: INFO: Pod "pod-secrets-091b9f81-c2ad-4ceb-aed0-1a89c165663b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.466045ms
May 25 03:41:38.217: INFO: Pod "pod-secrets-091b9f81-c2ad-4ceb-aed0-1a89c165663b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014074545s
May 25 03:41:40.230: INFO: Pod "pod-secrets-091b9f81-c2ad-4ceb-aed0-1a89c165663b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027644447s
STEP: Saw pod success
May 25 03:41:40.230: INFO: Pod "pod-secrets-091b9f81-c2ad-4ceb-aed0-1a89c165663b" satisfied condition "Succeeded or Failed"
May 25 03:41:40.234: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-secrets-091b9f81-c2ad-4ceb-aed0-1a89c165663b container secret-env-test: <nil>
STEP: delete the pod
May 25 03:41:40.274: INFO: Waiting for pod pod-secrets-091b9f81-c2ad-4ceb-aed0-1a89c165663b to disappear
May 25 03:41:40.278: INFO: Pod pod-secrets-091b9f81-c2ad-4ceb-aed0-1a89c165663b no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:41:40.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9338" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":339,"completed":177,"skipped":2558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:41:40.292: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0525 03:41:41.449418      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 25 03:42:43.479: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:42:43.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4943" for this suite.

• [SLOW TEST:63.208 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":339,"completed":178,"skipped":2612,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:42:43.501: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
May 25 03:42:43.571: INFO: Waiting up to 5m0s for pod "client-containers-28bc4de2-0d8b-4809-b7b7-266465cc4052" in namespace "containers-99" to be "Succeeded or Failed"
May 25 03:42:43.577: INFO: Pod "client-containers-28bc4de2-0d8b-4809-b7b7-266465cc4052": Phase="Pending", Reason="", readiness=false. Elapsed: 5.921622ms
May 25 03:42:45.583: INFO: Pod "client-containers-28bc4de2-0d8b-4809-b7b7-266465cc4052": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012106236s
STEP: Saw pod success
May 25 03:42:45.583: INFO: Pod "client-containers-28bc4de2-0d8b-4809-b7b7-266465cc4052" satisfied condition "Succeeded or Failed"
May 25 03:42:45.587: INFO: Trying to get logs from node 10.10.103.202-worker pod client-containers-28bc4de2-0d8b-4809-b7b7-266465cc4052 container agnhost-container: <nil>
STEP: delete the pod
May 25 03:42:45.612: INFO: Waiting for pod client-containers-28bc4de2-0d8b-4809-b7b7-266465cc4052 to disappear
May 25 03:42:45.615: INFO: Pod client-containers-28bc4de2-0d8b-4809-b7b7-266465cc4052 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:42:45.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-99" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":339,"completed":179,"skipped":2636,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:42:45.629: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2480
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 25 03:42:45.680: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 25 03:42:45.714: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:42:47.723: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 03:42:49.727: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 03:42:51.727: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 03:42:53.720: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 03:42:55.721: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 03:42:57.727: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 03:42:59.728: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 03:43:01.723: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 03:43:03.727: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 03:43:05.721: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 03:43:07.725: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 25 03:43:07.733: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 25 03:43:11.761: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 25 03:43:11.761: INFO: Breadth first check of 192.168.171.190 on host 10.10.103.201...
May 25 03:43:11.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.31.122:9080/dial?request=hostname&protocol=http&host=192.168.171.190&port=8080&tries=1'] Namespace:pod-network-test-2480 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 03:43:11.764: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 03:43:11.941: INFO: Waiting for responses: map[]
May 25 03:43:11.941: INFO: reached 192.168.171.190 after 0/1 tries
May 25 03:43:11.941: INFO: Breadth first check of 192.168.31.124 on host 10.10.103.202...
May 25 03:43:11.947: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.31.122:9080/dial?request=hostname&protocol=http&host=192.168.31.124&port=8080&tries=1'] Namespace:pod-network-test-2480 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 03:43:11.947: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 03:43:12.110: INFO: Waiting for responses: map[]
May 25 03:43:12.110: INFO: reached 192.168.31.124 after 0/1 tries
May 25 03:43:12.110: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:43:12.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2480" for this suite.

• [SLOW TEST:26.498 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":339,"completed":180,"skipped":2648,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:43:12.127: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
May 25 03:43:12.208: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May 25 03:43:14.217: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:43:15.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4750" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":339,"completed":181,"skipped":2661,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:43:15.259: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 25 03:43:15.344: INFO: The status of Pod pod-update-e3971ae0-30e7-4c1a-9bb7-4f8317dc3822 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:43:17.357: INFO: The status of Pod pod-update-e3971ae0-30e7-4c1a-9bb7-4f8317dc3822 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 25 03:43:17.896: INFO: Successfully updated pod "pod-update-e3971ae0-30e7-4c1a-9bb7-4f8317dc3822"
STEP: verifying the updated pod is in kubernetes
May 25 03:43:17.905: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:43:17.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2216" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":339,"completed":182,"skipped":2669,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:43:17.920: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:43:17.987: INFO: The status of Pod server-envvars-11911731-e3e5-48cf-87c3-a61925202c1d is Pending, waiting for it to be Running (with Ready = true)
May 25 03:43:19.997: INFO: The status of Pod server-envvars-11911731-e3e5-48cf-87c3-a61925202c1d is Pending, waiting for it to be Running (with Ready = true)
May 25 03:43:21.997: INFO: The status of Pod server-envvars-11911731-e3e5-48cf-87c3-a61925202c1d is Running (Ready = true)
May 25 03:43:22.026: INFO: Waiting up to 5m0s for pod "client-envvars-325cfbc3-1deb-40c6-913e-b35741d5b983" in namespace "pods-9963" to be "Succeeded or Failed"
May 25 03:43:22.035: INFO: Pod "client-envvars-325cfbc3-1deb-40c6-913e-b35741d5b983": Phase="Pending", Reason="", readiness=false. Elapsed: 8.459735ms
May 25 03:43:24.043: INFO: Pod "client-envvars-325cfbc3-1deb-40c6-913e-b35741d5b983": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016476892s
STEP: Saw pod success
May 25 03:43:24.043: INFO: Pod "client-envvars-325cfbc3-1deb-40c6-913e-b35741d5b983" satisfied condition "Succeeded or Failed"
May 25 03:43:24.047: INFO: Trying to get logs from node 10.10.103.202-worker pod client-envvars-325cfbc3-1deb-40c6-913e-b35741d5b983 container env3cont: <nil>
STEP: delete the pod
May 25 03:43:24.072: INFO: Waiting for pod client-envvars-325cfbc3-1deb-40c6-913e-b35741d5b983 to disappear
May 25 03:43:24.077: INFO: Pod client-envvars-325cfbc3-1deb-40c6-913e-b35741d5b983 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:43:24.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9963" for this suite.

• [SLOW TEST:6.168 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":339,"completed":183,"skipped":2676,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:43:24.088: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 03:43:24.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12d803f1-98d7-458f-93f6-970cccbb0878" in namespace "downward-api-5208" to be "Succeeded or Failed"
May 25 03:43:24.153: INFO: Pod "downwardapi-volume-12d803f1-98d7-458f-93f6-970cccbb0878": Phase="Pending", Reason="", readiness=false. Elapsed: 4.703902ms
May 25 03:43:26.159: INFO: Pod "downwardapi-volume-12d803f1-98d7-458f-93f6-970cccbb0878": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010631626s
STEP: Saw pod success
May 25 03:43:26.159: INFO: Pod "downwardapi-volume-12d803f1-98d7-458f-93f6-970cccbb0878" satisfied condition "Succeeded or Failed"
May 25 03:43:26.163: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-12d803f1-98d7-458f-93f6-970cccbb0878 container client-container: <nil>
STEP: delete the pod
May 25 03:43:26.192: INFO: Waiting for pod downwardapi-volume-12d803f1-98d7-458f-93f6-970cccbb0878 to disappear
May 25 03:43:26.197: INFO: Pod downwardapi-volume-12d803f1-98d7-458f-93f6-970cccbb0878 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:43:26.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5208" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":184,"skipped":2682,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:43:26.209: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-25739cc4-0f34-4fe7-8770-ca4f92abf00d
STEP: Creating a pod to test consume configMaps
May 25 03:43:26.271: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5ba9cd7-1b56-44e8-8371-c145e47b5f7f" in namespace "projected-2372" to be "Succeeded or Failed"
May 25 03:43:26.275: INFO: Pod "pod-projected-configmaps-a5ba9cd7-1b56-44e8-8371-c145e47b5f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.927215ms
May 25 03:43:28.281: INFO: Pod "pod-projected-configmaps-a5ba9cd7-1b56-44e8-8371-c145e47b5f7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010600745s
STEP: Saw pod success
May 25 03:43:28.281: INFO: Pod "pod-projected-configmaps-a5ba9cd7-1b56-44e8-8371-c145e47b5f7f" satisfied condition "Succeeded or Failed"
May 25 03:43:28.287: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-projected-configmaps-a5ba9cd7-1b56-44e8-8371-c145e47b5f7f container agnhost-container: <nil>
STEP: delete the pod
May 25 03:43:28.312: INFO: Waiting for pod pod-projected-configmaps-a5ba9cd7-1b56-44e8-8371-c145e47b5f7f to disappear
May 25 03:43:28.316: INFO: Pod pod-projected-configmaps-a5ba9cd7-1b56-44e8-8371-c145e47b5f7f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:43:28.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2372" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":185,"skipped":2693,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:43:28.331: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-08271e96-11de-4f0f-b08f-323487db0463
STEP: Creating a pod to test consume configMaps
May 25 03:43:28.445: INFO: Waiting up to 5m0s for pod "pod-configmaps-40474d96-fde4-449e-9ca9-911cc6f7a23e" in namespace "configmap-5432" to be "Succeeded or Failed"
May 25 03:43:28.450: INFO: Pod "pod-configmaps-40474d96-fde4-449e-9ca9-911cc6f7a23e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.225466ms
May 25 03:43:30.458: INFO: Pod "pod-configmaps-40474d96-fde4-449e-9ca9-911cc6f7a23e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012118026s
May 25 03:43:32.470: INFO: Pod "pod-configmaps-40474d96-fde4-449e-9ca9-911cc6f7a23e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024729887s
STEP: Saw pod success
May 25 03:43:32.470: INFO: Pod "pod-configmaps-40474d96-fde4-449e-9ca9-911cc6f7a23e" satisfied condition "Succeeded or Failed"
May 25 03:43:32.474: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-configmaps-40474d96-fde4-449e-9ca9-911cc6f7a23e container agnhost-container: <nil>
STEP: delete the pod
May 25 03:43:32.503: INFO: Waiting for pod pod-configmaps-40474d96-fde4-449e-9ca9-911cc6f7a23e to disappear
May 25 03:43:32.506: INFO: Pod pod-configmaps-40474d96-fde4-449e-9ca9-911cc6f7a23e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:43:32.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5432" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":186,"skipped":2729,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:43:32.519: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:43:32.566: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 25 03:43:33.605: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:43:34.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5518" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":339,"completed":187,"skipped":2793,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:43:34.627: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
May 25 03:43:34.690: INFO: Waiting up to 5m0s for pod "pod-8d406746-355f-4dc4-a217-2f67e38ac909" in namespace "emptydir-4702" to be "Succeeded or Failed"
May 25 03:43:34.694: INFO: Pod "pod-8d406746-355f-4dc4-a217-2f67e38ac909": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098829ms
May 25 03:43:36.700: INFO: Pod "pod-8d406746-355f-4dc4-a217-2f67e38ac909": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009992817s
May 25 03:43:38.705: INFO: Pod "pod-8d406746-355f-4dc4-a217-2f67e38ac909": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01547401s
STEP: Saw pod success
May 25 03:43:38.705: INFO: Pod "pod-8d406746-355f-4dc4-a217-2f67e38ac909" satisfied condition "Succeeded or Failed"
May 25 03:43:38.709: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-8d406746-355f-4dc4-a217-2f67e38ac909 container test-container: <nil>
STEP: delete the pod
May 25 03:43:38.734: INFO: Waiting for pod pod-8d406746-355f-4dc4-a217-2f67e38ac909 to disappear
May 25 03:43:38.739: INFO: Pod pod-8d406746-355f-4dc4-a217-2f67e38ac909 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:43:38.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4702" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":188,"skipped":2812,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:43:38.750: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
May 25 03:43:38.819: INFO: Waiting up to 5m0s for pod "var-expansion-941ea99d-1d35-4399-b777-f85a6490e66e" in namespace "var-expansion-4295" to be "Succeeded or Failed"
May 25 03:43:38.823: INFO: Pod "var-expansion-941ea99d-1d35-4399-b777-f85a6490e66e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.833706ms
May 25 03:43:40.829: INFO: Pod "var-expansion-941ea99d-1d35-4399-b777-f85a6490e66e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00985846s
May 25 03:43:42.836: INFO: Pod "var-expansion-941ea99d-1d35-4399-b777-f85a6490e66e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016814996s
STEP: Saw pod success
May 25 03:43:42.836: INFO: Pod "var-expansion-941ea99d-1d35-4399-b777-f85a6490e66e" satisfied condition "Succeeded or Failed"
May 25 03:43:42.840: INFO: Trying to get logs from node 10.10.103.202-worker pod var-expansion-941ea99d-1d35-4399-b777-f85a6490e66e container dapi-container: <nil>
STEP: delete the pod
May 25 03:43:42.869: INFO: Waiting for pod var-expansion-941ea99d-1d35-4399-b777-f85a6490e66e to disappear
May 25 03:43:42.873: INFO: Pod var-expansion-941ea99d-1d35-4399-b777-f85a6490e66e no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:43:42.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4295" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":339,"completed":189,"skipped":2816,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:43:42.885: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-cc9e59c3-bcaf-406e-8a60-cf59c726dee7 in namespace container-probe-8721
May 25 03:43:46.961: INFO: Started pod busybox-cc9e59c3-bcaf-406e-8a60-cf59c726dee7 in namespace container-probe-8721
STEP: checking the pod's current state and verifying that restartCount is present
May 25 03:43:46.965: INFO: Initial restart count of pod busybox-cc9e59c3-bcaf-406e-8a60-cf59c726dee7 is 0
May 25 03:44:35.248: INFO: Restart count of pod container-probe-8721/busybox-cc9e59c3-bcaf-406e-8a60-cf59c726dee7 is now 1 (48.283027073s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:44:35.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8721" for this suite.

• [SLOW TEST:52.391 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":190,"skipped":2824,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:44:35.277: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:44:35.363: INFO: Creating ReplicaSet my-hostname-basic-1422fd4d-89c6-45af-9f52-4c0fa02434fa
May 25 03:44:35.374: INFO: Pod name my-hostname-basic-1422fd4d-89c6-45af-9f52-4c0fa02434fa: Found 0 pods out of 1
May 25 03:44:40.385: INFO: Pod name my-hostname-basic-1422fd4d-89c6-45af-9f52-4c0fa02434fa: Found 1 pods out of 1
May 25 03:44:40.385: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1422fd4d-89c6-45af-9f52-4c0fa02434fa" is running
May 25 03:44:40.389: INFO: Pod "my-hostname-basic-1422fd4d-89c6-45af-9f52-4c0fa02434fa-4w6vl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 03:44:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 03:44:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 03:44:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 03:44:35 +0000 UTC Reason: Message:}])
May 25 03:44:40.390: INFO: Trying to dial the pod
May 25 03:44:45.416: INFO: Controller my-hostname-basic-1422fd4d-89c6-45af-9f52-4c0fa02434fa: Got expected result from replica 1 [my-hostname-basic-1422fd4d-89c6-45af-9f52-4c0fa02434fa-4w6vl]: "my-hostname-basic-1422fd4d-89c6-45af-9f52-4c0fa02434fa-4w6vl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:44:45.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3692" for this suite.

• [SLOW TEST:10.154 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":191,"skipped":2875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:44:45.433: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 03:44:46.270: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 03:44:48.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757511086, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757511086, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757511086, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757511086, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 03:44:51.315: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:44:51.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8318" for this suite.
STEP: Destroying namespace "webhook-8318-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.057 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":339,"completed":192,"skipped":2940,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:44:51.490: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May 25 03:44:51.570: INFO: Waiting up to 5m0s for pod "downward-api-9e415640-1c22-4a89-ae6c-15b75f06e475" in namespace "downward-api-5313" to be "Succeeded or Failed"
May 25 03:44:51.574: INFO: Pod "downward-api-9e415640-1c22-4a89-ae6c-15b75f06e475": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016931ms
May 25 03:44:53.581: INFO: Pod "downward-api-9e415640-1c22-4a89-ae6c-15b75f06e475": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011375361s
May 25 03:44:55.597: INFO: Pod "downward-api-9e415640-1c22-4a89-ae6c-15b75f06e475": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027122527s
STEP: Saw pod success
May 25 03:44:55.597: INFO: Pod "downward-api-9e415640-1c22-4a89-ae6c-15b75f06e475" satisfied condition "Succeeded or Failed"
May 25 03:44:55.601: INFO: Trying to get logs from node 10.10.103.202-worker pod downward-api-9e415640-1c22-4a89-ae6c-15b75f06e475 container dapi-container: <nil>
STEP: delete the pod
May 25 03:44:55.636: INFO: Waiting for pod downward-api-9e415640-1c22-4a89-ae6c-15b75f06e475 to disappear
May 25 03:44:55.639: INFO: Pod downward-api-9e415640-1c22-4a89-ae6c-15b75f06e475 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:44:55.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5313" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":339,"completed":193,"skipped":2941,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:44:55.653: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:44:55.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-883" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":339,"completed":194,"skipped":2949,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:44:55.768: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 25 03:44:55.831: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:44:55.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-191" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":339,"completed":195,"skipped":2985,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:44:55.868: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
May 25 03:44:55.929: INFO: Waiting up to 5m0s for pod "var-expansion-c59c7510-8ffe-45e1-8caa-c4d64196efe9" in namespace "var-expansion-8641" to be "Succeeded or Failed"
May 25 03:44:55.933: INFO: Pod "var-expansion-c59c7510-8ffe-45e1-8caa-c4d64196efe9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.960172ms
May 25 03:44:57.939: INFO: Pod "var-expansion-c59c7510-8ffe-45e1-8caa-c4d64196efe9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010533659s
STEP: Saw pod success
May 25 03:44:57.939: INFO: Pod "var-expansion-c59c7510-8ffe-45e1-8caa-c4d64196efe9" satisfied condition "Succeeded or Failed"
May 25 03:44:57.943: INFO: Trying to get logs from node 10.10.103.202-worker pod var-expansion-c59c7510-8ffe-45e1-8caa-c4d64196efe9 container dapi-container: <nil>
STEP: delete the pod
May 25 03:44:57.965: INFO: Waiting for pod var-expansion-c59c7510-8ffe-45e1-8caa-c4d64196efe9 to disappear
May 25 03:44:57.970: INFO: Pod var-expansion-c59c7510-8ffe-45e1-8caa-c4d64196efe9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:44:57.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8641" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":339,"completed":196,"skipped":2993,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:44:57.982: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
May 25 03:44:58.035: INFO: Major version: 1
STEP: Confirm minor version
May 25 03:44:58.035: INFO: cleanMinorVersion: 21
May 25 03:44:58.035: INFO: Minor version: 21
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:44:58.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1778" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":339,"completed":197,"skipped":3027,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:44:58.048: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
May 25 03:44:58.110: INFO: Pod name sample-pod: Found 0 pods out of 1
May 25 03:45:03.122: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:45:03.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3632" for this suite.

• [SLOW TEST:5.139 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":339,"completed":198,"skipped":3078,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:45:03.188: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
May 25 03:45:03.257: INFO: Waiting up to 5m0s for pod "pod-4b974b2c-7202-4f4b-8678-a14c2e73fcee" in namespace "emptydir-8357" to be "Succeeded or Failed"
May 25 03:45:03.261: INFO: Pod "pod-4b974b2c-7202-4f4b-8678-a14c2e73fcee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.357472ms
May 25 03:45:05.269: INFO: Pod "pod-4b974b2c-7202-4f4b-8678-a14c2e73fcee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012046358s
STEP: Saw pod success
May 25 03:45:05.269: INFO: Pod "pod-4b974b2c-7202-4f4b-8678-a14c2e73fcee" satisfied condition "Succeeded or Failed"
May 25 03:45:05.272: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-4b974b2c-7202-4f4b-8678-a14c2e73fcee container test-container: <nil>
STEP: delete the pod
May 25 03:45:05.296: INFO: Waiting for pod pod-4b974b2c-7202-4f4b-8678-a14c2e73fcee to disappear
May 25 03:45:05.300: INFO: Pod pod-4b974b2c-7202-4f4b-8678-a14c2e73fcee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:45:05.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8357" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":199,"skipped":3090,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:45:05.313: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-610b5fc7-3cfa-4f4e-932e-5d006cb9dda8
STEP: Creating a pod to test consume configMaps
May 25 03:45:05.377: INFO: Waiting up to 5m0s for pod "pod-configmaps-2de420b2-3e75-49e7-98b0-3107d347fc56" in namespace "configmap-8587" to be "Succeeded or Failed"
May 25 03:45:05.381: INFO: Pod "pod-configmaps-2de420b2-3e75-49e7-98b0-3107d347fc56": Phase="Pending", Reason="", readiness=false. Elapsed: 3.680673ms
May 25 03:45:07.389: INFO: Pod "pod-configmaps-2de420b2-3e75-49e7-98b0-3107d347fc56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01210338s
May 25 03:45:09.397: INFO: Pod "pod-configmaps-2de420b2-3e75-49e7-98b0-3107d347fc56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019220507s
STEP: Saw pod success
May 25 03:45:09.397: INFO: Pod "pod-configmaps-2de420b2-3e75-49e7-98b0-3107d347fc56" satisfied condition "Succeeded or Failed"
May 25 03:45:09.405: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-configmaps-2de420b2-3e75-49e7-98b0-3107d347fc56 container agnhost-container: <nil>
STEP: delete the pod
May 25 03:45:09.434: INFO: Waiting for pod pod-configmaps-2de420b2-3e75-49e7-98b0-3107d347fc56 to disappear
May 25 03:45:09.438: INFO: Pod pod-configmaps-2de420b2-3e75-49e7-98b0-3107d347fc56 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:45:09.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8587" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":200,"skipped":3144,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:45:09.451: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:45:20.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9513" for this suite.

• [SLOW TEST:11.144 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":339,"completed":201,"skipped":3156,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:45:20.595: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 25 03:45:20.675: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8514  1a15dadc-e98e-4d08-a2e2-a0c52e454eb5 850630 0 2021-05-25 03:45:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-25 03:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 03:45:20.675: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8514  1a15dadc-e98e-4d08-a2e2-a0c52e454eb5 850631 0 2021-05-25 03:45:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-25 03:45:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 03:45:20.676: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8514  1a15dadc-e98e-4d08-a2e2-a0c52e454eb5 850632 0 2021-05-25 03:45:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-25 03:45:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 25 03:45:30.715: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8514  1a15dadc-e98e-4d08-a2e2-a0c52e454eb5 850668 0 2021-05-25 03:45:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-25 03:45:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 03:45:30.715: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8514  1a15dadc-e98e-4d08-a2e2-a0c52e454eb5 850669 0 2021-05-25 03:45:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-25 03:45:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 25 03:45:30.715: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8514  1a15dadc-e98e-4d08-a2e2-a0c52e454eb5 850670 0 2021-05-25 03:45:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-25 03:45:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:45:30.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8514" for this suite.

• [SLOW TEST:10.133 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":339,"completed":202,"skipped":3157,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:45:30.729: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
May 25 03:45:30.805: INFO: The status of Pod pod-hostip-90dda28c-ce62-491f-831b-e59763e568b8 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:45:32.813: INFO: The status of Pod pod-hostip-90dda28c-ce62-491f-831b-e59763e568b8 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:45:34.813: INFO: The status of Pod pod-hostip-90dda28c-ce62-491f-831b-e59763e568b8 is Running (Ready = true)
May 25 03:45:34.821: INFO: Pod pod-hostip-90dda28c-ce62-491f-831b-e59763e568b8 has hostIP: 10.10.103.202
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:45:34.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-469" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":339,"completed":203,"skipped":3162,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:45:34.835: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:149
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 25 03:45:34.931: INFO: starting watch
STEP: patching
STEP: updating
May 25 03:45:34.943: INFO: waiting for watch events with expected annotations
May 25 03:45:34.943: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:45:34.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-4592" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":339,"completed":204,"skipped":3171,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:45:34.984: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:45:35.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9591" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":339,"completed":205,"skipped":3192,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:45:35.049: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-251.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-251.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-251.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 03:45:39.155: INFO: DNS probes using dns-test-fd0026d2-4aa6-4bab-a81d-9d17aaa6a5e5 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-251.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-251.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-251.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 03:45:43.234: INFO: File wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local from pod  dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 25 03:45:43.238: INFO: File jessie_udp@dns-test-service-3.dns-251.svc.cluster.local from pod  dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 25 03:45:43.239: INFO: Lookups using dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 failed for: [wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local jessie_udp@dns-test-service-3.dns-251.svc.cluster.local]

May 25 03:45:48.246: INFO: File wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local from pod  dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 25 03:45:48.250: INFO: File jessie_udp@dns-test-service-3.dns-251.svc.cluster.local from pod  dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 25 03:45:48.250: INFO: Lookups using dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 failed for: [wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local jessie_udp@dns-test-service-3.dns-251.svc.cluster.local]

May 25 03:45:53.246: INFO: File wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local from pod  dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 25 03:45:53.250: INFO: File jessie_udp@dns-test-service-3.dns-251.svc.cluster.local from pod  dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 25 03:45:53.250: INFO: Lookups using dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 failed for: [wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local jessie_udp@dns-test-service-3.dns-251.svc.cluster.local]

May 25 03:45:58.245: INFO: File wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local from pod  dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 25 03:45:58.249: INFO: File jessie_udp@dns-test-service-3.dns-251.svc.cluster.local from pod  dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 25 03:45:58.249: INFO: Lookups using dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 failed for: [wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local jessie_udp@dns-test-service-3.dns-251.svc.cluster.local]

May 25 03:46:03.245: INFO: File wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local from pod  dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 25 03:46:03.250: INFO: File jessie_udp@dns-test-service-3.dns-251.svc.cluster.local from pod  dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 25 03:46:03.250: INFO: Lookups using dns-251/dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 failed for: [wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local jessie_udp@dns-test-service-3.dns-251.svc.cluster.local]

May 25 03:46:08.248: INFO: DNS probes using dns-test-6ab8588a-7224-4c1e-b75b-ce4015a2ccd1 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-251.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-251.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-251.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-251.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 03:46:12.334: INFO: DNS probes using dns-test-3316793e-609d-4e79-b011-42d9b48afbae succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:46:12.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-251" for this suite.

• [SLOW TEST:37.356 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":339,"completed":206,"skipped":3200,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:46:12.406: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 03:46:12.465: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8cfd445a-6726-4e0a-89ba-514400ee141c" in namespace "projected-9914" to be "Succeeded or Failed"
May 25 03:46:12.469: INFO: Pod "downwardapi-volume-8cfd445a-6726-4e0a-89ba-514400ee141c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049868ms
May 25 03:46:14.480: INFO: Pod "downwardapi-volume-8cfd445a-6726-4e0a-89ba-514400ee141c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015316874s
May 25 03:46:16.486: INFO: Pod "downwardapi-volume-8cfd445a-6726-4e0a-89ba-514400ee141c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020858022s
STEP: Saw pod success
May 25 03:46:16.486: INFO: Pod "downwardapi-volume-8cfd445a-6726-4e0a-89ba-514400ee141c" satisfied condition "Succeeded or Failed"
May 25 03:46:16.490: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-8cfd445a-6726-4e0a-89ba-514400ee141c container client-container: <nil>
STEP: delete the pod
May 25 03:46:16.526: INFO: Waiting for pod downwardapi-volume-8cfd445a-6726-4e0a-89ba-514400ee141c to disappear
May 25 03:46:16.530: INFO: Pod downwardapi-volume-8cfd445a-6726-4e0a-89ba-514400ee141c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:46:16.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9914" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":207,"skipped":3241,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:46:16.544: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:46:24.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4717" for this suite.

• [SLOW TEST:8.095 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":339,"completed":208,"skipped":3264,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:46:24.639: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-da23e04f-61d7-4758-bc6f-8eecc8b7d35f
STEP: Creating a pod to test consume configMaps
May 25 03:46:24.717: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-20807f07-f7f0-4f48-b2f9-5c1d20bfddba" in namespace "projected-8215" to be "Succeeded or Failed"
May 25 03:46:24.723: INFO: Pod "pod-projected-configmaps-20807f07-f7f0-4f48-b2f9-5c1d20bfddba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.448487ms
May 25 03:46:26.731: INFO: Pod "pod-projected-configmaps-20807f07-f7f0-4f48-b2f9-5c1d20bfddba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014031978s
STEP: Saw pod success
May 25 03:46:26.731: INFO: Pod "pod-projected-configmaps-20807f07-f7f0-4f48-b2f9-5c1d20bfddba" satisfied condition "Succeeded or Failed"
May 25 03:46:26.735: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-projected-configmaps-20807f07-f7f0-4f48-b2f9-5c1d20bfddba container agnhost-container: <nil>
STEP: delete the pod
May 25 03:46:26.763: INFO: Waiting for pod pod-projected-configmaps-20807f07-f7f0-4f48-b2f9-5c1d20bfddba to disappear
May 25 03:46:26.767: INFO: Pod pod-projected-configmaps-20807f07-f7f0-4f48-b2f9-5c1d20bfddba no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:46:26.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8215" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":209,"skipped":3271,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:46:26.780: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 03:46:28.089: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 03:46:31.121: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:46:31.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6916" for this suite.
STEP: Destroying namespace "webhook-6916-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":339,"completed":210,"skipped":3273,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:46:31.205: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:46:31.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8265" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":211,"skipped":3286,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:46:31.340: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
May 25 03:46:35.928: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8020 pod-service-account-2c6da35e-4925-476a-aafb-c3723f7972c2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 25 03:46:36.418: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8020 pod-service-account-2c6da35e-4925-476a-aafb-c3723f7972c2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 25 03:46:36.814: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8020 pod-service-account-2c6da35e-4925-476a-aafb-c3723f7972c2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:46:37.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8020" for this suite.

• [SLOW TEST:5.875 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":339,"completed":212,"skipped":3293,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:46:37.215: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 25 03:46:37.264: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 25 03:46:37.274: INFO: Waiting for terminating namespaces to be deleted...
May 25 03:46:37.279: INFO: 
Logging pods the apiserver thinks is on node 10.10.103.201-worker before test
May 25 03:46:37.291: INFO: calico-kube-controllers-867fdc777c-xgnbl from calico-system started at 2021-05-25 03:19:31 +0000 UTC (1 container statuses recorded)
May 25 03:46:37.291: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 25 03:46:37.291: INFO: calico-node-gf4hs from calico-system started at 2021-05-21 05:16:19 +0000 UTC (1 container statuses recorded)
May 25 03:46:37.291: INFO: 	Container calico-node ready: true, restart count 0
May 25 03:46:37.291: INFO: calico-typha-584858976f-vj6gj from calico-system started at 2021-05-21 05:16:25 +0000 UTC (1 container statuses recorded)
May 25 03:46:37.291: INFO: 	Container calico-typha ready: true, restart count 0
May 25 03:46:37.291: INFO: coredns-768c7df75d-h7src from kube-system started at 2021-05-25 03:19:31 +0000 UTC (1 container statuses recorded)
May 25 03:46:37.291: INFO: 	Container coredns ready: true, restart count 0
May 25 03:46:37.291: INFO: coredns-768c7df75d-s7x9f from kube-system started at 2021-05-25 03:19:31 +0000 UTC (1 container statuses recorded)
May 25 03:46:37.291: INFO: 	Container coredns ready: true, restart count 0
May 25 03:46:37.291: INFO: kube-proxy-s6ql6 from kube-system started at 2021-05-21 05:14:25 +0000 UTC (1 container statuses recorded)
May 25 03:46:37.291: INFO: 	Container kube-proxy ready: true, restart count 1
May 25 03:46:37.291: INFO: sonobuoy from sonobuoy started at 2021-05-25 02:26:58 +0000 UTC (1 container statuses recorded)
May 25 03:46:37.291: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 25 03:46:37.291: INFO: sonobuoy-e2e-job-58d7b3bc039b4826 from sonobuoy started at 2021-05-25 02:27:00 +0000 UTC (2 container statuses recorded)
May 25 03:46:37.291: INFO: 	Container e2e ready: true, restart count 0
May 25 03:46:37.291: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 03:46:37.291: INFO: sonobuoy-systemd-logs-daemon-set-ebbefc146b074e77-p6vlh from sonobuoy started at 2021-05-25 02:27:00 +0000 UTC (2 container statuses recorded)
May 25 03:46:37.291: INFO: 	Container sonobuoy-worker ready: false, restart count 8
May 25 03:46:37.291: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 03:46:37.291: INFO: tigera-operator-5b47ff5dcc-25sz7 from tigera-operator started at 2021-05-21 05:15:37 +0000 UTC (1 container statuses recorded)
May 25 03:46:37.291: INFO: 	Container tigera-operator ready: true, restart count 2
May 25 03:46:37.291: INFO: 
Logging pods the apiserver thinks is on node 10.10.103.202-worker before test
May 25 03:46:37.305: INFO: calico-node-k6h5c from calico-system started at 2021-05-21 05:16:19 +0000 UTC (1 container statuses recorded)
May 25 03:46:37.305: INFO: 	Container calico-node ready: true, restart count 0
May 25 03:46:37.305: INFO: calico-typha-584858976f-x82v6 from calico-system started at 2021-05-21 05:16:18 +0000 UTC (1 container statuses recorded)
May 25 03:46:37.305: INFO: 	Container calico-typha ready: true, restart count 0
May 25 03:46:37.305: INFO: kube-proxy-fvq49 from kube-system started at 2021-05-21 05:14:38 +0000 UTC (1 container statuses recorded)
May 25 03:46:37.305: INFO: 	Container kube-proxy ready: true, restart count 0
May 25 03:46:37.306: INFO: sonobuoy-systemd-logs-daemon-set-ebbefc146b074e77-kj9h6 from sonobuoy started at 2021-05-25 02:27:00 +0000 UTC (2 container statuses recorded)
May 25 03:46:37.306: INFO: 	Container sonobuoy-worker ready: false, restart count 8
May 25 03:46:37.306: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 03:46:37.306: INFO: pod-service-account-2c6da35e-4925-476a-aafb-c3723f7972c2 from svcaccounts-8020 started at 2021-05-25 03:46:31 +0000 UTC (1 container statuses recorded)
May 25 03:46:37.306: INFO: 	Container test ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d95e1f6c-a875-484a-9b6b-e25a5b17aa93 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.10.103.202 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-d95e1f6c-a875-484a-9b6b-e25a5b17aa93 off the node 10.10.103.202-worker
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d95e1f6c-a875-484a-9b6b-e25a5b17aa93
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:51:45.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8192" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.260 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":339,"completed":213,"skipped":3316,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:51:45.475: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:51:45.524: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:51:51.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6046" for this suite.

• [SLOW TEST:6.384 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":339,"completed":214,"skipped":3328,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:51:51.859: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:51:51.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-513" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":339,"completed":215,"skipped":3329,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:51:51.905: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
May 25 03:51:51.937: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-7948 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:51:52.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7948" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":339,"completed":216,"skipped":3345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:51:52.031: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:51:52.060: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 25 03:51:52.066: INFO: Pod name sample-pod: Found 0 pods out of 1
May 25 03:51:57.071: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 25 03:51:57.071: INFO: Creating deployment "test-rolling-update-deployment"
May 25 03:51:57.077: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 25 03:51:57.081: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 25 03:51:59.100: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 25 03:51:59.108: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757511517, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757511517, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757511517, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757511517, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-585b757574\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 03:52:01.115: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 25 03:52:01.127: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3211  4b9220e0-59d4-40b9-b0ba-5eb994b30805 852353 1 2021-05-25 03:51:57 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-05-25 03:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-25 03:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f447e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-25 03:51:57 +0000 UTC,LastTransitionTime:2021-05-25 03:51:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-05-25 03:51:59 +0000 UTC,LastTransitionTime:2021-05-25 03:51:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 25 03:52:01.133: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-3211  aa472a51-cb07-4754-badf-7d4edaa48840 852342 1 2021-05-25 03:51:57 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 4b9220e0-59d4-40b9-b0ba-5eb994b30805 0xc003f44ca7 0xc003f44ca8}] []  [{kube-controller-manager Update apps/v1 2021-05-25 03:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9220e0-59d4-40b9-b0ba-5eb994b30805\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f44d38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 25 03:52:01.133: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 25 03:52:01.133: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3211  799bca41-5851-4046-8b4e-df5400ba15da 852351 2 2021-05-25 03:51:52 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 4b9220e0-59d4-40b9-b0ba-5eb994b30805 0xc003f44b97 0xc003f44b98}] []  [{e2e.test Update apps/v1 2021-05-25 03:51:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-25 03:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b9220e0-59d4-40b9-b0ba-5eb994b30805\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f44c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 25 03:52:01.138: INFO: Pod "test-rolling-update-deployment-585b757574-txhd9" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-txhd9 test-rolling-update-deployment-585b757574- deployment-3211  29d4d10a-c2ae-4348-aaaf-0ed80ac65851 852341 0 2021-05-25 03:51:57 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/podIP:192.168.31.102/32 cni.projectcalico.org/podIPs:192.168.31.102/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 aa472a51-cb07-4754-badf-7d4edaa48840 0xc00510f267 0xc00510f268}] []  [{kube-controller-manager Update v1 2021-05-25 03:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa472a51-cb07-4754-badf-7d4edaa48840\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 03:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 03:51:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.31.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4h7kk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4h7kk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:51:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:51:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:51:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:51:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:192.168.31.102,StartTime:2021-05-25 03:51:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 03:51:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker://sha256:a51db9feaeb94e79df8275b6c4921047efdd40b23de5153754aedde5fd42a900,ContainerID:docker://c099f21660c37340cd12f61235b4fccffed0ba8a637ebfe800bdf07444add869,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.31.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:52:01.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3211" for this suite.

• [SLOW TEST:9.121 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":217,"skipped":3367,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:52:01.153: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:52:01.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6822" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":339,"completed":218,"skipped":3379,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:52:01.255: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-1669/configmap-test-3e90ed99-8488-4fbd-8f0d-dd823ad2863a
STEP: Creating a pod to test consume configMaps
May 25 03:52:01.320: INFO: Waiting up to 5m0s for pod "pod-configmaps-836777dd-5215-48ec-8af8-f180885d5ccc" in namespace "configmap-1669" to be "Succeeded or Failed"
May 25 03:52:01.324: INFO: Pod "pod-configmaps-836777dd-5215-48ec-8af8-f180885d5ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008644ms
May 25 03:52:03.331: INFO: Pod "pod-configmaps-836777dd-5215-48ec-8af8-f180885d5ccc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011112012s
STEP: Saw pod success
May 25 03:52:03.331: INFO: Pod "pod-configmaps-836777dd-5215-48ec-8af8-f180885d5ccc" satisfied condition "Succeeded or Failed"
May 25 03:52:03.335: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-configmaps-836777dd-5215-48ec-8af8-f180885d5ccc container env-test: <nil>
STEP: delete the pod
May 25 03:52:03.387: INFO: Waiting for pod pod-configmaps-836777dd-5215-48ec-8af8-f180885d5ccc to disappear
May 25 03:52:03.391: INFO: Pod pod-configmaps-836777dd-5215-48ec-8af8-f180885d5ccc no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:52:03.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1669" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":219,"skipped":3390,"failed":0}
SSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:52:03.404: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:52:07.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7894" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":339,"completed":220,"skipped":3395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:52:07.523: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-d4ceb5bb-866a-4476-8eda-4443d1769c33
STEP: Creating a pod to test consume configMaps
May 25 03:52:07.586: INFO: Waiting up to 5m0s for pod "pod-configmaps-44ed3801-f82b-4621-b959-f2fb6a6bdf20" in namespace "configmap-3942" to be "Succeeded or Failed"
May 25 03:52:07.590: INFO: Pod "pod-configmaps-44ed3801-f82b-4621-b959-f2fb6a6bdf20": Phase="Pending", Reason="", readiness=false. Elapsed: 3.725313ms
May 25 03:52:09.598: INFO: Pod "pod-configmaps-44ed3801-f82b-4621-b959-f2fb6a6bdf20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01160452s
May 25 03:52:11.610: INFO: Pod "pod-configmaps-44ed3801-f82b-4621-b959-f2fb6a6bdf20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023290045s
STEP: Saw pod success
May 25 03:52:11.610: INFO: Pod "pod-configmaps-44ed3801-f82b-4621-b959-f2fb6a6bdf20" satisfied condition "Succeeded or Failed"
May 25 03:52:11.614: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-configmaps-44ed3801-f82b-4621-b959-f2fb6a6bdf20 container agnhost-container: <nil>
STEP: delete the pod
May 25 03:52:11.650: INFO: Waiting for pod pod-configmaps-44ed3801-f82b-4621-b959-f2fb6a6bdf20 to disappear
May 25 03:52:11.654: INFO: Pod pod-configmaps-44ed3801-f82b-4621-b959-f2fb6a6bdf20 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:52:11.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3942" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":221,"skipped":3420,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:52:11.666: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:52:11.763: INFO: Create a RollingUpdate DaemonSet
May 25 03:52:11.770: INFO: Check that daemon pods launch on every node of the cluster
May 25 03:52:11.775: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:52:11.780: INFO: Number of nodes with available pods: 0
May 25 03:52:11.780: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:52:12.865: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:52:12.872: INFO: Number of nodes with available pods: 0
May 25 03:52:12.872: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:52:13.790: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:52:13.796: INFO: Number of nodes with available pods: 1
May 25 03:52:13.796: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 03:52:14.791: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:52:14.795: INFO: Number of nodes with available pods: 2
May 25 03:52:14.795: INFO: Number of running nodes: 2, number of available pods: 2
May 25 03:52:14.795: INFO: Update the DaemonSet to trigger a rollout
May 25 03:52:14.807: INFO: Updating DaemonSet daemon-set
May 25 03:52:18.835: INFO: Roll back the DaemonSet before rollout is complete
May 25 03:52:18.848: INFO: Updating DaemonSet daemon-set
May 25 03:52:18.848: INFO: Make sure DaemonSet rollback is complete
May 25 03:52:18.857: INFO: Wrong image for pod: daemon-set-6vjgj. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
May 25 03:52:18.857: INFO: Pod daemon-set-6vjgj is not available
May 25 03:52:18.862: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:52:19.876: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:52:20.880: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:52:21.873: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:52:22.983: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:52:23.879: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:52:24.880: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 25 03:52:25.870: INFO: Pod daemon-set-bfdfh is not available
May 25 03:52:25.884: INFO: DaemonSet pods can't tolerate node 10.10.103.200-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3701, will wait for the garbage collector to delete the pods
May 25 03:52:25.966: INFO: Deleting DaemonSet.extensions daemon-set took: 11.527325ms
May 25 03:52:26.067: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.638862ms
May 25 03:52:38.477: INFO: Number of nodes with available pods: 0
May 25 03:52:38.477: INFO: Number of running nodes: 0, number of available pods: 0
May 25 03:52:38.482: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"852705"},"items":null}

May 25 03:52:38.486: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"852705"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:52:38.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3701" for this suite.

• [SLOW TEST:26.847 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":339,"completed":222,"skipped":3424,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:52:38.514: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:52:40.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8530" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":223,"skipped":3451,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:52:40.623: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-e16c4b92-502d-4d8b-bcfc-908cb7200a51
STEP: Creating a pod to test consume secrets
May 25 03:52:40.698: INFO: Waiting up to 5m0s for pod "pod-secrets-b5aeb527-70c6-4413-93c7-8333961f5e7a" in namespace "secrets-9217" to be "Succeeded or Failed"
May 25 03:52:40.703: INFO: Pod "pod-secrets-b5aeb527-70c6-4413-93c7-8333961f5e7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.552167ms
May 25 03:52:42.710: INFO: Pod "pod-secrets-b5aeb527-70c6-4413-93c7-8333961f5e7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01136136s
STEP: Saw pod success
May 25 03:52:42.710: INFO: Pod "pod-secrets-b5aeb527-70c6-4413-93c7-8333961f5e7a" satisfied condition "Succeeded or Failed"
May 25 03:52:42.714: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-secrets-b5aeb527-70c6-4413-93c7-8333961f5e7a container secret-volume-test: <nil>
STEP: delete the pod
May 25 03:52:42.742: INFO: Waiting for pod pod-secrets-b5aeb527-70c6-4413-93c7-8333961f5e7a to disappear
May 25 03:52:42.746: INFO: Pod pod-secrets-b5aeb527-70c6-4413-93c7-8333961f5e7a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:52:42.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9217" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":224,"skipped":3469,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:52:42.759: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:52:42.816: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 25 03:52:48.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-238 --namespace=crd-publish-openapi-238 create -f -'
May 25 03:52:49.055: INFO: stderr: ""
May 25 03:52:49.055: INFO: stdout: "e2e-test-crd-publish-openapi-8758-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 25 03:52:49.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-238 --namespace=crd-publish-openapi-238 delete e2e-test-crd-publish-openapi-8758-crds test-cr'
May 25 03:52:49.278: INFO: stderr: ""
May 25 03:52:49.278: INFO: stdout: "e2e-test-crd-publish-openapi-8758-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 25 03:52:49.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-238 --namespace=crd-publish-openapi-238 apply -f -'
May 25 03:52:49.778: INFO: stderr: ""
May 25 03:52:49.778: INFO: stdout: "e2e-test-crd-publish-openapi-8758-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 25 03:52:49.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-238 --namespace=crd-publish-openapi-238 delete e2e-test-crd-publish-openapi-8758-crds test-cr'
May 25 03:52:49.986: INFO: stderr: ""
May 25 03:52:49.986: INFO: stdout: "e2e-test-crd-publish-openapi-8758-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 25 03:52:49.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=crd-publish-openapi-238 explain e2e-test-crd-publish-openapi-8758-crds'
May 25 03:52:50.477: INFO: stderr: ""
May 25 03:52:50.477: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8758-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:52:56.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-238" for this suite.

• [SLOW TEST:13.977 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":339,"completed":225,"skipped":3482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:52:56.736: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-2fb4fa2e-6ea2-4183-8cff-897d5346cf4e
May 25 03:52:56.775: INFO: Pod name my-hostname-basic-2fb4fa2e-6ea2-4183-8cff-897d5346cf4e: Found 0 pods out of 1
May 25 03:53:01.793: INFO: Pod name my-hostname-basic-2fb4fa2e-6ea2-4183-8cff-897d5346cf4e: Found 1 pods out of 1
May 25 03:53:01.793: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-2fb4fa2e-6ea2-4183-8cff-897d5346cf4e" are running
May 25 03:53:01.797: INFO: Pod "my-hostname-basic-2fb4fa2e-6ea2-4183-8cff-897d5346cf4e-c99sr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 03:52:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 03:52:58 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 03:52:58 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-25 03:52:56 +0000 UTC Reason: Message:}])
May 25 03:53:01.798: INFO: Trying to dial the pod
May 25 03:53:06.816: INFO: Controller my-hostname-basic-2fb4fa2e-6ea2-4183-8cff-897d5346cf4e: Got expected result from replica 1 [my-hostname-basic-2fb4fa2e-6ea2-4183-8cff-897d5346cf4e-c99sr]: "my-hostname-basic-2fb4fa2e-6ea2-4183-8cff-897d5346cf4e-c99sr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:53:06.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2464" for this suite.

• [SLOW TEST:10.096 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":226,"skipped":3514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:53:06.833: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:53:06.900: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Creating first CR 
May 25 03:53:09.534: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-25T03:53:09Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-25T03:53:09Z]] name:name1 resourceVersion:852936 uid:cec1f2e7-4c64-49ac-b27b-53b540ce3677] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 25 03:53:19.555: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-25T03:53:19Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-25T03:53:19Z]] name:name2 resourceVersion:852977 uid:816aae14-43d1-47f1-b170-124b48d84a88] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 25 03:53:29.571: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-25T03:53:09Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-25T03:53:29Z]] name:name1 resourceVersion:853006 uid:cec1f2e7-4c64-49ac-b27b-53b540ce3677] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 25 03:53:39.591: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-25T03:53:19Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-25T03:53:39Z]] name:name2 resourceVersion:853031 uid:816aae14-43d1-47f1-b170-124b48d84a88] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 25 03:53:49.609: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-25T03:53:09Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-25T03:53:29Z]] name:name1 resourceVersion:853065 uid:cec1f2e7-4c64-49ac-b27b-53b540ce3677] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 25 03:53:59.623: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-25T03:53:19Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-25T03:53:39Z]] name:name2 resourceVersion:853093 uid:816aae14-43d1-47f1-b170-124b48d84a88] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:54:10.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5785" for this suite.

• [SLOW TEST:63.335 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":339,"completed":227,"skipped":3594,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:54:10.168: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:54:27.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5239" for this suite.

• [SLOW TEST:17.162 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":339,"completed":228,"skipped":3596,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:54:27.331: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:54:27.393: INFO: The status of Pod busybox-scheduling-23aa2bdd-4945-454c-8f13-cbb29302c271 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:54:29.398: INFO: The status of Pod busybox-scheduling-23aa2bdd-4945-454c-8f13-cbb29302c271 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:54:29.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5663" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":339,"completed":229,"skipped":3620,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:54:29.452: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0525 03:54:29.510469      23 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:56:01.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7260" for this suite.

• [SLOW TEST:92.119 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":339,"completed":230,"skipped":3629,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:56:01.571: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-5812
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5812
STEP: Deleting pre-stop pod
May 25 03:56:10.688: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:56:10.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5812" for this suite.

• [SLOW TEST:9.163 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":339,"completed":231,"skipped":3646,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:56:10.735: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:56:10.797: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5630
I0525 03:56:10.821075      23 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5630, replica count: 1
I0525 03:56:11.873063      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0525 03:56:12.874185      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 03:56:12.989: INFO: Created: latency-svc-kbs84
May 25 03:56:13.005: INFO: Got endpoints: latency-svc-kbs84 [30.876086ms]
May 25 03:56:13.024: INFO: Created: latency-svc-sl275
May 25 03:56:13.034: INFO: Got endpoints: latency-svc-sl275 [28.426767ms]
May 25 03:56:13.040: INFO: Created: latency-svc-dcr8k
May 25 03:56:13.048: INFO: Got endpoints: latency-svc-dcr8k [42.194013ms]
May 25 03:56:13.062: INFO: Created: latency-svc-7cts8
May 25 03:56:13.078: INFO: Created: latency-svc-hpscp
May 25 03:56:13.095: INFO: Got endpoints: latency-svc-7cts8 [89.099182ms]
May 25 03:56:13.113: INFO: Got endpoints: latency-svc-hpscp [106.481722ms]
May 25 03:56:13.120: INFO: Created: latency-svc-njztx
May 25 03:56:13.126: INFO: Got endpoints: latency-svc-njztx [119.331332ms]
May 25 03:56:13.133: INFO: Created: latency-svc-r8j8b
May 25 03:56:13.145: INFO: Got endpoints: latency-svc-r8j8b [31.684977ms]
May 25 03:56:13.146: INFO: Created: latency-svc-spvbj
May 25 03:56:13.155: INFO: Got endpoints: latency-svc-spvbj [147.910308ms]
May 25 03:56:13.169: INFO: Created: latency-svc-g4hwr
May 25 03:56:13.177: INFO: Got endpoints: latency-svc-g4hwr [171.292514ms]
May 25 03:56:13.183: INFO: Created: latency-svc-snvkn
May 25 03:56:13.192: INFO: Got endpoints: latency-svc-snvkn [184.93352ms]
May 25 03:56:13.201: INFO: Created: latency-svc-dsdw5
May 25 03:56:13.211: INFO: Got endpoints: latency-svc-dsdw5 [203.921766ms]
May 25 03:56:13.219: INFO: Created: latency-svc-7sf2b
May 25 03:56:13.229: INFO: Got endpoints: latency-svc-7sf2b [222.032566ms]
May 25 03:56:13.239: INFO: Created: latency-svc-fgw8k
May 25 03:56:13.249: INFO: Got endpoints: latency-svc-fgw8k [242.240881ms]
May 25 03:56:13.253: INFO: Created: latency-svc-7stzx
May 25 03:56:13.261: INFO: Got endpoints: latency-svc-7stzx [254.280636ms]
May 25 03:56:13.278: INFO: Created: latency-svc-6rdtb
May 25 03:56:13.286: INFO: Got endpoints: latency-svc-6rdtb [279.305809ms]
May 25 03:56:13.290: INFO: Created: latency-svc-685fs
May 25 03:56:13.299: INFO: Got endpoints: latency-svc-685fs [292.169146ms]
May 25 03:56:13.304: INFO: Created: latency-svc-hk4z9
May 25 03:56:13.314: INFO: Got endpoints: latency-svc-hk4z9 [307.43559ms]
May 25 03:56:13.321: INFO: Created: latency-svc-chnbb
May 25 03:56:13.333: INFO: Got endpoints: latency-svc-chnbb [299.283896ms]
May 25 03:56:13.340: INFO: Created: latency-svc-lbsjx
May 25 03:56:13.364: INFO: Got endpoints: latency-svc-lbsjx [315.275046ms]
May 25 03:56:13.371: INFO: Created: latency-svc-c7xfl
May 25 03:56:13.382: INFO: Got endpoints: latency-svc-c7xfl [286.927594ms]
May 25 03:56:13.387: INFO: Created: latency-svc-98snc
May 25 03:56:13.397: INFO: Got endpoints: latency-svc-98snc [271.3464ms]
May 25 03:56:13.402: INFO: Created: latency-svc-8swlx
May 25 03:56:13.414: INFO: Got endpoints: latency-svc-8swlx [269.233507ms]
May 25 03:56:13.420: INFO: Created: latency-svc-xkr67
May 25 03:56:13.427: INFO: Got endpoints: latency-svc-xkr67 [271.227599ms]
May 25 03:56:13.435: INFO: Created: latency-svc-2tlr9
May 25 03:56:13.446: INFO: Got endpoints: latency-svc-2tlr9 [269.015448ms]
May 25 03:56:13.451: INFO: Created: latency-svc-56lnr
May 25 03:56:13.464: INFO: Got endpoints: latency-svc-56lnr [272.150827ms]
May 25 03:56:13.468: INFO: Created: latency-svc-94p29
May 25 03:56:13.473: INFO: Got endpoints: latency-svc-94p29 [261.780544ms]
May 25 03:56:13.479: INFO: Created: latency-svc-bz6v5
May 25 03:56:13.493: INFO: Got endpoints: latency-svc-bz6v5 [263.926587ms]
May 25 03:56:13.496: INFO: Created: latency-svc-cnr82
May 25 03:56:13.509: INFO: Got endpoints: latency-svc-cnr82 [259.996206ms]
May 25 03:56:13.515: INFO: Created: latency-svc-xzvsg
May 25 03:56:13.524: INFO: Got endpoints: latency-svc-xzvsg [262.848166ms]
May 25 03:56:13.528: INFO: Created: latency-svc-j9sc6
May 25 03:56:13.534: INFO: Got endpoints: latency-svc-j9sc6 [247.92323ms]
May 25 03:56:13.542: INFO: Created: latency-svc-bknvh
May 25 03:56:13.548: INFO: Got endpoints: latency-svc-bknvh [248.405504ms]
May 25 03:56:13.556: INFO: Created: latency-svc-v6x99
May 25 03:56:13.562: INFO: Got endpoints: latency-svc-v6x99 [247.664781ms]
May 25 03:56:13.568: INFO: Created: latency-svc-wzdh6
May 25 03:56:13.573: INFO: Got endpoints: latency-svc-wzdh6 [240.334027ms]
May 25 03:56:13.580: INFO: Created: latency-svc-bxkrb
May 25 03:56:13.605: INFO: Got endpoints: latency-svc-bxkrb [241.15715ms]
May 25 03:56:13.607: INFO: Created: latency-svc-qm9f9
May 25 03:56:13.616: INFO: Got endpoints: latency-svc-qm9f9 [234.13729ms]
May 25 03:56:13.622: INFO: Created: latency-svc-zzpdh
May 25 03:56:13.628: INFO: Got endpoints: latency-svc-zzpdh [230.571858ms]
May 25 03:56:13.632: INFO: Created: latency-svc-bk5sl
May 25 03:56:13.637: INFO: Got endpoints: latency-svc-bk5sl [223.13611ms]
May 25 03:56:13.640: INFO: Created: latency-svc-p8r7s
May 25 03:56:13.647: INFO: Got endpoints: latency-svc-p8r7s [220.088465ms]
May 25 03:56:13.657: INFO: Created: latency-svc-x44qh
May 25 03:56:13.665: INFO: Got endpoints: latency-svc-x44qh [219.09832ms]
May 25 03:56:13.670: INFO: Created: latency-svc-ccw62
May 25 03:56:13.679: INFO: Got endpoints: latency-svc-ccw62 [215.075529ms]
May 25 03:56:13.685: INFO: Created: latency-svc-jlqvg
May 25 03:56:13.690: INFO: Got endpoints: latency-svc-jlqvg [217.291738ms]
May 25 03:56:13.695: INFO: Created: latency-svc-h64c2
May 25 03:56:13.707: INFO: Got endpoints: latency-svc-h64c2 [213.84324ms]
May 25 03:56:13.710: INFO: Created: latency-svc-qg77r
May 25 03:56:13.720: INFO: Got endpoints: latency-svc-qg77r [210.061724ms]
May 25 03:56:13.726: INFO: Created: latency-svc-2kbb4
May 25 03:56:13.731: INFO: Got endpoints: latency-svc-2kbb4 [206.650656ms]
May 25 03:56:13.734: INFO: Created: latency-svc-79bhp
May 25 03:56:13.747: INFO: Created: latency-svc-jlhbv
May 25 03:56:13.754: INFO: Created: latency-svc-8ptlk
May 25 03:56:13.754: INFO: Got endpoints: latency-svc-79bhp [219.692423ms]
May 25 03:56:13.764: INFO: Created: latency-svc-9479f
May 25 03:56:13.770: INFO: Created: latency-svc-jzdqp
May 25 03:56:13.782: INFO: Created: latency-svc-9ltz2
May 25 03:56:13.793: INFO: Created: latency-svc-bmvz8
May 25 03:56:13.808: INFO: Got endpoints: latency-svc-jlhbv [260.004529ms]
May 25 03:56:13.826: INFO: Created: latency-svc-2g82s
May 25 03:56:13.835: INFO: Created: latency-svc-gd7xc
May 25 03:56:13.847: INFO: Got endpoints: latency-svc-8ptlk [285.602807ms]
May 25 03:56:13.848: INFO: Created: latency-svc-nhbjm
May 25 03:56:13.873: INFO: Created: latency-svc-7tcbl
May 25 03:56:13.908: INFO: Got endpoints: latency-svc-9479f [335.007628ms]
May 25 03:56:13.958: INFO: Got endpoints: latency-svc-jzdqp [352.899362ms]
May 25 03:56:13.961: INFO: Created: latency-svc-95qdz
May 25 03:56:13.986: INFO: Created: latency-svc-msd2g
May 25 03:56:14.003: INFO: Got endpoints: latency-svc-9ltz2 [386.442728ms]
May 25 03:56:14.003: INFO: Created: latency-svc-c9sj6
May 25 03:56:14.012: INFO: Created: latency-svc-fxcxl
May 25 03:56:14.025: INFO: Created: latency-svc-76fss
May 25 03:56:14.044: INFO: Created: latency-svc-g7wjt
May 25 03:56:14.051: INFO: Got endpoints: latency-svc-bmvz8 [423.404054ms]
May 25 03:56:14.056: INFO: Created: latency-svc-jtvqq
May 25 03:56:14.061: INFO: Created: latency-svc-drbdz
May 25 03:56:14.069: INFO: Created: latency-svc-qrpq6
May 25 03:56:14.078: INFO: Created: latency-svc-jzhxw
May 25 03:56:14.086: INFO: Created: latency-svc-jxhlk
May 25 03:56:14.099: INFO: Got endpoints: latency-svc-2g82s [461.50204ms]
May 25 03:56:14.112: INFO: Created: latency-svc-xdvhz
May 25 03:56:14.147: INFO: Got endpoints: latency-svc-gd7xc [500.730638ms]
May 25 03:56:14.167: INFO: Created: latency-svc-r76gn
May 25 03:56:14.196: INFO: Got endpoints: latency-svc-nhbjm [531.259287ms]
May 25 03:56:14.211: INFO: Created: latency-svc-47xfz
May 25 03:56:14.252: INFO: Got endpoints: latency-svc-7tcbl [572.515373ms]
May 25 03:56:14.271: INFO: Created: latency-svc-6ppmk
May 25 03:56:14.297: INFO: Got endpoints: latency-svc-95qdz [606.814831ms]
May 25 03:56:14.311: INFO: Created: latency-svc-qrwkd
May 25 03:56:14.346: INFO: Got endpoints: latency-svc-msd2g [639.111087ms]
May 25 03:56:14.362: INFO: Created: latency-svc-phxl7
May 25 03:56:14.397: INFO: Got endpoints: latency-svc-c9sj6 [677.532416ms]
May 25 03:56:14.411: INFO: Created: latency-svc-4pkzj
May 25 03:56:14.447: INFO: Got endpoints: latency-svc-fxcxl [716.04648ms]
May 25 03:56:14.459: INFO: Created: latency-svc-2vjlt
May 25 03:56:14.498: INFO: Got endpoints: latency-svc-76fss [743.545216ms]
May 25 03:56:14.510: INFO: Created: latency-svc-l7nz6
May 25 03:56:14.548: INFO: Got endpoints: latency-svc-g7wjt [739.782381ms]
May 25 03:56:14.567: INFO: Created: latency-svc-tsk2v
May 25 03:56:14.598: INFO: Got endpoints: latency-svc-jtvqq [750.788084ms]
May 25 03:56:14.615: INFO: Created: latency-svc-dxcmh
May 25 03:56:14.648: INFO: Got endpoints: latency-svc-drbdz [739.13362ms]
May 25 03:56:14.663: INFO: Created: latency-svc-8522w
May 25 03:56:14.698: INFO: Got endpoints: latency-svc-qrpq6 [739.716976ms]
May 25 03:56:14.713: INFO: Created: latency-svc-nkqz5
May 25 03:56:14.747: INFO: Got endpoints: latency-svc-jzhxw [744.718645ms]
May 25 03:56:14.762: INFO: Created: latency-svc-8qmxf
May 25 03:56:14.797: INFO: Got endpoints: latency-svc-jxhlk [745.706036ms]
May 25 03:56:14.812: INFO: Created: latency-svc-vqbkt
May 25 03:56:14.847: INFO: Got endpoints: latency-svc-xdvhz [748.393126ms]
May 25 03:56:14.864: INFO: Created: latency-svc-2qcx6
May 25 03:56:14.897: INFO: Got endpoints: latency-svc-r76gn [749.419975ms]
May 25 03:56:14.914: INFO: Created: latency-svc-8s65l
May 25 03:56:14.948: INFO: Got endpoints: latency-svc-47xfz [751.901999ms]
May 25 03:56:14.966: INFO: Created: latency-svc-x5b9v
May 25 03:56:14.998: INFO: Got endpoints: latency-svc-6ppmk [746.029634ms]
May 25 03:56:15.026: INFO: Created: latency-svc-qqwn2
May 25 03:56:15.051: INFO: Got endpoints: latency-svc-qrwkd [753.667597ms]
May 25 03:56:15.068: INFO: Created: latency-svc-m4fzb
May 25 03:56:15.098: INFO: Got endpoints: latency-svc-phxl7 [751.566325ms]
May 25 03:56:15.115: INFO: Created: latency-svc-js9qp
May 25 03:56:15.148: INFO: Got endpoints: latency-svc-4pkzj [750.377793ms]
May 25 03:56:15.165: INFO: Created: latency-svc-qdpln
May 25 03:56:15.199: INFO: Got endpoints: latency-svc-2vjlt [751.689966ms]
May 25 03:56:15.215: INFO: Created: latency-svc-qbr5g
May 25 03:56:15.247: INFO: Got endpoints: latency-svc-l7nz6 [749.556618ms]
May 25 03:56:15.263: INFO: Created: latency-svc-btc2x
May 25 03:56:15.298: INFO: Got endpoints: latency-svc-tsk2v [750.030945ms]
May 25 03:56:15.314: INFO: Created: latency-svc-4npcj
May 25 03:56:15.348: INFO: Got endpoints: latency-svc-dxcmh [749.623525ms]
May 25 03:56:15.365: INFO: Created: latency-svc-7fzs7
May 25 03:56:15.398: INFO: Got endpoints: latency-svc-8522w [750.032349ms]
May 25 03:56:15.415: INFO: Created: latency-svc-bxdpp
May 25 03:56:15.448: INFO: Got endpoints: latency-svc-nkqz5 [750.745262ms]
May 25 03:56:15.464: INFO: Created: latency-svc-9lwlf
May 25 03:56:15.498: INFO: Got endpoints: latency-svc-8qmxf [750.60318ms]
May 25 03:56:15.516: INFO: Created: latency-svc-8ksxs
May 25 03:56:15.547: INFO: Got endpoints: latency-svc-vqbkt [749.885301ms]
May 25 03:56:15.564: INFO: Created: latency-svc-czjrz
May 25 03:56:15.598: INFO: Got endpoints: latency-svc-2qcx6 [750.835707ms]
May 25 03:56:15.614: INFO: Created: latency-svc-rqzbm
May 25 03:56:15.648: INFO: Got endpoints: latency-svc-8s65l [751.055271ms]
May 25 03:56:15.667: INFO: Created: latency-svc-lf4gb
May 25 03:56:15.697: INFO: Got endpoints: latency-svc-x5b9v [749.085831ms]
May 25 03:56:15.713: INFO: Created: latency-svc-qkx6w
May 25 03:56:15.750: INFO: Got endpoints: latency-svc-qqwn2 [752.340103ms]
May 25 03:56:15.787: INFO: Created: latency-svc-zf62l
May 25 03:56:15.800: INFO: Got endpoints: latency-svc-m4fzb [749.306146ms]
May 25 03:56:15.819: INFO: Created: latency-svc-5f8gh
May 25 03:56:15.848: INFO: Got endpoints: latency-svc-js9qp [750.156815ms]
May 25 03:56:15.887: INFO: Created: latency-svc-wz9dw
May 25 03:56:15.902: INFO: Got endpoints: latency-svc-qdpln [754.194539ms]
May 25 03:56:15.922: INFO: Created: latency-svc-zcfgl
May 25 03:56:15.954: INFO: Got endpoints: latency-svc-qbr5g [755.749275ms]
May 25 03:56:15.972: INFO: Created: latency-svc-t675z
May 25 03:56:16.001: INFO: Got endpoints: latency-svc-btc2x [753.228996ms]
May 25 03:56:16.020: INFO: Created: latency-svc-bs7t6
May 25 03:56:16.047: INFO: Got endpoints: latency-svc-4npcj [749.559426ms]
May 25 03:56:16.163: INFO: Created: latency-svc-h9fq4
May 25 03:56:16.189: INFO: Got endpoints: latency-svc-7fzs7 [840.801515ms]
May 25 03:56:16.189: INFO: Got endpoints: latency-svc-bxdpp [791.175544ms]
May 25 03:56:16.204: INFO: Got endpoints: latency-svc-9lwlf [755.807197ms]
May 25 03:56:16.232: INFO: Created: latency-svc-27td7
May 25 03:56:16.248: INFO: Created: latency-svc-t8dgh
May 25 03:56:16.254: INFO: Got endpoints: latency-svc-8ksxs [755.5128ms]
May 25 03:56:16.258: INFO: Created: latency-svc-sdcn9
May 25 03:56:16.277: INFO: Created: latency-svc-rq62p
May 25 03:56:16.299: INFO: Got endpoints: latency-svc-czjrz [751.766479ms]
May 25 03:56:16.317: INFO: Created: latency-svc-vpgvk
May 25 03:56:16.348: INFO: Got endpoints: latency-svc-rqzbm [749.499688ms]
May 25 03:56:16.368: INFO: Created: latency-svc-qkxx2
May 25 03:56:16.397: INFO: Got endpoints: latency-svc-lf4gb [748.861332ms]
May 25 03:56:16.413: INFO: Created: latency-svc-4j5k9
May 25 03:56:16.447: INFO: Got endpoints: latency-svc-qkx6w [749.804979ms]
May 25 03:56:16.463: INFO: Created: latency-svc-5gzlv
May 25 03:56:16.497: INFO: Got endpoints: latency-svc-zf62l [746.115645ms]
May 25 03:56:16.513: INFO: Created: latency-svc-m8crr
May 25 03:56:16.549: INFO: Got endpoints: latency-svc-5f8gh [748.495069ms]
May 25 03:56:16.564: INFO: Created: latency-svc-4kbkl
May 25 03:56:16.598: INFO: Got endpoints: latency-svc-wz9dw [749.942752ms]
May 25 03:56:16.614: INFO: Created: latency-svc-pjrvn
May 25 03:56:16.649: INFO: Got endpoints: latency-svc-zcfgl [746.66801ms]
May 25 03:56:16.679: INFO: Created: latency-svc-g4r9j
May 25 03:56:16.699: INFO: Got endpoints: latency-svc-t675z [744.204412ms]
May 25 03:56:16.715: INFO: Created: latency-svc-q56zj
May 25 03:56:16.752: INFO: Got endpoints: latency-svc-bs7t6 [751.601401ms]
May 25 03:56:16.767: INFO: Created: latency-svc-wdznt
May 25 03:56:16.798: INFO: Got endpoints: latency-svc-h9fq4 [750.397634ms]
May 25 03:56:16.813: INFO: Created: latency-svc-66bcm
May 25 03:56:16.848: INFO: Got endpoints: latency-svc-27td7 [658.631314ms]
May 25 03:56:16.863: INFO: Created: latency-svc-ncjv6
May 25 03:56:16.896: INFO: Got endpoints: latency-svc-t8dgh [706.698559ms]
May 25 03:56:16.910: INFO: Created: latency-svc-j85jh
May 25 03:56:16.948: INFO: Got endpoints: latency-svc-sdcn9 [743.43125ms]
May 25 03:56:16.965: INFO: Created: latency-svc-zqdbh
May 25 03:56:16.997: INFO: Got endpoints: latency-svc-rq62p [743.370912ms]
May 25 03:56:17.014: INFO: Created: latency-svc-z4ngk
May 25 03:56:17.048: INFO: Got endpoints: latency-svc-vpgvk [749.211748ms]
May 25 03:56:17.065: INFO: Created: latency-svc-cmkn8
May 25 03:56:17.098: INFO: Got endpoints: latency-svc-qkxx2 [749.80427ms]
May 25 03:56:17.117: INFO: Created: latency-svc-vgvjw
May 25 03:56:17.150: INFO: Got endpoints: latency-svc-4j5k9 [753.344402ms]
May 25 03:56:17.167: INFO: Created: latency-svc-kkjvd
May 25 03:56:17.198: INFO: Got endpoints: latency-svc-5gzlv [750.197022ms]
May 25 03:56:17.214: INFO: Created: latency-svc-lvqpp
May 25 03:56:17.249: INFO: Got endpoints: latency-svc-m8crr [752.005727ms]
May 25 03:56:17.266: INFO: Created: latency-svc-jgwtr
May 25 03:56:17.303: INFO: Got endpoints: latency-svc-4kbkl [754.530171ms]
May 25 03:56:17.322: INFO: Created: latency-svc-wq2zt
May 25 03:56:17.348: INFO: Got endpoints: latency-svc-pjrvn [749.555751ms]
May 25 03:56:17.364: INFO: Created: latency-svc-zzx24
May 25 03:56:17.399: INFO: Got endpoints: latency-svc-g4r9j [750.057572ms]
May 25 03:56:17.420: INFO: Created: latency-svc-qxlqc
May 25 03:56:17.448: INFO: Got endpoints: latency-svc-q56zj [749.0376ms]
May 25 03:56:17.467: INFO: Created: latency-svc-sqfvr
May 25 03:56:17.498: INFO: Got endpoints: latency-svc-wdznt [745.485514ms]
May 25 03:56:17.516: INFO: Created: latency-svc-4zc6t
May 25 03:56:17.550: INFO: Got endpoints: latency-svc-66bcm [752.27776ms]
May 25 03:56:17.570: INFO: Created: latency-svc-r744k
May 25 03:56:17.600: INFO: Got endpoints: latency-svc-ncjv6 [752.491713ms]
May 25 03:56:17.628: INFO: Created: latency-svc-8df9r
May 25 03:56:17.660: INFO: Got endpoints: latency-svc-j85jh [764.087206ms]
May 25 03:56:17.678: INFO: Created: latency-svc-lpdmq
May 25 03:56:17.699: INFO: Got endpoints: latency-svc-zqdbh [750.671794ms]
May 25 03:56:17.715: INFO: Created: latency-svc-m5b4x
May 25 03:56:17.747: INFO: Got endpoints: latency-svc-z4ngk [749.949823ms]
May 25 03:56:17.767: INFO: Created: latency-svc-lx6ww
May 25 03:56:17.798: INFO: Got endpoints: latency-svc-cmkn8 [749.7076ms]
May 25 03:56:17.817: INFO: Created: latency-svc-f9xcn
May 25 03:56:17.848: INFO: Got endpoints: latency-svc-vgvjw [750.127873ms]
May 25 03:56:17.865: INFO: Created: latency-svc-l8m2z
May 25 03:56:17.897: INFO: Got endpoints: latency-svc-kkjvd [746.820274ms]
May 25 03:56:17.917: INFO: Created: latency-svc-ctk25
May 25 03:56:17.948: INFO: Got endpoints: latency-svc-lvqpp [750.447735ms]
May 25 03:56:17.968: INFO: Created: latency-svc-nsj26
May 25 03:56:18.000: INFO: Got endpoints: latency-svc-jgwtr [751.522036ms]
May 25 03:56:18.022: INFO: Created: latency-svc-xqspm
May 25 03:56:18.050: INFO: Got endpoints: latency-svc-wq2zt [747.012576ms]
May 25 03:56:18.071: INFO: Created: latency-svc-4sgqb
May 25 03:56:18.103: INFO: Got endpoints: latency-svc-zzx24 [754.535778ms]
May 25 03:56:18.123: INFO: Created: latency-svc-9gqvr
May 25 03:56:18.149: INFO: Got endpoints: latency-svc-qxlqc [749.486421ms]
May 25 03:56:18.174: INFO: Created: latency-svc-xwhn7
May 25 03:56:18.203: INFO: Got endpoints: latency-svc-sqfvr [754.983923ms]
May 25 03:56:18.227: INFO: Created: latency-svc-lgs2x
May 25 03:56:18.250: INFO: Got endpoints: latency-svc-4zc6t [751.58865ms]
May 25 03:56:18.266: INFO: Created: latency-svc-9vs9h
May 25 03:56:18.298: INFO: Got endpoints: latency-svc-r744k [747.56277ms]
May 25 03:56:18.315: INFO: Created: latency-svc-s9fjg
May 25 03:56:18.348: INFO: Got endpoints: latency-svc-8df9r [747.745841ms]
May 25 03:56:18.370: INFO: Created: latency-svc-8fhtv
May 25 03:56:18.397: INFO: Got endpoints: latency-svc-lpdmq [737.474228ms]
May 25 03:56:18.445: INFO: Created: latency-svc-g4qxb
May 25 03:56:18.473: INFO: Got endpoints: latency-svc-m5b4x [774.252824ms]
May 25 03:56:18.515: INFO: Got endpoints: latency-svc-lx6ww [768.214043ms]
May 25 03:56:18.517: INFO: Created: latency-svc-nkswc
May 25 03:56:18.547: INFO: Created: latency-svc-hwzj4
May 25 03:56:18.554: INFO: Got endpoints: latency-svc-f9xcn [755.960176ms]
May 25 03:56:18.571: INFO: Created: latency-svc-pw5g2
May 25 03:56:18.597: INFO: Got endpoints: latency-svc-l8m2z [749.509132ms]
May 25 03:56:18.627: INFO: Created: latency-svc-vqrq8
May 25 03:56:18.650: INFO: Got endpoints: latency-svc-ctk25 [752.729133ms]
May 25 03:56:18.666: INFO: Created: latency-svc-krsdg
May 25 03:56:18.697: INFO: Got endpoints: latency-svc-nsj26 [748.884477ms]
May 25 03:56:18.712: INFO: Created: latency-svc-c467v
May 25 03:56:18.747: INFO: Got endpoints: latency-svc-xqspm [746.797987ms]
May 25 03:56:18.769: INFO: Created: latency-svc-bqzjs
May 25 03:56:18.797: INFO: Got endpoints: latency-svc-4sgqb [747.020708ms]
May 25 03:56:18.815: INFO: Created: latency-svc-wklsh
May 25 03:56:18.848: INFO: Got endpoints: latency-svc-9gqvr [745.105512ms]
May 25 03:56:18.864: INFO: Created: latency-svc-dzp99
May 25 03:56:18.898: INFO: Got endpoints: latency-svc-xwhn7 [749.563053ms]
May 25 03:56:18.916: INFO: Created: latency-svc-qmq4t
May 25 03:56:18.948: INFO: Got endpoints: latency-svc-lgs2x [744.958561ms]
May 25 03:56:18.966: INFO: Created: latency-svc-jpdzv
May 25 03:56:18.996: INFO: Got endpoints: latency-svc-9vs9h [746.766919ms]
May 25 03:56:19.017: INFO: Created: latency-svc-hl4xd
May 25 03:56:19.062: INFO: Got endpoints: latency-svc-s9fjg [763.797036ms]
May 25 03:56:19.081: INFO: Created: latency-svc-9qdwg
May 25 03:56:19.101: INFO: Got endpoints: latency-svc-8fhtv [752.443976ms]
May 25 03:56:19.120: INFO: Created: latency-svc-vtx8z
May 25 03:56:19.150: INFO: Got endpoints: latency-svc-g4qxb [752.074966ms]
May 25 03:56:19.169: INFO: Created: latency-svc-4j2th
May 25 03:56:19.199: INFO: Got endpoints: latency-svc-nkswc [726.098687ms]
May 25 03:56:19.215: INFO: Created: latency-svc-fjr4l
May 25 03:56:19.250: INFO: Got endpoints: latency-svc-hwzj4 [734.329249ms]
May 25 03:56:19.266: INFO: Created: latency-svc-47b79
May 25 03:56:19.299: INFO: Got endpoints: latency-svc-pw5g2 [744.958092ms]
May 25 03:56:19.316: INFO: Created: latency-svc-bbbx4
May 25 03:56:19.348: INFO: Got endpoints: latency-svc-vqrq8 [750.263674ms]
May 25 03:56:19.365: INFO: Created: latency-svc-bvvcl
May 25 03:56:19.398: INFO: Got endpoints: latency-svc-krsdg [747.790052ms]
May 25 03:56:19.415: INFO: Created: latency-svc-9s4cj
May 25 03:56:19.448: INFO: Got endpoints: latency-svc-c467v [750.51318ms]
May 25 03:56:19.463: INFO: Created: latency-svc-z25rv
May 25 03:56:19.498: INFO: Got endpoints: latency-svc-bqzjs [750.623836ms]
May 25 03:56:19.514: INFO: Created: latency-svc-f6rv6
May 25 03:56:19.547: INFO: Got endpoints: latency-svc-wklsh [749.352722ms]
May 25 03:56:19.564: INFO: Created: latency-svc-b4bx2
May 25 03:56:19.597: INFO: Got endpoints: latency-svc-dzp99 [749.04034ms]
May 25 03:56:19.613: INFO: Created: latency-svc-t5w4j
May 25 03:56:19.653: INFO: Got endpoints: latency-svc-qmq4t [755.0758ms]
May 25 03:56:19.669: INFO: Created: latency-svc-5mnws
May 25 03:56:19.698: INFO: Got endpoints: latency-svc-jpdzv [750.039685ms]
May 25 03:56:19.714: INFO: Created: latency-svc-45996
May 25 03:56:19.751: INFO: Got endpoints: latency-svc-hl4xd [754.101937ms]
May 25 03:56:19.766: INFO: Created: latency-svc-9vd5n
May 25 03:56:19.797: INFO: Got endpoints: latency-svc-9qdwg [735.640421ms]
May 25 03:56:19.812: INFO: Created: latency-svc-rzfcp
May 25 03:56:19.848: INFO: Got endpoints: latency-svc-vtx8z [747.541689ms]
May 25 03:56:19.863: INFO: Created: latency-svc-46nhg
May 25 03:56:19.898: INFO: Got endpoints: latency-svc-4j2th [748.442431ms]
May 25 03:56:19.913: INFO: Created: latency-svc-ld5b9
May 25 03:56:19.948: INFO: Got endpoints: latency-svc-fjr4l [749.371178ms]
May 25 03:56:19.964: INFO: Created: latency-svc-4ztmq
May 25 03:56:19.998: INFO: Got endpoints: latency-svc-47b79 [748.049869ms]
May 25 03:56:20.014: INFO: Created: latency-svc-7vlm7
May 25 03:56:20.055: INFO: Got endpoints: latency-svc-bbbx4 [755.577453ms]
May 25 03:56:20.071: INFO: Created: latency-svc-6xlj4
May 25 03:56:20.101: INFO: Got endpoints: latency-svc-bvvcl [752.971681ms]
May 25 03:56:20.117: INFO: Created: latency-svc-pt4hn
May 25 03:56:20.149: INFO: Got endpoints: latency-svc-9s4cj [751.244785ms]
May 25 03:56:20.166: INFO: Created: latency-svc-bmx9b
May 25 03:56:20.197: INFO: Got endpoints: latency-svc-z25rv [749.221181ms]
May 25 03:56:20.217: INFO: Created: latency-svc-6z2dm
May 25 03:56:20.251: INFO: Got endpoints: latency-svc-f6rv6 [753.412655ms]
May 25 03:56:20.269: INFO: Created: latency-svc-b9zx7
May 25 03:56:20.296: INFO: Got endpoints: latency-svc-b4bx2 [749.407556ms]
May 25 03:56:20.313: INFO: Created: latency-svc-77pgc
May 25 03:56:20.349: INFO: Got endpoints: latency-svc-t5w4j [751.866184ms]
May 25 03:56:20.769: INFO: Got endpoints: latency-svc-45996 [1.071053261s]
May 25 03:56:20.770: INFO: Created: latency-svc-rq898
May 25 03:56:20.770: INFO: Got endpoints: latency-svc-5mnws [1.116381295s]
May 25 03:56:20.778: INFO: Got endpoints: latency-svc-9vd5n [1.027135266s]
May 25 03:56:20.778: INFO: Got endpoints: latency-svc-46nhg [929.64548ms]
May 25 03:56:20.778: INFO: Got endpoints: latency-svc-rzfcp [980.879217ms]
May 25 03:56:20.787: INFO: Got endpoints: latency-svc-7vlm7 [788.657785ms]
May 25 03:56:20.790: INFO: Got endpoints: latency-svc-4ztmq [841.236594ms]
May 25 03:56:20.790: INFO: Got endpoints: latency-svc-ld5b9 [892.26721ms]
May 25 03:56:20.801: INFO: Got endpoints: latency-svc-6xlj4 [746.114337ms]
May 25 03:56:20.806: INFO: Created: latency-svc-sngwl
May 25 03:56:20.818: INFO: Created: latency-svc-gq8d5
May 25 03:56:20.831: INFO: Created: latency-svc-2964p
May 25 03:56:20.841: INFO: Created: latency-svc-rlvf8
May 25 03:56:20.848: INFO: Got endpoints: latency-svc-pt4hn [747.092964ms]
May 25 03:56:20.855: INFO: Created: latency-svc-ls4gj
May 25 03:56:20.871: INFO: Created: latency-svc-7scck
May 25 03:56:20.885: INFO: Created: latency-svc-gbbkb
May 25 03:56:20.897: INFO: Created: latency-svc-pqgkg
May 25 03:56:20.906: INFO: Got endpoints: latency-svc-bmx9b [756.313932ms]
May 25 03:56:20.912: INFO: Created: latency-svc-tfd2r
May 25 03:56:20.950: INFO: Got endpoints: latency-svc-6z2dm [752.415228ms]
May 25 03:56:20.997: INFO: Got endpoints: latency-svc-b9zx7 [746.082389ms]
May 25 03:56:21.053: INFO: Got endpoints: latency-svc-77pgc [756.88744ms]
May 25 03:56:21.099: INFO: Got endpoints: latency-svc-rq898 [750.160585ms]
May 25 03:56:21.148: INFO: Got endpoints: latency-svc-sngwl [378.547217ms]
May 25 03:56:21.198: INFO: Got endpoints: latency-svc-gq8d5 [429.087658ms]
May 25 03:56:21.250: INFO: Got endpoints: latency-svc-2964p [471.656556ms]
May 25 03:56:21.298: INFO: Got endpoints: latency-svc-rlvf8 [519.508733ms]
May 25 03:56:21.347: INFO: Got endpoints: latency-svc-ls4gj [569.582329ms]
May 25 03:56:21.398: INFO: Got endpoints: latency-svc-7scck [611.042034ms]
May 25 03:56:21.448: INFO: Got endpoints: latency-svc-gbbkb [657.700622ms]
May 25 03:56:21.507: INFO: Got endpoints: latency-svc-pqgkg [716.107849ms]
May 25 03:56:21.552: INFO: Got endpoints: latency-svc-tfd2r [750.671004ms]
May 25 03:56:21.552: INFO: Latencies: [28.426767ms 31.684977ms 42.194013ms 89.099182ms 106.481722ms 119.331332ms 147.910308ms 171.292514ms 184.93352ms 203.921766ms 206.650656ms 210.061724ms 213.84324ms 215.075529ms 217.291738ms 219.09832ms 219.692423ms 220.088465ms 222.032566ms 223.13611ms 230.571858ms 234.13729ms 240.334027ms 241.15715ms 242.240881ms 247.664781ms 247.92323ms 248.405504ms 254.280636ms 259.996206ms 260.004529ms 261.780544ms 262.848166ms 263.926587ms 269.015448ms 269.233507ms 271.227599ms 271.3464ms 272.150827ms 279.305809ms 285.602807ms 286.927594ms 292.169146ms 299.283896ms 307.43559ms 315.275046ms 335.007628ms 352.899362ms 378.547217ms 386.442728ms 423.404054ms 429.087658ms 461.50204ms 471.656556ms 500.730638ms 519.508733ms 531.259287ms 569.582329ms 572.515373ms 606.814831ms 611.042034ms 639.111087ms 657.700622ms 658.631314ms 677.532416ms 706.698559ms 716.04648ms 716.107849ms 726.098687ms 734.329249ms 735.640421ms 737.474228ms 739.13362ms 739.716976ms 739.782381ms 743.370912ms 743.43125ms 743.545216ms 744.204412ms 744.718645ms 744.958092ms 744.958561ms 745.105512ms 745.485514ms 745.706036ms 746.029634ms 746.082389ms 746.114337ms 746.115645ms 746.66801ms 746.766919ms 746.797987ms 746.820274ms 747.012576ms 747.020708ms 747.092964ms 747.541689ms 747.56277ms 747.745841ms 747.790052ms 748.049869ms 748.393126ms 748.442431ms 748.495069ms 748.861332ms 748.884477ms 749.0376ms 749.04034ms 749.085831ms 749.211748ms 749.221181ms 749.306146ms 749.352722ms 749.371178ms 749.407556ms 749.419975ms 749.486421ms 749.499688ms 749.509132ms 749.555751ms 749.556618ms 749.559426ms 749.563053ms 749.623525ms 749.7076ms 749.80427ms 749.804979ms 749.885301ms 749.942752ms 749.949823ms 750.030945ms 750.032349ms 750.039685ms 750.057572ms 750.127873ms 750.156815ms 750.160585ms 750.197022ms 750.263674ms 750.377793ms 750.397634ms 750.447735ms 750.51318ms 750.60318ms 750.623836ms 750.671004ms 750.671794ms 750.745262ms 750.788084ms 750.835707ms 751.055271ms 751.244785ms 751.522036ms 751.566325ms 751.58865ms 751.601401ms 751.689966ms 751.766479ms 751.866184ms 751.901999ms 752.005727ms 752.074966ms 752.27776ms 752.340103ms 752.415228ms 752.443976ms 752.491713ms 752.729133ms 752.971681ms 753.228996ms 753.344402ms 753.412655ms 753.667597ms 754.101937ms 754.194539ms 754.530171ms 754.535778ms 754.983923ms 755.0758ms 755.5128ms 755.577453ms 755.749275ms 755.807197ms 755.960176ms 756.313932ms 756.88744ms 763.797036ms 764.087206ms 768.214043ms 774.252824ms 788.657785ms 791.175544ms 840.801515ms 841.236594ms 892.26721ms 929.64548ms 980.879217ms 1.027135266s 1.071053261s 1.116381295s]
May 25 03:56:21.552: INFO: 50 %ile: 748.049869ms
May 25 03:56:21.552: INFO: 90 %ile: 755.577453ms
May 25 03:56:21.552: INFO: 99 %ile: 1.071053261s
May 25 03:56:21.552: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:56:21.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5630" for this suite.

• [SLOW TEST:10.834 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":339,"completed":232,"skipped":3659,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:56:21.571: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:56:45.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9218" for this suite.

• [SLOW TEST:24.439 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":339,"completed":233,"skipped":3672,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:56:46.010: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:56:57.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1687" for this suite.

• [SLOW TEST:11.154 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":339,"completed":234,"skipped":3675,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:56:57.165: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 03:56:57.227: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0756a1c6-62be-409a-b2ff-212e6854759f" in namespace "downward-api-4476" to be "Succeeded or Failed"
May 25 03:56:57.231: INFO: Pod "downwardapi-volume-0756a1c6-62be-409a-b2ff-212e6854759f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.921551ms
May 25 03:56:59.239: INFO: Pod "downwardapi-volume-0756a1c6-62be-409a-b2ff-212e6854759f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012414053s
STEP: Saw pod success
May 25 03:56:59.239: INFO: Pod "downwardapi-volume-0756a1c6-62be-409a-b2ff-212e6854759f" satisfied condition "Succeeded or Failed"
May 25 03:56:59.244: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-0756a1c6-62be-409a-b2ff-212e6854759f container client-container: <nil>
STEP: delete the pod
May 25 03:56:59.297: INFO: Waiting for pod downwardapi-volume-0756a1c6-62be-409a-b2ff-212e6854759f to disappear
May 25 03:56:59.305: INFO: Pod downwardapi-volume-0756a1c6-62be-409a-b2ff-212e6854759f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:56:59.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4476" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":235,"skipped":3688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:56:59.321: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May 25 03:56:59.386: INFO: Waiting up to 5m0s for pod "downward-api-fe9fbe54-34c2-47b5-b538-689559f122b6" in namespace "downward-api-9274" to be "Succeeded or Failed"
May 25 03:56:59.391: INFO: Pod "downward-api-fe9fbe54-34c2-47b5-b538-689559f122b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13385ms
May 25 03:57:01.400: INFO: Pod "downward-api-fe9fbe54-34c2-47b5-b538-689559f122b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013171623s
STEP: Saw pod success
May 25 03:57:01.400: INFO: Pod "downward-api-fe9fbe54-34c2-47b5-b538-689559f122b6" satisfied condition "Succeeded or Failed"
May 25 03:57:01.404: INFO: Trying to get logs from node 10.10.103.202-worker pod downward-api-fe9fbe54-34c2-47b5-b538-689559f122b6 container dapi-container: <nil>
STEP: delete the pod
May 25 03:57:01.433: INFO: Waiting for pod downward-api-fe9fbe54-34c2-47b5-b538-689559f122b6 to disappear
May 25 03:57:01.437: INFO: Pod downward-api-fe9fbe54-34c2-47b5-b538-689559f122b6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:57:01.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9274" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":339,"completed":236,"skipped":3722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:57:01.452: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 25 03:57:01.523: INFO: Waiting up to 5m0s for pod "pod-d785c482-507e-45d3-a92d-2f7000a478bd" in namespace "emptydir-7963" to be "Succeeded or Failed"
May 25 03:57:01.527: INFO: Pod "pod-d785c482-507e-45d3-a92d-2f7000a478bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040851ms
May 25 03:57:03.538: INFO: Pod "pod-d785c482-507e-45d3-a92d-2f7000a478bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015060819s
May 25 03:57:05.552: INFO: Pod "pod-d785c482-507e-45d3-a92d-2f7000a478bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028926546s
STEP: Saw pod success
May 25 03:57:05.552: INFO: Pod "pod-d785c482-507e-45d3-a92d-2f7000a478bd" satisfied condition "Succeeded or Failed"
May 25 03:57:05.556: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-d785c482-507e-45d3-a92d-2f7000a478bd container test-container: <nil>
STEP: delete the pod
May 25 03:57:05.586: INFO: Waiting for pod pod-d785c482-507e-45d3-a92d-2f7000a478bd to disappear
May 25 03:57:05.591: INFO: Pod pod-d785c482-507e-45d3-a92d-2f7000a478bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:57:05.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7963" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":237,"skipped":3766,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:57:05.608: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 03:57:06.547: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 03:57:08.574: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757511826, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757511826, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757511826, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757511826, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 03:57:11.604: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 25 03:57:11.652: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:57:11.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5568" for this suite.
STEP: Destroying namespace "webhook-5568-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.150 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":339,"completed":238,"skipped":3779,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:57:11.758: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:57:11.831: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:57:12.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3877" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":339,"completed":239,"skipped":3790,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:57:12.406: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:57:19.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3543" for this suite.

• [SLOW TEST:7.115 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":339,"completed":240,"skipped":3795,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:57:19.521: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 03:57:19.590: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf226ff3-e42f-4e65-b5f0-27b364bd2505" in namespace "projected-6256" to be "Succeeded or Failed"
May 25 03:57:19.597: INFO: Pod "downwardapi-volume-bf226ff3-e42f-4e65-b5f0-27b364bd2505": Phase="Pending", Reason="", readiness=false. Elapsed: 6.91017ms
May 25 03:57:21.608: INFO: Pod "downwardapi-volume-bf226ff3-e42f-4e65-b5f0-27b364bd2505": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018423867s
May 25 03:57:23.620: INFO: Pod "downwardapi-volume-bf226ff3-e42f-4e65-b5f0-27b364bd2505": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030146124s
STEP: Saw pod success
May 25 03:57:23.620: INFO: Pod "downwardapi-volume-bf226ff3-e42f-4e65-b5f0-27b364bd2505" satisfied condition "Succeeded or Failed"
May 25 03:57:23.624: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-bf226ff3-e42f-4e65-b5f0-27b364bd2505 container client-container: <nil>
STEP: delete the pod
May 25 03:57:23.661: INFO: Waiting for pod downwardapi-volume-bf226ff3-e42f-4e65-b5f0-27b364bd2505 to disappear
May 25 03:57:23.665: INFO: Pod downwardapi-volume-bf226ff3-e42f-4e65-b5f0-27b364bd2505 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:57:23.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6256" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":241,"skipped":3806,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:57:23.678: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 25 03:57:23.744: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 03:57:29.547: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:57:53.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9776" for this suite.

• [SLOW TEST:29.718 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":339,"completed":242,"skipped":3808,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:57:53.396: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:57:53.446: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 25 03:57:58.458: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 25 03:57:58.458: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 25 03:57:58.489: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4316  96b9081e-a58b-4077-bcfd-fb06a289b27b 856083 1 2021-05-25 03:57:58 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-05-25 03:57:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0077cc958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 25 03:57:58.494: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
May 25 03:57:58.494: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 25 03:57:58.495: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4316  5cb4e920-165a-4fc6-bbcf-a497bc5d7540 856087 1 2021-05-25 03:57:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 96b9081e-a58b-4077-bcfd-fb06a289b27b 0xc0077cccc7 0xc0077cccc8}] []  [{e2e.test Update apps/v1 2021-05-25 03:57:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-25 03:57:58 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"96b9081e-a58b-4077-bcfd-fb06a289b27b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0077ccd68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 25 03:57:58.500: INFO: Pod "test-cleanup-controller-jxbw5" is available:
&Pod{ObjectMeta:{test-cleanup-controller-jxbw5 test-cleanup-controller- deployment-4316  2ef2ac5f-05f3-417f-a9e0-900f1a5b39bd 856073 0 2021-05-25 03:57:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:192.168.31.118/32 cni.projectcalico.org/podIPs:192.168.31.118/32] [{apps/v1 ReplicaSet test-cleanup-controller 5cb4e920-165a-4fc6-bbcf-a497bc5d7540 0xc007764c67 0xc007764c68}] []  [{kube-controller-manager Update v1 2021-05-25 03:57:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cb4e920-165a-4fc6-bbcf-a497bc5d7540\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 03:57:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 03:57:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.31.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bv9t7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bv9t7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:57:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:57:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:57:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 03:57:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:192.168.31.118,StartTime:2021-05-25 03:57:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 03:57:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://1b5dfab1190639a6a570674e678707d21f6e4a911bec0229d964eb5b0fa0b528,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.31.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:57:58.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4316" for this suite.

• [SLOW TEST:5.119 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":339,"completed":243,"skipped":3809,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:57:58.515: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 03:57:58.601: INFO: Got root ca configmap in namespace "svcaccounts-4792"
May 25 03:57:58.608: INFO: Deleted root ca configmap in namespace "svcaccounts-4792"
STEP: waiting for a new root ca configmap created
May 25 03:57:59.117: INFO: Recreated root ca configmap in namespace "svcaccounts-4792"
May 25 03:57:59.126: INFO: Updated root ca configmap in namespace "svcaccounts-4792"
STEP: waiting for the root ca configmap reconciled
May 25 03:57:59.635: INFO: Reconciled root ca configmap in namespace "svcaccounts-4792"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:57:59.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4792" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":339,"completed":244,"skipped":3822,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:57:59.650: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9877
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9877
I0525 03:57:59.738481      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9877, replica count: 2
May 25 03:58:02.790: INFO: Creating new exec pod
I0525 03:58:02.790829      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 03:58:07.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:08.197: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:08.197: INFO: stdout: ""
May 25 03:58:09.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:09.578: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:09.578: INFO: stdout: ""
May 25 03:58:10.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:10.575: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:10.575: INFO: stdout: ""
May 25 03:58:11.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:11.570: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:11.570: INFO: stdout: ""
May 25 03:58:12.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:12.583: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:12.583: INFO: stdout: ""
May 25 03:58:13.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:13.573: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:13.573: INFO: stdout: ""
May 25 03:58:14.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:14.591: INFO: stderr: "+ echo+  hostName\nnc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:14.591: INFO: stdout: ""
May 25 03:58:15.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:15.572: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:15.572: INFO: stdout: ""
May 25 03:58:16.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:16.585: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:16.586: INFO: stdout: ""
May 25 03:58:17.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:17.571: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:17.571: INFO: stdout: ""
May 25 03:58:18.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:18.565: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:18.566: INFO: stdout: ""
May 25 03:58:19.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:19.607: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:19.607: INFO: stdout: ""
May 25 03:58:20.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:20.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:20.566: INFO: stdout: ""
May 25 03:58:21.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:21.599: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:21.599: INFO: stdout: ""
May 25 03:58:22.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:22.575: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:22.575: INFO: stdout: ""
May 25 03:58:23.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:23.564: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:23.564: INFO: stdout: ""
May 25 03:58:24.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:24.584: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:24.584: INFO: stdout: ""
May 25 03:58:25.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:25.563: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:25.563: INFO: stdout: ""
May 25 03:58:26.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:26.561: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:26.561: INFO: stdout: ""
May 25 03:58:27.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:27.704: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:27.704: INFO: stdout: ""
May 25 03:58:28.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:28.574: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:28.574: INFO: stdout: ""
May 25 03:58:29.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:29.567: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:29.567: INFO: stdout: ""
May 25 03:58:30.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:30.569: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:30.569: INFO: stdout: ""
May 25 03:58:31.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:31.570: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:31.570: INFO: stdout: ""
May 25 03:58:32.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:32.590: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:32.590: INFO: stdout: ""
May 25 03:58:33.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 03:58:33.564: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 03:58:33.564: INFO: stdout: "externalname-service-64s8k"
May 25 03:58:33.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.40.171 80'
May 25 03:58:33.949: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.40.171 80\nConnection to 10.104.40.171 80 port [tcp/http] succeeded!\n"
May 25 03:58:33.949: INFO: stdout: ""
May 25 03:58:34.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.40.171 80'
May 25 03:58:35.323: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.40.171 80\nConnection to 10.104.40.171 80 port [tcp/http] succeeded!\n"
May 25 03:58:35.323: INFO: stdout: ""
May 25 03:58:35.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.40.171 80'
May 25 03:58:36.338: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.40.171 80\nConnection to 10.104.40.171 80 port [tcp/http] succeeded!\n"
May 25 03:58:36.338: INFO: stdout: ""
May 25 03:58:36.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.40.171 80'
May 25 03:58:37.316: INFO: stderr: "+ echo+  hostName\nnc -v -t -w 2 10.104.40.171 80\nConnection to 10.104.40.171 80 port [tcp/http] succeeded!\n"
May 25 03:58:37.316: INFO: stdout: ""
May 25 03:58:37.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.40.171 80'
May 25 03:58:38.308: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.40.171 80\nConnection to 10.104.40.171 80 port [tcp/http] succeeded!\n"
May 25 03:58:38.308: INFO: stdout: ""
May 25 03:58:38.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-9877 exec execpodzp2qf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.40.171 80'
May 25 03:58:39.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.40.171 80\nConnection to 10.104.40.171 80 port [tcp/http] succeeded!\n"
May 25 03:58:39.329: INFO: stdout: "externalname-service-64s8k"
May 25 03:58:39.329: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:58:39.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9877" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:39.743 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":339,"completed":245,"skipped":3822,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:58:39.393: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-59f8dde6-d4dc-4bb3-be82-b88e3b859f43
STEP: Creating a pod to test consume configMaps
May 25 03:58:39.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-639b5fed-5050-49fe-ad97-52da911d3607" in namespace "configmap-6383" to be "Succeeded or Failed"
May 25 03:58:39.473: INFO: Pod "pod-configmaps-639b5fed-5050-49fe-ad97-52da911d3607": Phase="Pending", Reason="", readiness=false. Elapsed: 5.257315ms
May 25 03:58:41.487: INFO: Pod "pod-configmaps-639b5fed-5050-49fe-ad97-52da911d3607": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018715125s
STEP: Saw pod success
May 25 03:58:41.487: INFO: Pod "pod-configmaps-639b5fed-5050-49fe-ad97-52da911d3607" satisfied condition "Succeeded or Failed"
May 25 03:58:41.499: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-configmaps-639b5fed-5050-49fe-ad97-52da911d3607 container agnhost-container: <nil>
STEP: delete the pod
May 25 03:58:41.530: INFO: Waiting for pod pod-configmaps-639b5fed-5050-49fe-ad97-52da911d3607 to disappear
May 25 03:58:41.535: INFO: Pod pod-configmaps-639b5fed-5050-49fe-ad97-52da911d3607 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 03:58:41.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6383" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":246,"skipped":3828,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 03:58:41.547: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-0eac4498-893e-41d6-b0a5-f9ea50ab4ad9
STEP: Creating the pod
May 25 03:58:41.621: INFO: The status of Pod pod-projected-configmaps-7f30934a-dc94-4685-be52-bc5aeecac970 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:58:43.629: INFO: The status of Pod pod-projected-configmaps-7f30934a-dc94-4685-be52-bc5aeecac970 is Pending, waiting for it to be Running (with Ready = true)
May 25 03:58:45.626: INFO: The status of Pod pod-projected-configmaps-7f30934a-dc94-4685-be52-bc5aeecac970 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-0eac4498-893e-41d6-b0a5-f9ea50ab4ad9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:00:06.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8060" for this suite.

• [SLOW TEST:84.766 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":247,"skipped":3845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:00:06.314: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-4568
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4568
STEP: Creating statefulset with conflicting port in namespace statefulset-4568
STEP: Waiting until pod test-pod will start running in namespace statefulset-4568
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4568
May 25 04:00:10.514: INFO: Observed stateful pod in namespace: statefulset-4568, name: ss-0, uid: 55fe8f61-a96a-4802-99d8-448f6a868d40, status phase: Pending. Waiting for statefulset controller to delete.
May 25 04:00:10.863: INFO: Observed stateful pod in namespace: statefulset-4568, name: ss-0, uid: 55fe8f61-a96a-4802-99d8-448f6a868d40, status phase: Failed. Waiting for statefulset controller to delete.
May 25 04:00:10.877: INFO: Observed stateful pod in namespace: statefulset-4568, name: ss-0, uid: 55fe8f61-a96a-4802-99d8-448f6a868d40, status phase: Failed. Waiting for statefulset controller to delete.
May 25 04:00:10.884: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4568
STEP: Removing pod with conflicting port in namespace statefulset-4568
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4568 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
May 25 04:00:14.927: INFO: Deleting all statefulset in ns statefulset-4568
May 25 04:00:14.931: INFO: Scaling statefulset ss to 0
May 25 04:00:24.968: INFO: Waiting for statefulset status.replicas updated to 0
May 25 04:00:24.971: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:00:25.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4568" for this suite.

• [SLOW TEST:18.710 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":339,"completed":248,"skipped":3881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:00:25.025: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:00:27.105: INFO: Deleting pod "var-expansion-5eeba2e1-cb11-4e92-bbc6-0027a43ec9d9" in namespace "var-expansion-7841"
May 25 04:00:27.114: INFO: Wait up to 5m0s for pod "var-expansion-5eeba2e1-cb11-4e92-bbc6-0027a43ec9d9" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:00:39.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7841" for this suite.

• [SLOW TEST:14.168 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":339,"completed":249,"skipped":3928,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:00:39.193: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:00:39.282: INFO: The status of Pod busybox-host-aliasesca6d5f72-2377-4faf-9fd0-ca419a139d2b is Pending, waiting for it to be Running (with Ready = true)
May 25 04:00:41.291: INFO: The status of Pod busybox-host-aliasesca6d5f72-2377-4faf-9fd0-ca419a139d2b is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:00:41.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2105" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":250,"skipped":3936,"failed":0}
SS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:00:41.321: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
May 25 04:00:41.400: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
May 25 04:00:43.453: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
May 25 04:00:45.483: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:00:47.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-8804" for this suite.

• [SLOW TEST:6.189 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":339,"completed":251,"skipped":3938,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:00:47.510: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 25 04:00:52.101: INFO: Successfully updated pod "adopt-release-nq4tb"
STEP: Checking that the Job readopts the Pod
May 25 04:00:52.101: INFO: Waiting up to 15m0s for pod "adopt-release-nq4tb" in namespace "job-923" to be "adopted"
May 25 04:00:52.109: INFO: Pod "adopt-release-nq4tb": Phase="Running", Reason="", readiness=true. Elapsed: 7.447093ms
May 25 04:00:54.118: INFO: Pod "adopt-release-nq4tb": Phase="Running", Reason="", readiness=true. Elapsed: 2.016964938s
May 25 04:00:54.119: INFO: Pod "adopt-release-nq4tb" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 25 04:00:54.644: INFO: Successfully updated pod "adopt-release-nq4tb"
STEP: Checking that the Job releases the Pod
May 25 04:00:54.644: INFO: Waiting up to 15m0s for pod "adopt-release-nq4tb" in namespace "job-923" to be "released"
May 25 04:00:54.655: INFO: Pod "adopt-release-nq4tb": Phase="Running", Reason="", readiness=true. Elapsed: 11.546319ms
May 25 04:00:56.666: INFO: Pod "adopt-release-nq4tb": Phase="Running", Reason="", readiness=true. Elapsed: 2.022325529s
May 25 04:00:56.666: INFO: Pod "adopt-release-nq4tb" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:00:56.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-923" for this suite.

• [SLOW TEST:9.169 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":339,"completed":252,"skipped":3947,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:00:56.680: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-3611d4c8-890a-4bf6-92f9-ca9dc092c94f
STEP: Creating a pod to test consume secrets
May 25 04:00:56.755: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-812cbd18-3937-41cc-b499-e795bec97ffc" in namespace "projected-114" to be "Succeeded or Failed"
May 25 04:00:56.761: INFO: Pod "pod-projected-secrets-812cbd18-3937-41cc-b499-e795bec97ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.971265ms
May 25 04:00:58.771: INFO: Pod "pod-projected-secrets-812cbd18-3937-41cc-b499-e795bec97ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015821172s
May 25 04:01:00.783: INFO: Pod "pod-projected-secrets-812cbd18-3937-41cc-b499-e795bec97ffc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027992026s
STEP: Saw pod success
May 25 04:01:00.783: INFO: Pod "pod-projected-secrets-812cbd18-3937-41cc-b499-e795bec97ffc" satisfied condition "Succeeded or Failed"
May 25 04:01:00.788: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-projected-secrets-812cbd18-3937-41cc-b499-e795bec97ffc container projected-secret-volume-test: <nil>
STEP: delete the pod
May 25 04:01:00.821: INFO: Waiting for pod pod-projected-secrets-812cbd18-3937-41cc-b499-e795bec97ffc to disappear
May 25 04:01:00.825: INFO: Pod pod-projected-secrets-812cbd18-3937-41cc-b499-e795bec97ffc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:01:00.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-114" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":253,"skipped":3952,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:01:00.838: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-c931d35f-f5e0-4925-afe5-55f5b8fe1504
STEP: Creating secret with name secret-projected-all-test-volume-ae0dfe19-608b-4794-a7bb-c271367bd392
STEP: Creating a pod to test Check all projections for projected volume plugin
May 25 04:01:00.910: INFO: Waiting up to 5m0s for pod "projected-volume-3f67dd35-2d43-42aa-b3d0-5d714a385315" in namespace "projected-493" to be "Succeeded or Failed"
May 25 04:01:00.915: INFO: Pod "projected-volume-3f67dd35-2d43-42aa-b3d0-5d714a385315": Phase="Pending", Reason="", readiness=false. Elapsed: 5.026518ms
May 25 04:01:02.922: INFO: Pod "projected-volume-3f67dd35-2d43-42aa-b3d0-5d714a385315": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012397189s
STEP: Saw pod success
May 25 04:01:02.923: INFO: Pod "projected-volume-3f67dd35-2d43-42aa-b3d0-5d714a385315" satisfied condition "Succeeded or Failed"
May 25 04:01:02.926: INFO: Trying to get logs from node 10.10.103.202-worker pod projected-volume-3f67dd35-2d43-42aa-b3d0-5d714a385315 container projected-all-volume-test: <nil>
STEP: delete the pod
May 25 04:01:02.950: INFO: Waiting for pod projected-volume-3f67dd35-2d43-42aa-b3d0-5d714a385315 to disappear
May 25 04:01:02.954: INFO: Pod projected-volume-3f67dd35-2d43-42aa-b3d0-5d714a385315 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:01:02.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-493" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":339,"completed":254,"skipped":3973,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:01:02.966: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:01:03.019: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:01:04.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4712" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":339,"completed":255,"skipped":3988,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:01:04.061: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-1f36a31c-ed78-445d-ba1d-801a645430b3
STEP: Creating a pod to test consume secrets
May 25 04:01:04.151: INFO: Waiting up to 5m0s for pod "pod-secrets-9e4deac5-1bee-4341-bcb3-560cf415028e" in namespace "secrets-5913" to be "Succeeded or Failed"
May 25 04:01:04.156: INFO: Pod "pod-secrets-9e4deac5-1bee-4341-bcb3-560cf415028e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069682ms
May 25 04:01:06.162: INFO: Pod "pod-secrets-9e4deac5-1bee-4341-bcb3-560cf415028e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010527215s
STEP: Saw pod success
May 25 04:01:06.162: INFO: Pod "pod-secrets-9e4deac5-1bee-4341-bcb3-560cf415028e" satisfied condition "Succeeded or Failed"
May 25 04:01:06.166: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-secrets-9e4deac5-1bee-4341-bcb3-560cf415028e container secret-volume-test: <nil>
STEP: delete the pod
May 25 04:01:06.190: INFO: Waiting for pod pod-secrets-9e4deac5-1bee-4341-bcb3-560cf415028e to disappear
May 25 04:01:06.193: INFO: Pod pod-secrets-9e4deac5-1bee-4341-bcb3-560cf415028e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:01:06.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5913" for this suite.
STEP: Destroying namespace "secret-namespace-2909" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":339,"completed":256,"skipped":3990,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:01:06.214: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-7153
STEP: creating replication controller nodeport-test in namespace services-7153
I0525 04:01:06.288863      23 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-7153, replica count: 2
May 25 04:01:09.339: INFO: Creating new exec pod
I0525 04:01:09.339810      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 04:01:14.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:14.764: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:14.764: INFO: stdout: ""
May 25 04:01:15.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:16.159: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:16.159: INFO: stdout: ""
May 25 04:01:16.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:17.173: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:17.173: INFO: stdout: ""
May 25 04:01:17.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:18.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:18.145: INFO: stdout: ""
May 25 04:01:18.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:19.155: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:19.155: INFO: stdout: ""
May 25 04:01:19.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:20.141: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:20.141: INFO: stdout: ""
May 25 04:01:20.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:21.150: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:21.150: INFO: stdout: ""
May 25 04:01:21.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:22.142: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:22.142: INFO: stdout: ""
May 25 04:01:22.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:23.150: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:23.150: INFO: stdout: ""
May 25 04:01:23.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:24.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:24.145: INFO: stdout: ""
May 25 04:01:24.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:25.137: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:25.137: INFO: stdout: ""
May 25 04:01:25.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:26.141: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:26.141: INFO: stdout: ""
May 25 04:01:26.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:27.148: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:27.148: INFO: stdout: ""
May 25 04:01:27.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:28.129: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:28.129: INFO: stdout: ""
May 25 04:01:28.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:29.162: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:29.162: INFO: stdout: ""
May 25 04:01:29.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:30.137: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:30.137: INFO: stdout: ""
May 25 04:01:30.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:31.139: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:31.139: INFO: stdout: ""
May 25 04:01:31.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:32.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:32.153: INFO: stdout: ""
May 25 04:01:32.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:33.188: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:33.188: INFO: stdout: ""
May 25 04:01:33.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:34.146: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:34.146: INFO: stdout: ""
May 25 04:01:34.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:35.147: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:35.147: INFO: stdout: ""
May 25 04:01:35.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:36.163: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:36.163: INFO: stdout: ""
May 25 04:01:36.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:37.136: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:37.136: INFO: stdout: ""
May 25 04:01:37.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:38.161: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:38.161: INFO: stdout: ""
May 25 04:01:38.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:39.146: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:39.147: INFO: stdout: ""
May 25 04:01:39.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:40.161: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:40.161: INFO: stdout: ""
May 25 04:01:40.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:41.132: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:41.132: INFO: stdout: ""
May 25 04:01:41.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:42.138: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:42.138: INFO: stdout: ""
May 25 04:01:42.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:43.134: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:43.134: INFO: stdout: ""
May 25 04:01:43.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:44.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:44.145: INFO: stdout: ""
May 25 04:01:44.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:45.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:45.149: INFO: stdout: ""
May 25 04:01:45.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:46.164: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:46.164: INFO: stdout: ""
May 25 04:01:46.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:47.138: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:47.138: INFO: stdout: ""
May 25 04:01:47.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:48.144: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:48.144: INFO: stdout: ""
May 25 04:01:48.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:49.160: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:49.160: INFO: stdout: ""
May 25 04:01:49.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:50.141: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:50.142: INFO: stdout: ""
May 25 04:01:50.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:51.160: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:51.160: INFO: stdout: ""
May 25 04:01:51.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:52.142: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:52.142: INFO: stdout: ""
May 25 04:01:52.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:53.150: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:53.150: INFO: stdout: ""
May 25 04:01:53.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:54.136: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:54.136: INFO: stdout: ""
May 25 04:01:54.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:55.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:55.153: INFO: stdout: ""
May 25 04:01:55.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:56.151: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:56.151: INFO: stdout: ""
May 25 04:01:56.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:57.136: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:57.136: INFO: stdout: ""
May 25 04:01:57.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:58.155: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:58.155: INFO: stdout: ""
May 25 04:01:58.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:01:59.143: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:01:59.143: INFO: stdout: ""
May 25 04:01:59.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:00.311: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:00.311: INFO: stdout: ""
May 25 04:02:00.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:01.147: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:01.147: INFO: stdout: ""
May 25 04:02:01.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:02.148: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:02.148: INFO: stdout: ""
May 25 04:02:02.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:03.129: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:03.129: INFO: stdout: ""
May 25 04:02:03.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:04.134: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:04.134: INFO: stdout: ""
May 25 04:02:04.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:05.139: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:05.139: INFO: stdout: ""
May 25 04:02:05.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:06.148: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:06.148: INFO: stdout: ""
May 25 04:02:06.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:07.150: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:07.150: INFO: stdout: ""
May 25 04:02:07.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:08.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:08.145: INFO: stdout: ""
May 25 04:02:08.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:09.144: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:09.144: INFO: stdout: ""
May 25 04:02:09.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:10.143: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:10.143: INFO: stdout: ""
May 25 04:02:10.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:11.160: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:11.160: INFO: stdout: ""
May 25 04:02:11.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:12.151: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:12.151: INFO: stdout: ""
May 25 04:02:12.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:13.164: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:13.164: INFO: stdout: ""
May 25 04:02:13.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:14.157: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:14.157: INFO: stdout: ""
May 25 04:02:14.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:15.147: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:15.147: INFO: stdout: ""
May 25 04:02:15.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:16.157: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:16.157: INFO: stdout: ""
May 25 04:02:16.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:17.141: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:17.141: INFO: stdout: ""
May 25 04:02:17.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:18.150: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:18.150: INFO: stdout: ""
May 25 04:02:18.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:19.130: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:19.130: INFO: stdout: ""
May 25 04:02:19.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:20.155: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:20.155: INFO: stdout: ""
May 25 04:02:20.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:21.158: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:21.158: INFO: stdout: ""
May 25 04:02:21.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:22.141: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:22.141: INFO: stdout: ""
May 25 04:02:22.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:23.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:23.149: INFO: stdout: ""
May 25 04:02:23.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:24.143: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:24.143: INFO: stdout: ""
May 25 04:02:24.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:25.143: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:25.143: INFO: stdout: ""
May 25 04:02:25.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:26.164: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:26.164: INFO: stdout: ""
May 25 04:02:26.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:27.146: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:27.146: INFO: stdout: ""
May 25 04:02:27.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:28.144: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:28.144: INFO: stdout: ""
May 25 04:02:28.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:29.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:29.145: INFO: stdout: ""
May 25 04:02:29.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:30.156: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:30.156: INFO: stdout: ""
May 25 04:02:30.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:31.143: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:31.143: INFO: stdout: ""
May 25 04:02:31.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:32.141: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:32.141: INFO: stdout: ""
May 25 04:02:32.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:33.147: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:33.147: INFO: stdout: ""
May 25 04:02:33.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:34.141: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:34.141: INFO: stdout: ""
May 25 04:02:34.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:35.133: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:35.133: INFO: stdout: ""
May 25 04:02:35.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 25 04:02:36.144: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 25 04:02:36.144: INFO: stdout: "nodeport-test-ttnts"
May 25 04:02:36.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:36.516: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:36.516: INFO: stdout: ""
May 25 04:02:37.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:37.897: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:37.897: INFO: stdout: ""
May 25 04:02:38.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:38.894: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:38.895: INFO: stdout: ""
May 25 04:02:39.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:39.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:39.886: INFO: stdout: ""
May 25 04:02:40.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:40.898: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:40.898: INFO: stdout: ""
May 25 04:02:41.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:41.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:41.891: INFO: stdout: ""
May 25 04:02:42.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:42.896: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:42.896: INFO: stdout: ""
May 25 04:02:43.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:43.888: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:43.888: INFO: stdout: ""
May 25 04:02:44.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:44.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:44.886: INFO: stdout: ""
May 25 04:02:45.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:45.899: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:45.899: INFO: stdout: ""
May 25 04:02:46.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:46.930: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:46.930: INFO: stdout: ""
May 25 04:02:47.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:47.879: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:47.879: INFO: stdout: ""
May 25 04:02:48.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:48.894: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:48.894: INFO: stdout: ""
May 25 04:02:49.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:49.981: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:49.981: INFO: stdout: ""
May 25 04:02:50.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:50.960: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:50.961: INFO: stdout: ""
May 25 04:02:51.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:51.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:51.886: INFO: stdout: ""
May 25 04:02:52.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.192.172 80'
May 25 04:02:52.902: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.192.172 80\nConnection to 10.107.192.172 80 port [tcp/http] succeeded!\n"
May 25 04:02:52.902: INFO: stdout: "nodeport-test-qhp2s"
May 25 04:02:52.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.201 31738'
May 25 04:02:53.286: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.201 31738\nConnection to 10.10.103.201 31738 port [tcp/*] succeeded!\n"
May 25 04:02:53.286: INFO: stdout: ""
May 25 04:02:54.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.201 31738'
May 25 04:02:54.678: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.201 31738\nConnection to 10.10.103.201 31738 port [tcp/*] succeeded!\n"
May 25 04:02:54.678: INFO: stdout: ""
May 25 04:02:55.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.201 31738'
May 25 04:02:55.659: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.201 31738\nConnection to 10.10.103.201 31738 port [tcp/*] succeeded!\n"
May 25 04:02:55.659: INFO: stdout: ""
May 25 04:02:56.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.201 31738'
May 25 04:02:56.669: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.201 31738\nConnection to 10.10.103.201 31738 port [tcp/*] succeeded!\n"
May 25 04:02:56.669: INFO: stdout: ""
May 25 04:02:57.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.201 31738'
May 25 04:02:57.664: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.201 31738\nConnection to 10.10.103.201 31738 port [tcp/*] succeeded!\n"
May 25 04:02:57.664: INFO: stdout: "nodeport-test-qhp2s"
May 25 04:02:57.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.202 31738'
May 25 04:02:58.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.202 31738\nConnection to 10.10.103.202 31738 port [tcp/*] succeeded!\n"
May 25 04:02:58.034: INFO: stdout: ""
May 25 04:02:59.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.202 31738'
May 25 04:02:59.420: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.202 31738\nConnection to 10.10.103.202 31738 port [tcp/*] succeeded!\n"
May 25 04:02:59.420: INFO: stdout: ""
May 25 04:03:00.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-7153 exec execpodzmjfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.202 31738'
May 25 04:03:00.413: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.202 31738\nConnection to 10.10.103.202 31738 port [tcp/*] succeeded!\n"
May 25 04:03:00.413: INFO: stdout: "nodeport-test-qhp2s"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:03:00.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7153" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:114.218 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":339,"completed":257,"skipped":4010,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:03:00.432: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 04:03:01.849: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 04:03:03.872: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512181, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512181, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512181, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512181, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 04:03:06.906: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:03:19.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7850" for this suite.
STEP: Destroying namespace "webhook-7850-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.816 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":339,"completed":258,"skipped":4040,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:03:19.249: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:03:19.351: INFO: The status of Pod busybox-readonly-fsdd924126-c5b0-4087-9340-fe4dcdad1fd4 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:03:21.358: INFO: The status of Pod busybox-readonly-fsdd924126-c5b0-4087-9340-fe4dcdad1fd4 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:03:21.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1399" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":259,"skipped":4050,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:03:21.409: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-6322f10b-3d80-4505-b004-f13c27c51f92
STEP: Creating configMap with name cm-test-opt-upd-4984300b-64ed-4c3f-bf24-867754baad09
STEP: Creating the pod
May 25 04:03:21.490: INFO: The status of Pod pod-configmaps-98519700-abad-4ba9-a006-cb1c107e5730 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:03:23.501: INFO: The status of Pod pod-configmaps-98519700-abad-4ba9-a006-cb1c107e5730 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:03:25.506: INFO: The status of Pod pod-configmaps-98519700-abad-4ba9-a006-cb1c107e5730 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-6322f10b-3d80-4505-b004-f13c27c51f92
STEP: Updating configmap cm-test-opt-upd-4984300b-64ed-4c3f-bf24-867754baad09
STEP: Creating configMap with name cm-test-opt-create-ede9cf43-205e-4f14-902b-eca8f5bca257
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:03:27.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1671" for this suite.

• [SLOW TEST:6.224 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":260,"skipped":4060,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:03:27.633: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 25 04:03:27.708: INFO: Waiting up to 5m0s for pod "pod-c356098e-d9f1-4e2b-a92d-a88cc15aa014" in namespace "emptydir-4049" to be "Succeeded or Failed"
May 25 04:03:27.712: INFO: Pod "pod-c356098e-d9f1-4e2b-a92d-a88cc15aa014": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825451ms
May 25 04:03:29.722: INFO: Pod "pod-c356098e-d9f1-4e2b-a92d-a88cc15aa014": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013783717s
May 25 04:03:31.731: INFO: Pod "pod-c356098e-d9f1-4e2b-a92d-a88cc15aa014": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022647559s
STEP: Saw pod success
May 25 04:03:31.731: INFO: Pod "pod-c356098e-d9f1-4e2b-a92d-a88cc15aa014" satisfied condition "Succeeded or Failed"
May 25 04:03:31.737: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-c356098e-d9f1-4e2b-a92d-a88cc15aa014 container test-container: <nil>
STEP: delete the pod
May 25 04:03:31.770: INFO: Waiting for pod pod-c356098e-d9f1-4e2b-a92d-a88cc15aa014 to disappear
May 25 04:03:31.774: INFO: Pod pod-c356098e-d9f1-4e2b-a92d-a88cc15aa014 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:03:31.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4049" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":261,"skipped":4060,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:03:31.796: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-sgm8
STEP: Creating a pod to test atomic-volume-subpath
May 25 04:03:31.869: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sgm8" in namespace "subpath-2904" to be "Succeeded or Failed"
May 25 04:03:31.874: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.856331ms
May 25 04:03:33.881: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011762403s
May 25 04:03:35.893: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Running", Reason="", readiness=true. Elapsed: 4.024186557s
May 25 04:03:37.905: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Running", Reason="", readiness=true. Elapsed: 6.036231711s
May 25 04:03:39.921: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Running", Reason="", readiness=true. Elapsed: 8.052208484s
May 25 04:03:41.931: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Running", Reason="", readiness=true. Elapsed: 10.061844312s
May 25 04:03:43.947: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Running", Reason="", readiness=true. Elapsed: 12.077864919s
May 25 04:03:45.956: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Running", Reason="", readiness=true. Elapsed: 14.086789648s
May 25 04:03:47.968: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Running", Reason="", readiness=true. Elapsed: 16.099327271s
May 25 04:03:49.987: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Running", Reason="", readiness=true. Elapsed: 18.118365387s
May 25 04:03:51.996: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Running", Reason="", readiness=true. Elapsed: 20.127528079s
May 25 04:03:54.012: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Running", Reason="", readiness=true. Elapsed: 22.142684065s
May 25 04:03:56.028: INFO: Pod "pod-subpath-test-configmap-sgm8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.159228495s
STEP: Saw pod success
May 25 04:03:56.028: INFO: Pod "pod-subpath-test-configmap-sgm8" satisfied condition "Succeeded or Failed"
May 25 04:03:56.033: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-subpath-test-configmap-sgm8 container test-container-subpath-configmap-sgm8: <nil>
STEP: delete the pod
May 25 04:03:56.070: INFO: Waiting for pod pod-subpath-test-configmap-sgm8 to disappear
May 25 04:03:56.074: INFO: Pod pod-subpath-test-configmap-sgm8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-sgm8
May 25 04:03:56.074: INFO: Deleting pod "pod-subpath-test-configmap-sgm8" in namespace "subpath-2904"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:03:56.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2904" for this suite.

• [SLOW TEST:24.295 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":339,"completed":262,"skipped":4074,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:03:56.091: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
May 25 04:03:56.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6721 cluster-info'
May 25 04:03:56.348: INFO: stderr: ""
May 25 04:03:56.348: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:03:56.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6721" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":339,"completed":263,"skipped":4080,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:03:56.367: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
May 25 04:03:56.429: INFO: The status of Pod annotationupdate63e5471e-0590-4962-a5f0-e35f6456b200 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:03:58.434: INFO: The status of Pod annotationupdate63e5471e-0590-4962-a5f0-e35f6456b200 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:04:00.435: INFO: The status of Pod annotationupdate63e5471e-0590-4962-a5f0-e35f6456b200 is Running (Ready = true)
May 25 04:04:00.973: INFO: Successfully updated pod "annotationupdate63e5471e-0590-4962-a5f0-e35f6456b200"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:04:03.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9095" for this suite.

• [SLOW TEST:6.654 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":264,"skipped":4081,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:04:03.021: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 04:04:03.087: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fcf1383-2704-4dcf-8cbd-bdfe05c3a755" in namespace "downward-api-8695" to be "Succeeded or Failed"
May 25 04:04:03.093: INFO: Pod "downwardapi-volume-0fcf1383-2704-4dcf-8cbd-bdfe05c3a755": Phase="Pending", Reason="", readiness=false. Elapsed: 5.720473ms
May 25 04:04:05.100: INFO: Pod "downwardapi-volume-0fcf1383-2704-4dcf-8cbd-bdfe05c3a755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012895602s
STEP: Saw pod success
May 25 04:04:05.100: INFO: Pod "downwardapi-volume-0fcf1383-2704-4dcf-8cbd-bdfe05c3a755" satisfied condition "Succeeded or Failed"
May 25 04:04:05.105: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-0fcf1383-2704-4dcf-8cbd-bdfe05c3a755 container client-container: <nil>
STEP: delete the pod
May 25 04:04:05.132: INFO: Waiting for pod downwardapi-volume-0fcf1383-2704-4dcf-8cbd-bdfe05c3a755 to disappear
May 25 04:04:05.138: INFO: Pod downwardapi-volume-0fcf1383-2704-4dcf-8cbd-bdfe05c3a755 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:04:05.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8695" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":265,"skipped":4091,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:04:05.151: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
May 25 04:04:05.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-9194 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
May 25 04:04:05.446: INFO: stderr: ""
May 25 04:04:05.446: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 25 04:04:05.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-9194 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
May 25 04:04:06.141: INFO: stderr: ""
May 25 04:04:06.141: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
May 25 04:04:06.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-9194 delete pods e2e-test-httpd-pod'
May 25 04:04:18.423: INFO: stderr: ""
May 25 04:04:18.423: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:04:18.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9194" for this suite.

• [SLOW TEST:13.292 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:903
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":339,"completed":266,"skipped":4092,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:04:18.443: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-95c0f428-7b03-4d61-85f3-cc1fa498c9f9
STEP: Creating the pod
May 25 04:04:18.514: INFO: The status of Pod pod-configmaps-7fb1a7ea-011a-4faf-b0b9-a48a9bf92b29 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:04:20.523: INFO: The status of Pod pod-configmaps-7fb1a7ea-011a-4faf-b0b9-a48a9bf92b29 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-95c0f428-7b03-4d61-85f3-cc1fa498c9f9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:04:22.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-493" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":267,"skipped":4104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:04:22.584: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-3a4d9fe3-d9d0-4504-9b7e-f7597a60c689
STEP: Creating a pod to test consume configMaps
May 25 04:04:22.652: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e03387c-4c7a-423f-aa37-52d88e3ab34a" in namespace "configmap-1343" to be "Succeeded or Failed"
May 25 04:04:22.658: INFO: Pod "pod-configmaps-8e03387c-4c7a-423f-aa37-52d88e3ab34a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054675ms
May 25 04:04:24.667: INFO: Pod "pod-configmaps-8e03387c-4c7a-423f-aa37-52d88e3ab34a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014821578s
STEP: Saw pod success
May 25 04:04:24.667: INFO: Pod "pod-configmaps-8e03387c-4c7a-423f-aa37-52d88e3ab34a" satisfied condition "Succeeded or Failed"
May 25 04:04:24.671: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-configmaps-8e03387c-4c7a-423f-aa37-52d88e3ab34a container agnhost-container: <nil>
STEP: delete the pod
May 25 04:04:24.694: INFO: Waiting for pod pod-configmaps-8e03387c-4c7a-423f-aa37-52d88e3ab34a to disappear
May 25 04:04:24.698: INFO: Pod pod-configmaps-8e03387c-4c7a-423f-aa37-52d88e3ab34a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:04:24.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1343" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":268,"skipped":4138,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:04:24.711: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
W0525 04:04:24.768219      23 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:09:24.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7642" for this suite.

• [SLOW TEST:300.106 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":339,"completed":269,"skipped":4151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:09:24.819: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1386
STEP: creating an pod
May 25 04:09:24.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-713 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
May 25 04:09:25.161: INFO: stderr: ""
May 25 04:09:25.161: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
May 25 04:09:25.161: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 25 04:09:25.161: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-713" to be "running and ready, or succeeded"
May 25 04:09:25.166: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.566828ms
May 25 04:09:27.178: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016536764s
May 25 04:09:29.193: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.0319836s
May 25 04:09:29.193: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 25 04:09:29.193: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 25 04:09:29.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-713 logs logs-generator logs-generator'
May 25 04:09:29.457: INFO: stderr: ""
May 25 04:09:29.457: INFO: stdout: "I0525 04:09:26.840739       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/jrb 462\nI0525 04:09:27.041011       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/klz 433\nI0525 04:09:27.241472       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/hjkw 421\nI0525 04:09:27.440873       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/pg9c 518\nI0525 04:09:27.641308       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/9d67 448\nI0525 04:09:27.841779       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/v56t 507\nI0525 04:09:28.041308       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/7gpl 263\nI0525 04:09:28.241822       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/js2q 278\nI0525 04:09:28.441269       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/4bdj 534\nI0525 04:09:28.641720       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/m8j5 526\nI0525 04:09:28.841276       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/xxwx 217\nI0525 04:09:29.041742       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/ljsp 380\nI0525 04:09:29.241219       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/c78 501\nI0525 04:09:29.441762       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/nzb 327\n"
STEP: limiting log lines
May 25 04:09:29.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-713 logs logs-generator logs-generator --tail=1'
May 25 04:09:29.682: INFO: stderr: ""
May 25 04:09:29.682: INFO: stdout: "I0525 04:09:29.641287       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/gcp 375\n"
May 25 04:09:29.682: INFO: got output "I0525 04:09:29.641287       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/gcp 375\n"
STEP: limiting log bytes
May 25 04:09:29.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-713 logs logs-generator logs-generator --limit-bytes=1'
May 25 04:09:29.914: INFO: stderr: ""
May 25 04:09:29.914: INFO: stdout: "I"
May 25 04:09:29.914: INFO: got output "I"
STEP: exposing timestamps
May 25 04:09:29.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-713 logs logs-generator logs-generator --tail=1 --timestamps'
May 25 04:09:30.167: INFO: stderr: ""
May 25 04:09:30.167: INFO: stdout: "2021-05-25T04:09:30.041637545Z I0525 04:09:30.041293       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/dbl 452\n"
May 25 04:09:30.167: INFO: got output "2021-05-25T04:09:30.041637545Z I0525 04:09:30.041293       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/dbl 452\n"
STEP: restricting to a time range
May 25 04:09:32.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-713 logs logs-generator logs-generator --since=1s'
May 25 04:09:32.894: INFO: stderr: ""
May 25 04:09:32.894: INFO: stdout: "I0525 04:09:32.042122       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/kkg 211\nI0525 04:09:32.241678       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/ffxq 471\nI0525 04:09:32.441238       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/zq9 222\nI0525 04:09:32.641754       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/b67 292\nI0525 04:09:32.841254       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/qlc 469\n"
May 25 04:09:32.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-713 logs logs-generator logs-generator --since=24h'
May 25 04:09:33.121: INFO: stderr: ""
May 25 04:09:33.121: INFO: stdout: "I0525 04:09:26.840739       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/jrb 462\nI0525 04:09:27.041011       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/klz 433\nI0525 04:09:27.241472       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/hjkw 421\nI0525 04:09:27.440873       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/pg9c 518\nI0525 04:09:27.641308       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/9d67 448\nI0525 04:09:27.841779       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/v56t 507\nI0525 04:09:28.041308       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/7gpl 263\nI0525 04:09:28.241822       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/js2q 278\nI0525 04:09:28.441269       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/4bdj 534\nI0525 04:09:28.641720       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/m8j5 526\nI0525 04:09:28.841276       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/xxwx 217\nI0525 04:09:29.041742       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/ljsp 380\nI0525 04:09:29.241219       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/c78 501\nI0525 04:09:29.441762       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/nzb 327\nI0525 04:09:29.641287       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/gcp 375\nI0525 04:09:29.841811       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/px7g 424\nI0525 04:09:30.041293       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/dbl 452\nI0525 04:09:30.241786       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/s5r 295\nI0525 04:09:30.441295       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/xr8 268\nI0525 04:09:30.641806       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/fkf 422\nI0525 04:09:30.841300       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/fzd 207\nI0525 04:09:31.040762       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/mgs 571\nI0525 04:09:31.241330       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/gld 435\nI0525 04:09:31.440789       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/bfq 351\nI0525 04:09:31.641254       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/fvk 261\nI0525 04:09:31.840951       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/jv7l 576\nI0525 04:09:32.042122       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/kkg 211\nI0525 04:09:32.241678       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/ffxq 471\nI0525 04:09:32.441238       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/zq9 222\nI0525 04:09:32.641754       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/b67 292\nI0525 04:09:32.841254       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/qlc 469\nI0525 04:09:33.041776       1 logs_generator.go:76] 31 POST /api/v1/namespaces/default/pods/dhvb 413\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1391
May 25 04:09:33.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-713 delete pod logs-generator'
May 25 04:09:38.400: INFO: stderr: ""
May 25 04:09:38.400: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:09:38.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-713" for this suite.

• [SLOW TEST:13.598 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1383
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":339,"completed":270,"skipped":4226,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:09:38.418: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
May 25 04:09:38.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 create -f -'
May 25 04:09:39.004: INFO: stderr: ""
May 25 04:09:39.004: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 25 04:09:39.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 04:09:39.218: INFO: stderr: ""
May 25 04:09:39.218: INFO: stdout: "update-demo-nautilus-67554 update-demo-nautilus-hnjp4 "
May 25 04:09:39.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-67554 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 04:09:39.422: INFO: stderr: ""
May 25 04:09:39.422: INFO: stdout: ""
May 25 04:09:39.422: INFO: update-demo-nautilus-67554 is created but not running
May 25 04:09:44.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 04:09:44.634: INFO: stderr: ""
May 25 04:09:44.634: INFO: stdout: "update-demo-nautilus-67554 update-demo-nautilus-hnjp4 "
May 25 04:09:44.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-67554 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 04:09:44.835: INFO: stderr: ""
May 25 04:09:44.835: INFO: stdout: "true"
May 25 04:09:44.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-67554 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 04:09:45.035: INFO: stderr: ""
May 25 04:09:45.035: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 25 04:09:45.035: INFO: validating pod update-demo-nautilus-67554
May 25 04:09:45.042: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 04:09:45.042: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 04:09:45.043: INFO: update-demo-nautilus-67554 is verified up and running
May 25 04:09:45.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-hnjp4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 04:09:45.241: INFO: stderr: ""
May 25 04:09:45.241: INFO: stdout: "true"
May 25 04:09:45.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-hnjp4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 04:09:45.440: INFO: stderr: ""
May 25 04:09:45.440: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 25 04:09:45.440: INFO: validating pod update-demo-nautilus-hnjp4
May 25 04:09:45.447: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 04:09:45.447: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 04:09:45.447: INFO: update-demo-nautilus-hnjp4 is verified up and running
STEP: scaling down the replication controller
May 25 04:09:45.451: INFO: scanned /root for discovery docs: <nil>
May 25 04:09:45.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 25 04:09:46.726: INFO: stderr: ""
May 25 04:09:46.726: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 25 04:09:46.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 04:09:46.941: INFO: stderr: ""
May 25 04:09:46.941: INFO: stdout: "update-demo-nautilus-67554 update-demo-nautilus-hnjp4 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 25 04:09:51.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 04:09:52.140: INFO: stderr: ""
May 25 04:09:52.140: INFO: stdout: "update-demo-nautilus-67554 "
May 25 04:09:52.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-67554 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 04:09:52.337: INFO: stderr: ""
May 25 04:09:52.337: INFO: stdout: "true"
May 25 04:09:52.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-67554 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 04:09:52.529: INFO: stderr: ""
May 25 04:09:52.529: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 25 04:09:52.529: INFO: validating pod update-demo-nautilus-67554
May 25 04:09:52.535: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 04:09:52.535: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 04:09:52.535: INFO: update-demo-nautilus-67554 is verified up and running
STEP: scaling up the replication controller
May 25 04:09:52.538: INFO: scanned /root for discovery docs: <nil>
May 25 04:09:52.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 25 04:09:53.780: INFO: stderr: ""
May 25 04:09:53.780: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 25 04:09:53.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 04:09:53.992: INFO: stderr: ""
May 25 04:09:53.992: INFO: stdout: "update-demo-nautilus-67554 update-demo-nautilus-zbpmm "
May 25 04:09:53.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-67554 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 04:09:54.195: INFO: stderr: ""
May 25 04:09:54.195: INFO: stdout: "true"
May 25 04:09:54.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-67554 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 04:09:54.395: INFO: stderr: ""
May 25 04:09:54.395: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 25 04:09:54.395: INFO: validating pod update-demo-nautilus-67554
May 25 04:09:54.403: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 04:09:54.403: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 04:09:54.403: INFO: update-demo-nautilus-67554 is verified up and running
May 25 04:09:54.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-zbpmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 04:09:54.596: INFO: stderr: ""
May 25 04:09:54.596: INFO: stdout: ""
May 25 04:09:54.596: INFO: update-demo-nautilus-zbpmm is created but not running
May 25 04:09:59.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 25 04:09:59.808: INFO: stderr: ""
May 25 04:09:59.808: INFO: stdout: "update-demo-nautilus-67554 update-demo-nautilus-zbpmm "
May 25 04:09:59.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-67554 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 04:10:00.012: INFO: stderr: ""
May 25 04:10:00.012: INFO: stdout: "true"
May 25 04:10:00.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-67554 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 04:10:00.208: INFO: stderr: ""
May 25 04:10:00.208: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 25 04:10:00.208: INFO: validating pod update-demo-nautilus-67554
May 25 04:10:00.214: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 04:10:00.214: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 04:10:00.214: INFO: update-demo-nautilus-67554 is verified up and running
May 25 04:10:00.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-zbpmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 25 04:10:00.404: INFO: stderr: ""
May 25 04:10:00.404: INFO: stdout: "true"
May 25 04:10:00.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods update-demo-nautilus-zbpmm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 25 04:10:00.606: INFO: stderr: ""
May 25 04:10:00.606: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
May 25 04:10:00.606: INFO: validating pod update-demo-nautilus-zbpmm
May 25 04:10:00.613: INFO: got data: {
  "image": "nautilus.jpg"
}

May 25 04:10:00.613: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 25 04:10:00.613: INFO: update-demo-nautilus-zbpmm is verified up and running
STEP: using delete to clean up resources
May 25 04:10:00.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 delete --grace-period=0 --force -f -'
May 25 04:10:00.825: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 04:10:00.825: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 25 04:10:00.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get rc,svc -l name=update-demo --no-headers'
May 25 04:10:01.052: INFO: stderr: "No resources found in kubectl-4707 namespace.\n"
May 25 04:10:01.052: INFO: stdout: ""
May 25 04:10:01.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-4707 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 25 04:10:01.261: INFO: stderr: ""
May 25 04:10:01.261: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:10:01.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4707" for this suite.

• [SLOW TEST:22.857 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":339,"completed":271,"skipped":4229,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:10:01.276: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
May 25 04:10:01.335: INFO: Waiting up to 5m0s for pod "pod-a1b138bf-d4a8-4eb2-a01f-d89fe7f7f17d" in namespace "emptydir-9793" to be "Succeeded or Failed"
May 25 04:10:01.339: INFO: Pod "pod-a1b138bf-d4a8-4eb2-a01f-d89fe7f7f17d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.764894ms
May 25 04:10:03.348: INFO: Pod "pod-a1b138bf-d4a8-4eb2-a01f-d89fe7f7f17d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01292833s
STEP: Saw pod success
May 25 04:10:03.348: INFO: Pod "pod-a1b138bf-d4a8-4eb2-a01f-d89fe7f7f17d" satisfied condition "Succeeded or Failed"
May 25 04:10:03.352: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-a1b138bf-d4a8-4eb2-a01f-d89fe7f7f17d container test-container: <nil>
STEP: delete the pod
May 25 04:10:03.376: INFO: Waiting for pod pod-a1b138bf-d4a8-4eb2-a01f-d89fe7f7f17d to disappear
May 25 04:10:03.381: INFO: Pod pod-a1b138bf-d4a8-4eb2-a01f-d89fe7f7f17d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:10:03.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9793" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":272,"skipped":4230,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:10:03.393: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 25 04:10:03.448: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 25 04:10:28.992: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:10:35.195: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:10:58.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3753" for this suite.

• [SLOW TEST:55.400 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":339,"completed":273,"skipped":4239,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:10:58.793: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 04:10:59.822: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 25 04:11:01.835: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512659, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512659, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512659, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512659, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 04:11:04.860: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:11:04.871: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2496-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:11:08.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9898" for this suite.
STEP: Destroying namespace "webhook-9898-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.386 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":339,"completed":274,"skipped":4240,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:11:08.179: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
May 25 04:11:08.248: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 25 04:11:10.255: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 25 04:11:12.261: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
May 25 04:11:12.291: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 25 04:11:14.302: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 25 04:11:16.301: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 25 04:11:16.328: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 25 04:11:16.333: INFO: Pod pod-with-poststart-exec-hook still exists
May 25 04:11:18.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 25 04:11:18.347: INFO: Pod pod-with-poststart-exec-hook still exists
May 25 04:11:20.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 25 04:11:20.347: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:11:20.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6710" for this suite.

• [SLOW TEST:12.185 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":339,"completed":275,"skipped":4241,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:11:20.364: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:11:20.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2283" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":339,"completed":276,"skipped":4246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:11:20.479: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May 25 04:11:20.538: INFO: Waiting up to 5m0s for pod "downward-api-6c0a4bd6-9895-42e5-bfed-f9dcd0e06df9" in namespace "downward-api-8055" to be "Succeeded or Failed"
May 25 04:11:20.543: INFO: Pod "downward-api-6c0a4bd6-9895-42e5-bfed-f9dcd0e06df9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.815126ms
May 25 04:11:22.552: INFO: Pod "downward-api-6c0a4bd6-9895-42e5-bfed-f9dcd0e06df9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014248366s
STEP: Saw pod success
May 25 04:11:22.552: INFO: Pod "downward-api-6c0a4bd6-9895-42e5-bfed-f9dcd0e06df9" satisfied condition "Succeeded or Failed"
May 25 04:11:22.556: INFO: Trying to get logs from node 10.10.103.202-worker pod downward-api-6c0a4bd6-9895-42e5-bfed-f9dcd0e06df9 container dapi-container: <nil>
STEP: delete the pod
May 25 04:11:22.582: INFO: Waiting for pod downward-api-6c0a4bd6-9895-42e5-bfed-f9dcd0e06df9 to disappear
May 25 04:11:22.587: INFO: Pod downward-api-6c0a4bd6-9895-42e5-bfed-f9dcd0e06df9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:11:22.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8055" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":339,"completed":277,"skipped":4272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:11:22.599: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 25 04:11:22.655: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:11:29.052: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:11:51.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1971" for this suite.

• [SLOW TEST:29.209 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":339,"completed":278,"skipped":4305,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:11:51.809: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-6180
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
May 25 04:11:51.885: INFO: Found 0 stateful pods, waiting for 3
May 25 04:12:01.903: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 25 04:12:01.903: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 25 04:12:01.903: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 25 04:12:01.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-6180 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 04:12:02.303: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 04:12:02.304: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 04:12:02.304: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
May 25 04:12:12.350: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 25 04:12:22.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-6180 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 04:12:22.756: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 04:12:22.756: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 04:12:22.756: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
May 25 04:12:42.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-6180 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 25 04:12:43.170: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 25 04:12:43.170: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 25 04:12:43.170: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 25 04:12:53.221: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 25 04:13:03.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=statefulset-6180 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 25 04:13:03.755: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 25 04:13:03.755: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 25 04:13:03.755: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 25 04:13:13.790: INFO: Waiting for StatefulSet statefulset-6180/ss2 to complete update
May 25 04:13:13.790: INFO: Waiting for Pod statefulset-6180/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
May 25 04:13:23.805: INFO: Deleting all statefulset in ns statefulset-6180
May 25 04:13:23.808: INFO: Scaling statefulset ss2 to 0
May 25 04:13:33.835: INFO: Waiting for statefulset status.replicas updated to 0
May 25 04:13:33.839: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:13:33.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6180" for this suite.

• [SLOW TEST:102.078 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":339,"completed":279,"skipped":4316,"failed":0}
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:13:33.888: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:13:33.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4657" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":280,"skipped":4316,"failed":0}
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:13:33.959: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
May 25 04:13:34.020: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 25 04:13:36.029: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 25 04:13:38.027: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
May 25 04:13:38.050: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 25 04:13:40.059: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 25 04:13:42.060: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
May 25 04:13:42.073: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 25 04:13:42.078: INFO: Pod pod-with-prestop-exec-hook still exists
May 25 04:13:44.079: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 25 04:13:44.091: INFO: Pod pod-with-prestop-exec-hook still exists
May 25 04:13:46.078: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 25 04:13:46.086: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:13:46.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7276" for this suite.

• [SLOW TEST:12.176 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":339,"completed":281,"skipped":4320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:13:46.136: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-2be7e459-9483-4693-ad2a-21d762988cd7
STEP: Creating a pod to test consume configMaps
May 25 04:13:46.216: INFO: Waiting up to 5m0s for pod "pod-configmaps-77fda31d-736f-44be-942f-1b0d67152315" in namespace "configmap-1153" to be "Succeeded or Failed"
May 25 04:13:46.223: INFO: Pod "pod-configmaps-77fda31d-736f-44be-942f-1b0d67152315": Phase="Pending", Reason="", readiness=false. Elapsed: 6.579382ms
May 25 04:13:48.233: INFO: Pod "pod-configmaps-77fda31d-736f-44be-942f-1b0d67152315": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016194805s
May 25 04:13:50.242: INFO: Pod "pod-configmaps-77fda31d-736f-44be-942f-1b0d67152315": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02506677s
STEP: Saw pod success
May 25 04:13:50.242: INFO: Pod "pod-configmaps-77fda31d-736f-44be-942f-1b0d67152315" satisfied condition "Succeeded or Failed"
May 25 04:13:50.245: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-configmaps-77fda31d-736f-44be-942f-1b0d67152315 container configmap-volume-test: <nil>
STEP: delete the pod
May 25 04:13:50.275: INFO: Waiting for pod pod-configmaps-77fda31d-736f-44be-942f-1b0d67152315 to disappear
May 25 04:13:50.279: INFO: Pod pod-configmaps-77fda31d-736f-44be-942f-1b0d67152315 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:13:50.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1153" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":282,"skipped":4363,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:13:50.292: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-182054db-eed9-4e82-ba1d-664ef1d4e869
STEP: Creating a pod to test consume secrets
May 25 04:13:50.357: INFO: Waiting up to 5m0s for pod "pod-secrets-d415c8c2-8b51-4625-b132-b22ef1757e1e" in namespace "secrets-9634" to be "Succeeded or Failed"
May 25 04:13:50.363: INFO: Pod "pod-secrets-d415c8c2-8b51-4625-b132-b22ef1757e1e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.910888ms
May 25 04:13:52.374: INFO: Pod "pod-secrets-d415c8c2-8b51-4625-b132-b22ef1757e1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016680181s
May 25 04:13:54.384: INFO: Pod "pod-secrets-d415c8c2-8b51-4625-b132-b22ef1757e1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027351477s
STEP: Saw pod success
May 25 04:13:54.384: INFO: Pod "pod-secrets-d415c8c2-8b51-4625-b132-b22ef1757e1e" satisfied condition "Succeeded or Failed"
May 25 04:13:54.388: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-secrets-d415c8c2-8b51-4625-b132-b22ef1757e1e container secret-volume-test: <nil>
STEP: delete the pod
May 25 04:13:54.423: INFO: Waiting for pod pod-secrets-d415c8c2-8b51-4625-b132-b22ef1757e1e to disappear
May 25 04:13:54.429: INFO: Pod pod-secrets-d415c8c2-8b51-4625-b132-b22ef1757e1e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:13:54.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9634" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":283,"skipped":4385,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:13:54.444: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-d472f77b-2189-4736-9416-5a079201609d
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:13:54.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4569" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":339,"completed":284,"skipped":4387,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:13:54.523: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:13:54.607: INFO: The status of Pod pod-secrets-2db646c0-0477-4545-954d-f693b57c8039 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:13:56.616: INFO: The status of Pod pod-secrets-2db646c0-0477-4545-954d-f693b57c8039 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:13:56.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1012" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":339,"completed":285,"skipped":4388,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:13:56.663: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:13:56.740: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 25 04:13:56.750: INFO: Number of nodes with available pods: 0
May 25 04:13:56.750: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 25 04:13:56.774: INFO: Number of nodes with available pods: 0
May 25 04:13:56.774: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 04:13:57.780: INFO: Number of nodes with available pods: 0
May 25 04:13:57.780: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 04:13:58.781: INFO: Number of nodes with available pods: 1
May 25 04:13:58.781: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 25 04:13:58.812: INFO: Number of nodes with available pods: 1
May 25 04:13:58.812: INFO: Number of running nodes: 0, number of available pods: 1
May 25 04:13:59.820: INFO: Number of nodes with available pods: 0
May 25 04:13:59.820: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 25 04:13:59.835: INFO: Number of nodes with available pods: 0
May 25 04:13:59.835: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 04:14:00.844: INFO: Number of nodes with available pods: 0
May 25 04:14:00.844: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 04:14:01.843: INFO: Number of nodes with available pods: 0
May 25 04:14:01.843: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 04:14:02.846: INFO: Number of nodes with available pods: 0
May 25 04:14:02.846: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 04:14:03.840: INFO: Number of nodes with available pods: 0
May 25 04:14:03.840: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 04:14:04.848: INFO: Number of nodes with available pods: 0
May 25 04:14:04.848: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 04:14:05.845: INFO: Number of nodes with available pods: 0
May 25 04:14:05.845: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 04:14:06.843: INFO: Number of nodes with available pods: 0
May 25 04:14:06.843: INFO: Node 10.10.103.201-worker is running more than one daemon pod
May 25 04:14:07.845: INFO: Number of nodes with available pods: 1
May 25 04:14:07.845: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3055, will wait for the garbage collector to delete the pods
May 25 04:14:07.912: INFO: Deleting DaemonSet.extensions daemon-set took: 6.213368ms
May 25 04:14:08.013: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.134359ms
May 25 04:14:11.624: INFO: Number of nodes with available pods: 0
May 25 04:14:11.624: INFO: Number of running nodes: 0, number of available pods: 0
May 25 04:14:11.628: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"860869"},"items":null}

May 25 04:14:11.631: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"860869"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:14:11.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3055" for this suite.

• [SLOW TEST:15.008 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":339,"completed":286,"skipped":4393,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:14:11.672: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1931.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1931.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1931.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1931.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1931.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1931.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 04:14:15.785: INFO: DNS probes using dns-1931/dns-test-44c3d0a3-5681-47ae-abdf-da15914aedb5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:14:15.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1931" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":339,"completed":287,"skipped":4396,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:14:15.815: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-806.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-806.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-806.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-806.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-806.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-806.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-806.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 25 04:14:19.908: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:19.912: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:19.917: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:19.921: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:19.935: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:19.940: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:19.944: INFO: Unable to read jessie_udp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:19.948: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:19.957: INFO: Lookups using dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local jessie_udp@dns-test-service-2.dns-806.svc.cluster.local jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local]

May 25 04:14:24.967: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:24.971: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:24.977: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:24.981: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:24.995: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:24.999: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:25.004: INFO: Unable to read jessie_udp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:25.008: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:25.017: INFO: Lookups using dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local jessie_udp@dns-test-service-2.dns-806.svc.cluster.local jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local]

May 25 04:14:29.965: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:29.969: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:29.973: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:29.978: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:29.991: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:29.995: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:29.999: INFO: Unable to read jessie_udp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:30.003: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:30.011: INFO: Lookups using dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local jessie_udp@dns-test-service-2.dns-806.svc.cluster.local jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local]

May 25 04:14:34.968: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:34.972: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:34.976: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:34.981: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:34.996: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:35.000: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:35.005: INFO: Unable to read jessie_udp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:35.010: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:35.019: INFO: Lookups using dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local jessie_udp@dns-test-service-2.dns-806.svc.cluster.local jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local]

May 25 04:14:39.965: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:39.970: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:39.975: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:39.980: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:39.994: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:39.999: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:40.004: INFO: Unable to read jessie_udp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:40.008: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:40.018: INFO: Lookups using dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local jessie_udp@dns-test-service-2.dns-806.svc.cluster.local jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local]

May 25 04:14:44.963: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:44.968: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:44.973: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:44.977: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:44.991: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:44.996: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:45.002: INFO: Unable to read jessie_udp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:45.006: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local from pod dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c: the server could not find the requested resource (get pods dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c)
May 25 04:14:45.014: INFO: Lookups using dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local wheezy_udp@dns-test-service-2.dns-806.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-806.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-806.svc.cluster.local jessie_udp@dns-test-service-2.dns-806.svc.cluster.local jessie_tcp@dns-test-service-2.dns-806.svc.cluster.local]

May 25 04:14:50.017: INFO: DNS probes using dns-806/dns-test-e5362681-5842-4f08-aa0c-ae7877f4d18c succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:14:50.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-806" for this suite.

• [SLOW TEST:34.271 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":339,"completed":288,"skipped":4399,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:14:50.086: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
May 25 04:14:50.138: INFO: created test-podtemplate-1
May 25 04:14:50.144: INFO: created test-podtemplate-2
May 25 04:14:50.149: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 25 04:14:50.153: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 25 04:14:50.168: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:14:50.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1978" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":339,"completed":289,"skipped":4416,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:14:50.184: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:14:50.237: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 25 04:14:55.247: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 25 04:14:55.247: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 25 04:14:57.254: INFO: Creating deployment "test-rollover-deployment"
May 25 04:14:57.272: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 25 04:14:59.287: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 25 04:14:59.298: INFO: Ensure that both replica sets have 1 created replica
May 25 04:14:59.307: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 25 04:14:59.322: INFO: Updating deployment test-rollover-deployment
May 25 04:14:59.322: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 25 04:15:01.342: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 25 04:15:01.352: INFO: Make sure deployment "test-rollover-deployment" is complete
May 25 04:15:01.362: INFO: all replica sets need to contain the pod-template-hash label
May 25 04:15:01.362: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512899, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 04:15:03.378: INFO: all replica sets need to contain the pod-template-hash label
May 25 04:15:03.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512901, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 04:15:05.376: INFO: all replica sets need to contain the pod-template-hash label
May 25 04:15:05.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512901, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 04:15:07.378: INFO: all replica sets need to contain the pod-template-hash label
May 25 04:15:07.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512901, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 04:15:09.377: INFO: all replica sets need to contain the pod-template-hash label
May 25 04:15:09.377: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512901, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 04:15:11.379: INFO: all replica sets need to contain the pod-template-hash label
May 25 04:15:11.379: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512901, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757512897, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 25 04:15:13.377: INFO: 
May 25 04:15:13.377: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 25 04:15:13.396: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6196  2a162e8b-e877-43c9-9e12-0afa1e7faaa8 861262 2 2021-05-25 04:14:57 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-25 04:14:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-25 04:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00787d5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-25 04:14:57 +0000 UTC,LastTransitionTime:2021-05-25 04:14:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-05-25 04:15:11 +0000 UTC,LastTransitionTime:2021-05-25 04:14:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 25 04:15:13.400: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-6196  e107f595-2151-4174-ae80-f5e03db5726a 861251 2 2021-05-25 04:14:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 2a162e8b-e877-43c9-9e12-0afa1e7faaa8 0xc007764300 0xc007764301}] []  [{kube-controller-manager Update apps/v1 2021-05-25 04:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a162e8b-e877-43c9-9e12-0afa1e7faaa8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007764398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 25 04:15:13.400: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 25 04:15:13.401: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6196  3d6c0fab-29df-4318-a100-51e51279c5cd 861260 2 2021-05-25 04:14:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 2a162e8b-e877-43c9-9e12-0afa1e7faaa8 0xc0077640f7 0xc0077640f8}] []  [{e2e.test Update apps/v1 2021-05-25 04:14:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-25 04:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a162e8b-e877-43c9-9e12-0afa1e7faaa8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007764198 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 25 04:15:13.401: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-6196  67fbb356-e968-4ae7-b986-11f0fd1715db 861198 2 2021-05-25 04:14:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 2a162e8b-e877-43c9-9e12-0afa1e7faaa8 0xc007764207 0xc007764208}] []  [{kube-controller-manager Update apps/v1 2021-05-25 04:14:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a162e8b-e877-43c9-9e12-0afa1e7faaa8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007764298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 25 04:15:13.405: INFO: Pod "test-rollover-deployment-98c5f4599-d48mq" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-d48mq test-rollover-deployment-98c5f4599- deployment-6196  203f97cf-826e-477b-b177-b7f162538a86 861224 0 2021-05-25 04:14:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/podIP:192.168.31.93/32 cni.projectcalico.org/podIPs:192.168.31.93/32] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 e107f595-2151-4174-ae80-f5e03db5726a 0xc00787d9c0 0xc00787d9c1}] []  [{kube-controller-manager Update v1 2021-05-25 04:14:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e107f595-2151-4174-ae80-f5e03db5726a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 04:15:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 04:15:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.31.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pw7hr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pw7hr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:14:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:15:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:15:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:14:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:192.168.31.93,StartTime:2021-05-25 04:14:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 04:15:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker://sha256:a51db9feaeb94e79df8275b6c4921047efdd40b23de5153754aedde5fd42a900,ContainerID:docker://3520e751b73ae524d9c7641ccf9984bf2ace8d057c68e9731e5a1e3f356cb9a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.31.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:15:13.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6196" for this suite.

• [SLOW TEST:23.237 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":339,"completed":290,"skipped":4447,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:15:13.422: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
May 25 04:15:13.485: INFO: Waiting up to 5m0s for pod "security-context-a16f6fad-15ec-4e9d-99c3-e1e493f66afd" in namespace "security-context-297" to be "Succeeded or Failed"
May 25 04:15:13.490: INFO: Pod "security-context-a16f6fad-15ec-4e9d-99c3-e1e493f66afd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.252333ms
May 25 04:15:15.498: INFO: Pod "security-context-a16f6fad-15ec-4e9d-99c3-e1e493f66afd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012091749s
STEP: Saw pod success
May 25 04:15:15.498: INFO: Pod "security-context-a16f6fad-15ec-4e9d-99c3-e1e493f66afd" satisfied condition "Succeeded or Failed"
May 25 04:15:15.502: INFO: Trying to get logs from node 10.10.103.202-worker pod security-context-a16f6fad-15ec-4e9d-99c3-e1e493f66afd container test-container: <nil>
STEP: delete the pod
May 25 04:15:15.528: INFO: Waiting for pod security-context-a16f6fad-15ec-4e9d-99c3-e1e493f66afd to disappear
May 25 04:15:15.533: INFO: Pod security-context-a16f6fad-15ec-4e9d-99c3-e1e493f66afd no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:15:15.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-297" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":291,"skipped":4480,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:15:15.547: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
May 25 04:15:15.634: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
May 25 04:15:15.661: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:15:15.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6285" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":339,"completed":292,"skipped":4500,"failed":0}
SSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:15:15.710: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-9335/secret-test-fb22a2a1-d8c6-4202-9d57-2ed64bb8f430
STEP: Creating a pod to test consume secrets
May 25 04:15:15.774: INFO: Waiting up to 5m0s for pod "pod-configmaps-d10bbd27-d74f-4a5f-8618-6e601f1863da" in namespace "secrets-9335" to be "Succeeded or Failed"
May 25 04:15:15.779: INFO: Pod "pod-configmaps-d10bbd27-d74f-4a5f-8618-6e601f1863da": Phase="Pending", Reason="", readiness=false. Elapsed: 5.433313ms
May 25 04:15:17.789: INFO: Pod "pod-configmaps-d10bbd27-d74f-4a5f-8618-6e601f1863da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015260931s
May 25 04:15:19.797: INFO: Pod "pod-configmaps-d10bbd27-d74f-4a5f-8618-6e601f1863da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023154912s
STEP: Saw pod success
May 25 04:15:19.797: INFO: Pod "pod-configmaps-d10bbd27-d74f-4a5f-8618-6e601f1863da" satisfied condition "Succeeded or Failed"
May 25 04:15:19.801: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-configmaps-d10bbd27-d74f-4a5f-8618-6e601f1863da container env-test: <nil>
STEP: delete the pod
May 25 04:15:19.836: INFO: Waiting for pod pod-configmaps-d10bbd27-d74f-4a5f-8618-6e601f1863da to disappear
May 25 04:15:19.839: INFO: Pod pod-configmaps-d10bbd27-d74f-4a5f-8618-6e601f1863da no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:15:19.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9335" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":293,"skipped":4507,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:15:19.851: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6907
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 25 04:15:19.904: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 25 04:15:19.942: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:15:21.952: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:15:23.951: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:15:25.954: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:15:27.954: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:15:29.953: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:15:31.952: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:15:33.954: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:15:35.951: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:15:37.953: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:15:39.983: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:15:41.954: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 25 04:15:41.961: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 25 04:15:46.015: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 25 04:15:46.015: INFO: Going to poll 192.168.171.140 on port 8081 at least 0 times, with a maximum of 34 tries before failing
May 25 04:15:46.020: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.171.140 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6907 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:15:46.020: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:15:47.182: INFO: Found all 1 expected endpoints: [netserver-0]
May 25 04:15:47.182: INFO: Going to poll 192.168.31.114 on port 8081 at least 0 times, with a maximum of 34 tries before failing
May 25 04:15:47.195: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.31.114 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6907 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:15:47.195: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:15:48.353: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:15:48.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6907" for this suite.

• [SLOW TEST:28.523 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":294,"skipped":4550,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:15:48.375: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 25 04:15:48.438: INFO: Waiting up to 5m0s for pod "pod-bc1067d9-89bf-4bb4-954b-b3e1291660a9" in namespace "emptydir-4447" to be "Succeeded or Failed"
May 25 04:15:48.444: INFO: Pod "pod-bc1067d9-89bf-4bb4-954b-b3e1291660a9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.369037ms
May 25 04:15:50.551: INFO: Pod "pod-bc1067d9-89bf-4bb4-954b-b3e1291660a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.112063113s
May 25 04:15:52.564: INFO: Pod "pod-bc1067d9-89bf-4bb4-954b-b3e1291660a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.125528763s
STEP: Saw pod success
May 25 04:15:52.564: INFO: Pod "pod-bc1067d9-89bf-4bb4-954b-b3e1291660a9" satisfied condition "Succeeded or Failed"
May 25 04:15:52.568: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-bc1067d9-89bf-4bb4-954b-b3e1291660a9 container test-container: <nil>
STEP: delete the pod
May 25 04:15:52.604: INFO: Waiting for pod pod-bc1067d9-89bf-4bb4-954b-b3e1291660a9 to disappear
May 25 04:15:52.616: INFO: Pod pod-bc1067d9-89bf-4bb4-954b-b3e1291660a9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:15:52.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4447" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":295,"skipped":4552,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:15:52.628: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 25 04:15:56.719: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5842 PodName:var-expansion-595608ea-9241-403c-92d4-2d9de58641a3 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:15:56.719: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: test for file in mounted path
May 25 04:15:56.878: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5842 PodName:var-expansion-595608ea-9241-403c-92d4-2d9de58641a3 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:15:56.878: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: updating the annotation value
May 25 04:15:57.556: INFO: Successfully updated pod "var-expansion-595608ea-9241-403c-92d4-2d9de58641a3"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 25 04:15:57.561: INFO: Deleting pod "var-expansion-595608ea-9241-403c-92d4-2d9de58641a3" in namespace "var-expansion-5842"
May 25 04:15:57.568: INFO: Wait up to 5m0s for pod "var-expansion-595608ea-9241-403c-92d4-2d9de58641a3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:16:31.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5842" for this suite.

• [SLOW TEST:38.968 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":339,"completed":296,"skipped":4552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:16:31.597: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-00f62ba3-0012-499a-949b-cfed80793afa
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:16:35.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-34" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":297,"skipped":4585,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:16:35.728: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-7570/configmap-test-87189f35-0eb4-4a34-8ba6-c80e4ff4e252
STEP: Creating a pod to test consume configMaps
May 25 04:16:35.792: INFO: Waiting up to 5m0s for pod "pod-configmaps-5543cb5c-db82-473f-b4ef-75b7789c0bcd" in namespace "configmap-7570" to be "Succeeded or Failed"
May 25 04:16:35.798: INFO: Pod "pod-configmaps-5543cb5c-db82-473f-b4ef-75b7789c0bcd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033165ms
May 25 04:16:37.807: INFO: Pod "pod-configmaps-5543cb5c-db82-473f-b4ef-75b7789c0bcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015342094s
May 25 04:16:39.816: INFO: Pod "pod-configmaps-5543cb5c-db82-473f-b4ef-75b7789c0bcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02398425s
STEP: Saw pod success
May 25 04:16:39.816: INFO: Pod "pod-configmaps-5543cb5c-db82-473f-b4ef-75b7789c0bcd" satisfied condition "Succeeded or Failed"
May 25 04:16:39.823: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-configmaps-5543cb5c-db82-473f-b4ef-75b7789c0bcd container env-test: <nil>
STEP: delete the pod
May 25 04:16:39.856: INFO: Waiting for pod pod-configmaps-5543cb5c-db82-473f-b4ef-75b7789c0bcd to disappear
May 25 04:16:39.860: INFO: Pod pod-configmaps-5543cb5c-db82-473f-b4ef-75b7789c0bcd no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:16:39.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7570" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":339,"completed":298,"skipped":4590,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:16:39.874: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 25 04:16:39.930: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 25 04:16:39.937: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 25 04:16:39.937: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 25 04:16:39.951: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 25 04:16:39.951: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 25 04:16:39.963: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 25 04:16:39.963: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 25 04:16:47.014: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:16:47.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4072" for this suite.

• [SLOW TEST:7.176 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":339,"completed":299,"skipped":4598,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:16:47.050: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
May 25 04:18:47.662: INFO: Successfully updated pod "var-expansion-83100ddf-05ef-4901-8f02-6c0673951f1d"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 25 04:18:49.693: INFO: Deleting pod "var-expansion-83100ddf-05ef-4901-8f02-6c0673951f1d" in namespace "var-expansion-6120"
May 25 04:18:49.704: INFO: Wait up to 5m0s for pod "var-expansion-83100ddf-05ef-4901-8f02-6c0673951f1d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:19:23.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6120" for this suite.

• [SLOW TEST:156.686 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":339,"completed":300,"skipped":4600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:19:23.738: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
W0525 04:19:23.796386      23 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: getting
STEP: listing
STEP: watching
May 25 04:19:23.812: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 25 04:19:23.822: INFO: starting watch
STEP: patching
STEP: updating
May 25 04:19:23.846: INFO: waiting for watch events with expected annotations
May 25 04:19:23.846: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:19:23.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8891" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":339,"completed":301,"skipped":4622,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:19:23.916: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-711
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 25 04:19:23.963: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 25 04:19:23.995: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:19:26.004: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:19:28.007: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:19:30.007: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:19:32.007: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:19:34.006: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:19:36.007: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:19:38.006: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:19:40.008: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:19:42.005: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:19:44.006: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 25 04:19:46.004: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 25 04:19:46.013: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
May 25 04:19:50.069: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
May 25 04:19:50.069: INFO: Going to poll 192.168.171.189 on port 8080 at least 0 times, with a maximum of 34 tries before failing
May 25 04:19:50.072: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.171.189:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-711 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:19:50.072: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:19:50.258: INFO: Found all 1 expected endpoints: [netserver-0]
May 25 04:19:50.258: INFO: Going to poll 192.168.31.123 on port 8080 at least 0 times, with a maximum of 34 tries before failing
May 25 04:19:50.264: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.31.123:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-711 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:19:50.264: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:19:50.419: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:19:50.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-711" for this suite.

• [SLOW TEST:26.518 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":302,"skipped":4630,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:19:50.435: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-0cf8f31a-2ee0-4c84-a2c3-501e836a1e2b
STEP: Creating a pod to test consume configMaps
May 25 04:19:50.514: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-116127e1-fec6-4bb9-9189-6262b36d903f" in namespace "projected-8821" to be "Succeeded or Failed"
May 25 04:19:50.518: INFO: Pod "pod-projected-configmaps-116127e1-fec6-4bb9-9189-6262b36d903f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.671092ms
May 25 04:19:52.527: INFO: Pod "pod-projected-configmaps-116127e1-fec6-4bb9-9189-6262b36d903f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012777116s
STEP: Saw pod success
May 25 04:19:52.527: INFO: Pod "pod-projected-configmaps-116127e1-fec6-4bb9-9189-6262b36d903f" satisfied condition "Succeeded or Failed"
May 25 04:19:52.531: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-projected-configmaps-116127e1-fec6-4bb9-9189-6262b36d903f container agnhost-container: <nil>
STEP: delete the pod
May 25 04:19:52.575: INFO: Waiting for pod pod-projected-configmaps-116127e1-fec6-4bb9-9189-6262b36d903f to disappear
May 25 04:19:52.580: INFO: Pod pod-projected-configmaps-116127e1-fec6-4bb9-9189-6262b36d903f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:19:52.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8821" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":303,"skipped":4634,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:19:52.593: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-5d3cab30-4e0f-444c-a8b2-a852de1fa625
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:19:52.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-537" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":339,"completed":304,"skipped":4639,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:19:52.676: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 25 04:19:54.757: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:19:54.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3030" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":305,"skipped":4649,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:19:54.787: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:19:54.842: INFO: Creating simple deployment test-new-deployment
May 25 04:19:54.857: INFO: deployment "test-new-deployment" doesn't have the required revision set
May 25 04:19:56.870: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757513194, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757513194, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757513194, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757513194, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-847dcfb7fb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 25 04:19:58.925: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1227  bc18b8a0-2a76-4938-becc-e81c6dad1d8f 862629 3 2021-05-25 04:19:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-05-25 04:19:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-25 04:19:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005162488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-25 04:19:57 +0000 UTC,LastTransitionTime:2021-05-25 04:19:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-05-25 04:19:57 +0000 UTC,LastTransitionTime:2021-05-25 04:19:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 25 04:19:58.946: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-1227  7849442b-8720-4ac1-a535-ba7361fc0a84 862630 2 2021-05-25 04:19:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment bc18b8a0-2a76-4938-becc-e81c6dad1d8f 0xc005162897 0xc005162898}] []  [{kube-controller-manager Update apps/v1 2021-05-25 04:19:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc18b8a0-2a76-4938-becc-e81c6dad1d8f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005162908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 25 04:19:58.953: INFO: Pod "test-new-deployment-847dcfb7fb-mlf7h" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-mlf7h test-new-deployment-847dcfb7fb- deployment-1227  0d64a5fa-12be-412c-9c67-6dab64c15d03 862605 0 2021-05-25 04:19:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:192.168.31.124/32 cni.projectcalico.org/podIPs:192.168.31.124/32] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 7849442b-8720-4ac1-a535-ba7361fc0a84 0xc003f52c57 0xc003f52c58}] []  [{kube-controller-manager Update v1 2021-05-25 04:19:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7849442b-8720-4ac1-a535-ba7361fc0a84\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 04:19:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 04:19:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.31.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9twn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9twn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:19:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:19:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:19:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:192.168.31.124,StartTime:2021-05-25 04:19:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 04:19:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://bcd1334e5c4212d923956f2ac9c320bd9d480cc9b97fa64c3a19f6000c0d4842,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.31.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:19:58.953: INFO: Pod "test-new-deployment-847dcfb7fb-xhl72" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-xhl72 test-new-deployment-847dcfb7fb- deployment-1227  2b882ff0-0206-4df5-85c9-764621f3a9d1 862633 0 2021-05-25 04:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 7849442b-8720-4ac1-a535-ba7361fc0a84 0xc003f52e57 0xc003f52e58}] []  [{kube-controller-manager Update v1 2021-05-25 04:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7849442b-8720-4ac1-a535-ba7361fc0a84\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqtgf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqtgf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:19:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:19:58.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1227" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":339,"completed":306,"skipped":4658,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:19:58.970: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1308
STEP: creating the pod
May 25 04:19:59.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3793 create -f -'
May 25 04:19:59.711: INFO: stderr: ""
May 25 04:19:59.711: INFO: stdout: "pod/pause created\n"
May 25 04:19:59.711: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 25 04:19:59.711: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3793" to be "running and ready"
May 25 04:19:59.718: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.740101ms
May 25 04:20:01.723: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011593296s
May 25 04:20:01.723: INFO: Pod "pause" satisfied condition "running and ready"
May 25 04:20:01.723: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
May 25 04:20:01.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3793 label pods pause testing-label=testing-label-value'
May 25 04:20:01.943: INFO: stderr: ""
May 25 04:20:01.944: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 25 04:20:01.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3793 get pod pause -L testing-label'
May 25 04:20:02.139: INFO: stderr: ""
May 25 04:20:02.139: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 25 04:20:02.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3793 label pods pause testing-label-'
May 25 04:20:02.365: INFO: stderr: ""
May 25 04:20:02.365: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 25 04:20:02.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3793 get pod pause -L testing-label'
May 25 04:20:02.559: INFO: stderr: ""
May 25 04:20:02.559: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: using delete to clean up resources
May 25 04:20:02.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3793 delete --grace-period=0 --force -f -'
May 25 04:20:02.771: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 25 04:20:02.772: INFO: stdout: "pod \"pause\" force deleted\n"
May 25 04:20:02.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3793 get rc,svc -l name=pause --no-headers'
May 25 04:20:02.979: INFO: stderr: "No resources found in kubectl-3793 namespace.\n"
May 25 04:20:02.979: INFO: stdout: ""
May 25 04:20:02.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-3793 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 25 04:20:03.184: INFO: stderr: ""
May 25 04:20:03.184: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:20:03.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3793" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":339,"completed":307,"skipped":4658,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:20:03.222: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
May 25 04:20:03.297: INFO: Waiting up to 5m0s for pod "client-containers-b7cfe4ef-5ac9-478e-871a-58412692d864" in namespace "containers-6112" to be "Succeeded or Failed"
May 25 04:20:03.301: INFO: Pod "client-containers-b7cfe4ef-5ac9-478e-871a-58412692d864": Phase="Pending", Reason="", readiness=false. Elapsed: 4.208066ms
May 25 04:20:05.307: INFO: Pod "client-containers-b7cfe4ef-5ac9-478e-871a-58412692d864": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010093166s
May 25 04:20:07.323: INFO: Pod "client-containers-b7cfe4ef-5ac9-478e-871a-58412692d864": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025867383s
STEP: Saw pod success
May 25 04:20:07.323: INFO: Pod "client-containers-b7cfe4ef-5ac9-478e-871a-58412692d864" satisfied condition "Succeeded or Failed"
May 25 04:20:07.327: INFO: Trying to get logs from node 10.10.103.202-worker pod client-containers-b7cfe4ef-5ac9-478e-871a-58412692d864 container agnhost-container: <nil>
STEP: delete the pod
May 25 04:20:07.363: INFO: Waiting for pod client-containers-b7cfe4ef-5ac9-478e-871a-58412692d864 to disappear
May 25 04:20:07.366: INFO: Pod client-containers-b7cfe4ef-5ac9-478e-871a-58412692d864 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:20:07.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6112" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":339,"completed":308,"skipped":4676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:20:07.379: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:20:23.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6368" for this suite.

• [SLOW TEST:16.282 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":339,"completed":309,"skipped":4722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:20:23.663: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:20:27.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7392" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":339,"completed":310,"skipped":4773,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:20:28.008: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May 25 04:20:28.070: INFO: Waiting up to 5m0s for pod "downward-api-0e4a9de6-d59d-4733-9970-b4a679ed7033" in namespace "downward-api-4563" to be "Succeeded or Failed"
May 25 04:20:28.076: INFO: Pod "downward-api-0e4a9de6-d59d-4733-9970-b4a679ed7033": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071045ms
May 25 04:20:30.080: INFO: Pod "downward-api-0e4a9de6-d59d-4733-9970-b4a679ed7033": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010429168s
STEP: Saw pod success
May 25 04:20:30.080: INFO: Pod "downward-api-0e4a9de6-d59d-4733-9970-b4a679ed7033" satisfied condition "Succeeded or Failed"
May 25 04:20:30.084: INFO: Trying to get logs from node 10.10.103.202-worker pod downward-api-0e4a9de6-d59d-4733-9970-b4a679ed7033 container dapi-container: <nil>
STEP: delete the pod
May 25 04:20:30.116: INFO: Waiting for pod downward-api-0e4a9de6-d59d-4733-9970-b4a679ed7033 to disappear
May 25 04:20:30.121: INFO: Pod downward-api-0e4a9de6-d59d-4733-9970-b4a679ed7033 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:20:30.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4563" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":339,"completed":311,"skipped":4779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:20:30.133: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:20:31.541: INFO: Checking APIGroup: apiregistration.k8s.io
May 25 04:20:31.542: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 25 04:20:31.542: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.542: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 25 04:20:31.542: INFO: Checking APIGroup: apps
May 25 04:20:31.543: INFO: PreferredVersion.GroupVersion: apps/v1
May 25 04:20:31.543: INFO: Versions found [{apps/v1 v1}]
May 25 04:20:31.543: INFO: apps/v1 matches apps/v1
May 25 04:20:31.543: INFO: Checking APIGroup: events.k8s.io
May 25 04:20:31.544: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 25 04:20:31.544: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.544: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 25 04:20:31.544: INFO: Checking APIGroup: authentication.k8s.io
May 25 04:20:31.545: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 25 04:20:31.545: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.545: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 25 04:20:31.545: INFO: Checking APIGroup: authorization.k8s.io
May 25 04:20:31.546: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 25 04:20:31.546: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.546: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 25 04:20:31.546: INFO: Checking APIGroup: autoscaling
May 25 04:20:31.546: INFO: PreferredVersion.GroupVersion: autoscaling/v1
May 25 04:20:31.546: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 25 04:20:31.546: INFO: autoscaling/v1 matches autoscaling/v1
May 25 04:20:31.546: INFO: Checking APIGroup: batch
May 25 04:20:31.547: INFO: PreferredVersion.GroupVersion: batch/v1
May 25 04:20:31.547: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 25 04:20:31.547: INFO: batch/v1 matches batch/v1
May 25 04:20:31.547: INFO: Checking APIGroup: certificates.k8s.io
May 25 04:20:31.548: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 25 04:20:31.548: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.548: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 25 04:20:31.548: INFO: Checking APIGroup: networking.k8s.io
May 25 04:20:31.549: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 25 04:20:31.549: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.549: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 25 04:20:31.549: INFO: Checking APIGroup: extensions
May 25 04:20:31.550: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
May 25 04:20:31.550: INFO: Versions found [{extensions/v1beta1 v1beta1}]
May 25 04:20:31.550: INFO: extensions/v1beta1 matches extensions/v1beta1
May 25 04:20:31.550: INFO: Checking APIGroup: policy
May 25 04:20:31.550: INFO: PreferredVersion.GroupVersion: policy/v1
May 25 04:20:31.550: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
May 25 04:20:31.550: INFO: policy/v1 matches policy/v1
May 25 04:20:31.550: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 25 04:20:31.551: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 25 04:20:31.551: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.551: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 25 04:20:31.551: INFO: Checking APIGroup: storage.k8s.io
May 25 04:20:31.552: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 25 04:20:31.552: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.552: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 25 04:20:31.552: INFO: Checking APIGroup: admissionregistration.k8s.io
May 25 04:20:31.552: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 25 04:20:31.552: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.552: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 25 04:20:31.552: INFO: Checking APIGroup: apiextensions.k8s.io
May 25 04:20:31.553: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 25 04:20:31.553: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.553: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 25 04:20:31.553: INFO: Checking APIGroup: scheduling.k8s.io
May 25 04:20:31.554: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 25 04:20:31.554: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.554: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 25 04:20:31.554: INFO: Checking APIGroup: coordination.k8s.io
May 25 04:20:31.555: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 25 04:20:31.555: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.555: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 25 04:20:31.555: INFO: Checking APIGroup: node.k8s.io
May 25 04:20:31.555: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 25 04:20:31.555: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.555: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 25 04:20:31.555: INFO: Checking APIGroup: discovery.k8s.io
May 25 04:20:31.556: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May 25 04:20:31.556: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.556: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May 25 04:20:31.556: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 25 04:20:31.557: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
May 25 04:20:31.557: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May 25 04:20:31.557: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
May 25 04:20:31.557: INFO: Checking APIGroup: crd.projectcalico.org
May 25 04:20:31.558: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
May 25 04:20:31.558: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
May 25 04:20:31.558: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
May 25 04:20:31.558: INFO: Checking APIGroup: operator.tigera.io
May 25 04:20:31.559: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
May 25 04:20:31.559: INFO: Versions found [{operator.tigera.io/v1 v1}]
May 25 04:20:31.559: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:20:31.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5583" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":339,"completed":312,"skipped":4813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:20:31.569: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 25 04:20:31.597: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 25 04:20:31.602: INFO: Waiting for terminating namespaces to be deleted...
May 25 04:20:31.604: INFO: 
Logging pods the apiserver thinks is on node 10.10.103.201-worker before test
May 25 04:20:31.611: INFO: calico-kube-controllers-867fdc777c-xgnbl from calico-system started at 2021-05-25 03:19:31 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.611: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 25 04:20:31.611: INFO: calico-node-gf4hs from calico-system started at 2021-05-21 05:16:19 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.611: INFO: 	Container calico-node ready: true, restart count 0
May 25 04:20:31.611: INFO: calico-typha-584858976f-vj6gj from calico-system started at 2021-05-21 05:16:25 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.611: INFO: 	Container calico-typha ready: true, restart count 0
May 25 04:20:31.611: INFO: coredns-768c7df75d-h7src from kube-system started at 2021-05-25 03:19:31 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.611: INFO: 	Container coredns ready: true, restart count 0
May 25 04:20:31.611: INFO: coredns-768c7df75d-s7x9f from kube-system started at 2021-05-25 03:19:31 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.611: INFO: 	Container coredns ready: true, restart count 0
May 25 04:20:31.611: INFO: kube-proxy-s6ql6 from kube-system started at 2021-05-21 05:14:25 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.611: INFO: 	Container kube-proxy ready: true, restart count 1
May 25 04:20:31.611: INFO: sonobuoy from sonobuoy started at 2021-05-25 02:26:58 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.611: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 25 04:20:31.611: INFO: sonobuoy-e2e-job-58d7b3bc039b4826 from sonobuoy started at 2021-05-25 02:27:00 +0000 UTC (2 container statuses recorded)
May 25 04:20:31.611: INFO: 	Container e2e ready: true, restart count 0
May 25 04:20:31.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 04:20:31.611: INFO: sonobuoy-systemd-logs-daemon-set-ebbefc146b074e77-p6vlh from sonobuoy started at 2021-05-25 02:27:00 +0000 UTC (2 container statuses recorded)
May 25 04:20:31.611: INFO: 	Container sonobuoy-worker ready: false, restart count 15
May 25 04:20:31.611: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 04:20:31.611: INFO: tigera-operator-5b47ff5dcc-25sz7 from tigera-operator started at 2021-05-21 05:15:37 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.611: INFO: 	Container tigera-operator ready: true, restart count 2
May 25 04:20:31.611: INFO: 
Logging pods the apiserver thinks is on node 10.10.103.202-worker before test
May 25 04:20:31.616: INFO: calico-node-k6h5c from calico-system started at 2021-05-21 05:16:19 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.616: INFO: 	Container calico-node ready: true, restart count 0
May 25 04:20:31.616: INFO: calico-typha-584858976f-x82v6 from calico-system started at 2021-05-21 05:16:18 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.616: INFO: 	Container calico-typha ready: true, restart count 0
May 25 04:20:31.616: INFO: kube-proxy-fvq49 from kube-system started at 2021-05-21 05:14:38 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.616: INFO: 	Container kube-proxy ready: true, restart count 0
May 25 04:20:31.616: INFO: rc-test-c4d9q from replication-controller-7392 started at 2021-05-25 04:20:23 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.616: INFO: 	Container rc-test ready: true, restart count 0
May 25 04:20:31.616: INFO: rc-test-nfdzd from replication-controller-7392 started at 2021-05-25 04:20:25 +0000 UTC (1 container statuses recorded)
May 25 04:20:31.616: INFO: 	Container rc-test ready: true, restart count 0
May 25 04:20:31.616: INFO: sonobuoy-systemd-logs-daemon-set-ebbefc146b074e77-kj9h6 from sonobuoy started at 2021-05-25 02:27:00 +0000 UTC (2 container statuses recorded)
May 25 04:20:31.616: INFO: 	Container sonobuoy-worker ready: false, restart count 15
May 25 04:20:31.616: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16823466bb00714b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:20:32.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4216" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":339,"completed":313,"skipped":4870,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:20:32.662: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:20:32.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9646" for this suite.
STEP: Destroying namespace "nspatchtest-44447867-def1-4879-9994-ad8284bcbece-6276" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":339,"completed":314,"skipped":4877,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:20:32.788: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 04:20:34.245: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 25 04:20:36.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757513234, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757513234, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757513234, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757513234, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 04:20:39.280: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:20:39.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9968" for this suite.
STEP: Destroying namespace "webhook-9968-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.683 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":339,"completed":315,"skipped":4878,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:20:39.471: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:20:43.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3087" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":316,"skipped":4903,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:20:43.597: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:20:47.682: INFO: Deleting pod "var-expansion-610c7354-2369-4929-84a2-e1998b0c5bb3" in namespace "var-expansion-2146"
May 25 04:20:47.692: INFO: Wait up to 5m0s for pod "var-expansion-610c7354-2369-4929-84a2-e1998b0c5bb3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:20:59.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2146" for this suite.

• [SLOW TEST:16.128 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":339,"completed":317,"skipped":4924,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:20:59.726: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:20:59.829: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"251a58e4-1b7d-4b15-9725-b61aa5841f1c", Controller:(*bool)(0xc00437f4b2), BlockOwnerDeletion:(*bool)(0xc00437f4b3)}}
May 25 04:20:59.840: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ee3818fd-cbea-4563-82f7-92a4355ab0f0", Controller:(*bool)(0xc00437f6ea), BlockOwnerDeletion:(*bool)(0xc00437f6eb)}}
May 25 04:20:59.851: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"783b6ca5-d2c3-4e9e-ac7a-bd5fdcfe9366", Controller:(*bool)(0xc0008b6aba), BlockOwnerDeletion:(*bool)(0xc0008b6abb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:21:04.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5887" for this suite.

• [SLOW TEST:5.156 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":339,"completed":318,"skipped":4926,"failed":0}
SS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:21:04.882: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
May 25 04:21:04.948: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May 25 04:21:06.954: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May 25 04:21:08.958: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
May 25 04:21:08.977: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May 25 04:21:10.986: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May 25 04:21:12.986: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 25 04:21:12.990: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-152 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:21:12.990: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:21:13.147: INFO: Exec stderr: ""
May 25 04:21:13.147: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-152 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:21:13.147: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:21:13.294: INFO: Exec stderr: ""
May 25 04:21:13.294: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-152 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:21:13.294: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:21:13.445: INFO: Exec stderr: ""
May 25 04:21:13.446: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-152 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:21:13.446: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:21:13.598: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 25 04:21:13.598: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-152 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:21:13.598: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:21:13.751: INFO: Exec stderr: ""
May 25 04:21:13.751: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-152 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:21:13.751: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:21:13.917: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 25 04:21:13.917: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-152 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:21:13.917: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:21:14.068: INFO: Exec stderr: ""
May 25 04:21:14.068: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-152 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:21:14.068: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:21:14.221: INFO: Exec stderr: ""
May 25 04:21:14.221: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-152 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:21:14.221: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:21:14.378: INFO: Exec stderr: ""
May 25 04:21:14.378: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-152 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 25 04:21:14.378: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
May 25 04:21:14.541: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:21:14.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-152" for this suite.

• [SLOW TEST:9.677 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":319,"skipped":4928,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:21:14.560: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
May 25 04:21:14.628: INFO: Waiting up to 5m0s for pod "var-expansion-20af7393-7dc0-4a46-abca-bce6be8fd1c8" in namespace "var-expansion-899" to be "Succeeded or Failed"
May 25 04:21:14.632: INFO: Pod "var-expansion-20af7393-7dc0-4a46-abca-bce6be8fd1c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.431831ms
May 25 04:21:16.641: INFO: Pod "var-expansion-20af7393-7dc0-4a46-abca-bce6be8fd1c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013428936s
STEP: Saw pod success
May 25 04:21:16.641: INFO: Pod "var-expansion-20af7393-7dc0-4a46-abca-bce6be8fd1c8" satisfied condition "Succeeded or Failed"
May 25 04:21:16.645: INFO: Trying to get logs from node 10.10.103.202-worker pod var-expansion-20af7393-7dc0-4a46-abca-bce6be8fd1c8 container dapi-container: <nil>
STEP: delete the pod
May 25 04:21:16.671: INFO: Waiting for pod var-expansion-20af7393-7dc0-4a46-abca-bce6be8fd1c8 to disappear
May 25 04:21:16.674: INFO: Pod var-expansion-20af7393-7dc0-4a46-abca-bce6be8fd1c8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:21:16.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-899" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":339,"completed":320,"skipped":4934,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:21:16.689: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
May 25 04:21:16.752: INFO: Waiting up to 5m0s for pod "test-pod-340f4735-8a21-4610-8427-f7100e708ff1" in namespace "svcaccounts-2803" to be "Succeeded or Failed"
May 25 04:21:16.757: INFO: Pod "test-pod-340f4735-8a21-4610-8427-f7100e708ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.131608ms
May 25 04:21:18.763: INFO: Pod "test-pod-340f4735-8a21-4610-8427-f7100e708ff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010673266s
STEP: Saw pod success
May 25 04:21:18.763: INFO: Pod "test-pod-340f4735-8a21-4610-8427-f7100e708ff1" satisfied condition "Succeeded or Failed"
May 25 04:21:18.771: INFO: Trying to get logs from node 10.10.103.202-worker pod test-pod-340f4735-8a21-4610-8427-f7100e708ff1 container agnhost-container: <nil>
STEP: delete the pod
May 25 04:21:18.794: INFO: Waiting for pod test-pod-340f4735-8a21-4610-8427-f7100e708ff1 to disappear
May 25 04:21:18.799: INFO: Pod test-pod-340f4735-8a21-4610-8427-f7100e708ff1 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:21:18.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2803" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":339,"completed":321,"skipped":4979,"failed":0}
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:21:18.811: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
May 25 04:21:18.858: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:21:23.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2747" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":339,"completed":322,"skipped":4983,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:21:23.041: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 04:21:23.107: INFO: Waiting up to 5m0s for pod "downwardapi-volume-deabccca-b3eb-4a47-a97a-bd2683aa6cf4" in namespace "projected-4111" to be "Succeeded or Failed"
May 25 04:21:23.112: INFO: Pod "downwardapi-volume-deabccca-b3eb-4a47-a97a-bd2683aa6cf4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.899671ms
May 25 04:21:25.117: INFO: Pod "downwardapi-volume-deabccca-b3eb-4a47-a97a-bd2683aa6cf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009774124s
STEP: Saw pod success
May 25 04:21:25.117: INFO: Pod "downwardapi-volume-deabccca-b3eb-4a47-a97a-bd2683aa6cf4" satisfied condition "Succeeded or Failed"
May 25 04:21:25.121: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-deabccca-b3eb-4a47-a97a-bd2683aa6cf4 container client-container: <nil>
STEP: delete the pod
May 25 04:21:25.148: INFO: Waiting for pod downwardapi-volume-deabccca-b3eb-4a47-a97a-bd2683aa6cf4 to disappear
May 25 04:21:25.152: INFO: Pod downwardapi-volume-deabccca-b3eb-4a47-a97a-bd2683aa6cf4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:21:25.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4111" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":323,"skipped":5002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:21:25.167: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
May 25 04:21:27.295: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:21:29.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9485" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":339,"completed":324,"skipped":5062,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:21:29.327: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
May 25 04:21:29.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6745 api-versions'
May 25 04:21:29.570: INFO: stderr: ""
May 25 04:21:29.570: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noperator.tigera.io/v1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:21:29.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6745" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":339,"completed":325,"skipped":5064,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:21:29.584: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-eafb2ade-495b-4883-a500-d82f67c0cea9
STEP: Creating secret with name s-test-opt-upd-e0eb36d5-e912-4f16-a63e-933f06465728
STEP: Creating the pod
May 25 04:21:29.664: INFO: The status of Pod pod-secrets-31ef7a9d-cc37-4cb7-b9f9-fc4cce0b7474 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:21:31.671: INFO: The status of Pod pod-secrets-31ef7a9d-cc37-4cb7-b9f9-fc4cce0b7474 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:21:33.673: INFO: The status of Pod pod-secrets-31ef7a9d-cc37-4cb7-b9f9-fc4cce0b7474 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-eafb2ade-495b-4883-a500-d82f67c0cea9
STEP: Updating secret s-test-opt-upd-e0eb36d5-e912-4f16-a63e-933f06465728
STEP: Creating secret with name s-test-opt-create-6b9ab604-a82f-4ed7-a987-e9493a486998
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:21:37.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2476" for this suite.

• [SLOW TEST:8.219 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":326,"skipped":5069,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:21:37.804: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 04:21:37.876: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b1fd4c2-3fc3-401f-ab41-fc4c74f4dc0f" in namespace "projected-3777" to be "Succeeded or Failed"
May 25 04:21:37.881: INFO: Pod "downwardapi-volume-6b1fd4c2-3fc3-401f-ab41-fc4c74f4dc0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.735418ms
May 25 04:21:39.896: INFO: Pod "downwardapi-volume-6b1fd4c2-3fc3-401f-ab41-fc4c74f4dc0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019500334s
STEP: Saw pod success
May 25 04:21:39.896: INFO: Pod "downwardapi-volume-6b1fd4c2-3fc3-401f-ab41-fc4c74f4dc0f" satisfied condition "Succeeded or Failed"
May 25 04:21:39.900: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-6b1fd4c2-3fc3-401f-ab41-fc4c74f4dc0f container client-container: <nil>
STEP: delete the pod
May 25 04:21:39.930: INFO: Waiting for pod downwardapi-volume-6b1fd4c2-3fc3-401f-ab41-fc4c74f4dc0f to disappear
May 25 04:21:39.934: INFO: Pod downwardapi-volume-6b1fd4c2-3fc3-401f-ab41-fc4c74f4dc0f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:21:39.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3777" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":327,"skipped":5098,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:21:39.953: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 25 04:21:40.024: INFO: The status of Pod pod-update-activedeadlineseconds-33f900f4-7568-4a7e-aa78-c81f26b37c2c is Pending, waiting for it to be Running (with Ready = true)
May 25 04:21:42.033: INFO: The status of Pod pod-update-activedeadlineseconds-33f900f4-7568-4a7e-aa78-c81f26b37c2c is Pending, waiting for it to be Running (with Ready = true)
May 25 04:21:44.032: INFO: The status of Pod pod-update-activedeadlineseconds-33f900f4-7568-4a7e-aa78-c81f26b37c2c is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 25 04:21:44.560: INFO: Successfully updated pod "pod-update-activedeadlineseconds-33f900f4-7568-4a7e-aa78-c81f26b37c2c"
May 25 04:21:44.560: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-33f900f4-7568-4a7e-aa78-c81f26b37c2c" in namespace "pods-6303" to be "terminated due to deadline exceeded"
May 25 04:21:44.566: INFO: Pod "pod-update-activedeadlineseconds-33f900f4-7568-4a7e-aa78-c81f26b37c2c": Phase="Running", Reason="", readiness=true. Elapsed: 5.904321ms
May 25 04:21:46.576: INFO: Pod "pod-update-activedeadlineseconds-33f900f4-7568-4a7e-aa78-c81f26b37c2c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.016114683s
May 25 04:21:46.577: INFO: Pod "pod-update-activedeadlineseconds-33f900f4-7568-4a7e-aa78-c81f26b37c2c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:21:46.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6303" for this suite.

• [SLOW TEST:6.638 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":339,"completed":328,"skipped":5133,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:21:46.591: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-l2dl
STEP: Creating a pod to test atomic-volume-subpath
May 25 04:21:46.661: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-l2dl" in namespace "subpath-1586" to be "Succeeded or Failed"
May 25 04:21:46.664: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369774ms
May 25 04:21:48.672: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Running", Reason="", readiness=true. Elapsed: 2.011094703s
May 25 04:21:50.686: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Running", Reason="", readiness=true. Elapsed: 4.025791237s
May 25 04:21:52.700: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Running", Reason="", readiness=true. Elapsed: 6.03936674s
May 25 04:21:54.800: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Running", Reason="", readiness=true. Elapsed: 8.139846287s
May 25 04:21:56.822: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Running", Reason="", readiness=true. Elapsed: 10.161208095s
May 25 04:21:58.845: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Running", Reason="", readiness=true. Elapsed: 12.1842308s
May 25 04:22:00.861: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Running", Reason="", readiness=true. Elapsed: 14.200310208s
May 25 04:22:02.875: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Running", Reason="", readiness=true. Elapsed: 16.214289115s
May 25 04:22:04.928: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Running", Reason="", readiness=true. Elapsed: 18.267356986s
May 25 04:22:06.949: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Running", Reason="", readiness=true. Elapsed: 20.288667642s
May 25 04:22:08.957: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Running", Reason="", readiness=true. Elapsed: 22.296206013s
May 25 04:22:10.971: INFO: Pod "pod-subpath-test-projected-l2dl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.310163396s
STEP: Saw pod success
May 25 04:22:10.971: INFO: Pod "pod-subpath-test-projected-l2dl" satisfied condition "Succeeded or Failed"
May 25 04:22:10.975: INFO: Trying to get logs from node 10.10.103.202-worker pod pod-subpath-test-projected-l2dl container test-container-subpath-projected-l2dl: <nil>
STEP: delete the pod
May 25 04:22:11.018: INFO: Waiting for pod pod-subpath-test-projected-l2dl to disappear
May 25 04:22:11.022: INFO: Pod pod-subpath-test-projected-l2dl no longer exists
STEP: Deleting pod pod-subpath-test-projected-l2dl
May 25 04:22:11.022: INFO: Deleting pod "pod-subpath-test-projected-l2dl" in namespace "subpath-1586"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:22:11.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1586" for this suite.

• [SLOW TEST:24.449 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":339,"completed":329,"skipped":5144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:22:11.041: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 25 04:22:11.091: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 25 04:22:11.106: INFO: Waiting for terminating namespaces to be deleted...
May 25 04:22:11.111: INFO: 
Logging pods the apiserver thinks is on node 10.10.103.201-worker before test
May 25 04:22:11.123: INFO: calico-kube-controllers-867fdc777c-xgnbl from calico-system started at 2021-05-25 03:19:31 +0000 UTC (1 container statuses recorded)
May 25 04:22:11.123: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 25 04:22:11.123: INFO: calico-node-gf4hs from calico-system started at 2021-05-21 05:16:19 +0000 UTC (1 container statuses recorded)
May 25 04:22:11.123: INFO: 	Container calico-node ready: true, restart count 0
May 25 04:22:11.123: INFO: calico-typha-584858976f-vj6gj from calico-system started at 2021-05-21 05:16:25 +0000 UTC (1 container statuses recorded)
May 25 04:22:11.123: INFO: 	Container calico-typha ready: true, restart count 0
May 25 04:22:11.123: INFO: coredns-768c7df75d-h7src from kube-system started at 2021-05-25 03:19:31 +0000 UTC (1 container statuses recorded)
May 25 04:22:11.123: INFO: 	Container coredns ready: true, restart count 0
May 25 04:22:11.123: INFO: coredns-768c7df75d-s7x9f from kube-system started at 2021-05-25 03:19:31 +0000 UTC (1 container statuses recorded)
May 25 04:22:11.123: INFO: 	Container coredns ready: true, restart count 0
May 25 04:22:11.123: INFO: kube-proxy-s6ql6 from kube-system started at 2021-05-21 05:14:25 +0000 UTC (1 container statuses recorded)
May 25 04:22:11.123: INFO: 	Container kube-proxy ready: true, restart count 1
May 25 04:22:11.123: INFO: sonobuoy from sonobuoy started at 2021-05-25 02:26:58 +0000 UTC (1 container statuses recorded)
May 25 04:22:11.123: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 25 04:22:11.123: INFO: sonobuoy-e2e-job-58d7b3bc039b4826 from sonobuoy started at 2021-05-25 02:27:00 +0000 UTC (2 container statuses recorded)
May 25 04:22:11.123: INFO: 	Container e2e ready: true, restart count 0
May 25 04:22:11.123: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 25 04:22:11.123: INFO: sonobuoy-systemd-logs-daemon-set-ebbefc146b074e77-p6vlh from sonobuoy started at 2021-05-25 02:27:00 +0000 UTC (2 container statuses recorded)
May 25 04:22:11.123: INFO: 	Container sonobuoy-worker ready: false, restart count 15
May 25 04:22:11.123: INFO: 	Container systemd-logs ready: true, restart count 0
May 25 04:22:11.123: INFO: tigera-operator-5b47ff5dcc-25sz7 from tigera-operator started at 2021-05-21 05:15:37 +0000 UTC (1 container statuses recorded)
May 25 04:22:11.123: INFO: 	Container tigera-operator ready: true, restart count 2
May 25 04:22:11.123: INFO: 
Logging pods the apiserver thinks is on node 10.10.103.202-worker before test
May 25 04:22:11.135: INFO: calico-node-k6h5c from calico-system started at 2021-05-21 05:16:19 +0000 UTC (1 container statuses recorded)
May 25 04:22:11.135: INFO: 	Container calico-node ready: true, restart count 0
May 25 04:22:11.135: INFO: calico-typha-584858976f-x82v6 from calico-system started at 2021-05-21 05:16:18 +0000 UTC (1 container statuses recorded)
May 25 04:22:11.135: INFO: 	Container calico-typha ready: true, restart count 0
May 25 04:22:11.135: INFO: kube-proxy-fvq49 from kube-system started at 2021-05-21 05:14:38 +0000 UTC (1 container statuses recorded)
May 25 04:22:11.135: INFO: 	Container kube-proxy ready: true, restart count 0
May 25 04:22:11.135: INFO: sonobuoy-systemd-logs-daemon-set-ebbefc146b074e77-kj9h6 from sonobuoy started at 2021-05-25 02:27:00 +0000 UTC (2 container statuses recorded)
May 25 04:22:11.135: INFO: 	Container sonobuoy-worker ready: false, restart count 15
May 25 04:22:11.135: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c0109262-a0c3-4151-82ad-9f84fd7262a8 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c0109262-a0c3-4151-82ad-9f84fd7262a8 off the node 10.10.103.202-worker
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c0109262-a0c3-4151-82ad-9f84fd7262a8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:22:17.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5461" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.228 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":339,"completed":330,"skipped":5175,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:22:17.269: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:22:17.321: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:22:20.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8724" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":339,"completed":331,"skipped":5179,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:22:20.637: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
May 25 04:22:20.710: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 25 04:22:22.715: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 25 04:22:24.717: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
May 25 04:22:24.729: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 25 04:22:26.737: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 25 04:22:26.764: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 25 04:22:26.769: INFO: Pod pod-with-poststart-http-hook still exists
May 25 04:22:28.769: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 25 04:22:28.779: INFO: Pod pod-with-poststart-http-hook still exists
May 25 04:22:30.770: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 25 04:22:30.778: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:22:30.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6507" for this suite.

• [SLOW TEST:10.154 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":339,"completed":332,"skipped":5208,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:22:30.792: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May 25 04:22:30.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb17d7ba-d1e5-4336-a942-ed5db0107d3f" in namespace "downward-api-4122" to be "Succeeded or Failed"
May 25 04:22:30.857: INFO: Pod "downwardapi-volume-eb17d7ba-d1e5-4336-a942-ed5db0107d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.945269ms
May 25 04:22:32.864: INFO: Pod "downwardapi-volume-eb17d7ba-d1e5-4336-a942-ed5db0107d3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013015039s
STEP: Saw pod success
May 25 04:22:32.864: INFO: Pod "downwardapi-volume-eb17d7ba-d1e5-4336-a942-ed5db0107d3f" satisfied condition "Succeeded or Failed"
May 25 04:22:32.868: INFO: Trying to get logs from node 10.10.103.202-worker pod downwardapi-volume-eb17d7ba-d1e5-4336-a942-ed5db0107d3f container client-container: <nil>
STEP: delete the pod
May 25 04:22:32.889: INFO: Waiting for pod downwardapi-volume-eb17d7ba-d1e5-4336-a942-ed5db0107d3f to disappear
May 25 04:22:32.893: INFO: Pod downwardapi-volume-eb17d7ba-d1e5-4336-a942-ed5db0107d3f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:22:32.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4122" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":333,"skipped":5285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:22:32.907: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 25 04:22:33.832: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 25 04:22:35.846: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757513353, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757513353, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757513353, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757513353, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 25 04:22:38.866: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:22:38.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7579" for this suite.
STEP: Destroying namespace "webhook-7579-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.121 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":339,"completed":334,"skipped":5329,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:22:39.028: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
May 25 04:22:39.104: INFO: namespace kubectl-6777
May 25 04:22:39.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6777 create -f -'
May 25 04:22:39.762: INFO: stderr: ""
May 25 04:22:39.762: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 25 04:22:40.773: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 04:22:40.773: INFO: Found 0 / 1
May 25 04:22:41.771: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 04:22:41.771: INFO: Found 0 / 1
May 25 04:22:42.773: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 04:22:42.773: INFO: Found 1 / 1
May 25 04:22:42.773: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 25 04:22:42.778: INFO: Selector matched 1 pods for map[app:agnhost]
May 25 04:22:42.778: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 25 04:22:42.778: INFO: wait on agnhost-primary startup in kubectl-6777 
May 25 04:22:42.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6777 logs agnhost-primary-jn4rt agnhost-primary'
May 25 04:22:43.007: INFO: stderr: ""
May 25 04:22:43.007: INFO: stdout: "Paused\n"
STEP: exposing RC
May 25 04:22:43.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6777 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 25 04:22:43.261: INFO: stderr: ""
May 25 04:22:43.261: INFO: stdout: "service/rm2 exposed\n"
May 25 04:22:43.275: INFO: Service rm2 in namespace kubectl-6777 found.
STEP: exposing service
May 25 04:22:45.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=kubectl-6777 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 25 04:22:45.514: INFO: stderr: ""
May 25 04:22:45.514: INFO: stdout: "service/rm3 exposed\n"
May 25 04:22:45.520: INFO: Service rm3 in namespace kubectl-6777 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:22:47.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6777" for this suite.

• [SLOW TEST:8.522 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1223
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":339,"completed":335,"skipped":5331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:22:47.551: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-9021
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9021 to expose endpoints map[]
May 25 04:22:47.640: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May 25 04:22:48.660: INFO: successfully validated that service multi-endpoint-test in namespace services-9021 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9021
May 25 04:22:48.675: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:22:50.683: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9021 to expose endpoints map[pod1:[100]]
May 25 04:22:50.701: INFO: successfully validated that service multi-endpoint-test in namespace services-9021 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9021
May 25 04:22:50.713: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 25 04:22:52.721: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9021 to expose endpoints map[pod1:[100] pod2:[101]]
May 25 04:22:52.746: INFO: successfully validated that service multi-endpoint-test in namespace services-9021 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-9021
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9021 to expose endpoints map[pod2:[101]]
May 25 04:22:52.812: INFO: successfully validated that service multi-endpoint-test in namespace services-9021 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9021
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9021 to expose endpoints map[]
May 25 04:22:52.852: INFO: successfully validated that service multi-endpoint-test in namespace services-9021 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:22:52.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9021" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:5.358 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":339,"completed":336,"skipped":5357,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:22:52.910: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May 25 04:22:52.968: INFO: Creating deployment "webserver-deployment"
May 25 04:22:52.976: INFO: Waiting for observed generation 1
May 25 04:22:54.990: INFO: Waiting for all required pods to come up
May 25 04:22:54.998: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 25 04:23:03.028: INFO: Waiting for deployment "webserver-deployment" to complete
May 25 04:23:03.036: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 25 04:23:03.048: INFO: Updating deployment webserver-deployment
May 25 04:23:03.048: INFO: Waiting for observed generation 2
May 25 04:23:05.064: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 25 04:23:05.076: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 25 04:23:05.080: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 25 04:23:05.092: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 25 04:23:05.092: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 25 04:23:05.096: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 25 04:23:05.117: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 25 04:23:05.117: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 25 04:23:05.133: INFO: Updating deployment webserver-deployment
May 25 04:23:05.134: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 25 04:23:05.147: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 25 04:23:05.151: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
May 25 04:23:05.181: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5342  483b9838-121f-44af-a4a0-f95eb8255c97 864721 3 2021-05-25 04:22:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-25 04:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-25 04:23:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00437eb78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-25 04:23:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-05-25 04:23:03 +0000 UTC,LastTransitionTime:2021-05-25 04:22:52 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 25 04:23:05.199: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-5342  e32e5cbe-1fc3-494f-b44c-ea947adc0210 864726 3 2021-05-25 04:23:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 483b9838-121f-44af-a4a0-f95eb8255c97 0xc0050a3e07 0xc0050a3e08}] []  [{kube-controller-manager Update apps/v1 2021-05-25 04:23:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"483b9838-121f-44af-a4a0-f95eb8255c97\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050a3e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 25 04:23:05.199: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 25 04:23:05.200: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-5342  a69ff296-655a-4ce3-8791-952602cf9218 864722 3 2021-05-25 04:22:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 483b9838-121f-44af-a4a0-f95eb8255c97 0xc0050a3ee7 0xc0050a3ee8}] []  [{kube-controller-manager Update apps/v1 2021-05-25 04:22:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"483b9838-121f-44af-a4a0-f95eb8255c97\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050a3f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 25 04:23:05.263: INFO: Pod "webserver-deployment-795d758f88-h6m5g" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-h6m5g webserver-deployment-795d758f88- deployment-5342  6af2cb7a-4342-46a0-b67c-e98754f3405a 864703 0 2021-05-25 04:23:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e32e5cbe-1fc3-494f-b44c-ea947adc0210 0xc00437ef47 0xc00437ef48}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e32e5cbe-1fc3-494f-b44c-ea947adc0210\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-25 04:23:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v6vl8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v6vl8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:,StartTime:2021-05-25 04:23:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.263: INFO: Pod "webserver-deployment-795d758f88-hkxlf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hkxlf webserver-deployment-795d758f88- deployment-5342  30570b98-a177-4833-9c9e-423eb598ace2 864734 0 2021-05-25 04:23:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e32e5cbe-1fc3-494f-b44c-ea947adc0210 0xc00437f117 0xc00437f118}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e32e5cbe-1fc3-494f-b44c-ea947adc0210\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vctd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vctd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.264: INFO: Pod "webserver-deployment-795d758f88-lhv4v" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lhv4v webserver-deployment-795d758f88- deployment-5342  158743af-a706-4ecd-8c35-921c03f729be 864735 0 2021-05-25 04:23:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e32e5cbe-1fc3-494f-b44c-ea947adc0210 0xc00437f270 0xc00437f271}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e32e5cbe-1fc3-494f-b44c-ea947adc0210\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7552n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7552n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.264: INFO: Pod "webserver-deployment-795d758f88-mgclv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mgclv webserver-deployment-795d758f88- deployment-5342  4e395c10-251f-4636-8552-6f104f9b3556 864698 0 2021-05-25 04:23:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e32e5cbe-1fc3-494f-b44c-ea947adc0210 0xc00437f3e0 0xc00437f3e1}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e32e5cbe-1fc3-494f-b44c-ea947adc0210\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-25 04:23:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gkc68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gkc68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:,StartTime:2021-05-25 04:23:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.265: INFO: Pod "webserver-deployment-795d758f88-pdwfs" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pdwfs webserver-deployment-795d758f88- deployment-5342  bfea08e6-1354-46d9-8029-4abc847bd397 864723 0 2021-05-25 04:23:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.31.98/32 cni.projectcalico.org/podIPs:192.168.31.98/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e32e5cbe-1fc3-494f-b44c-ea947adc0210 0xc00437f5e7 0xc00437f5e8}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e32e5cbe-1fc3-494f-b44c-ea947adc0210\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-25 04:23:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-25 04:23:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8xplw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8xplw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:,StartTime:2021-05-25 04:23:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.265: INFO: Pod "webserver-deployment-795d758f88-pks2p" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pks2p webserver-deployment-795d758f88- deployment-5342  6e07cf8a-004f-4be4-9207-9e0650d62d3e 864716 0 2021-05-25 04:23:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.171.144/32 cni.projectcalico.org/podIPs:192.168.171.144/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e32e5cbe-1fc3-494f-b44c-ea947adc0210 0xc00437f7d7 0xc00437f7d8}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e32e5cbe-1fc3-494f-b44c-ea947adc0210\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-25 04:23:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-25 04:23:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bvbcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bvbcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.201-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.201,PodIP:,StartTime:2021-05-25 04:23:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.266: INFO: Pod "webserver-deployment-795d758f88-tqfnt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tqfnt webserver-deployment-795d758f88- deployment-5342  e9d94484-3452-495b-bfca-95870d9d9235 864740 0 2021-05-25 04:23:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e32e5cbe-1fc3-494f-b44c-ea947adc0210 0xc00437f9d7 0xc00437f9d8}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e32e5cbe-1fc3-494f-b44c-ea947adc0210\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8jvv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8jvv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.266: INFO: Pod "webserver-deployment-795d758f88-vb5hw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vb5hw webserver-deployment-795d758f88- deployment-5342  7986e31a-fc0d-478e-9f97-9e2f3e6e84c1 864670 0 2021-05-25 04:23:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e32e5cbe-1fc3-494f-b44c-ea947adc0210 0xc00437fb50 0xc00437fb51}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e32e5cbe-1fc3-494f-b44c-ea947adc0210\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-25 04:23:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k7bgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:,StartTime:2021-05-25 04:23:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.266: INFO: Pod "webserver-deployment-847dcfb7fb-2kd47" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2kd47 webserver-deployment-847dcfb7fb- deployment-5342  4fc93a70-6bd5-4bbb-932f-fe4d9a7b3a22 864731 0 2021-05-25 04:23:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00437fd37 0xc00437fd38}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5f8vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5f8vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.267: INFO: Pod "webserver-deployment-847dcfb7fb-2s7pf" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2s7pf webserver-deployment-847dcfb7fb- deployment-5342  3b896853-ba90-4eb9-ac05-f4bdb16c4e4b 864528 0 2021-05-25 04:22:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:192.168.171.143/32 cni.projectcalico.org/podIPs:192.168.171.143/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00437fe80 0xc00437fe81}] []  [{kube-controller-manager Update v1 2021-05-25 04:22:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 04:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 04:22:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.171.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6qvj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6qvj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.201-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.201,PodIP:192.168.171.143,StartTime:2021-05-25 04:22:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 04:22:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://a9ec26e4d364be0e9f83ce6fe00146678e96e021f5fe70bcb8dc969aeeacbf6b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.171.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.267: INFO: Pod "webserver-deployment-847dcfb7fb-482n7" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-482n7 webserver-deployment-847dcfb7fb- deployment-5342  06fda8f9-12ad-4b93-b6a8-1411cbae05ff 864736 0 2021-05-25 04:23:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739a067 0xc00739a068}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9gqsh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9gqsh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.268: INFO: Pod "webserver-deployment-847dcfb7fb-5llqr" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-5llqr webserver-deployment-847dcfb7fb- deployment-5342  32ef3bfb-8ebf-49d5-81d8-87033996f9ba 864644 0 2021-05-25 04:22:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:192.168.31.91/32 cni.projectcalico.org/podIPs:192.168.31.91/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739a1d0 0xc00739a1d1}] []  [{kube-controller-manager Update v1 2021-05-25 04:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 04:22:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 04:23:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.31.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2c55f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2c55f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:192.168.31.91,StartTime:2021-05-25 04:22:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 04:22:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://f9cba8db2be9d69f03206988608598067da928ac2da6bd1b83923b09594d3aa7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.31.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.268: INFO: Pod "webserver-deployment-847dcfb7fb-bxs5w" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-bxs5w webserver-deployment-847dcfb7fb- deployment-5342  b2abf338-fb8a-4780-9641-a4ed649dc30d 864737 0 2021-05-25 04:23:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739a3c7 0xc00739a3c8}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vrh2n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vrh2n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.268: INFO: Pod "webserver-deployment-847dcfb7fb-g577w" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-g577w webserver-deployment-847dcfb7fb- deployment-5342  c9998a6e-eba9-4751-8db2-4a622facdb36 864531 0 2021-05-25 04:22:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:192.168.171.138/32 cni.projectcalico.org/podIPs:192.168.171.138/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739a5c0 0xc00739a5c1}] []  [{kube-controller-manager Update v1 2021-05-25 04:22:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 04:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 04:22:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.171.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gdbs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gdbs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.201-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.201,PodIP:192.168.171.138,StartTime:2021-05-25 04:22:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 04:22:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://4b536257fb11b1b4c63e2c1063a477c618b8db35c65b5e486421cd86c373ae4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.171.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.269: INFO: Pod "webserver-deployment-847dcfb7fb-gqxmb" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-gqxmb webserver-deployment-847dcfb7fb- deployment-5342  a12bfbb9-7b78-4aeb-9457-b606e599c570 864730 0 2021-05-25 04:23:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739a9c7 0xc00739a9c8}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmzz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmzz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.269: INFO: Pod "webserver-deployment-847dcfb7fb-gxsth" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-gxsth webserver-deployment-847dcfb7fb- deployment-5342  109ae9d0-56e1-4f2d-a4fc-111c28197bc4 864632 0 2021-05-25 04:22:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:192.168.31.97/32 cni.projectcalico.org/podIPs:192.168.31.97/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739ab30 0xc00739ab31}] []  [{kube-controller-manager Update v1 2021-05-25 04:22:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 04:22:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 04:22:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.31.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pr77w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pr77w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:192.168.31.97,StartTime:2021-05-25 04:22:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 04:22:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://471eeea09f27fd0711d9293297932763fb32944ba1379095334d7c5159c6d558,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.31.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.270: INFO: Pod "webserver-deployment-847dcfb7fb-j4z5r" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-j4z5r webserver-deployment-847dcfb7fb- deployment-5342  50512208-47f7-4595-8ce3-9143c413e6c5 864738 0 2021-05-25 04:23:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739ad17 0xc00739ad18}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rlkzx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rlkzx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.270: INFO: Pod "webserver-deployment-847dcfb7fb-jxz94" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-jxz94 webserver-deployment-847dcfb7fb- deployment-5342  516b5090-97d4-44b8-a5c3-b6c46296b347 864728 0 2021-05-25 04:23:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739ae60 0xc00739ae61}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4f98h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4f98h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.201-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:23:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.270: INFO: Pod "webserver-deployment-847dcfb7fb-r28n5" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-r28n5 webserver-deployment-847dcfb7fb- deployment-5342  01689017-f0eb-4b05-be56-d9c0f154da93 864606 0 2021-05-25 04:22:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:192.168.31.94/32 cni.projectcalico.org/podIPs:192.168.31.94/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739afe0 0xc00739afe1}] []  [{kube-controller-manager Update v1 2021-05-25 04:22:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 04:22:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 04:22:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.31.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-szmjr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-szmjr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:192.168.31.94,StartTime:2021-05-25 04:22:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 04:22:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://a7136928307c50e46b3de19fb9cd73f1d9e8f089b951cdf0aae014e8cb9ae7cb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.31.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.271: INFO: Pod "webserver-deployment-847dcfb7fb-t9dhx" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-t9dhx webserver-deployment-847dcfb7fb- deployment-5342  ee5f3b0a-3fec-42d0-bcde-57fdf51b6bb4 864638 0 2021-05-25 04:22:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:192.168.31.95/32 cni.projectcalico.org/podIPs:192.168.31.95/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739b1e7 0xc00739b1e8}] []  [{kube-controller-manager Update v1 2021-05-25 04:22:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 04:22:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 04:23:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.31.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jcd2f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jcd2f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:192.168.31.95,StartTime:2021-05-25 04:22:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 04:22:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://6242ff4b861848f4bfb3fb38f3da9617a3d81d637fc565c675de1c6ffc0eaa38,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.31.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.271: INFO: Pod "webserver-deployment-847dcfb7fb-ww6nt" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-ww6nt webserver-deployment-847dcfb7fb- deployment-5342  1f1401e9-34a6-4c3c-b2a9-d2d5fe8ceaf1 864739 0 2021-05-25 04:23:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739b3d7 0xc00739b3d8}] []  [{kube-controller-manager Update v1 2021-05-25 04:23:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ql2wb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ql2wb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.272: INFO: Pod "webserver-deployment-847dcfb7fb-zcbds" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-zcbds webserver-deployment-847dcfb7fb- deployment-5342  e2512b05-ef71-42be-93f6-079cdacf0ee6 864641 0 2021-05-25 04:22:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:192.168.31.96/32 cni.projectcalico.org/podIPs:192.168.31.96/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739b8f0 0xc00739b8f1}] []  [{kube-controller-manager Update v1 2021-05-25 04:22:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 04:22:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 04:23:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.31.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mvxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mvxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:192.168.31.96,StartTime:2021-05-25 04:22:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 04:22:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://cb5d68c862edaf78cf65e9972ee6897f6e4ec0f5ffdbd125c498b5004b642338,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.31.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 25 04:23:05.275: INFO: Pod "webserver-deployment-847dcfb7fb-zhzzd" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-zhzzd webserver-deployment-847dcfb7fb- deployment-5342  cd5ca472-fc40-4a1a-a93c-3084a7d2e5e9 864616 0 2021-05-25 04:22:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:192.168.31.100/32 cni.projectcalico.org/podIPs:192.168.31.100/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a69ff296-655a-4ce3-8791-952602cf9218 0xc00739bad7 0xc00739bad8}] []  [{kube-controller-manager Update v1 2021-05-25 04:22:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a69ff296-655a-4ce3-8791-952602cf9218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-25 04:22:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-25 04:22:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.31.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vm2rx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vm2rx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.103.202-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-25 04:22:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.202,PodIP:192.168.31.100,StartTime:2021-05-25 04:22:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-25 04:22:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://420e202adad7b5ffcee15d4076ec01471930d1e98b68ca09559e8f3ef2dfa294,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.31.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:23:05.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5342" for this suite.

• [SLOW TEST:12.455 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":339,"completed":337,"skipped":5416,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:23:05.365: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 25 04:23:14.715: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:23:14.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2575" for this suite.

• [SLOW TEST:9.386 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":338,"skipped":5422,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 25 04:23:14.752: INFO: >>> kubeConfig: /tmp/kubeconfig-649648601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8918
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8918
I0525 04:23:14.858378      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8918, replica count: 2
I0525 04:23:17.910125      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 04:23:20.911: INFO: Creating new exec pod
I0525 04:23:20.911297      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 25 04:23:25.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:26.473: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:26.473: INFO: stdout: ""
May 25 04:23:27.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:27.846: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:27.846: INFO: stdout: ""
May 25 04:23:28.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:28.849: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:28.849: INFO: stdout: ""
May 25 04:23:29.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:29.863: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:29.863: INFO: stdout: ""
May 25 04:23:30.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:30.839: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:30.839: INFO: stdout: ""
May 25 04:23:31.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:31.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:31.871: INFO: stdout: ""
May 25 04:23:32.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:32.844: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:32.844: INFO: stdout: ""
May 25 04:23:33.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:33.863: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:33.863: INFO: stdout: ""
May 25 04:23:34.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:34.847: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:34.847: INFO: stdout: ""
May 25 04:23:35.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:35.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:35.860: INFO: stdout: ""
May 25 04:23:36.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:36.866: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:36.866: INFO: stdout: ""
May 25 04:23:37.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:37.850: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:37.850: INFO: stdout: ""
May 25 04:23:38.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:38.858: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:38.858: INFO: stdout: ""
May 25 04:23:39.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:39.849: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:39.849: INFO: stdout: ""
May 25 04:23:40.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:40.846: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:40.846: INFO: stdout: ""
May 25 04:23:41.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:41.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:41.851: INFO: stdout: ""
May 25 04:23:42.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:42.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:42.862: INFO: stdout: ""
May 25 04:23:43.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:43.849: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:43.850: INFO: stdout: ""
May 25 04:23:44.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:44.849: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:44.849: INFO: stdout: ""
May 25 04:23:45.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:45.849: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:45.849: INFO: stdout: ""
May 25 04:23:46.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:46.847: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:46.847: INFO: stdout: ""
May 25 04:23:47.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:47.881: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:47.881: INFO: stdout: ""
May 25 04:23:48.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:48.858: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:48.858: INFO: stdout: ""
May 25 04:23:49.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:49.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:49.851: INFO: stdout: ""
May 25 04:23:50.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:50.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:50.862: INFO: stdout: ""
May 25 04:23:51.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:51.864: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:51.864: INFO: stdout: ""
May 25 04:23:52.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:52.857: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:52.857: INFO: stdout: ""
May 25 04:23:53.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:53.847: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:53.847: INFO: stdout: ""
May 25 04:23:54.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:54.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:54.868: INFO: stdout: ""
May 25 04:23:55.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:55.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:55.862: INFO: stdout: ""
May 25 04:23:56.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:56.850: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:56.850: INFO: stdout: ""
May 25 04:23:57.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:57.854: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:57.854: INFO: stdout: ""
May 25 04:23:58.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:58.849: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:58.849: INFO: stdout: ""
May 25 04:23:59.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:23:59.859: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:23:59.860: INFO: stdout: ""
May 25 04:24:00.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:00.853: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:00.853: INFO: stdout: ""
May 25 04:24:01.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:01.858: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:01.858: INFO: stdout: ""
May 25 04:24:02.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:02.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:02.856: INFO: stdout: ""
May 25 04:24:03.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:03.861: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:03.861: INFO: stdout: ""
May 25 04:24:04.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:04.864: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:04.864: INFO: stdout: ""
May 25 04:24:05.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:05.859: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:05.859: INFO: stdout: ""
May 25 04:24:06.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:06.852: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:06.852: INFO: stdout: ""
May 25 04:24:07.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:07.864: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:07.864: INFO: stdout: ""
May 25 04:24:08.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:08.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:08.856: INFO: stdout: ""
May 25 04:24:09.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:09.858: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:09.858: INFO: stdout: ""
May 25 04:24:10.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:10.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:10.856: INFO: stdout: ""
May 25 04:24:11.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:11.852: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:11.853: INFO: stdout: ""
May 25 04:24:12.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:12.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:12.865: INFO: stdout: ""
May 25 04:24:13.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:13.855: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:13.855: INFO: stdout: ""
May 25 04:24:14.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:14.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:14.856: INFO: stdout: ""
May 25 04:24:15.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:15.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:15.865: INFO: stdout: ""
May 25 04:24:16.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:16.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:16.851: INFO: stdout: ""
May 25 04:24:17.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:17.858: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:17.858: INFO: stdout: ""
May 25 04:24:18.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:18.847: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:18.847: INFO: stdout: ""
May 25 04:24:19.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:19.838: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:19.838: INFO: stdout: ""
May 25 04:24:20.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:20.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:20.851: INFO: stdout: ""
May 25 04:24:21.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:21.845: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:21.845: INFO: stdout: ""
May 25 04:24:22.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:22.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:22.865: INFO: stdout: ""
May 25 04:24:23.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:23.850: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:23.850: INFO: stdout: ""
May 25 04:24:24.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:24.845: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:24.845: INFO: stdout: ""
May 25 04:24:25.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:25.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:25.856: INFO: stdout: ""
May 25 04:24:26.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:26.864: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:26.864: INFO: stdout: ""
May 25 04:24:27.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:27.863: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:27.863: INFO: stdout: ""
May 25 04:24:28.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:28.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:28.860: INFO: stdout: ""
May 25 04:24:29.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:29.855: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:29.855: INFO: stdout: ""
May 25 04:24:30.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:30.855: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:30.855: INFO: stdout: ""
May 25 04:24:31.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:31.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:31.871: INFO: stdout: ""
May 25 04:24:32.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:32.861: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:32.861: INFO: stdout: ""
May 25 04:24:33.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:33.870: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:33.871: INFO: stdout: ""
May 25 04:24:34.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:34.855: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:34.855: INFO: stdout: ""
May 25 04:24:35.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 25 04:24:35.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 25 04:24:35.865: INFO: stdout: "externalname-service-zl4bb"
May 25 04:24:35.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:36.242: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:36.242: INFO: stdout: ""
May 25 04:24:37.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:37.615: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:37.615: INFO: stdout: ""
May 25 04:24:38.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:38.616: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:38.616: INFO: stdout: ""
May 25 04:24:39.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:39.627: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:39.627: INFO: stdout: ""
May 25 04:24:40.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:40.621: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:40.621: INFO: stdout: ""
May 25 04:24:41.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:41.656: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:41.656: INFO: stdout: ""
May 25 04:24:42.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:42.627: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:42.627: INFO: stdout: ""
May 25 04:24:43.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:43.631: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:43.631: INFO: stdout: ""
May 25 04:24:44.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:44.623: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:44.623: INFO: stdout: ""
May 25 04:24:45.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:45.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:45.619: INFO: stdout: ""
May 25 04:24:46.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:46.618: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:46.618: INFO: stdout: ""
May 25 04:24:47.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:47.615: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:47.615: INFO: stdout: ""
May 25 04:24:48.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:48.624: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:48.625: INFO: stdout: ""
May 25 04:24:49.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:49.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:49.619: INFO: stdout: ""
May 25 04:24:50.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:50.628: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:50.628: INFO: stdout: ""
May 25 04:24:51.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:51.622: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:51.622: INFO: stdout: ""
May 25 04:24:52.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:52.638: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:52.638: INFO: stdout: ""
May 25 04:24:53.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:53.618: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:53.618: INFO: stdout: ""
May 25 04:24:54.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:54.610: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:54.610: INFO: stdout: ""
May 25 04:24:55.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:55.624: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:55.624: INFO: stdout: ""
May 25 04:24:56.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:56.618: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:56.618: INFO: stdout: ""
May 25 04:24:57.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:57.633: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:57.633: INFO: stdout: ""
May 25 04:24:58.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:58.613: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:58.613: INFO: stdout: ""
May 25 04:24:59.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:24:59.620: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:24:59.620: INFO: stdout: ""
May 25 04:25:00.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:25:00.608: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:25:00.608: INFO: stdout: ""
May 25 04:25:01.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:25:01.628: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:25:01.628: INFO: stdout: ""
May 25 04:25:02.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:25:02.614: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:25:02.614: INFO: stdout: ""
May 25 04:25:03.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:25:03.624: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:25:03.624: INFO: stdout: ""
May 25 04:25:04.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:25:04.631: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:25:04.631: INFO: stdout: ""
May 25 04:25:05.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:25:05.625: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:25:05.625: INFO: stdout: ""
May 25 04:25:06.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.81.58 80'
May 25 04:25:06.623: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.81.58 80\nConnection to 10.96.81.58 80 port [tcp/http] succeeded!\n"
May 25 04:25:06.623: INFO: stdout: "externalname-service-zl4bb"
May 25 04:25:06.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.201 32163'
May 25 04:25:07.010: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.201 32163\nConnection to 10.10.103.201 32163 port [tcp/*] succeeded!\n"
May 25 04:25:07.010: INFO: stdout: "externalname-service-c8zz9"
May 25 04:25:07.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.202 32163'
May 25 04:25:07.373: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.202 32163\nConnection to 10.10.103.202 32163 port [tcp/*] succeeded!\n"
May 25 04:25:07.373: INFO: stdout: ""
May 25 04:25:08.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-649648601 --namespace=services-8918 exec execpodgznx7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.103.202 32163'
May 25 04:25:08.738: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.103.202 32163\nConnection to 10.10.103.202 32163 port [tcp/*] succeeded!\n"
May 25 04:25:08.738: INFO: stdout: "externalname-service-c8zz9"
May 25 04:25:08.738: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 25 04:25:08.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8918" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:114.055 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":339,"completed":339,"skipped":5432,"failed":0}
May 25 04:25:08.807: INFO: Running AfterSuite actions on all nodes
May 25 04:25:08.807: INFO: Running AfterSuite actions on node 1
May 25 04:25:08.807: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":339,"completed":339,"skipped":5432,"failed":0}

Ran 339 of 5771 Specs in 7082.494 seconds
SUCCESS! -- 339 Passed | 0 Failed | 0 Pending | 5432 Skipped
PASS

Ginkgo ran 1 suite in 1h58m6.757342549s
Test Suite Passed
