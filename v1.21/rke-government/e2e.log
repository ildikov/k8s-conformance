I0708 01:44:58.334769      20 e2e.go:129] Starting e2e run "056299fd-c67f-45b1-9c38-56a7ea751ee1" on Ginkgo node 1
{"msg":"Test Suite starting","total":339,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1625708696 - Will randomize all specs
Will run 339 of 5771 specs

Jul  8 01:44:58.362: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 01:44:58.366: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul  8 01:44:58.391: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul  8 01:44:58.427: INFO: The status of Pod helm-install-rke2-canal-zzkr7 is Succeeded, skipping waiting
Jul  8 01:44:58.427: INFO: The status of Pod helm-install-rke2-coredns-7vm9g is Succeeded, skipping waiting
Jul  8 01:44:58.427: INFO: The status of Pod helm-install-rke2-kube-proxy-tqnww is Succeeded, skipping waiting
Jul  8 01:44:58.427: INFO: The status of Pod helm-install-rke2-metrics-server-xbzj4 is Succeeded, skipping waiting
Jul  8 01:44:58.427: INFO: 22 / 26 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul  8 01:44:58.427: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jul  8 01:44:58.427: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul  8 01:44:58.434: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul  8 01:44:58.434: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'rke2-canal' (0 seconds elapsed)
Jul  8 01:44:58.434: INFO: e2e test version: v1.21.2
Jul  8 01:44:58.435: INFO: kube-apiserver version: v1.21.2+rke2r1
Jul  8 01:44:58.435: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 01:44:58.441: INFO: Cluster IP family: ipv4
S
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:44:58.442: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
W0708 01:44:58.476720      20 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jul  8 01:44:58.476: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jul  8 01:44:58.483: INFO: PSP annotation exists on dry run pod: "global-unrestricted-psp"; assuming PodSecurityPolicy is enabled
W0708 01:44:58.485857      20 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
W0708 01:44:58.489617      20 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jul  8 01:44:58.495: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:44:58.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5381" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":339,"completed":1,"skipped":1,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:44:58.666: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3866
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0708 01:45:08.854343      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul  8 01:50:08.859: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:50:08.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3866" for this suite.

• [SLOW TEST:310.201 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":339,"completed":2,"skipped":26,"failed":0}
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:50:08.868: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3143
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:50:09.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3143" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":339,"completed":3,"skipped":27,"failed":0}
S
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:50:09.049: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9287
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 01:50:09.236: INFO: The status of Pod test-webserver-90c5ef9f-56e6-4cdc-a1ee-2971fe4015cf is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:50:11.243: INFO: The status of Pod test-webserver-90c5ef9f-56e6-4cdc-a1ee-2971fe4015cf is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:50:13.247: INFO: The status of Pod test-webserver-90c5ef9f-56e6-4cdc-a1ee-2971fe4015cf is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:50:15.243: INFO: The status of Pod test-webserver-90c5ef9f-56e6-4cdc-a1ee-2971fe4015cf is Running (Ready = false)
Jul  8 01:50:17.244: INFO: The status of Pod test-webserver-90c5ef9f-56e6-4cdc-a1ee-2971fe4015cf is Running (Ready = false)
Jul  8 01:50:19.242: INFO: The status of Pod test-webserver-90c5ef9f-56e6-4cdc-a1ee-2971fe4015cf is Running (Ready = false)
Jul  8 01:50:21.243: INFO: The status of Pod test-webserver-90c5ef9f-56e6-4cdc-a1ee-2971fe4015cf is Running (Ready = false)
Jul  8 01:50:23.243: INFO: The status of Pod test-webserver-90c5ef9f-56e6-4cdc-a1ee-2971fe4015cf is Running (Ready = false)
Jul  8 01:50:25.243: INFO: The status of Pod test-webserver-90c5ef9f-56e6-4cdc-a1ee-2971fe4015cf is Running (Ready = false)
Jul  8 01:50:27.243: INFO: The status of Pod test-webserver-90c5ef9f-56e6-4cdc-a1ee-2971fe4015cf is Running (Ready = false)
Jul  8 01:50:29.242: INFO: The status of Pod test-webserver-90c5ef9f-56e6-4cdc-a1ee-2971fe4015cf is Running (Ready = false)
Jul  8 01:50:31.243: INFO: The status of Pod test-webserver-90c5ef9f-56e6-4cdc-a1ee-2971fe4015cf is Running (Ready = true)
Jul  8 01:50:31.245: INFO: Container started at 2021-07-08 01:50:14 +0000 UTC, pod became ready at 2021-07-08 01:50:29 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:50:31.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9287" for this suite.

• [SLOW TEST:22.207 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":339,"completed":4,"skipped":28,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:50:31.256: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8396
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 01:50:31.433: INFO: The status of Pod pod-secrets-590c5985-7cf0-4940-8541-e4f3d794da6e is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:50:33.440: INFO: The status of Pod pod-secrets-590c5985-7cf0-4940-8541-e4f3d794da6e is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:50:33.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8396" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":339,"completed":5,"skipped":53,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:50:33.511: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-4977
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0708 01:50:33.667917      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul  8 01:50:33.683: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul  8 01:50:33.686: INFO: starting watch
STEP: patching
STEP: updating
Jul  8 01:50:33.714: INFO: waiting for watch events with expected annotations
Jul  8 01:50:33.714: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:50:33.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4977" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":339,"completed":6,"skipped":81,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:50:33.770: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5155
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-21edfec7-9698-49ff-b06b-f66cfed05127
STEP: Creating a pod to test consume secrets
Jul  8 01:50:33.930: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1fda44bc-4d0f-4703-9198-8e73e800eea7" in namespace "projected-5155" to be "Succeeded or Failed"
Jul  8 01:50:33.934: INFO: Pod "pod-projected-secrets-1fda44bc-4d0f-4703-9198-8e73e800eea7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.154243ms
Jul  8 01:50:35.941: INFO: Pod "pod-projected-secrets-1fda44bc-4d0f-4703-9198-8e73e800eea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01079218s
STEP: Saw pod success
Jul  8 01:50:35.941: INFO: Pod "pod-projected-secrets-1fda44bc-4d0f-4703-9198-8e73e800eea7" satisfied condition "Succeeded or Failed"
Jul  8 01:50:35.944: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-projected-secrets-1fda44bc-4d0f-4703-9198-8e73e800eea7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  8 01:50:35.980: INFO: Waiting for pod pod-projected-secrets-1fda44bc-4d0f-4703-9198-8e73e800eea7 to disappear
Jul  8 01:50:35.983: INFO: Pod pod-projected-secrets-1fda44bc-4d0f-4703-9198-8e73e800eea7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:50:35.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5155" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":7,"skipped":101,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:50:35.992: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3500
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-3500/configmap-test-ffa2356c-d871-4464-8e28-0f24ba9fc080
STEP: Creating a pod to test consume configMaps
Jul  8 01:50:36.153: INFO: Waiting up to 5m0s for pod "pod-configmaps-d7b3e296-14e8-4faa-a4f3-09714f3e6065" in namespace "configmap-3500" to be "Succeeded or Failed"
Jul  8 01:50:36.157: INFO: Pod "pod-configmaps-d7b3e296-14e8-4faa-a4f3-09714f3e6065": Phase="Pending", Reason="", readiness=false. Elapsed: 3.905224ms
Jul  8 01:50:38.161: INFO: Pod "pod-configmaps-d7b3e296-14e8-4faa-a4f3-09714f3e6065": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007724085s
Jul  8 01:50:40.169: INFO: Pod "pod-configmaps-d7b3e296-14e8-4faa-a4f3-09714f3e6065": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01542062s
STEP: Saw pod success
Jul  8 01:50:40.169: INFO: Pod "pod-configmaps-d7b3e296-14e8-4faa-a4f3-09714f3e6065" satisfied condition "Succeeded or Failed"
Jul  8 01:50:40.175: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-configmaps-d7b3e296-14e8-4faa-a4f3-09714f3e6065 container env-test: <nil>
STEP: delete the pod
Jul  8 01:50:40.195: INFO: Waiting for pod pod-configmaps-d7b3e296-14e8-4faa-a4f3-09714f3e6065 to disappear
Jul  8 01:50:40.197: INFO: Pod pod-configmaps-d7b3e296-14e8-4faa-a4f3-09714f3e6065 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:50:40.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3500" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":339,"completed":8,"skipped":103,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:50:40.207: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9484
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-85bba32a-1c41-43b0-a0e3-1e37d0680997
STEP: Creating secret with name s-test-opt-upd-f5f44242-811e-499c-bb81-1b4f93eba757
STEP: Creating the pod
Jul  8 01:50:40.387: INFO: The status of Pod pod-projected-secrets-c9fa1694-c8a3-4795-8de9-4698eb92ef22 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:50:42.395: INFO: The status of Pod pod-projected-secrets-c9fa1694-c8a3-4795-8de9-4698eb92ef22 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-85bba32a-1c41-43b0-a0e3-1e37d0680997
STEP: Updating secret s-test-opt-upd-f5f44242-811e-499c-bb81-1b4f93eba757
STEP: Creating secret with name s-test-opt-create-40e20d8a-0b08-4e38-98af-39fb0e6835c6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:50:46.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9484" for this suite.

• [SLOW TEST:6.260 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":9,"skipped":106,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:50:46.468: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul  8 01:50:46.620: INFO: Pod name pod-release: Found 0 pods out of 1
Jul  8 01:50:51.638: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:50:52.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5500" for this suite.

• [SLOW TEST:6.205 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":339,"completed":10,"skipped":118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:50:52.676: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Jul  8 01:50:52.820: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jul  8 01:50:52.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8388 create -f -'
Jul  8 01:50:53.618: INFO: stderr: ""
Jul  8 01:50:53.618: INFO: stdout: "service/agnhost-replica created\n"
Jul  8 01:50:53.619: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jul  8 01:50:53.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8388 create -f -'
Jul  8 01:50:54.242: INFO: stderr: ""
Jul  8 01:50:54.242: INFO: stdout: "service/agnhost-primary created\n"
Jul  8 01:50:54.242: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul  8 01:50:54.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8388 create -f -'
Jul  8 01:50:54.855: INFO: stderr: ""
Jul  8 01:50:54.855: INFO: stdout: "service/frontend created\n"
Jul  8 01:50:54.855: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul  8 01:50:54.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8388 create -f -'
Jul  8 01:50:55.506: INFO: stderr: ""
Jul  8 01:50:55.506: INFO: stdout: "deployment.apps/frontend created\n"
Jul  8 01:50:55.506: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul  8 01:50:55.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8388 create -f -'
Jul  8 01:50:56.076: INFO: stderr: ""
Jul  8 01:50:56.076: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jul  8 01:50:56.076: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul  8 01:50:56.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8388 create -f -'
Jul  8 01:50:56.699: INFO: stderr: ""
Jul  8 01:50:56.699: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jul  8 01:50:56.699: INFO: Waiting for all frontend pods to be Running.
Jul  8 01:51:06.750: INFO: Waiting for frontend to serve content.
Jul  8 01:51:06.758: INFO: Trying to add a new entry to the guestbook.
Jul  8 01:51:06.765: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul  8 01:51:06.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8388 delete --grace-period=0 --force -f -'
Jul  8 01:51:06.923: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  8 01:51:06.923: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jul  8 01:51:06.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8388 delete --grace-period=0 --force -f -'
Jul  8 01:51:07.102: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  8 01:51:07.102: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul  8 01:51:07.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8388 delete --grace-period=0 --force -f -'
Jul  8 01:51:07.208: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  8 01:51:07.208: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul  8 01:51:07.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8388 delete --grace-period=0 --force -f -'
Jul  8 01:51:07.316: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  8 01:51:07.316: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul  8 01:51:07.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8388 delete --grace-period=0 --force -f -'
Jul  8 01:51:07.521: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  8 01:51:07.521: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul  8 01:51:07.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8388 delete --grace-period=0 --force -f -'
Jul  8 01:51:07.795: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  8 01:51:07.795: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:51:07.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8388" for this suite.

• [SLOW TEST:15.197 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:336
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":339,"completed":11,"skipped":161,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:51:07.873: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 01:51:08.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdaf0ec4-b9b3-4abc-b3b1-7000a6c4bc4e" in namespace "downward-api-1427" to be "Succeeded or Failed"
Jul  8 01:51:08.071: INFO: Pod "downwardapi-volume-bdaf0ec4-b9b3-4abc-b3b1-7000a6c4bc4e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.706549ms
Jul  8 01:51:10.078: INFO: Pod "downwardapi-volume-bdaf0ec4-b9b3-4abc-b3b1-7000a6c4bc4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030469331s
STEP: Saw pod success
Jul  8 01:51:10.078: INFO: Pod "downwardapi-volume-bdaf0ec4-b9b3-4abc-b3b1-7000a6c4bc4e" satisfied condition "Succeeded or Failed"
Jul  8 01:51:10.080: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-bdaf0ec4-b9b3-4abc-b3b1-7000a6c4bc4e container client-container: <nil>
STEP: delete the pod
Jul  8 01:51:10.102: INFO: Waiting for pod downwardapi-volume-bdaf0ec4-b9b3-4abc-b3b1-7000a6c4bc4e to disappear
Jul  8 01:51:10.104: INFO: Pod downwardapi-volume-bdaf0ec4-b9b3-4abc-b3b1-7000a6c4bc4e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:51:10.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1427" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":339,"completed":12,"skipped":163,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:51:10.114: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4022
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4022.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4022.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4022.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4022.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4022.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4022.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4022.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4022.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4022.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4022.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4022.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 38.221.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.221.38_udp@PTR;check="$$(dig +tcp +noall +answer +search 38.221.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.221.38_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4022.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4022.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4022.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4022.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4022.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4022.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4022.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4022.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4022.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4022.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4022.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 38.221.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.221.38_udp@PTR;check="$$(dig +tcp +noall +answer +search 38.221.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.221.38_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  8 01:51:22.347: INFO: Unable to read wheezy_udp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:22.350: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:22.352: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:22.355: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:22.371: INFO: Unable to read jessie_udp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:22.374: INFO: Unable to read jessie_tcp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:22.376: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:22.378: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:22.391: INFO: Lookups using dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80 failed for: [wheezy_udp@dns-test-service.dns-4022.svc.cluster.local wheezy_tcp@dns-test-service.dns-4022.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local jessie_udp@dns-test-service.dns-4022.svc.cluster.local jessie_tcp@dns-test-service.dns-4022.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local]

Jul  8 01:51:27.395: INFO: Unable to read wheezy_udp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:27.398: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:27.401: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:27.404: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:27.421: INFO: Unable to read jessie_udp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:27.423: INFO: Unable to read jessie_tcp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:27.425: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:27.427: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:27.446: INFO: Lookups using dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80 failed for: [wheezy_udp@dns-test-service.dns-4022.svc.cluster.local wheezy_tcp@dns-test-service.dns-4022.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local jessie_udp@dns-test-service.dns-4022.svc.cluster.local jessie_tcp@dns-test-service.dns-4022.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local]

Jul  8 01:51:32.396: INFO: Unable to read wheezy_udp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:32.399: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:32.401: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:32.404: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:32.422: INFO: Unable to read jessie_udp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:32.432: INFO: Unable to read jessie_tcp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:32.435: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:32.438: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:32.472: INFO: Lookups using dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80 failed for: [wheezy_udp@dns-test-service.dns-4022.svc.cluster.local wheezy_tcp@dns-test-service.dns-4022.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local jessie_udp@dns-test-service.dns-4022.svc.cluster.local jessie_tcp@dns-test-service.dns-4022.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local]

Jul  8 01:51:37.395: INFO: Unable to read wheezy_udp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:37.398: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:37.400: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:37.406: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:37.422: INFO: Unable to read jessie_udp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:37.424: INFO: Unable to read jessie_tcp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:37.427: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:37.429: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:37.443: INFO: Lookups using dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80 failed for: [wheezy_udp@dns-test-service.dns-4022.svc.cluster.local wheezy_tcp@dns-test-service.dns-4022.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local jessie_udp@dns-test-service.dns-4022.svc.cluster.local jessie_tcp@dns-test-service.dns-4022.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local]

Jul  8 01:51:42.418: INFO: Unable to read jessie_udp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:42.423: INFO: Unable to read jessie_tcp@dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:42.426: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:42.428: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local from pod dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80: the server could not find the requested resource (get pods dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80)
Jul  8 01:51:42.442: INFO: Lookups using dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80 failed for: [jessie_udp@dns-test-service.dns-4022.svc.cluster.local jessie_tcp@dns-test-service.dns-4022.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4022.svc.cluster.local]

Jul  8 01:51:47.446: INFO: DNS probes using dns-4022/dns-test-8c1471e8-26c5-4987-8d4c-a57d7f1bbd80 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:51:47.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4022" for this suite.

• [SLOW TEST:37.523 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":339,"completed":13,"skipped":171,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:51:47.637: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3156
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 01:51:47.781: INFO: Creating ReplicaSet my-hostname-basic-2f4b5deb-924e-4bb9-9990-7514666f67be
Jul  8 01:51:47.790: INFO: Pod name my-hostname-basic-2f4b5deb-924e-4bb9-9990-7514666f67be: Found 0 pods out of 1
Jul  8 01:51:52.804: INFO: Pod name my-hostname-basic-2f4b5deb-924e-4bb9-9990-7514666f67be: Found 1 pods out of 1
Jul  8 01:51:52.804: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2f4b5deb-924e-4bb9-9990-7514666f67be" is running
Jul  8 01:51:52.807: INFO: Pod "my-hostname-basic-2f4b5deb-924e-4bb9-9990-7514666f67be-h8xld" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-08 01:51:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-08 01:51:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-08 01:51:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-08 01:51:47 +0000 UTC Reason: Message:}])
Jul  8 01:51:52.807: INFO: Trying to dial the pod
Jul  8 01:51:57.824: INFO: Controller my-hostname-basic-2f4b5deb-924e-4bb9-9990-7514666f67be: Got expected result from replica 1 [my-hostname-basic-2f4b5deb-924e-4bb9-9990-7514666f67be-h8xld]: "my-hostname-basic-2f4b5deb-924e-4bb9-9990-7514666f67be-h8xld", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:51:57.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3156" for this suite.

• [SLOW TEST:10.200 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":14,"skipped":174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:51:57.837: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7395
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  8 01:52:00.227: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:52:00.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7395" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":15,"skipped":202,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:52:00.255: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul  8 01:52:00.437: INFO: The status of Pod pod-update-activedeadlineseconds-b1c99f25-e24d-4769-a443-40d62282dde6 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:52:02.467: INFO: The status of Pod pod-update-activedeadlineseconds-b1c99f25-e24d-4769-a443-40d62282dde6 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul  8 01:52:03.006: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b1c99f25-e24d-4769-a443-40d62282dde6"
Jul  8 01:52:03.006: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b1c99f25-e24d-4769-a443-40d62282dde6" in namespace "pods-9608" to be "terminated due to deadline exceeded"
Jul  8 01:52:03.013: INFO: Pod "pod-update-activedeadlineseconds-b1c99f25-e24d-4769-a443-40d62282dde6": Phase="Running", Reason="", readiness=true. Elapsed: 6.226349ms
Jul  8 01:52:05.020: INFO: Pod "pod-update-activedeadlineseconds-b1c99f25-e24d-4769-a443-40d62282dde6": Phase="Running", Reason="", readiness=true. Elapsed: 2.013396957s
Jul  8 01:52:07.027: INFO: Pod "pod-update-activedeadlineseconds-b1c99f25-e24d-4769-a443-40d62282dde6": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.020513783s
Jul  8 01:52:07.027: INFO: Pod "pod-update-activedeadlineseconds-b1c99f25-e24d-4769-a443-40d62282dde6" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:52:07.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9608" for this suite.

• [SLOW TEST:6.782 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":339,"completed":16,"skipped":222,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:52:07.037: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-830
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:52:07.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-830" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":339,"completed":17,"skipped":225,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:52:07.196: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4808
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 01:52:07.337: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:52:10.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4808" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":339,"completed":18,"skipped":281,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:52:10.483: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul  8 01:52:10.639: INFO: Waiting up to 5m0s for pod "downward-api-8f4f17f6-6e1b-4568-8ffa-bd2220bef804" in namespace "downward-api-2955" to be "Succeeded or Failed"
Jul  8 01:52:10.655: INFO: Pod "downward-api-8f4f17f6-6e1b-4568-8ffa-bd2220bef804": Phase="Pending", Reason="", readiness=false. Elapsed: 15.762468ms
Jul  8 01:52:12.661: INFO: Pod "downward-api-8f4f17f6-6e1b-4568-8ffa-bd2220bef804": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021984833s
STEP: Saw pod success
Jul  8 01:52:12.661: INFO: Pod "downward-api-8f4f17f6-6e1b-4568-8ffa-bd2220bef804" satisfied condition "Succeeded or Failed"
Jul  8 01:52:12.664: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downward-api-8f4f17f6-6e1b-4568-8ffa-bd2220bef804 container dapi-container: <nil>
STEP: delete the pod
Jul  8 01:52:12.683: INFO: Waiting for pod downward-api-8f4f17f6-6e1b-4568-8ffa-bd2220bef804 to disappear
Jul  8 01:52:12.685: INFO: Pod downward-api-8f4f17f6-6e1b-4568-8ffa-bd2220bef804 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:52:12.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2955" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":339,"completed":19,"skipped":286,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:52:12.702: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4924
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4924.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4924.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  8 01:52:14.893: INFO: DNS probes using dns-4924/dns-test-be01791d-3751-4317-aedd-44b328902913 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:52:14.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4924" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":339,"completed":20,"skipped":292,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:52:14.951: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1826
Jul  8 01:52:15.115: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:52:17.121: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jul  8 01:52:17.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1826 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul  8 01:52:17.387: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul  8 01:52:17.387: INFO: stdout: "iptables"
Jul  8 01:52:17.387: INFO: proxyMode: iptables
Jul  8 01:52:17.403: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul  8 01:52:17.406: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1826
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1826
I0708 01:52:17.433032      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1826, replica count: 3
I0708 01:52:20.483971      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  8 01:52:20.492: INFO: Creating new exec pod
Jul  8 01:52:23.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1826 exec execpod-affinityrvsz6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jul  8 01:52:23.668: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout 80\n+ echo hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jul  8 01:52:23.668: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 01:52:23.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1826 exec execpod-affinityrvsz6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.90.150 80'
Jul  8 01:52:23.842: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.90.150 80\nConnection to 10.43.90.150 80 port [tcp/http] succeeded!\n"
Jul  8 01:52:23.842: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 01:52:23.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1826 exec execpod-affinityrvsz6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.90.150:80/ ; done'
Jul  8 01:52:24.071: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n"
Jul  8 01:52:24.071: INFO: stdout: "\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn\naffinity-clusterip-timeout-sdxrn"
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Received response from host: affinity-clusterip-timeout-sdxrn
Jul  8 01:52:24.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1826 exec execpod-affinityrvsz6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.43.90.150:80/'
Jul  8 01:52:24.228: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n"
Jul  8 01:52:24.228: INFO: stdout: "affinity-clusterip-timeout-sdxrn"
Jul  8 01:52:44.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1826 exec execpod-affinityrvsz6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.43.90.150:80/'
Jul  8 01:52:44.405: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.43.90.150:80/\n"
Jul  8 01:52:44.405: INFO: stdout: "affinity-clusterip-timeout-wf6mx"
Jul  8 01:52:44.405: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1826, will wait for the garbage collector to delete the pods
Jul  8 01:52:44.512: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.252457ms
Jul  8 01:52:44.614: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.226055ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:52:58.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1826" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:43.663 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":21,"skipped":308,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:52:58.617: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 01:52:58.787: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e41322cf-81af-4ff6-a506-b8ffd78350f0" in namespace "downward-api-4933" to be "Succeeded or Failed"
Jul  8 01:52:58.791: INFO: Pod "downwardapi-volume-e41322cf-81af-4ff6-a506-b8ffd78350f0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.975031ms
Jul  8 01:53:00.800: INFO: Pod "downwardapi-volume-e41322cf-81af-4ff6-a506-b8ffd78350f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013823471s
STEP: Saw pod success
Jul  8 01:53:00.800: INFO: Pod "downwardapi-volume-e41322cf-81af-4ff6-a506-b8ffd78350f0" satisfied condition "Succeeded or Failed"
Jul  8 01:53:00.802: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-e41322cf-81af-4ff6-a506-b8ffd78350f0 container client-container: <nil>
STEP: delete the pod
Jul  8 01:53:00.828: INFO: Waiting for pod downwardapi-volume-e41322cf-81af-4ff6-a506-b8ffd78350f0 to disappear
Jul  8 01:53:00.832: INFO: Pod downwardapi-volume-e41322cf-81af-4ff6-a506-b8ffd78350f0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:53:00.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4933" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":22,"skipped":360,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:53:00.841: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3915
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-3915
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jul  8 01:53:00.996: INFO: Found 0 stateful pods, waiting for 3
Jul  8 01:53:11.009: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 01:53:11.009: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 01:53:11.009: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul  8 01:53:21.006: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 01:53:21.006: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 01:53:21.006: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul  8 01:53:31.005: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 01:53:31.005: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 01:53:31.005: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Jul  8 01:53:31.036: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul  8 01:53:41.080: INFO: Updating stateful set ss2
Jul  8 01:53:41.094: INFO: Waiting for Pod statefulset-3915/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Jul  8 01:53:51.190: INFO: Found 2 stateful pods, waiting for 3
Jul  8 01:54:01.201: INFO: Found 2 stateful pods, waiting for 3
Jul  8 01:54:11.199: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 01:54:11.199: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 01:54:11.199: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul  8 01:54:11.225: INFO: Updating stateful set ss2
Jul  8 01:54:11.247: INFO: Waiting for Pod statefulset-3915/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul  8 01:54:21.276: INFO: Updating stateful set ss2
Jul  8 01:54:21.286: INFO: Waiting for StatefulSet statefulset-3915/ss2 to complete update
Jul  8 01:54:21.286: INFO: Waiting for Pod statefulset-3915/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul  8 01:54:31.303: INFO: Waiting for StatefulSet statefulset-3915/ss2 to complete update
Jul  8 01:54:31.303: INFO: Waiting for Pod statefulset-3915/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul  8 01:54:41.296: INFO: Waiting for StatefulSet statefulset-3915/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul  8 01:54:51.299: INFO: Deleting all statefulset in ns statefulset-3915
Jul  8 01:54:51.301: INFO: Scaling statefulset ss2 to 0
Jul  8 01:55:21.331: INFO: Waiting for statefulset status.replicas updated to 0
Jul  8 01:55:21.334: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:55:21.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3915" for this suite.

• [SLOW TEST:140.566 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":339,"completed":23,"skipped":380,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:55:21.408: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-3e404c42-7b9e-4167-b5d2-23b21e2f1041
STEP: Creating a pod to test consume secrets
Jul  8 01:55:21.588: INFO: Waiting up to 5m0s for pod "pod-secrets-97fa3125-3760-4412-86a2-dd8a5404be08" in namespace "secrets-2289" to be "Succeeded or Failed"
Jul  8 01:55:21.595: INFO: Pod "pod-secrets-97fa3125-3760-4412-86a2-dd8a5404be08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.252013ms
Jul  8 01:55:23.601: INFO: Pod "pod-secrets-97fa3125-3760-4412-86a2-dd8a5404be08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012393987s
STEP: Saw pod success
Jul  8 01:55:23.601: INFO: Pod "pod-secrets-97fa3125-3760-4412-86a2-dd8a5404be08" satisfied condition "Succeeded or Failed"
Jul  8 01:55:23.603: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-secrets-97fa3125-3760-4412-86a2-dd8a5404be08 container secret-volume-test: <nil>
STEP: delete the pod
Jul  8 01:55:23.623: INFO: Waiting for pod pod-secrets-97fa3125-3760-4412-86a2-dd8a5404be08 to disappear
Jul  8 01:55:23.625: INFO: Pod pod-secrets-97fa3125-3760-4412-86a2-dd8a5404be08 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:55:23.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2289" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":24,"skipped":380,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:55:23.637: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1308
STEP: creating the pod
Jul  8 01:55:23.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-2068 create -f -'
Jul  8 01:55:24.158: INFO: stderr: ""
Jul  8 01:55:24.158: INFO: stdout: "pod/pause created\n"
Jul  8 01:55:24.158: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul  8 01:55:24.158: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2068" to be "running and ready"
Jul  8 01:55:24.162: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.405117ms
Jul  8 01:55:26.168: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010796304s
Jul  8 01:55:28.177: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.019030548s
Jul  8 01:55:28.177: INFO: Pod "pause" satisfied condition "running and ready"
Jul  8 01:55:28.177: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Jul  8 01:55:28.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-2068 label pods pause testing-label=testing-label-value'
Jul  8 01:55:28.325: INFO: stderr: ""
Jul  8 01:55:28.325: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul  8 01:55:28.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-2068 get pod pause -L testing-label'
Jul  8 01:55:28.451: INFO: stderr: ""
Jul  8 01:55:28.451: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul  8 01:55:28.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-2068 label pods pause testing-label-'
Jul  8 01:55:28.580: INFO: stderr: ""
Jul  8 01:55:28.580: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul  8 01:55:28.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-2068 get pod pause -L testing-label'
Jul  8 01:55:28.683: INFO: stderr: ""
Jul  8 01:55:28.683: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: using delete to clean up resources
Jul  8 01:55:28.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-2068 delete --grace-period=0 --force -f -'
Jul  8 01:55:28.857: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  8 01:55:28.857: INFO: stdout: "pod \"pause\" force deleted\n"
Jul  8 01:55:28.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-2068 get rc,svc -l name=pause --no-headers'
Jul  8 01:55:29.065: INFO: stderr: "No resources found in kubectl-2068 namespace.\n"
Jul  8 01:55:29.071: INFO: stdout: ""
Jul  8 01:55:29.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-2068 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  8 01:55:29.248: INFO: stderr: ""
Jul  8 01:55:29.248: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:55:29.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2068" for this suite.

• [SLOW TEST:5.622 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":339,"completed":25,"skipped":403,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:55:29.259: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul  8 01:55:29.429: INFO: Waiting up to 5m0s for pod "pod-8f76001a-3906-4624-84ff-2f0c8833e725" in namespace "emptydir-5463" to be "Succeeded or Failed"
Jul  8 01:55:29.434: INFO: Pod "pod-8f76001a-3906-4624-84ff-2f0c8833e725": Phase="Pending", Reason="", readiness=false. Elapsed: 4.982216ms
Jul  8 01:55:31.441: INFO: Pod "pod-8f76001a-3906-4624-84ff-2f0c8833e725": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012245165s
STEP: Saw pod success
Jul  8 01:55:31.441: INFO: Pod "pod-8f76001a-3906-4624-84ff-2f0c8833e725" satisfied condition "Succeeded or Failed"
Jul  8 01:55:31.444: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-8f76001a-3906-4624-84ff-2f0c8833e725 container test-container: <nil>
STEP: delete the pod
Jul  8 01:55:31.463: INFO: Waiting for pod pod-8f76001a-3906-4624-84ff-2f0c8833e725 to disappear
Jul  8 01:55:31.466: INFO: Pod pod-8f76001a-3906-4624-84ff-2f0c8833e725 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:55:31.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5463" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":26,"skipped":415,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:55:31.475: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-258
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0708 01:55:31.617436      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:01.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-258" for this suite.

• [SLOW TEST:90.185 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":339,"completed":27,"skipped":444,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:01.661: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6521
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 01:57:01.815: INFO: Got root ca configmap in namespace "svcaccounts-6521"
Jul  8 01:57:01.820: INFO: Deleted root ca configmap in namespace "svcaccounts-6521"
STEP: waiting for a new root ca configmap created
Jul  8 01:57:02.327: INFO: Recreated root ca configmap in namespace "svcaccounts-6521"
Jul  8 01:57:02.335: INFO: Updated root ca configmap in namespace "svcaccounts-6521"
STEP: waiting for the root ca configmap reconciled
Jul  8 01:57:02.841: INFO: Reconciled root ca configmap in namespace "svcaccounts-6521"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:02.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6521" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":339,"completed":28,"skipped":454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:02.853: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2977
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul  8 01:57:03.008: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  8 01:57:03.015: INFO: Waiting for terminating namespaces to be deleted...
Jul  8 01:57:03.017: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-228.us-east-2.compute.internal before test
Jul  8 01:57:03.028: INFO: etcd-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:35 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.028: INFO: 	Container etcd ready: true, restart count 0
Jul  8 01:57:03.028: INFO: helm-install-rke2-canal-zzkr7 from kube-system started at 2021-07-07 23:40:11 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.028: INFO: 	Container helm ready: false, restart count 0
Jul  8 01:57:03.028: INFO: helm-install-rke2-coredns-7vm9g from kube-system started at 2021-07-07 23:40:11 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.028: INFO: 	Container helm ready: false, restart count 0
Jul  8 01:57:03.028: INFO: helm-install-rke2-kube-proxy-tqnww from kube-system started at 2021-07-07 23:40:11 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.028: INFO: 	Container helm ready: false, restart count 0
Jul  8 01:57:03.029: INFO: helm-install-rke2-metrics-server-xbzj4 from kube-system started at 2021-07-07 23:40:54 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.029: INFO: 	Container helm ready: false, restart count 0
Jul  8 01:57:03.029: INFO: kube-apiserver-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:47 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.029: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul  8 01:57:03.029: INFO: kube-controller-manager-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:57 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.029: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul  8 01:57:03.029: INFO: kube-proxy-vvsjd from kube-system started at 2021-07-07 23:40:24 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.029: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 01:57:03.029: INFO: kube-scheduler-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:55 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.029: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul  8 01:57:03.029: INFO: rke2-canal-kcjjk from kube-system started at 2021-07-07 23:40:25 +0000 UTC (2 container statuses recorded)
Jul  8 01:57:03.029: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 01:57:03.029: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 01:57:03.029: INFO: rke2-coredns-rke2-coredns-65d668ddf9-bd9pd from kube-system started at 2021-07-07 23:40:52 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.029: INFO: 	Container coredns ready: true, restart count 0
Jul  8 01:57:03.029: INFO: rke2-metrics-server-6647ffc866-7z7j7 from kube-system started at 2021-07-07 23:41:00 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.029: INFO: 	Container metrics-server ready: true, restart count 0
Jul  8 01:57:03.029: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-84vqn from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 01:57:03.029: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 01:57:03.029: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  8 01:57:03.029: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-217.us-east-2.compute.internal before test
Jul  8 01:57:03.038: INFO: replace-27095156-98vh4 from cronjob-258 started at 2021-07-08 01:56:00 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.038: INFO: 	Container c ready: true, restart count 0
Jul  8 01:57:03.038: INFO: replace-27095157-84ldn from cronjob-258 started at 2021-07-08 01:57:00 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.038: INFO: 	Container c ready: true, restart count 0
Jul  8 01:57:03.038: INFO: kube-proxy-t4j4c from kube-system started at 2021-07-07 23:44:49 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.038: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 01:57:03.038: INFO: rke2-canal-9wlw7 from kube-system started at 2021-07-07 23:44:49 +0000 UTC (2 container statuses recorded)
Jul  8 01:57:03.038: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 01:57:03.038: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 01:57:03.038: INFO: sonobuoy from sonobuoy started at 2021-07-08 01:44:37 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.038: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  8 01:57:03.038: INFO: sonobuoy-e2e-job-1dfa1a3a74ca4282 from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 01:57:03.038: INFO: 	Container e2e ready: true, restart count 0
Jul  8 01:57:03.038: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 01:57:03.038: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-5s9xh from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 01:57:03.038: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 01:57:03.038: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  8 01:57:03.038: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-226.us-east-2.compute.internal before test
Jul  8 01:57:03.045: INFO: etcd-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:42:57 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.045: INFO: 	Container etcd ready: true, restart count 0
Jul  8 01:57:03.045: INFO: kube-apiserver-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:11 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.045: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul  8 01:57:03.045: INFO: kube-controller-manager-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:22 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.045: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul  8 01:57:03.045: INFO: kube-proxy-87mbg from kube-system started at 2021-07-07 23:43:19 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.045: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 01:57:03.045: INFO: kube-scheduler-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:22 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.045: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul  8 01:57:03.045: INFO: rke2-canal-8n79q from kube-system started at 2021-07-07 23:43:19 +0000 UTC (2 container statuses recorded)
Jul  8 01:57:03.045: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 01:57:03.045: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 01:57:03.045: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-2nxk4 from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 01:57:03.045: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 01:57:03.045: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  8 01:57:03.045: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-8-165.us-east-2.compute.internal before test
Jul  8 01:57:03.051: INFO: etcd-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:41 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.051: INFO: 	Container etcd ready: true, restart count 0
Jul  8 01:57:03.051: INFO: kube-apiserver-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:48 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.051: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul  8 01:57:03.051: INFO: kube-controller-manager-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:59 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.051: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul  8 01:57:03.051: INFO: kube-proxy-p4h8m from kube-system started at 2021-07-07 23:43:56 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.051: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 01:57:03.051: INFO: kube-scheduler-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:58 +0000 UTC (1 container statuses recorded)
Jul  8 01:57:03.051: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul  8 01:57:03.051: INFO: rke2-canal-vnj8b from kube-system started at 2021-07-07 23:43:56 +0000 UTC (2 container statuses recorded)
Jul  8 01:57:03.051: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 01:57:03.051: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 01:57:03.051: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-fndvb from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 01:57:03.051: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 01:57:03.051: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-32d6cc75-676b-4a0b-b01e-55d83882441a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-32d6cc75-676b-4a0b-b01e-55d83882441a off the node ip-172-31-6-217.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-32d6cc75-676b-4a0b-b01e-55d83882441a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:07.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2977" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":339,"completed":29,"skipped":507,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:07.182: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-7648
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:09.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7648" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":30,"skipped":515,"failed":0}
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:09.368: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 01:57:09.530: INFO: The status of Pod busybox-readonly-fsacced6d7-3173-4a05-9d6d-03a32d742bad is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:57:11.538: INFO: The status of Pod busybox-readonly-fsacced6d7-3173-4a05-9d6d-03a32d742bad is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:11.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5482" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":31,"skipped":519,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:11.554: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2022
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-500f0785-4139-41d6-9aba-076ed0aae186
STEP: Creating a pod to test consume secrets
Jul  8 01:57:11.736: INFO: Waiting up to 5m0s for pod "pod-secrets-453f659a-c466-43c9-aebc-3baa402e87f3" in namespace "secrets-2022" to be "Succeeded or Failed"
Jul  8 01:57:11.743: INFO: Pod "pod-secrets-453f659a-c466-43c9-aebc-3baa402e87f3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.318398ms
Jul  8 01:57:13.750: INFO: Pod "pod-secrets-453f659a-c466-43c9-aebc-3baa402e87f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013969692s
STEP: Saw pod success
Jul  8 01:57:13.750: INFO: Pod "pod-secrets-453f659a-c466-43c9-aebc-3baa402e87f3" satisfied condition "Succeeded or Failed"
Jul  8 01:57:13.752: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-secrets-453f659a-c466-43c9-aebc-3baa402e87f3 container secret-env-test: <nil>
STEP: delete the pod
Jul  8 01:57:13.768: INFO: Waiting for pod pod-secrets-453f659a-c466-43c9-aebc-3baa402e87f3 to disappear
Jul  8 01:57:13.775: INFO: Pod pod-secrets-453f659a-c466-43c9-aebc-3baa402e87f3 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:13.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2022" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":339,"completed":32,"skipped":524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:13.785: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1047
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jul  8 01:57:15.951: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1047 PodName:pod-sharedvolume-ac8678ce-0e6d-41c3-92c1-6fbc458ba0c3 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 01:57:15.952: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 01:57:16.020: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:16.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1047" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":339,"completed":33,"skipped":563,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:16.033: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 01:57:16.184: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78497309-c650-4dda-8e5d-50ad2817455a" in namespace "projected-7926" to be "Succeeded or Failed"
Jul  8 01:57:16.194: INFO: Pod "downwardapi-volume-78497309-c650-4dda-8e5d-50ad2817455a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.491449ms
Jul  8 01:57:18.200: INFO: Pod "downwardapi-volume-78497309-c650-4dda-8e5d-50ad2817455a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016081234s
STEP: Saw pod success
Jul  8 01:57:18.200: INFO: Pod "downwardapi-volume-78497309-c650-4dda-8e5d-50ad2817455a" satisfied condition "Succeeded or Failed"
Jul  8 01:57:18.203: INFO: Trying to get logs from node ip-172-31-6-226.us-east-2.compute.internal pod downwardapi-volume-78497309-c650-4dda-8e5d-50ad2817455a container client-container: <nil>
STEP: delete the pod
Jul  8 01:57:18.232: INFO: Waiting for pod downwardapi-volume-78497309-c650-4dda-8e5d-50ad2817455a to disappear
Jul  8 01:57:18.236: INFO: Pod downwardapi-volume-78497309-c650-4dda-8e5d-50ad2817455a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:18.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7926" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":34,"skipped":563,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:18.246: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-8415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul  8 01:57:18.427: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul  8 01:57:18.436: INFO: starting watch
STEP: patching
STEP: updating
Jul  8 01:57:18.459: INFO: waiting for watch events with expected annotations
Jul  8 01:57:18.459: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:18.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-8415" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":339,"completed":35,"skipped":575,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:18.536: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 01:57:18.955: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 01:57:21.978: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:22.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1310" for this suite.
STEP: Destroying namespace "webhook-1310-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":339,"completed":36,"skipped":577,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:22.152: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-987.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-987.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-987.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-987.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-987.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-987.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  8 01:57:24.391: INFO: DNS probes using dns-987/dns-test-456171ee-40e5-441a-9638-79406d217ec8 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:24.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-987" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":339,"completed":37,"skipped":592,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:24.465: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-7749
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jul  8 01:57:24.696: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jul  8 01:57:24.713: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:24.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7749" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":339,"completed":38,"skipped":628,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:24.747: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1775
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1775
STEP: creating service affinity-nodeport-transition in namespace services-1775
STEP: creating replication controller affinity-nodeport-transition in namespace services-1775
I0708 01:57:24.928619      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-1775, replica count: 3
I0708 01:57:27.979687      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  8 01:57:27.992: INFO: Creating new exec pod
Jul  8 01:57:31.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1775 exec execpod-affinitybvk6g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jul  8 01:57:31.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jul  8 01:57:31.222: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 01:57:31.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1775 exec execpod-affinitybvk6g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.18.33 80'
Jul  8 01:57:31.395: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.18.33 80\nConnection to 10.43.18.33 80 port [tcp/http] succeeded!\n"
Jul  8 01:57:31.395: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 01:57:31.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1775 exec execpod-affinitybvk6g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.6.226 30852'
Jul  8 01:57:31.567: INFO: stderr: "+ nc -v -t -w 2 172.31.6.226 30852\n+ echo hostName\nConnection to 172.31.6.226 30852 port [tcp/*] succeeded!\n"
Jul  8 01:57:31.568: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 01:57:31.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1775 exec execpod-affinitybvk6g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.8.165 30852'
Jul  8 01:57:31.738: INFO: stderr: "+ + nc -v -techo -w 2 hostName 172.31.8.165\n 30852\nConnection to 172.31.8.165 30852 port [tcp/*] succeeded!\n"
Jul  8 01:57:31.738: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 01:57:31.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1775 exec execpod-affinitybvk6g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.228:30852/ ; done'
Jul  8 01:57:31.990: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n"
Jul  8 01:57:31.990: INFO: stdout: "\naffinity-nodeport-transition-x24b8\naffinity-nodeport-transition-xv7tx\naffinity-nodeport-transition-xv7tx\naffinity-nodeport-transition-x24b8\naffinity-nodeport-transition-x24b8\naffinity-nodeport-transition-xv7tx\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-xv7tx\naffinity-nodeport-transition-x24b8\naffinity-nodeport-transition-x24b8\naffinity-nodeport-transition-x24b8\naffinity-nodeport-transition-xv7tx\naffinity-nodeport-transition-x24b8\naffinity-nodeport-transition-x24b8\naffinity-nodeport-transition-x24b8"
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-x24b8
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-xv7tx
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-xv7tx
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-x24b8
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-x24b8
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-xv7tx
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-xv7tx
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-x24b8
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-x24b8
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-x24b8
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-xv7tx
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-x24b8
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-x24b8
Jul  8 01:57:31.991: INFO: Received response from host: affinity-nodeport-transition-x24b8
Jul  8 01:57:32.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1775 exec execpod-affinitybvk6g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.228:30852/ ; done'
Jul  8 01:57:32.295: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30852/\n"
Jul  8 01:57:32.295: INFO: stdout: "\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849\naffinity-nodeport-transition-bz849"
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Received response from host: affinity-nodeport-transition-bz849
Jul  8 01:57:32.295: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1775, will wait for the garbage collector to delete the pods
Jul  8 01:57:32.387: INFO: Deleting ReplicationController affinity-nodeport-transition took: 13.752232ms
Jul  8 01:57:32.488: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.730491ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:48.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1775" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:23.917 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":39,"skipped":649,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:48.665: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3360
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jul  8 01:57:48.918: INFO: The status of Pod labelsupdate0af8d712-8971-4e39-b19e-9d1b276a5d41 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:57:50.923: INFO: The status of Pod labelsupdate0af8d712-8971-4e39-b19e-9d1b276a5d41 is Running (Ready = true)
Jul  8 01:57:51.449: INFO: Successfully updated pod "labelsupdate0af8d712-8971-4e39-b19e-9d1b276a5d41"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:55.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3360" for this suite.

• [SLOW TEST:6.835 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":40,"skipped":651,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:55.500: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-707
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul  8 01:57:55.661: INFO: The status of Pod pod-update-bbbe3359-ff22-4413-8b73-211a820c3aaa is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:57:57.668: INFO: The status of Pod pod-update-bbbe3359-ff22-4413-8b73-211a820c3aaa is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul  8 01:57:58.186: INFO: Successfully updated pod "pod-update-bbbe3359-ff22-4413-8b73-211a820c3aaa"
STEP: verifying the updated pod is in kubernetes
Jul  8 01:57:58.193: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:57:58.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-707" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":339,"completed":41,"skipped":655,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:57:58.204: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7934
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 01:57:58.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17b4d673-1723-4d21-bca4-37d585bc091a" in namespace "downward-api-7934" to be "Succeeded or Failed"
Jul  8 01:57:58.360: INFO: Pod "downwardapi-volume-17b4d673-1723-4d21-bca4-37d585bc091a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.082492ms
Jul  8 01:58:00.364: INFO: Pod "downwardapi-volume-17b4d673-1723-4d21-bca4-37d585bc091a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008468109s
STEP: Saw pod success
Jul  8 01:58:00.364: INFO: Pod "downwardapi-volume-17b4d673-1723-4d21-bca4-37d585bc091a" satisfied condition "Succeeded or Failed"
Jul  8 01:58:00.367: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-17b4d673-1723-4d21-bca4-37d585bc091a container client-container: <nil>
STEP: delete the pod
Jul  8 01:58:00.386: INFO: Waiting for pod downwardapi-volume-17b4d673-1723-4d21-bca4-37d585bc091a to disappear
Jul  8 01:58:00.389: INFO: Pod downwardapi-volume-17b4d673-1723-4d21-bca4-37d585bc091a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:58:00.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7934" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":42,"skipped":709,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:58:00.402: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-303
STEP: creating service affinity-clusterip in namespace services-303
STEP: creating replication controller affinity-clusterip in namespace services-303
I0708 01:58:00.602307      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-303, replica count: 3
I0708 01:58:03.655364      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  8 01:58:03.664: INFO: Creating new exec pod
Jul  8 01:58:06.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-303 exec execpod-affinityk7md2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jul  8 01:58:06.877: INFO: stderr: "+ echo hostName+ nc -v -t -w 2 affinity-clusterip 80\n\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jul  8 01:58:06.877: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 01:58:06.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-303 exec execpod-affinityk7md2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.130.41 80'
Jul  8 01:58:07.041: INFO: stderr: "+ nc -v -t -w 2 10.43.130.41 80\n+ echo hostName\nConnection to 10.43.130.41 80 port [tcp/http] succeeded!\n"
Jul  8 01:58:07.041: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 01:58:07.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-303 exec execpod-affinityk7md2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.130.41:80/ ; done'
Jul  8 01:58:07.322: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.130.41:80/\n"
Jul  8 01:58:07.322: INFO: stdout: "\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7\naffinity-clusterip-zvmj7"
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Received response from host: affinity-clusterip-zvmj7
Jul  8 01:58:07.322: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-303, will wait for the garbage collector to delete the pods
Jul  8 01:58:07.407: INFO: Deleting ReplicationController affinity-clusterip took: 5.430209ms
Jul  8 01:58:07.514: INFO: Terminating ReplicationController affinity-clusterip pods took: 107.433284ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:58:18.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-303" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:18.272 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":43,"skipped":715,"failed":0}
SS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:58:18.677: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-6914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul  8 01:58:19.291: INFO: starting watch
STEP: patching
STEP: updating
Jul  8 01:58:19.314: INFO: waiting for watch events with expected annotations
Jul  8 01:58:19.315: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:58:19.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6914" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":339,"completed":44,"skipped":717,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:58:19.374: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:58:19.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-333" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":339,"completed":45,"skipped":721,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:58:19.528: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5514
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4cgwh in namespace proxy-5514
I0708 01:58:19.691406      20 runners.go:190] Created replication controller with name: proxy-service-4cgwh, namespace: proxy-5514, replica count: 1
I0708 01:58:20.742939      20 runners.go:190] proxy-service-4cgwh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0708 01:58:21.743998      20 runners.go:190] proxy-service-4cgwh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  8 01:58:21.750: INFO: setup took 2.082083359s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul  8 01:58:21.760: INFO: (0) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 10.134678ms)
Jul  8 01:58:21.761: INFO: (0) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 10.433418ms)
Jul  8 01:58:21.765: INFO: (0) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 14.660986ms)
Jul  8 01:58:21.765: INFO: (0) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 15.320155ms)
Jul  8 01:58:21.766: INFO: (0) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 16.146985ms)
Jul  8 01:58:21.766: INFO: (0) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 16.025524ms)
Jul  8 01:58:21.778: INFO: (0) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 27.862283ms)
Jul  8 01:58:21.778: INFO: (0) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 27.538227ms)
Jul  8 01:58:21.778: INFO: (0) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 26.987685ms)
Jul  8 01:58:21.779: INFO: (0) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 28.230152ms)
Jul  8 01:58:21.779: INFO: (0) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 27.506279ms)
Jul  8 01:58:21.779: INFO: (0) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 27.795597ms)
Jul  8 01:58:21.779: INFO: (0) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 27.907417ms)
Jul  8 01:58:21.779: INFO: (0) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 28.771425ms)
Jul  8 01:58:21.779: INFO: (0) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 27.820426ms)
Jul  8 01:58:21.780: INFO: (0) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 28.563416ms)
Jul  8 01:58:21.785: INFO: (1) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 5.632892ms)
Jul  8 01:58:21.786: INFO: (1) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 6.205766ms)
Jul  8 01:58:21.789: INFO: (1) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 8.473762ms)
Jul  8 01:58:21.789: INFO: (1) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 9.313975ms)
Jul  8 01:58:21.790: INFO: (1) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 10.375712ms)
Jul  8 01:58:21.792: INFO: (1) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 11.696546ms)
Jul  8 01:58:21.792: INFO: (1) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 11.346193ms)
Jul  8 01:58:21.792: INFO: (1) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 11.698782ms)
Jul  8 01:58:21.792: INFO: (1) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 12.326736ms)
Jul  8 01:58:21.792: INFO: (1) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 12.059953ms)
Jul  8 01:58:21.793: INFO: (1) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 12.887151ms)
Jul  8 01:58:21.793: INFO: (1) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 13.181507ms)
Jul  8 01:58:21.794: INFO: (1) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 14.358209ms)
Jul  8 01:58:21.795: INFO: (1) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 14.973531ms)
Jul  8 01:58:21.795: INFO: (1) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 15.051469ms)
Jul  8 01:58:21.796: INFO: (1) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 15.368336ms)
Jul  8 01:58:21.801: INFO: (2) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 5.169732ms)
Jul  8 01:58:21.801: INFO: (2) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 5.367495ms)
Jul  8 01:58:21.813: INFO: (2) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 16.379697ms)
Jul  8 01:58:21.813: INFO: (2) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 16.815525ms)
Jul  8 01:58:21.813: INFO: (2) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 16.596492ms)
Jul  8 01:58:21.813: INFO: (2) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 16.830914ms)
Jul  8 01:58:21.813: INFO: (2) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 16.605753ms)
Jul  8 01:58:21.814: INFO: (2) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 17.475923ms)
Jul  8 01:58:21.814: INFO: (2) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 17.745881ms)
Jul  8 01:58:21.814: INFO: (2) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 17.086523ms)
Jul  8 01:58:21.816: INFO: (2) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 20.157946ms)
Jul  8 01:58:21.816: INFO: (2) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 19.736163ms)
Jul  8 01:58:21.816: INFO: (2) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 20.428471ms)
Jul  8 01:58:21.816: INFO: (2) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 20.164093ms)
Jul  8 01:58:21.817: INFO: (2) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 21.162308ms)
Jul  8 01:58:21.818: INFO: (2) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 21.897266ms)
Jul  8 01:58:21.829: INFO: (3) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 10.318045ms)
Jul  8 01:58:21.832: INFO: (3) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 13.106404ms)
Jul  8 01:58:21.832: INFO: (3) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 13.695723ms)
Jul  8 01:58:21.832: INFO: (3) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 13.651768ms)
Jul  8 01:58:21.833: INFO: (3) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 14.320305ms)
Jul  8 01:58:21.833: INFO: (3) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 14.39821ms)
Jul  8 01:58:21.833: INFO: (3) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 14.944111ms)
Jul  8 01:58:21.834: INFO: (3) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 15.37843ms)
Jul  8 01:58:21.834: INFO: (3) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 15.402032ms)
Jul  8 01:58:21.834: INFO: (3) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 15.044473ms)
Jul  8 01:58:21.835: INFO: (3) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 16.625029ms)
Jul  8 01:58:21.837: INFO: (3) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 18.707665ms)
Jul  8 01:58:21.837: INFO: (3) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 18.411103ms)
Jul  8 01:58:21.837: INFO: (3) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 18.162106ms)
Jul  8 01:58:21.837: INFO: (3) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 18.410759ms)
Jul  8 01:58:21.837: INFO: (3) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 18.894551ms)
Jul  8 01:58:21.847: INFO: (4) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 9.768962ms)
Jul  8 01:58:21.848: INFO: (4) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 9.928525ms)
Jul  8 01:58:21.850: INFO: (4) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 12.473796ms)
Jul  8 01:58:21.850: INFO: (4) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 12.046829ms)
Jul  8 01:58:21.850: INFO: (4) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 12.496062ms)
Jul  8 01:58:21.850: INFO: (4) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 12.672757ms)
Jul  8 01:58:21.851: INFO: (4) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 12.979491ms)
Jul  8 01:58:21.851: INFO: (4) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 13.05796ms)
Jul  8 01:58:21.851: INFO: (4) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 12.593659ms)
Jul  8 01:58:21.851: INFO: (4) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 12.730881ms)
Jul  8 01:58:21.851: INFO: (4) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 13.184202ms)
Jul  8 01:58:21.851: INFO: (4) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 12.607419ms)
Jul  8 01:58:21.851: INFO: (4) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 13.53361ms)
Jul  8 01:58:21.851: INFO: (4) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 13.036412ms)
Jul  8 01:58:21.851: INFO: (4) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 13.462228ms)
Jul  8 01:58:21.851: INFO: (4) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 12.919976ms)
Jul  8 01:58:21.860: INFO: (5) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 8.380001ms)
Jul  8 01:58:21.860: INFO: (5) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 8.381391ms)
Jul  8 01:58:21.860: INFO: (5) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 8.539277ms)
Jul  8 01:58:21.860: INFO: (5) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 8.727879ms)
Jul  8 01:58:21.860: INFO: (5) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 8.828598ms)
Jul  8 01:58:21.860: INFO: (5) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 8.900384ms)
Jul  8 01:58:21.860: INFO: (5) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 8.835648ms)
Jul  8 01:58:21.860: INFO: (5) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 8.828414ms)
Jul  8 01:58:21.860: INFO: (5) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 9.14078ms)
Jul  8 01:58:21.860: INFO: (5) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 8.721877ms)
Jul  8 01:58:21.865: INFO: (5) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 13.284253ms)
Jul  8 01:58:21.865: INFO: (5) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 13.432202ms)
Jul  8 01:58:21.865: INFO: (5) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 13.624185ms)
Jul  8 01:58:21.865: INFO: (5) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 13.622088ms)
Jul  8 01:58:21.866: INFO: (5) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 14.152322ms)
Jul  8 01:58:21.866: INFO: (5) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 14.21491ms)
Jul  8 01:58:21.870: INFO: (6) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 4.447231ms)
Jul  8 01:58:21.872: INFO: (6) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 5.974025ms)
Jul  8 01:58:21.872: INFO: (6) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 6.052851ms)
Jul  8 01:58:21.874: INFO: (6) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 7.876923ms)
Jul  8 01:58:21.877: INFO: (6) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 10.2934ms)
Jul  8 01:58:21.877: INFO: (6) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 10.559043ms)
Jul  8 01:58:21.877: INFO: (6) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 10.985413ms)
Jul  8 01:58:21.878: INFO: (6) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 11.684475ms)
Jul  8 01:58:21.878: INFO: (6) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 12.033716ms)
Jul  8 01:58:21.878: INFO: (6) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 12.035567ms)
Jul  8 01:58:21.878: INFO: (6) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 12.266151ms)
Jul  8 01:58:21.880: INFO: (6) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 14.506127ms)
Jul  8 01:58:21.881: INFO: (6) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 14.739389ms)
Jul  8 01:58:21.881: INFO: (6) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 15.092441ms)
Jul  8 01:58:21.882: INFO: (6) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 15.569164ms)
Jul  8 01:58:21.882: INFO: (6) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 15.71508ms)
Jul  8 01:58:21.890: INFO: (7) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 7.086245ms)
Jul  8 01:58:21.890: INFO: (7) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 7.525061ms)
Jul  8 01:58:21.890: INFO: (7) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 7.34461ms)
Jul  8 01:58:21.890: INFO: (7) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 8.029658ms)
Jul  8 01:58:21.890: INFO: (7) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 7.484293ms)
Jul  8 01:58:21.890: INFO: (7) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 8.304467ms)
Jul  8 01:58:21.890: INFO: (7) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 7.943382ms)
Jul  8 01:58:21.890: INFO: (7) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 7.575415ms)
Jul  8 01:58:21.890: INFO: (7) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 7.675451ms)
Jul  8 01:58:21.890: INFO: (7) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 7.845156ms)
Jul  8 01:58:21.892: INFO: (7) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 9.64719ms)
Jul  8 01:58:21.894: INFO: (7) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 11.120122ms)
Jul  8 01:58:21.894: INFO: (7) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 11.308424ms)
Jul  8 01:58:21.894: INFO: (7) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 11.328695ms)
Jul  8 01:58:21.894: INFO: (7) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 11.67864ms)
Jul  8 01:58:21.894: INFO: (7) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 12.154742ms)
Jul  8 01:58:21.898: INFO: (8) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 3.848725ms)
Jul  8 01:58:21.901: INFO: (8) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 6.277541ms)
Jul  8 01:58:21.904: INFO: (8) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 9.081729ms)
Jul  8 01:58:21.904: INFO: (8) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 9.396412ms)
Jul  8 01:58:21.904: INFO: (8) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 9.673246ms)
Jul  8 01:58:21.905: INFO: (8) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 9.898047ms)
Jul  8 01:58:21.905: INFO: (8) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 10.153134ms)
Jul  8 01:58:21.905: INFO: (8) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 10.288022ms)
Jul  8 01:58:21.905: INFO: (8) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 10.068815ms)
Jul  8 01:58:21.905: INFO: (8) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 9.883769ms)
Jul  8 01:58:21.906: INFO: (8) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 11.552721ms)
Jul  8 01:58:21.909: INFO: (8) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 13.801066ms)
Jul  8 01:58:21.909: INFO: (8) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 13.719693ms)
Jul  8 01:58:21.909: INFO: (8) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 14.088227ms)
Jul  8 01:58:21.909: INFO: (8) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 14.497717ms)
Jul  8 01:58:21.909: INFO: (8) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 15.110359ms)
Jul  8 01:58:21.920: INFO: (9) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 10.309768ms)
Jul  8 01:58:21.920: INFO: (9) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 10.488783ms)
Jul  8 01:58:21.920: INFO: (9) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 10.610541ms)
Jul  8 01:58:21.921: INFO: (9) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 11.766289ms)
Jul  8 01:58:21.921: INFO: (9) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 11.705411ms)
Jul  8 01:58:21.922: INFO: (9) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 12.050594ms)
Jul  8 01:58:21.922: INFO: (9) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 11.971925ms)
Jul  8 01:58:21.922: INFO: (9) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 11.851663ms)
Jul  8 01:58:21.922: INFO: (9) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 11.979632ms)
Jul  8 01:58:21.922: INFO: (9) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 12.512806ms)
Jul  8 01:58:21.924: INFO: (9) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 14.322881ms)
Jul  8 01:58:21.925: INFO: (9) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 15.531474ms)
Jul  8 01:58:21.925: INFO: (9) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 15.499492ms)
Jul  8 01:58:21.925: INFO: (9) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 15.417105ms)
Jul  8 01:58:21.925: INFO: (9) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 15.36906ms)
Jul  8 01:58:21.926: INFO: (9) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 15.749236ms)
Jul  8 01:58:21.930: INFO: (10) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 4.496438ms)
Jul  8 01:58:21.938: INFO: (10) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 11.718977ms)
Jul  8 01:58:21.938: INFO: (10) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 12.375753ms)
Jul  8 01:58:21.938: INFO: (10) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 11.692683ms)
Jul  8 01:58:21.939: INFO: (10) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 13.101727ms)
Jul  8 01:58:21.939: INFO: (10) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 13.422959ms)
Jul  8 01:58:21.940: INFO: (10) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 13.627007ms)
Jul  8 01:58:21.940: INFO: (10) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 13.91604ms)
Jul  8 01:58:21.940: INFO: (10) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 13.15535ms)
Jul  8 01:58:21.940: INFO: (10) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 13.946086ms)
Jul  8 01:58:21.940: INFO: (10) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 14.310202ms)
Jul  8 01:58:21.941: INFO: (10) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 14.483455ms)
Jul  8 01:58:21.941: INFO: (10) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 14.449643ms)
Jul  8 01:58:21.941: INFO: (10) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 14.765068ms)
Jul  8 01:58:21.945: INFO: (10) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 18.566866ms)
Jul  8 01:58:21.946: INFO: (10) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 19.226849ms)
Jul  8 01:58:21.958: INFO: (11) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 11.285978ms)
Jul  8 01:58:21.958: INFO: (11) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 11.086204ms)
Jul  8 01:58:21.958: INFO: (11) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 11.175627ms)
Jul  8 01:58:21.958: INFO: (11) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 11.272475ms)
Jul  8 01:58:21.959: INFO: (11) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 13.077743ms)
Jul  8 01:58:21.961: INFO: (11) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 14.561244ms)
Jul  8 01:58:21.961: INFO: (11) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 14.500131ms)
Jul  8 01:58:21.961: INFO: (11) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 14.672859ms)
Jul  8 01:58:21.961: INFO: (11) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 14.227035ms)
Jul  8 01:58:21.961: INFO: (11) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 14.357767ms)
Jul  8 01:58:21.961: INFO: (11) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 15.136185ms)
Jul  8 01:58:21.966: INFO: (11) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 19.426314ms)
Jul  8 01:58:21.966: INFO: (11) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 19.864505ms)
Jul  8 01:58:21.966: INFO: (11) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 20.212093ms)
Jul  8 01:58:21.966: INFO: (11) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 20.246802ms)
Jul  8 01:58:21.967: INFO: (11) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 21.205212ms)
Jul  8 01:58:21.974: INFO: (12) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 6.613345ms)
Jul  8 01:58:21.974: INFO: (12) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 6.879954ms)
Jul  8 01:58:21.976: INFO: (12) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 8.187156ms)
Jul  8 01:58:21.976: INFO: (12) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 8.685063ms)
Jul  8 01:58:21.976: INFO: (12) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 8.829234ms)
Jul  8 01:58:21.979: INFO: (12) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 11.478934ms)
Jul  8 01:58:21.979: INFO: (12) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 11.731822ms)
Jul  8 01:58:21.981: INFO: (12) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 13.407309ms)
Jul  8 01:58:21.981: INFO: (12) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 13.508236ms)
Jul  8 01:58:21.981: INFO: (12) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 13.624678ms)
Jul  8 01:58:21.981: INFO: (12) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 13.793988ms)
Jul  8 01:58:21.982: INFO: (12) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 14.025108ms)
Jul  8 01:58:21.982: INFO: (12) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 13.968752ms)
Jul  8 01:58:21.982: INFO: (12) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 14.007536ms)
Jul  8 01:58:21.982: INFO: (12) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 14.164615ms)
Jul  8 01:58:21.982: INFO: (12) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 14.614461ms)
Jul  8 01:58:21.987: INFO: (13) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 4.805459ms)
Jul  8 01:58:21.989: INFO: (13) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 6.588739ms)
Jul  8 01:58:21.989: INFO: (13) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 6.904423ms)
Jul  8 01:58:21.991: INFO: (13) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 8.033674ms)
Jul  8 01:58:21.991: INFO: (13) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 8.002891ms)
Jul  8 01:58:21.991: INFO: (13) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 8.54345ms)
Jul  8 01:58:21.991: INFO: (13) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 8.603353ms)
Jul  8 01:58:21.992: INFO: (13) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 9.396746ms)
Jul  8 01:58:21.992: INFO: (13) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 9.831661ms)
Jul  8 01:58:21.994: INFO: (13) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 11.289567ms)
Jul  8 01:58:21.994: INFO: (13) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 11.368949ms)
Jul  8 01:58:21.994: INFO: (13) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 11.173499ms)
Jul  8 01:58:21.994: INFO: (13) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 11.61697ms)
Jul  8 01:58:21.995: INFO: (13) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 12.052293ms)
Jul  8 01:58:21.996: INFO: (13) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 12.829517ms)
Jul  8 01:58:21.996: INFO: (13) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 12.967243ms)
Jul  8 01:58:22.004: INFO: (14) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 7.751436ms)
Jul  8 01:58:22.004: INFO: (14) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 8.057731ms)
Jul  8 01:58:22.005: INFO: (14) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 8.662496ms)
Jul  8 01:58:22.006: INFO: (14) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 9.564091ms)
Jul  8 01:58:22.006: INFO: (14) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 10.016453ms)
Jul  8 01:58:22.006: INFO: (14) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 10.198769ms)
Jul  8 01:58:22.006: INFO: (14) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 10.371091ms)
Jul  8 01:58:22.006: INFO: (14) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 10.506581ms)
Jul  8 01:58:22.007: INFO: (14) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 11.008348ms)
Jul  8 01:58:22.008: INFO: (14) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 11.364421ms)
Jul  8 01:58:22.008: INFO: (14) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 12.116156ms)
Jul  8 01:58:22.008: INFO: (14) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 11.746141ms)
Jul  8 01:58:22.008: INFO: (14) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 11.868574ms)
Jul  8 01:58:22.009: INFO: (14) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 12.880701ms)
Jul  8 01:58:22.010: INFO: (14) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 13.415728ms)
Jul  8 01:58:22.010: INFO: (14) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 13.636001ms)
Jul  8 01:58:22.014: INFO: (15) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 4.200394ms)
Jul  8 01:58:22.016: INFO: (15) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 5.387534ms)
Jul  8 01:58:22.021: INFO: (15) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 10.555412ms)
Jul  8 01:58:22.021: INFO: (15) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 10.897132ms)
Jul  8 01:58:22.021: INFO: (15) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 10.759404ms)
Jul  8 01:58:22.021: INFO: (15) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 10.722183ms)
Jul  8 01:58:22.021: INFO: (15) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 11.182108ms)
Jul  8 01:58:22.021: INFO: (15) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 11.016543ms)
Jul  8 01:58:22.022: INFO: (15) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 11.459171ms)
Jul  8 01:58:22.022: INFO: (15) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 11.69421ms)
Jul  8 01:58:22.022: INFO: (15) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 11.380899ms)
Jul  8 01:58:22.023: INFO: (15) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 12.085156ms)
Jul  8 01:58:22.023: INFO: (15) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 12.456308ms)
Jul  8 01:58:22.023: INFO: (15) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 13.062132ms)
Jul  8 01:58:22.023: INFO: (15) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 12.971235ms)
Jul  8 01:58:22.023: INFO: (15) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 13.202663ms)
Jul  8 01:58:22.033: INFO: (16) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 9.083041ms)
Jul  8 01:58:22.033: INFO: (16) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 9.265622ms)
Jul  8 01:58:22.033: INFO: (16) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 9.717452ms)
Jul  8 01:58:22.033: INFO: (16) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 9.695432ms)
Jul  8 01:58:22.033: INFO: (16) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 9.421696ms)
Jul  8 01:58:22.034: INFO: (16) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 9.60208ms)
Jul  8 01:58:22.034: INFO: (16) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 10.043373ms)
Jul  8 01:58:22.034: INFO: (16) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 9.889107ms)
Jul  8 01:58:22.039: INFO: (16) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 15.246476ms)
Jul  8 01:58:22.039: INFO: (16) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 15.741662ms)
Jul  8 01:58:22.040: INFO: (16) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 15.796273ms)
Jul  8 01:58:22.040: INFO: (16) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 15.603815ms)
Jul  8 01:58:22.040: INFO: (16) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 16.00982ms)
Jul  8 01:58:22.040: INFO: (16) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 15.741803ms)
Jul  8 01:58:22.040: INFO: (16) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 15.663843ms)
Jul  8 01:58:22.040: INFO: (16) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 15.966428ms)
Jul  8 01:58:22.047: INFO: (17) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 6.428876ms)
Jul  8 01:58:22.047: INFO: (17) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 7.114975ms)
Jul  8 01:58:22.047: INFO: (17) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 7.198129ms)
Jul  8 01:58:22.048: INFO: (17) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 7.420484ms)
Jul  8 01:58:22.048: INFO: (17) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 8.460352ms)
Jul  8 01:58:22.048: INFO: (17) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 7.946864ms)
Jul  8 01:58:22.048: INFO: (17) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 8.164653ms)
Jul  8 01:58:22.049: INFO: (17) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 8.460727ms)
Jul  8 01:58:22.049: INFO: (17) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 8.753282ms)
Jul  8 01:58:22.049: INFO: (17) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 8.57827ms)
Jul  8 01:58:22.049: INFO: (17) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 9.168497ms)
Jul  8 01:58:22.051: INFO: (17) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 10.604865ms)
Jul  8 01:58:22.052: INFO: (17) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 10.873196ms)
Jul  8 01:58:22.052: INFO: (17) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 12.302831ms)
Jul  8 01:58:22.052: INFO: (17) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 11.765457ms)
Jul  8 01:58:22.052: INFO: (17) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 12.051337ms)
Jul  8 01:58:22.064: INFO: (18) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 10.748027ms)
Jul  8 01:58:22.064: INFO: (18) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 11.056708ms)
Jul  8 01:58:22.064: INFO: (18) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 10.949644ms)
Jul  8 01:58:22.064: INFO: (18) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 10.820912ms)
Jul  8 01:58:22.064: INFO: (18) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 10.765507ms)
Jul  8 01:58:22.064: INFO: (18) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 11.13528ms)
Jul  8 01:58:22.064: INFO: (18) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 10.971006ms)
Jul  8 01:58:22.064: INFO: (18) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 10.945679ms)
Jul  8 01:58:22.064: INFO: (18) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 11.263275ms)
Jul  8 01:58:22.064: INFO: (18) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 11.517126ms)
Jul  8 01:58:22.065: INFO: (18) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 12.123177ms)
Jul  8 01:58:22.065: INFO: (18) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 11.933564ms)
Jul  8 01:58:22.065: INFO: (18) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 12.002601ms)
Jul  8 01:58:22.065: INFO: (18) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 12.220845ms)
Jul  8 01:58:22.065: INFO: (18) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 12.419222ms)
Jul  8 01:58:22.065: INFO: (18) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 11.949629ms)
Jul  8 01:58:22.074: INFO: (19) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:460/proxy/: tls baz (200; 8.521363ms)
Jul  8 01:58:22.075: INFO: (19) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 8.931762ms)
Jul  8 01:58:22.076: INFO: (19) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">... (200; 9.679846ms)
Jul  8 01:58:22.076: INFO: (19) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:1080/proxy/rewriteme">test<... (200; 10.182815ms)
Jul  8 01:58:22.076: INFO: (19) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 10.307812ms)
Jul  8 01:58:22.076: INFO: (19) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs/proxy/rewriteme">test</a> (200; 10.175473ms)
Jul  8 01:58:22.077: INFO: (19) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname2/proxy/: bar (200; 11.142726ms)
Jul  8 01:58:22.078: INFO: (19) /api/v1/namespaces/proxy-5514/services/proxy-service-4cgwh:portname1/proxy/: foo (200; 12.477819ms)
Jul  8 01:58:22.079: INFO: (19) /api/v1/namespaces/proxy-5514/pods/http:proxy-service-4cgwh-fffvs:162/proxy/: bar (200; 12.856873ms)
Jul  8 01:58:22.079: INFO: (19) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname2/proxy/: bar (200; 13.015344ms)
Jul  8 01:58:22.079: INFO: (19) /api/v1/namespaces/proxy-5514/services/http:proxy-service-4cgwh:portname1/proxy/: foo (200; 12.73878ms)
Jul  8 01:58:22.079: INFO: (19) /api/v1/namespaces/proxy-5514/pods/proxy-service-4cgwh-fffvs:160/proxy/: foo (200; 13.357059ms)
Jul  8 01:58:22.079: INFO: (19) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:462/proxy/: tls qux (200; 13.244846ms)
Jul  8 01:58:22.079: INFO: (19) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname2/proxy/: tls qux (200; 13.087469ms)
Jul  8 01:58:22.079: INFO: (19) /api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/: <a href="/api/v1/namespaces/proxy-5514/pods/https:proxy-service-4cgwh-fffvs:443/proxy/tlsrewritem... (200; 13.318485ms)
Jul  8 01:58:22.079: INFO: (19) /api/v1/namespaces/proxy-5514/services/https:proxy-service-4cgwh:tlsportname1/proxy/: tls baz (200; 13.638481ms)
STEP: deleting ReplicationController proxy-service-4cgwh in namespace proxy-5514, will wait for the garbage collector to delete the pods
Jul  8 01:58:22.140: INFO: Deleting ReplicationController proxy-service-4cgwh took: 6.703405ms
Jul  8 01:58:22.241: INFO: Terminating ReplicationController proxy-service-4cgwh pods took: 100.673151ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:58:25.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5514" for this suite.

• [SLOW TEST:5.826 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":339,"completed":46,"skipped":722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:58:25.358: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-9912
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:58:25.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9912" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":339,"completed":47,"skipped":760,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:58:25.550: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1386
STEP: creating an pod
Jul  8 01:58:25.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9716 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul  8 01:58:25.773: INFO: stderr: ""
Jul  8 01:58:25.773: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Jul  8 01:58:25.773: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul  8 01:58:25.773: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9716" to be "running and ready, or succeeded"
Jul  8 01:58:25.783: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.926234ms
Jul  8 01:58:27.790: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.016511379s
Jul  8 01:58:27.790: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul  8 01:58:27.790: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jul  8 01:58:27.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9716 logs logs-generator logs-generator'
Jul  8 01:58:27.922: INFO: stderr: ""
Jul  8 01:58:27.922: INFO: stdout: "I0708 01:58:26.574849       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/hcs 343\nI0708 01:58:26.774970       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/87r 563\nI0708 01:58:26.975598       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/kg4 546\nI0708 01:58:27.175919       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/7q5t 260\nI0708 01:58:27.375218       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/fmz 425\nI0708 01:58:27.575628       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/mzmj 565\nI0708 01:58:27.774920       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/lnz 585\n"
STEP: limiting log lines
Jul  8 01:58:27.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9716 logs logs-generator logs-generator --tail=1'
Jul  8 01:58:28.033: INFO: stderr: ""
Jul  8 01:58:28.033: INFO: stdout: "I0708 01:58:27.975216       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/wp5t 452\n"
Jul  8 01:58:28.033: INFO: got output "I0708 01:58:27.975216       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/wp5t 452\n"
STEP: limiting log bytes
Jul  8 01:58:28.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9716 logs logs-generator logs-generator --limit-bytes=1'
Jul  8 01:58:28.153: INFO: stderr: ""
Jul  8 01:58:28.153: INFO: stdout: "I"
Jul  8 01:58:28.153: INFO: got output "I"
STEP: exposing timestamps
Jul  8 01:58:28.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9716 logs logs-generator logs-generator --tail=1 --timestamps'
Jul  8 01:58:28.260: INFO: stderr: ""
Jul  8 01:58:28.260: INFO: stdout: "2021-07-08T01:58:28.175786371Z I0708 01:58:28.175672       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/vv7k 215\n"
Jul  8 01:58:28.260: INFO: got output "2021-07-08T01:58:28.175786371Z I0708 01:58:28.175672       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/vv7k 215\n"
STEP: restricting to a time range
Jul  8 01:58:30.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9716 logs logs-generator logs-generator --since=1s'
Jul  8 01:58:30.877: INFO: stderr: ""
Jul  8 01:58:30.878: INFO: stdout: "I0708 01:58:29.975235       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/hmcq 368\nI0708 01:58:30.175547       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/x8b 507\nI0708 01:58:30.375875       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/4nr 284\nI0708 01:58:30.575142       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/llj 454\nI0708 01:58:30.776401       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/z8t 256\n"
Jul  8 01:58:30.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9716 logs logs-generator logs-generator --since=24h'
Jul  8 01:58:30.968: INFO: stderr: ""
Jul  8 01:58:30.968: INFO: stdout: "I0708 01:58:26.574849       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/hcs 343\nI0708 01:58:26.774970       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/87r 563\nI0708 01:58:26.975598       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/kg4 546\nI0708 01:58:27.175919       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/7q5t 260\nI0708 01:58:27.375218       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/fmz 425\nI0708 01:58:27.575628       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/mzmj 565\nI0708 01:58:27.774920       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/lnz 585\nI0708 01:58:27.975216       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/wp5t 452\nI0708 01:58:28.175672       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/vv7k 215\nI0708 01:58:28.374945       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/p6k 212\nI0708 01:58:28.575249       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/6mn 580\nI0708 01:58:28.775555       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/g4xl 302\nI0708 01:58:28.975855       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/jd9 309\nI0708 01:58:29.175204       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/rp5h 414\nI0708 01:58:29.375498       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/wx4 388\nI0708 01:58:29.575804       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/fptv 518\nI0708 01:58:29.774934       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/79x4 578\nI0708 01:58:29.975235       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/hmcq 368\nI0708 01:58:30.175547       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/x8b 507\nI0708 01:58:30.375875       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/4nr 284\nI0708 01:58:30.575142       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/llj 454\nI0708 01:58:30.776401       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/z8t 256\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1391
Jul  8 01:58:30.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9716 delete pod logs-generator'
Jul  8 01:58:38.524: INFO: stderr: ""
Jul  8 01:58:38.524: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:58:38.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9716" for this suite.

• [SLOW TEST:12.999 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1383
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":339,"completed":48,"skipped":798,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:58:38.551: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4715
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul  8 01:58:38.724: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4715  86beb9ea-cf0d-4943-8a1a-e74f97f62ff8 21925 0 2021-07-08 01:58:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-08 01:58:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 01:58:38.724: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4715  86beb9ea-cf0d-4943-8a1a-e74f97f62ff8 21926 0 2021-07-08 01:58:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-08 01:58:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 01:58:38.724: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4715  86beb9ea-cf0d-4943-8a1a-e74f97f62ff8 21927 0 2021-07-08 01:58:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-08 01:58:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul  8 01:58:48.766: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4715  86beb9ea-cf0d-4943-8a1a-e74f97f62ff8 21963 0 2021-07-08 01:58:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-08 01:58:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 01:58:48.766: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4715  86beb9ea-cf0d-4943-8a1a-e74f97f62ff8 21964 0 2021-07-08 01:58:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-08 01:58:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 01:58:48.767: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4715  86beb9ea-cf0d-4943-8a1a-e74f97f62ff8 21965 0 2021-07-08 01:58:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-08 01:58:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:58:48.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4715" for this suite.

• [SLOW TEST:10.225 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":339,"completed":49,"skipped":800,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:58:48.777: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7485
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 01:58:48.926: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul  8 01:58:52.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-7485 --namespace=crd-publish-openapi-7485 create -f -'
Jul  8 01:58:53.132: INFO: stderr: ""
Jul  8 01:58:53.132: INFO: stdout: "e2e-test-crd-publish-openapi-6351-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul  8 01:58:53.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-7485 --namespace=crd-publish-openapi-7485 delete e2e-test-crd-publish-openapi-6351-crds test-cr'
Jul  8 01:58:53.262: INFO: stderr: ""
Jul  8 01:58:53.262: INFO: stdout: "e2e-test-crd-publish-openapi-6351-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul  8 01:58:53.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-7485 --namespace=crd-publish-openapi-7485 apply -f -'
Jul  8 01:58:53.509: INFO: stderr: ""
Jul  8 01:58:53.509: INFO: stdout: "e2e-test-crd-publish-openapi-6351-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul  8 01:58:53.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-7485 --namespace=crd-publish-openapi-7485 delete e2e-test-crd-publish-openapi-6351-crds test-cr'
Jul  8 01:58:53.591: INFO: stderr: ""
Jul  8 01:58:53.591: INFO: stdout: "e2e-test-crd-publish-openapi-6351-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jul  8 01:58:53.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-7485 explain e2e-test-crd-publish-openapi-6351-crds'
Jul  8 01:58:53.940: INFO: stderr: ""
Jul  8 01:58:53.940: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6351-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:58:56.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7485" for this suite.

• [SLOW TEST:8.172 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":339,"completed":50,"skipped":803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:58:56.950: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4114
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 01:58:57.099: INFO: Creating pod...
Jul  8 01:58:57.120: INFO: Pod Quantity: 1 Status: Pending
Jul  8 01:58:58.126: INFO: Pod Quantity: 1 Status: Pending
Jul  8 01:58:59.126: INFO: Pod Status: Running
Jul  8 01:58:59.126: INFO: Creating service...
Jul  8 01:58:59.135: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/pods/agnhost/proxy/some/path/with/DELETE
Jul  8 01:58:59.141: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul  8 01:58:59.141: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/pods/agnhost/proxy/some/path/with/GET
Jul  8 01:58:59.151: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul  8 01:58:59.151: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/pods/agnhost/proxy/some/path/with/HEAD
Jul  8 01:58:59.155: INFO: http.Client request:HEAD | StatusCode:200
Jul  8 01:58:59.155: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/pods/agnhost/proxy/some/path/with/OPTIONS
Jul  8 01:58:59.158: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul  8 01:58:59.158: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/pods/agnhost/proxy/some/path/with/PATCH
Jul  8 01:58:59.161: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul  8 01:58:59.161: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/pods/agnhost/proxy/some/path/with/POST
Jul  8 01:58:59.164: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul  8 01:58:59.164: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/pods/agnhost/proxy/some/path/with/PUT
Jul  8 01:58:59.168: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul  8 01:58:59.168: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/services/test-service/proxy/some/path/with/DELETE
Jul  8 01:58:59.174: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul  8 01:58:59.174: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/services/test-service/proxy/some/path/with/GET
Jul  8 01:58:59.178: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul  8 01:58:59.178: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/services/test-service/proxy/some/path/with/HEAD
Jul  8 01:58:59.186: INFO: http.Client request:HEAD | StatusCode:200
Jul  8 01:58:59.186: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/services/test-service/proxy/some/path/with/OPTIONS
Jul  8 01:58:59.192: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul  8 01:58:59.192: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/services/test-service/proxy/some/path/with/PATCH
Jul  8 01:58:59.197: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul  8 01:58:59.198: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/services/test-service/proxy/some/path/with/POST
Jul  8 01:58:59.202: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul  8 01:58:59.202: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4114/services/test-service/proxy/some/path/with/PUT
Jul  8 01:58:59.209: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:58:59.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4114" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":339,"completed":51,"skipped":839,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:58:59.220: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-552
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-552 to expose endpoints map[]
Jul  8 01:58:59.398: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jul  8 01:59:00.406: INFO: successfully validated that service endpoint-test2 in namespace services-552 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-552
Jul  8 01:59:00.441: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:59:02.448: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-552 to expose endpoints map[pod1:[80]]
Jul  8 01:59:02.456: INFO: successfully validated that service endpoint-test2 in namespace services-552 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-552
Jul  8 01:59:02.470: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:59:04.479: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 01:59:06.477: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-552 to expose endpoints map[pod1:[80] pod2:[80]]
Jul  8 01:59:06.488: INFO: successfully validated that service endpoint-test2 in namespace services-552 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-552
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-552 to expose endpoints map[pod2:[80]]
Jul  8 01:59:06.539: INFO: successfully validated that service endpoint-test2 in namespace services-552 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-552
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-552 to expose endpoints map[]
Jul  8 01:59:06.573: INFO: successfully validated that service endpoint-test2 in namespace services-552 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:59:06.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-552" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:7.419 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":339,"completed":52,"skipped":844,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:59:06.657: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6069.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6069.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6069.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6069.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6069.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6069.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  8 01:59:08.854: INFO: DNS probes using dns-6069/dns-test-2d364cc6-3d31-4ac8-9672-a94fb895023a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 01:59:08.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6069" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":339,"completed":53,"skipped":864,"failed":0}
S
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 01:59:08.919: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-cccf4587-f425-4a6c-937f-2149a4443b1b in namespace container-probe-2084
Jul  8 01:59:11.093: INFO: Started pod test-webserver-cccf4587-f425-4a6c-937f-2149a4443b1b in namespace container-probe-2084
STEP: checking the pod's current state and verifying that restartCount is present
Jul  8 01:59:11.096: INFO: Initial restart count of pod test-webserver-cccf4587-f425-4a6c-937f-2149a4443b1b is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:03:12.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2084" for this suite.

• [SLOW TEST:243.437 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":54,"skipped":865,"failed":0}
SSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:03:12.336: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-9015
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
W0708 02:03:12.556893      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:09:00.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9015" for this suite.

• [SLOW TEST:348.307 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":339,"completed":55,"skipped":868,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:09:00.643: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-2228
STEP: Creating a pod to test atomic-volume-subpath
Jul  8 02:09:00.818: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2228" in namespace "subpath-1929" to be "Succeeded or Failed"
Jul  8 02:09:00.844: INFO: Pod "pod-subpath-test-configmap-2228": Phase="Pending", Reason="", readiness=false. Elapsed: 26.309878ms
Jul  8 02:09:02.850: INFO: Pod "pod-subpath-test-configmap-2228": Phase="Running", Reason="", readiness=true. Elapsed: 2.032027248s
Jul  8 02:09:04.855: INFO: Pod "pod-subpath-test-configmap-2228": Phase="Running", Reason="", readiness=true. Elapsed: 4.037082919s
Jul  8 02:09:06.861: INFO: Pod "pod-subpath-test-configmap-2228": Phase="Running", Reason="", readiness=true. Elapsed: 6.043368566s
Jul  8 02:09:08.875: INFO: Pod "pod-subpath-test-configmap-2228": Phase="Running", Reason="", readiness=true. Elapsed: 8.05677408s
Jul  8 02:09:10.883: INFO: Pod "pod-subpath-test-configmap-2228": Phase="Running", Reason="", readiness=true. Elapsed: 10.065529373s
Jul  8 02:09:12.896: INFO: Pod "pod-subpath-test-configmap-2228": Phase="Running", Reason="", readiness=true. Elapsed: 12.078066875s
Jul  8 02:09:14.903: INFO: Pod "pod-subpath-test-configmap-2228": Phase="Running", Reason="", readiness=true. Elapsed: 14.084834605s
Jul  8 02:09:16.911: INFO: Pod "pod-subpath-test-configmap-2228": Phase="Running", Reason="", readiness=true. Elapsed: 16.093386545s
Jul  8 02:09:18.919: INFO: Pod "pod-subpath-test-configmap-2228": Phase="Running", Reason="", readiness=true. Elapsed: 18.101449819s
Jul  8 02:09:20.931: INFO: Pod "pod-subpath-test-configmap-2228": Phase="Running", Reason="", readiness=true. Elapsed: 20.113391456s
Jul  8 02:09:22.940: INFO: Pod "pod-subpath-test-configmap-2228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.121597246s
STEP: Saw pod success
Jul  8 02:09:22.940: INFO: Pod "pod-subpath-test-configmap-2228" satisfied condition "Succeeded or Failed"
Jul  8 02:09:22.953: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-subpath-test-configmap-2228 container test-container-subpath-configmap-2228: <nil>
STEP: delete the pod
Jul  8 02:09:23.000: INFO: Waiting for pod pod-subpath-test-configmap-2228 to disappear
Jul  8 02:09:23.005: INFO: Pod pod-subpath-test-configmap-2228 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-2228
Jul  8 02:09:23.005: INFO: Deleting pod "pod-subpath-test-configmap-2228" in namespace "subpath-1929"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:09:23.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1929" for this suite.

• [SLOW TEST:22.380 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":339,"completed":56,"skipped":872,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:09:23.023: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-81b22bd6-dcd7-4998-a7a5-e1523e57a2e7
STEP: Creating a pod to test consume configMaps
Jul  8 02:09:23.184: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af6e5ccd-db49-4bd8-a5ba-f15d83da2645" in namespace "projected-7559" to be "Succeeded or Failed"
Jul  8 02:09:23.191: INFO: Pod "pod-projected-configmaps-af6e5ccd-db49-4bd8-a5ba-f15d83da2645": Phase="Pending", Reason="", readiness=false. Elapsed: 6.746079ms
Jul  8 02:09:25.197: INFO: Pod "pod-projected-configmaps-af6e5ccd-db49-4bd8-a5ba-f15d83da2645": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013537407s
STEP: Saw pod success
Jul  8 02:09:25.197: INFO: Pod "pod-projected-configmaps-af6e5ccd-db49-4bd8-a5ba-f15d83da2645" satisfied condition "Succeeded or Failed"
Jul  8 02:09:25.200: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-projected-configmaps-af6e5ccd-db49-4bd8-a5ba-f15d83da2645 container agnhost-container: <nil>
STEP: delete the pod
Jul  8 02:09:25.222: INFO: Waiting for pod pod-projected-configmaps-af6e5ccd-db49-4bd8-a5ba-f15d83da2645 to disappear
Jul  8 02:09:25.224: INFO: Pod pod-projected-configmaps-af6e5ccd-db49-4bd8-a5ba-f15d83da2645 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:09:25.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7559" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":57,"skipped":874,"failed":0}
SSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:09:25.231: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-860
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jul  8 02:09:45.656: INFO: EndpointSlice for Service endpointslice-860/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:09:55.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-860" for this suite.

• [SLOW TEST:30.463 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":339,"completed":58,"skipped":877,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:09:55.694: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7827
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 02:09:56.516: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul  8 02:09:58.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761306996, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761306996, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761306996, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761306996, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 02:10:01.562: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:10:01.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7827" for this suite.
STEP: Destroying namespace "webhook-7827-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.357 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":339,"completed":59,"skipped":880,"failed":0}
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:10:02.053: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7568
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jul  8 02:10:02.258: INFO: PodSpec: initContainers in spec.initContainers
Jul  8 02:10:43.756: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-20bed07f-2dc0-4f8c-bc2f-ff116d1f202a", GenerateName:"", Namespace:"init-container-7568", SelfLink:"", UID:"a14893a3-70b2-4d52-b57e-6ec795d5ca14", ResourceVersion:"24106", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63761307002, loc:(*time.Location)(0x9dde5a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"258729567"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.42.3.59/32", "cni.projectcalico.org/podIPs":"10.42.3.59/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0046fde00), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046fde18)}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0046fde30), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046fde48)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0046fde60), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046fde78)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-dvdh5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0028266a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-dvdh5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-dvdh5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.4.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-dvdh5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001b1ad08), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-6-217.us-east-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00322a4d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001b1ad80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001b1ada0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001b1ada8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001b1adac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0036dcbc0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761307002, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761307002, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761307002, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761307002, loc:(*time.Location)(0x9dde5a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.6.217", PodIP:"10.42.3.59", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.42.3.59"}}, StartTime:(*v1.Time)(0xc0046fdea8), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00322a5b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00322a620)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"containerd://2dd609123211b9473116923d6562b3d4e1fe58647b270227d65dad894b39e1c9", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002826720), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002826700), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.4.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc001b1ae2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:10:43.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7568" for this suite.

• [SLOW TEST:41.737 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":339,"completed":60,"skipped":884,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:10:43.790: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1185
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul  8 02:10:43.947: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  8 02:10:43.955: INFO: Waiting for terminating namespaces to be deleted...
Jul  8 02:10:43.958: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-228.us-east-2.compute.internal before test
Jul  8 02:10:43.969: INFO: etcd-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:35 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container etcd ready: true, restart count 0
Jul  8 02:10:43.969: INFO: helm-install-rke2-canal-zzkr7 from kube-system started at 2021-07-07 23:40:11 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container helm ready: false, restart count 0
Jul  8 02:10:43.969: INFO: helm-install-rke2-coredns-7vm9g from kube-system started at 2021-07-07 23:40:11 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container helm ready: false, restart count 0
Jul  8 02:10:43.969: INFO: helm-install-rke2-kube-proxy-tqnww from kube-system started at 2021-07-07 23:40:11 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container helm ready: false, restart count 0
Jul  8 02:10:43.969: INFO: helm-install-rke2-metrics-server-xbzj4 from kube-system started at 2021-07-07 23:40:54 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container helm ready: false, restart count 0
Jul  8 02:10:43.969: INFO: kube-apiserver-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:47 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul  8 02:10:43.969: INFO: kube-controller-manager-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:57 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul  8 02:10:43.969: INFO: kube-proxy-vvsjd from kube-system started at 2021-07-07 23:40:24 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 02:10:43.969: INFO: kube-scheduler-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:55 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul  8 02:10:43.969: INFO: rke2-canal-kcjjk from kube-system started at 2021-07-07 23:40:25 +0000 UTC (2 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 02:10:43.969: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 02:10:43.969: INFO: rke2-coredns-rke2-coredns-65d668ddf9-bd9pd from kube-system started at 2021-07-07 23:40:52 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container coredns ready: true, restart count 0
Jul  8 02:10:43.969: INFO: rke2-metrics-server-6647ffc866-7z7j7 from kube-system started at 2021-07-07 23:41:00 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container metrics-server ready: true, restart count 0
Jul  8 02:10:43.969: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-84vqn from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 02:10:43.969: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 02:10:43.969: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  8 02:10:43.969: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-217.us-east-2.compute.internal before test
Jul  8 02:10:43.975: INFO: pod-init-20bed07f-2dc0-4f8c-bc2f-ff116d1f202a from init-container-7568 started at 2021-07-08 02:10:02 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.975: INFO: 	Container run1 ready: false, restart count 0
Jul  8 02:10:43.975: INFO: kube-proxy-t4j4c from kube-system started at 2021-07-07 23:44:49 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.975: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 02:10:43.975: INFO: rke2-canal-9wlw7 from kube-system started at 2021-07-07 23:44:49 +0000 UTC (2 container statuses recorded)
Jul  8 02:10:43.975: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 02:10:43.975: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 02:10:43.975: INFO: sonobuoy from sonobuoy started at 2021-07-08 01:44:37 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.975: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  8 02:10:43.975: INFO: sonobuoy-e2e-job-1dfa1a3a74ca4282 from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 02:10:43.975: INFO: 	Container e2e ready: true, restart count 0
Jul  8 02:10:43.975: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 02:10:43.975: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-5s9xh from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 02:10:43.975: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 02:10:43.975: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  8 02:10:43.975: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-226.us-east-2.compute.internal before test
Jul  8 02:10:43.982: INFO: etcd-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:42:57 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.982: INFO: 	Container etcd ready: true, restart count 0
Jul  8 02:10:43.982: INFO: kube-apiserver-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:11 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.982: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul  8 02:10:43.982: INFO: kube-controller-manager-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:22 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.982: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul  8 02:10:43.982: INFO: kube-proxy-87mbg from kube-system started at 2021-07-07 23:43:19 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.982: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 02:10:43.982: INFO: kube-scheduler-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:22 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.982: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul  8 02:10:43.982: INFO: rke2-canal-8n79q from kube-system started at 2021-07-07 23:43:19 +0000 UTC (2 container statuses recorded)
Jul  8 02:10:43.982: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 02:10:43.982: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 02:10:43.982: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-2nxk4 from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 02:10:43.982: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 02:10:43.982: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  8 02:10:43.982: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-8-165.us-east-2.compute.internal before test
Jul  8 02:10:43.989: INFO: etcd-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:41 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.989: INFO: 	Container etcd ready: true, restart count 0
Jul  8 02:10:43.989: INFO: kube-apiserver-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:48 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.989: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul  8 02:10:43.989: INFO: kube-controller-manager-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:59 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.989: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul  8 02:10:43.989: INFO: kube-proxy-p4h8m from kube-system started at 2021-07-07 23:43:56 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.989: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 02:10:43.989: INFO: kube-scheduler-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:58 +0000 UTC (1 container statuses recorded)
Jul  8 02:10:43.989: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul  8 02:10:43.989: INFO: rke2-canal-vnj8b from kube-system started at 2021-07-07 23:43:56 +0000 UTC (2 container statuses recorded)
Jul  8 02:10:43.989: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 02:10:43.989: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 02:10:43.989: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-fndvb from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 02:10:43.989: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 02:10:43.989: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node ip-172-31-3-228.us-east-2.compute.internal
STEP: verifying the node has the label node ip-172-31-6-217.us-east-2.compute.internal
STEP: verifying the node has the label node ip-172-31-6-226.us-east-2.compute.internal
STEP: verifying the node has the label node ip-172-31-8-165.us-east-2.compute.internal
Jul  8 02:10:44.084: INFO: Pod pod-init-20bed07f-2dc0-4f8c-bc2f-ff116d1f202a requesting resource cpu=100m on Node ip-172-31-6-217.us-east-2.compute.internal
Jul  8 02:10:44.084: INFO: Pod etcd-ip-172-31-3-228.us-east-2.compute.internal requesting resource cpu=0m on Node ip-172-31-3-228.us-east-2.compute.internal
Jul  8 02:10:44.084: INFO: Pod etcd-ip-172-31-6-226.us-east-2.compute.internal requesting resource cpu=0m on Node ip-172-31-6-226.us-east-2.compute.internal
Jul  8 02:10:44.084: INFO: Pod etcd-ip-172-31-8-165.us-east-2.compute.internal requesting resource cpu=0m on Node ip-172-31-8-165.us-east-2.compute.internal
Jul  8 02:10:44.084: INFO: Pod kube-apiserver-ip-172-31-3-228.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-3-228.us-east-2.compute.internal
Jul  8 02:10:44.084: INFO: Pod kube-apiserver-ip-172-31-6-226.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-6-226.us-east-2.compute.internal
Jul  8 02:10:44.084: INFO: Pod kube-apiserver-ip-172-31-8-165.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-8-165.us-east-2.compute.internal
Jul  8 02:10:44.084: INFO: Pod kube-controller-manager-ip-172-31-3-228.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-3-228.us-east-2.compute.internal
Jul  8 02:10:44.084: INFO: Pod kube-controller-manager-ip-172-31-6-226.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-6-226.us-east-2.compute.internal
Jul  8 02:10:44.084: INFO: Pod kube-controller-manager-ip-172-31-8-165.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-8-165.us-east-2.compute.internal
Jul  8 02:10:44.084: INFO: Pod kube-proxy-87mbg requesting resource cpu=0m on Node ip-172-31-6-226.us-east-2.compute.internal
Jul  8 02:10:44.084: INFO: Pod kube-proxy-p4h8m requesting resource cpu=0m on Node ip-172-31-8-165.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod kube-proxy-t4j4c requesting resource cpu=0m on Node ip-172-31-6-217.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod kube-proxy-vvsjd requesting resource cpu=0m on Node ip-172-31-3-228.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod kube-scheduler-ip-172-31-3-228.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-3-228.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod kube-scheduler-ip-172-31-6-226.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-6-226.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod kube-scheduler-ip-172-31-8-165.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-8-165.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod rke2-canal-8n79q requesting resource cpu=250m on Node ip-172-31-6-226.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod rke2-canal-9wlw7 requesting resource cpu=250m on Node ip-172-31-6-217.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod rke2-canal-kcjjk requesting resource cpu=250m on Node ip-172-31-3-228.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod rke2-canal-vnj8b requesting resource cpu=250m on Node ip-172-31-8-165.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod rke2-coredns-rke2-coredns-65d668ddf9-bd9pd requesting resource cpu=100m on Node ip-172-31-3-228.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod rke2-metrics-server-6647ffc866-7z7j7 requesting resource cpu=0m on Node ip-172-31-3-228.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-6-217.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod sonobuoy-e2e-job-1dfa1a3a74ca4282 requesting resource cpu=0m on Node ip-172-31-6-217.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-2nxk4 requesting resource cpu=0m on Node ip-172-31-6-226.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-5s9xh requesting resource cpu=0m on Node ip-172-31-6-217.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-84vqn requesting resource cpu=0m on Node ip-172-31-3-228.us-east-2.compute.internal
Jul  8 02:10:44.085: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-fndvb requesting resource cpu=0m on Node ip-172-31-8-165.us-east-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Jul  8 02:10:44.085: INFO: Creating a pod which consumes cpu=840m on Node ip-172-31-8-165.us-east-2.compute.internal
Jul  8 02:10:44.221: INFO: Creating a pod which consumes cpu=770m on Node ip-172-31-3-228.us-east-2.compute.internal
Jul  8 02:10:44.234: INFO: Creating a pod which consumes cpu=1155m on Node ip-172-31-6-217.us-east-2.compute.internal
Jul  8 02:10:44.247: INFO: Creating a pod which consumes cpu=840m on Node ip-172-31-6-226.us-east-2.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-43ed899e-5b41-4a64-9471-2579a4590059.168faeda926cf67d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1185/filler-pod-43ed899e-5b41-4a64-9471-2579a4590059 to ip-172-31-6-226.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-43ed899e-5b41-4a64-9471-2579a4590059.168faedabb05a0cd], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.4.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-43ed899e-5b41-4a64-9471-2579a4590059.168faedafcd638a5], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.4.1" in 1.104094048s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-43ed899e-5b41-4a64-9471-2579a4590059.168faedaffa75a50], Reason = [Created], Message = [Created container filler-pod-43ed899e-5b41-4a64-9471-2579a4590059]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-43ed899e-5b41-4a64-9471-2579a4590059.168faedb05a7d596], Reason = [Started], Message = [Started container filler-pod-43ed899e-5b41-4a64-9471-2579a4590059]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5fa7559b-3d50-48ef-9cc9-e824c3417095.168faeda91b94a73], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1185/filler-pod-5fa7559b-3d50-48ef-9cc9-e824c3417095 to ip-172-31-6-217.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5fa7559b-3d50-48ef-9cc9-e824c3417095.168faedac72f0277], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5fa7559b-3d50-48ef-9cc9-e824c3417095.168faedac9b95943], Reason = [Created], Message = [Created container filler-pod-5fa7559b-3d50-48ef-9cc9-e824c3417095]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5fa7559b-3d50-48ef-9cc9-e824c3417095.168faedacf46d9c1], Reason = [Started], Message = [Started container filler-pod-5fa7559b-3d50-48ef-9cc9-e824c3417095]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71c6885d-c266-43c9-b43c-d57b3857aa93.168faeda8eca90c0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1185/filler-pod-71c6885d-c266-43c9-b43c-d57b3857aa93 to ip-172-31-8-165.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71c6885d-c266-43c9-b43c-d57b3857aa93.168faedab42eeb1e], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.4.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71c6885d-c266-43c9-b43c-d57b3857aa93.168faedafb4140a7], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.4.1" in 1.192367147s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71c6885d-c266-43c9-b43c-d57b3857aa93.168faedafdafde85], Reason = [Created], Message = [Created container filler-pod-71c6885d-c266-43c9-b43c-d57b3857aa93]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71c6885d-c266-43c9-b43c-d57b3857aa93.168faedb02dfbdd8], Reason = [Started], Message = [Started container filler-pod-71c6885d-c266-43c9-b43c-d57b3857aa93]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf031bd7-9d64-4b4a-8575-31655966d336.168faeda8fff1ad0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1185/filler-pod-cf031bd7-9d64-4b4a-8575-31655966d336 to ip-172-31-3-228.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-cf031bd7-9d64-4b4a-8575-31655966d336.168faedad971b197], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-jhq55" : failed to sync configmap cache: timed out waiting for the condition]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf031bd7-9d64-4b4a-8575-31655966d336.168faedb21cc0a18], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.4.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf031bd7-9d64-4b4a-8575-31655966d336.168faedb6502c03f], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.4.1" in 1.12764365s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf031bd7-9d64-4b4a-8575-31655966d336.168faedb6933c1f0], Reason = [Created], Message = [Created container filler-pod-cf031bd7-9d64-4b4a-8575-31655966d336]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf031bd7-9d64-4b4a-8575-31655966d336.168faedb6fa081af], Reason = [Started], Message = [Started container filler-pod-cf031bd7-9d64-4b4a-8575-31655966d336]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168faedbf9eb90a6], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node ip-172-31-3-228.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-6-217.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-6-226.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-8-165.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:10:51.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1185" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:7.631 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":339,"completed":61,"skipped":896,"failed":0}
SSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:10:51.422: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-9016
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:10:57.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9016" for this suite.

• [SLOW TEST:6.255 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":339,"completed":62,"skipped":903,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:10:57.678: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
W0708 02:10:59.161064      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul  8 02:15:59.167: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:15:59.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2608" for this suite.

• [SLOW TEST:301.505 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":339,"completed":63,"skipped":907,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:15:59.183: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8563
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Jul  8 02:15:59.360: INFO: The status of Pod pod-hostip-9329eeb0-5bb2-4bfb-bfc7-0d367c4d3e06 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:16:01.375: INFO: The status of Pod pod-hostip-9329eeb0-5bb2-4bfb-bfc7-0d367c4d3e06 is Running (Ready = true)
Jul  8 02:16:01.382: INFO: Pod pod-hostip-9329eeb0-5bb2-4bfb-bfc7-0d367c4d3e06 has hostIP: 172.31.6.217
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:16:01.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8563" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":339,"completed":64,"skipped":911,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:16:01.399: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jul  8 02:16:01.576: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:16:03.582: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jul  8 02:16:03.596: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:16:05.603: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul  8 02:16:05.623: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  8 02:16:05.637: INFO: Pod pod-with-poststart-http-hook still exists
Jul  8 02:16:07.637: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  8 02:16:07.643: INFO: Pod pod-with-poststart-http-hook still exists
Jul  8 02:16:09.637: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  8 02:16:09.651: INFO: Pod pod-with-poststart-http-hook still exists
Jul  8 02:16:11.637: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  8 02:16:11.645: INFO: Pod pod-with-poststart-http-hook still exists
Jul  8 02:16:13.637: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  8 02:16:13.646: INFO: Pod pod-with-poststart-http-hook still exists
Jul  8 02:16:15.637: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  8 02:16:15.644: INFO: Pod pod-with-poststart-http-hook still exists
Jul  8 02:16:17.637: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  8 02:16:17.644: INFO: Pod pod-with-poststart-http-hook still exists
Jul  8 02:16:19.637: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  8 02:16:19.644: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:16:19.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8938" for this suite.

• [SLOW TEST:18.255 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":339,"completed":65,"skipped":919,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:16:19.655: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6445
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-5n6n
STEP: Creating a pod to test atomic-volume-subpath
Jul  8 02:16:19.844: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5n6n" in namespace "subpath-6445" to be "Succeeded or Failed"
Jul  8 02:16:19.849: INFO: Pod "pod-subpath-test-projected-5n6n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.748132ms
Jul  8 02:16:21.856: INFO: Pod "pod-subpath-test-projected-5n6n": Phase="Running", Reason="", readiness=true. Elapsed: 2.011772691s
Jul  8 02:16:23.864: INFO: Pod "pod-subpath-test-projected-5n6n": Phase="Running", Reason="", readiness=true. Elapsed: 4.019988348s
Jul  8 02:16:25.868: INFO: Pod "pod-subpath-test-projected-5n6n": Phase="Running", Reason="", readiness=true. Elapsed: 6.024176793s
Jul  8 02:16:27.875: INFO: Pod "pod-subpath-test-projected-5n6n": Phase="Running", Reason="", readiness=true. Elapsed: 8.030638877s
Jul  8 02:16:29.895: INFO: Pod "pod-subpath-test-projected-5n6n": Phase="Running", Reason="", readiness=true. Elapsed: 10.050894506s
Jul  8 02:16:31.903: INFO: Pod "pod-subpath-test-projected-5n6n": Phase="Running", Reason="", readiness=true. Elapsed: 12.058673314s
Jul  8 02:16:33.910: INFO: Pod "pod-subpath-test-projected-5n6n": Phase="Running", Reason="", readiness=true. Elapsed: 14.065896203s
Jul  8 02:16:35.915: INFO: Pod "pod-subpath-test-projected-5n6n": Phase="Running", Reason="", readiness=true. Elapsed: 16.070537199s
Jul  8 02:16:37.926: INFO: Pod "pod-subpath-test-projected-5n6n": Phase="Running", Reason="", readiness=true. Elapsed: 18.082160647s
Jul  8 02:16:39.934: INFO: Pod "pod-subpath-test-projected-5n6n": Phase="Running", Reason="", readiness=true. Elapsed: 20.089825782s
Jul  8 02:16:41.941: INFO: Pod "pod-subpath-test-projected-5n6n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.096574903s
STEP: Saw pod success
Jul  8 02:16:41.941: INFO: Pod "pod-subpath-test-projected-5n6n" satisfied condition "Succeeded or Failed"
Jul  8 02:16:41.943: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-subpath-test-projected-5n6n container test-container-subpath-projected-5n6n: <nil>
STEP: delete the pod
Jul  8 02:16:42.064: INFO: Waiting for pod pod-subpath-test-projected-5n6n to disappear
Jul  8 02:16:42.067: INFO: Pod pod-subpath-test-projected-5n6n no longer exists
STEP: Deleting pod pod-subpath-test-projected-5n6n
Jul  8 02:16:42.067: INFO: Deleting pod "pod-subpath-test-projected-5n6n" in namespace "subpath-6445"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:16:42.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6445" for this suite.

• [SLOW TEST:22.421 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":339,"completed":66,"skipped":927,"failed":0}
S
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:16:42.077: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6997
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Jul  8 02:16:42.275: INFO: Waiting up to 5m0s for pod "client-containers-f9bf17d7-0fcb-4b47-b3ff-9d1d8bf0d89a" in namespace "containers-6997" to be "Succeeded or Failed"
Jul  8 02:16:42.281: INFO: Pod "client-containers-f9bf17d7-0fcb-4b47-b3ff-9d1d8bf0d89a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.241133ms
Jul  8 02:16:44.288: INFO: Pod "client-containers-f9bf17d7-0fcb-4b47-b3ff-9d1d8bf0d89a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013317108s
STEP: Saw pod success
Jul  8 02:16:44.288: INFO: Pod "client-containers-f9bf17d7-0fcb-4b47-b3ff-9d1d8bf0d89a" satisfied condition "Succeeded or Failed"
Jul  8 02:16:44.291: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod client-containers-f9bf17d7-0fcb-4b47-b3ff-9d1d8bf0d89a container agnhost-container: <nil>
STEP: delete the pod
Jul  8 02:16:44.313: INFO: Waiting for pod client-containers-f9bf17d7-0fcb-4b47-b3ff-9d1d8bf0d89a to disappear
Jul  8 02:16:44.315: INFO: Pod client-containers-f9bf17d7-0fcb-4b47-b3ff-9d1d8bf0d89a no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:16:44.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6997" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":339,"completed":67,"skipped":928,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:16:44.326: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3446
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:16:44.540: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2da69265-7c19-4997-80ee-430677dc4510" in namespace "security-context-test-3446" to be "Succeeded or Failed"
Jul  8 02:16:44.547: INFO: Pod "alpine-nnp-false-2da69265-7c19-4997-80ee-430677dc4510": Phase="Pending", Reason="", readiness=false. Elapsed: 6.623659ms
Jul  8 02:16:46.553: INFO: Pod "alpine-nnp-false-2da69265-7c19-4997-80ee-430677dc4510": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012395622s
Jul  8 02:16:48.558: INFO: Pod "alpine-nnp-false-2da69265-7c19-4997-80ee-430677dc4510": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018148969s
Jul  8 02:16:48.558: INFO: Pod "alpine-nnp-false-2da69265-7c19-4997-80ee-430677dc4510" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:16:48.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3446" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":68,"skipped":938,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:16:48.704: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jul  8 02:16:48.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8714 create -f -'
Jul  8 02:16:49.408: INFO: stderr: ""
Jul  8 02:16:49.408: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul  8 02:16:50.417: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  8 02:16:50.417: INFO: Found 0 / 1
Jul  8 02:16:51.423: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  8 02:16:51.423: INFO: Found 1 / 1
Jul  8 02:16:51.423: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul  8 02:16:51.425: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  8 02:16:51.425: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  8 02:16:51.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8714 patch pod agnhost-primary-vtqc5 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul  8 02:16:51.549: INFO: stderr: ""
Jul  8 02:16:51.549: INFO: stdout: "pod/agnhost-primary-vtqc5 patched\n"
STEP: checking annotations
Jul  8 02:16:51.553: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  8 02:16:51.553: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:16:51.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8714" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":339,"completed":69,"skipped":946,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:16:51.565: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:16:51.717: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2c966e33-2155-4f16-afbb-46b3177ac541" in namespace "security-context-test-6358" to be "Succeeded or Failed"
Jul  8 02:16:51.722: INFO: Pod "busybox-readonly-false-2c966e33-2155-4f16-afbb-46b3177ac541": Phase="Pending", Reason="", readiness=false. Elapsed: 4.575497ms
Jul  8 02:16:53.733: INFO: Pod "busybox-readonly-false-2c966e33-2155-4f16-afbb-46b3177ac541": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016118714s
Jul  8 02:16:53.733: INFO: Pod "busybox-readonly-false-2c966e33-2155-4f16-afbb-46b3177ac541" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:16:53.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6358" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":339,"completed":70,"skipped":963,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:16:53.745: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul  8 02:16:53.927: INFO: Waiting up to 5m0s for pod "pod-8f7fc249-eaa2-4101-8c7f-f1346dfb93e0" in namespace "emptydir-2471" to be "Succeeded or Failed"
Jul  8 02:16:53.938: INFO: Pod "pod-8f7fc249-eaa2-4101-8c7f-f1346dfb93e0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.929925ms
Jul  8 02:16:55.942: INFO: Pod "pod-8f7fc249-eaa2-4101-8c7f-f1346dfb93e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014902241s
STEP: Saw pod success
Jul  8 02:16:55.942: INFO: Pod "pod-8f7fc249-eaa2-4101-8c7f-f1346dfb93e0" satisfied condition "Succeeded or Failed"
Jul  8 02:16:55.944: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-8f7fc249-eaa2-4101-8c7f-f1346dfb93e0 container test-container: <nil>
STEP: delete the pod
Jul  8 02:16:55.961: INFO: Waiting for pod pod-8f7fc249-eaa2-4101-8c7f-f1346dfb93e0 to disappear
Jul  8 02:16:55.966: INFO: Pod pod-8f7fc249-eaa2-4101-8c7f-f1346dfb93e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:16:55.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2471" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":71,"skipped":964,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:16:55.980: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8365
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:16:56.118: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:16:56.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8365" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":339,"completed":72,"skipped":968,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:16:57.056: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8413
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:17:13.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8413" for this suite.

• [SLOW TEST:16.377 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":339,"completed":73,"skipped":980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:17:13.434: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul  8 02:17:13.595: INFO: Waiting up to 5m0s for pod "pod-84740d69-3521-49cc-b19c-ec7a49a30e33" in namespace "emptydir-4856" to be "Succeeded or Failed"
Jul  8 02:17:13.604: INFO: Pod "pod-84740d69-3521-49cc-b19c-ec7a49a30e33": Phase="Pending", Reason="", readiness=false. Elapsed: 9.046816ms
Jul  8 02:17:15.613: INFO: Pod "pod-84740d69-3521-49cc-b19c-ec7a49a30e33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018641365s
STEP: Saw pod success
Jul  8 02:17:15.613: INFO: Pod "pod-84740d69-3521-49cc-b19c-ec7a49a30e33" satisfied condition "Succeeded or Failed"
Jul  8 02:17:15.616: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-84740d69-3521-49cc-b19c-ec7a49a30e33 container test-container: <nil>
STEP: delete the pod
Jul  8 02:17:15.640: INFO: Waiting for pod pod-84740d69-3521-49cc-b19c-ec7a49a30e33 to disappear
Jul  8 02:17:15.642: INFO: Pod pod-84740d69-3521-49cc-b19c-ec7a49a30e33 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:17:15.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4856" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":74,"skipped":1030,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:17:15.651: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-7243
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul  8 02:17:15.819: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul  8 02:17:15.821: INFO: starting watch
STEP: patching
STEP: updating
Jul  8 02:17:15.834: INFO: waiting for watch events with expected annotations
Jul  8 02:17:15.834: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:17:15.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7243" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":339,"completed":75,"skipped":1042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:17:15.883: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2536
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Jul  8 02:17:16.066: INFO: created test-event-1
Jul  8 02:17:16.070: INFO: created test-event-2
Jul  8 02:17:16.073: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jul  8 02:17:16.075: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jul  8 02:17:16.099: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:17:16.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2536" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":339,"completed":76,"skipped":1132,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:17:16.112: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9687
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:17:16.281: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul  8 02:17:16.292: INFO: Number of nodes with available pods: 0
Jul  8 02:17:16.293: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:17:17.301: INFO: Number of nodes with available pods: 0
Jul  8 02:17:17.301: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:17:18.303: INFO: Number of nodes with available pods: 3
Jul  8 02:17:18.303: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:17:19.302: INFO: Number of nodes with available pods: 3
Jul  8 02:17:19.302: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:17:20.307: INFO: Number of nodes with available pods: 3
Jul  8 02:17:20.307: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:17:21.313: INFO: Number of nodes with available pods: 3
Jul  8 02:17:21.313: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:17:22.305: INFO: Number of nodes with available pods: 3
Jul  8 02:17:22.305: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:17:23.304: INFO: Number of nodes with available pods: 4
Jul  8 02:17:23.304: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul  8 02:17:23.340: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:23.340: INFO: Wrong image for pod: daemon-set-4w7wc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:23.340: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:23.340: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:24.363: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:24.363: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:24.363: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:25.367: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:25.367: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:25.367: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:26.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:26.362: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:26.362: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:27.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:27.362: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:27.362: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:28.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:28.362: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:28.362: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:29.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:29.362: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:29.362: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:30.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:30.362: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:30.362: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:31.363: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:31.363: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:31.363: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:32.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:32.363: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:32.363: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:33.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:33.362: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:33.362: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:34.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:34.362: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:34.362: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:35.375: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:35.375: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:35.375: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:36.361: INFO: Pod daemon-set-2r278 is not available
Jul  8 02:17:36.361: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:36.361: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:36.361: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:37.363: INFO: Pod daemon-set-2r278 is not available
Jul  8 02:17:37.363: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:37.363: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:37.363: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:38.362: INFO: Pod daemon-set-2r278 is not available
Jul  8 02:17:38.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:38.362: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:38.362: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:39.363: INFO: Pod daemon-set-2r278 is not available
Jul  8 02:17:39.363: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:39.363: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:39.363: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:40.364: INFO: Pod daemon-set-2r278 is not available
Jul  8 02:17:40.364: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:40.364: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:40.364: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:41.365: INFO: Pod daemon-set-2r278 is not available
Jul  8 02:17:41.365: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:41.365: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:41.365: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:42.365: INFO: Pod daemon-set-2r278 is not available
Jul  8 02:17:42.365: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:42.365: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:42.365: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:43.372: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:43.372: INFO: Wrong image for pod: daemon-set-7qgxn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:43.372: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:44.364: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:44.364: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:45.365: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:45.365: INFO: Pod daemon-set-crwv8 is not available
Jul  8 02:17:45.365: INFO: Wrong image for pod: daemon-set-v6r5d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:46.361: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:47.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:48.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:49.363: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:50.365: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:51.363: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:52.363: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:53.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:54.363: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:55.362: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:56.361: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:57.366: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:58.361: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:58.361: INFO: Pod daemon-set-8bcjr is not available
Jul  8 02:17:59.364: INFO: Wrong image for pod: daemon-set-4mf96. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul  8 02:17:59.364: INFO: Pod daemon-set-8bcjr is not available
Jul  8 02:18:09.367: INFO: Pod daemon-set-jm5kq is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul  8 02:18:09.384: INFO: Number of nodes with available pods: 3
Jul  8 02:18:09.384: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:18:10.394: INFO: Number of nodes with available pods: 4
Jul  8 02:18:10.394: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9687, will wait for the garbage collector to delete the pods
Jul  8 02:18:10.466: INFO: Deleting DaemonSet.extensions daemon-set took: 8.372199ms
Jul  8 02:18:10.666: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.122781ms
Jul  8 02:18:21.077: INFO: Number of nodes with available pods: 0
Jul  8 02:18:21.077: INFO: Number of running nodes: 0, number of available pods: 0
Jul  8 02:18:21.079: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25910"},"items":null}

Jul  8 02:18:21.082: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25910"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:18:21.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9687" for this suite.

• [SLOW TEST:64.992 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":339,"completed":77,"skipped":1144,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:18:21.105: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1120
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1120 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1120;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1120 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1120;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1120.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1120.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1120.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1120.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1120.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1120.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1120.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1120.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1120.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1120.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1120.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 12.236.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.236.12_udp@PTR;check="$$(dig +tcp +noall +answer +search 12.236.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.236.12_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1120 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1120;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1120 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1120;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1120.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1120.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1120.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1120.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1120.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1120.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1120.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1120.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1120.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1120.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1120.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1120.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 12.236.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.236.12_udp@PTR;check="$$(dig +tcp +noall +answer +search 12.236.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.236.12_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  8 02:18:23.347: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.350: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.352: INFO: Unable to read wheezy_udp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.354: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.356: INFO: Unable to read wheezy_udp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.359: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.362: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.364: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.379: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.382: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.384: INFO: Unable to read jessie_udp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.386: INFO: Unable to read jessie_tcp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.388: INFO: Unable to read jessie_udp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.390: INFO: Unable to read jessie_tcp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.392: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.394: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:23.426: INFO: Lookups using dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1120 wheezy_tcp@dns-test-service.dns-1120 wheezy_udp@dns-test-service.dns-1120.svc wheezy_tcp@dns-test-service.dns-1120.svc wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1120 jessie_tcp@dns-test-service.dns-1120 jessie_udp@dns-test-service.dns-1120.svc jessie_tcp@dns-test-service.dns-1120.svc jessie_udp@_http._tcp.dns-test-service.dns-1120.svc jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc]

Jul  8 02:18:28.430: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.433: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.437: INFO: Unable to read wheezy_udp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.439: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.441: INFO: Unable to read wheezy_udp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.444: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.446: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.449: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.464: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.466: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.468: INFO: Unable to read jessie_udp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.471: INFO: Unable to read jessie_tcp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.473: INFO: Unable to read jessie_udp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.480: INFO: Unable to read jessie_tcp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.483: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.486: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:28.502: INFO: Lookups using dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1120 wheezy_tcp@dns-test-service.dns-1120 wheezy_udp@dns-test-service.dns-1120.svc wheezy_tcp@dns-test-service.dns-1120.svc wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1120 jessie_tcp@dns-test-service.dns-1120 jessie_udp@dns-test-service.dns-1120.svc jessie_tcp@dns-test-service.dns-1120.svc jessie_udp@_http._tcp.dns-test-service.dns-1120.svc jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc]

Jul  8 02:18:33.431: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.435: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.439: INFO: Unable to read wheezy_udp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.442: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.445: INFO: Unable to read wheezy_udp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.448: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.452: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.455: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.475: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.477: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.481: INFO: Unable to read jessie_udp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.491: INFO: Unable to read jessie_tcp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.495: INFO: Unable to read jessie_udp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.498: INFO: Unable to read jessie_tcp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.502: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.505: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:33.521: INFO: Lookups using dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1120 wheezy_tcp@dns-test-service.dns-1120 wheezy_udp@dns-test-service.dns-1120.svc wheezy_tcp@dns-test-service.dns-1120.svc wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1120 jessie_tcp@dns-test-service.dns-1120 jessie_udp@dns-test-service.dns-1120.svc jessie_tcp@dns-test-service.dns-1120.svc jessie_udp@_http._tcp.dns-test-service.dns-1120.svc jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc]

Jul  8 02:18:38.430: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.432: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.435: INFO: Unable to read wheezy_udp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.437: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.439: INFO: Unable to read wheezy_udp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.442: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.445: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.447: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.468: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.470: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.472: INFO: Unable to read jessie_udp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.474: INFO: Unable to read jessie_tcp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.479: INFO: Unable to read jessie_udp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.484: INFO: Unable to read jessie_tcp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.487: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.489: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:38.509: INFO: Lookups using dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1120 wheezy_tcp@dns-test-service.dns-1120 wheezy_udp@dns-test-service.dns-1120.svc wheezy_tcp@dns-test-service.dns-1120.svc wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1120 jessie_tcp@dns-test-service.dns-1120 jessie_udp@dns-test-service.dns-1120.svc jessie_tcp@dns-test-service.dns-1120.svc jessie_udp@_http._tcp.dns-test-service.dns-1120.svc jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc]

Jul  8 02:18:43.431: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.434: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.437: INFO: Unable to read wheezy_udp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.439: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.442: INFO: Unable to read wheezy_udp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.444: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.447: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.458: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.476: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.478: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.486: INFO: Unable to read jessie_udp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.492: INFO: Unable to read jessie_tcp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.500: INFO: Unable to read jessie_udp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.504: INFO: Unable to read jessie_tcp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.508: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.521: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:43.556: INFO: Lookups using dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1120 wheezy_tcp@dns-test-service.dns-1120 wheezy_udp@dns-test-service.dns-1120.svc wheezy_tcp@dns-test-service.dns-1120.svc wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1120 jessie_tcp@dns-test-service.dns-1120 jessie_udp@dns-test-service.dns-1120.svc jessie_tcp@dns-test-service.dns-1120.svc jessie_udp@_http._tcp.dns-test-service.dns-1120.svc jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc]

Jul  8 02:18:48.430: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.434: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.437: INFO: Unable to read wheezy_udp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.439: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.442: INFO: Unable to read wheezy_udp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.444: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.446: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.449: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.475: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.477: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.480: INFO: Unable to read jessie_udp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.482: INFO: Unable to read jessie_tcp@dns-test-service.dns-1120 from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.484: INFO: Unable to read jessie_udp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.487: INFO: Unable to read jessie_tcp@dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.489: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.492: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:48.506: INFO: Lookups using dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1120 wheezy_tcp@dns-test-service.dns-1120 wheezy_udp@dns-test-service.dns-1120.svc wheezy_tcp@dns-test-service.dns-1120.svc wheezy_udp@_http._tcp.dns-test-service.dns-1120.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1120.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1120 jessie_tcp@dns-test-service.dns-1120 jessie_udp@dns-test-service.dns-1120.svc jessie_tcp@dns-test-service.dns-1120.svc jessie_udp@_http._tcp.dns-test-service.dns-1120.svc jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc]

Jul  8 02:18:53.478: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:53.481: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc from pod dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a: the server could not find the requested resource (get pods dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a)
Jul  8 02:18:53.496: INFO: Lookups using dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a failed for: [jessie_udp@_http._tcp.dns-test-service.dns-1120.svc jessie_tcp@_http._tcp.dns-test-service.dns-1120.svc]

Jul  8 02:18:58.523: INFO: DNS probes using dns-1120/dns-test-28e56ad8-d34f-4da8-9a2b-740ad398e41a succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:18:58.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1120" for this suite.

• [SLOW TEST:37.638 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":339,"completed":78,"skipped":1154,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:18:58.745: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5358
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul  8 02:18:58.892: INFO: Waiting up to 5m0s for pod "pod-d02bd829-265a-4f0d-a561-dcb53bd5a425" in namespace "emptydir-5358" to be "Succeeded or Failed"
Jul  8 02:18:58.896: INFO: Pod "pod-d02bd829-265a-4f0d-a561-dcb53bd5a425": Phase="Pending", Reason="", readiness=false. Elapsed: 3.482592ms
Jul  8 02:19:00.904: INFO: Pod "pod-d02bd829-265a-4f0d-a561-dcb53bd5a425": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011251126s
STEP: Saw pod success
Jul  8 02:19:00.904: INFO: Pod "pod-d02bd829-265a-4f0d-a561-dcb53bd5a425" satisfied condition "Succeeded or Failed"
Jul  8 02:19:00.907: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-d02bd829-265a-4f0d-a561-dcb53bd5a425 container test-container: <nil>
STEP: delete the pod
Jul  8 02:19:00.963: INFO: Waiting for pod pod-d02bd829-265a-4f0d-a561-dcb53bd5a425 to disappear
Jul  8 02:19:00.966: INFO: Pod pod-d02bd829-265a-4f0d-a561-dcb53bd5a425 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:19:00.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5358" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":79,"skipped":1159,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:19:00.988: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:19:01.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6011 create -f -'
Jul  8 02:19:01.446: INFO: stderr: ""
Jul  8 02:19:01.446: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jul  8 02:19:01.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6011 create -f -'
Jul  8 02:19:01.756: INFO: stderr: ""
Jul  8 02:19:01.756: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul  8 02:19:02.764: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  8 02:19:02.764: INFO: Found 0 / 1
Jul  8 02:19:03.763: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  8 02:19:03.763: INFO: Found 1 / 1
Jul  8 02:19:03.763: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  8 02:19:03.773: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  8 02:19:03.773: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  8 02:19:03.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6011 describe pod agnhost-primary-67lfv'
Jul  8 02:19:03.900: INFO: stderr: ""
Jul  8 02:19:03.900: INFO: stdout: "Name:         agnhost-primary-67lfv\nNamespace:    kubectl-6011\nPriority:     0\nNode:         ip-172-31-6-217.us-east-2.compute.internal/172.31.6.217\nStart Time:   Thu, 08 Jul 2021 02:19:01 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.42.3.75/32\n              cni.projectcalico.org/podIPs: 10.42.3.75/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           10.42.3.75\nIPs:\n  IP:           10.42.3.75\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://80b8ee9805d5b37a5d6c133da9f81cb460f3d5b894f8aff5a6e62d5931d32ea4\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 08 Jul 2021 02:19:02 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-r4575 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-r4575:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6011/agnhost-primary-67lfv to ip-172-31-6-217.us-east-2.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jul  8 02:19:03.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6011 describe rc agnhost-primary'
Jul  8 02:19:04.019: INFO: stderr: ""
Jul  8 02:19:04.019: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6011\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-67lfv\n"
Jul  8 02:19:04.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6011 describe service agnhost-primary'
Jul  8 02:19:04.128: INFO: stderr: ""
Jul  8 02:19:04.128: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6011\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.43.136.148\nIPs:               10.43.136.148\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.42.3.75:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul  8 02:19:04.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6011 describe node ip-172-31-3-228.us-east-2.compute.internal'
Jul  8 02:19:04.257: INFO: stderr: ""
Jul  8 02:19:04.257: INFO: stdout: "Name:               ip-172-31-3-228.us-east-2.compute.internal\nRoles:              control-plane,etcd,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-2\n                    failure-domain.beta.kubernetes.io/zone=us-east-2a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-3-228.us-east-2.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=true\n                    node-role.kubernetes.io/etcd=true\n                    node-role.kubernetes.io/master=true\n                    node.kubernetes.io/instance-type=t3.medium\n                    topology.kubernetes.io/region=us-east-2\n                    topology.kubernetes.io/zone=us-east-2a\nAnnotations:        etcd.k3s.cattle.io/node-address: 172.31.3.228\n                    etcd.k3s.cattle.io/node-name: ip-172-31-3-228-7111b530\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"f2:2c:b2:24:02:e5\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.3.228\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.3.228/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.42.0.1\n                    rke2.io/node-args:\n                      [\"server\",\"--write-kubeconfig-mode\",\"0644\",\"--tls-san\",\"shyurke-route53.qa.rancher.space\",\"--cloud-provider-name\",\"aws\",\"--node-name\",\"ip-...\n                    rke2.io/node-config-hash: VUUGJ2QANZIDHURDVAVQX3ITLFEKL3HZ4LSZSSZXEPTNRTKYV2SA====\n                    rke2.io/node-env: {}\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 07 Jul 2021 23:39:53 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-3-228.us-east-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 08 Jul 2021 02:18:56 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 07 Jul 2021 23:40:49 +0000   Wed, 07 Jul 2021 23:40:49 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Thu, 08 Jul 2021 02:18:06 +0000   Wed, 07 Jul 2021 23:39:48 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 08 Jul 2021 02:18:06 +0000   Wed, 07 Jul 2021 23:39:48 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 08 Jul 2021 02:18:06 +0000   Wed, 07 Jul 2021 23:39:48 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 08 Jul 2021 02:18:06 +0000   Wed, 07 Jul 2021 23:40:43 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.3.228\n  ExternalIP:   18.116.14.21\n  Hostname:     ip-172-31-3-228.us-east-2.compute.internal\n  InternalDNS:  ip-172-31-3-228.us-east-2.compute.internal\n  ExternalDNS:  ec2-18-116-14-21.us-east-2.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           20263484Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3969472Ki\n  pods:                        110\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           19712317220\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3969472Ki\n  pods:                        110\nSystem Info:\n  Machine ID:                 ec2f10fa69bffc44b9a8cb2921fa14d7\n  System UUID:                ec2f10fa-69bf-fc44-b9a8-cb2921fa14d7\n  Boot ID:                    048a9431-418c-4521-a645-bff988cabb3a\n  Kernel Version:             5.4.0-1009-aws\n  OS Image:                   Ubuntu 20.04 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.4-k3s2\n  Kubelet Version:            v1.21.2+rke2r1\n  Kube-Proxy Version:         v1.21.2+rke2r1\nPodCIDR:                      10.42.0.0/24\nPodCIDRs:                     10.42.0.0/24\nProviderID:                   aws:///us-east-2a/i-0634fcfdb90b4794d\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                  ------------  ----------  ---------------  -------------  ---\n  kube-system                 etcd-ip-172-31-3-228.us-east-2.compute.internal                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         158m\n  kube-system                 kube-apiserver-ip-172-31-3-228.us-east-2.compute.internal             250m (12%)    0 (0%)      0 (0%)           0 (0%)         158m\n  kube-system                 kube-controller-manager-ip-172-31-3-228.us-east-2.compute.internal    200m (10%)    0 (0%)      0 (0%)           0 (0%)         159m\n  kube-system                 kube-proxy-vvsjd                                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         158m\n  kube-system                 kube-scheduler-ip-172-31-3-228.us-east-2.compute.internal             100m (5%)     0 (0%)      0 (0%)           0 (0%)         159m\n  kube-system                 rke2-canal-kcjjk                                                      250m (12%)    0 (0%)      0 (0%)           0 (0%)         158m\n  kube-system                 rke2-coredns-rke2-coredns-65d668ddf9-bd9pd                            100m (5%)     100m (5%)   128Mi (3%)       128Mi (3%)     158m\n  kube-system                 rke2-metrics-server-6647ffc866-7z7j7                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         158m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-84vqn               0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         900m (45%)  100m (5%)\n  memory                      128Mi (3%)  128Mi (3%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:                       <none>\n"
Jul  8 02:19:04.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6011 describe namespace kubectl-6011'
Jul  8 02:19:04.364: INFO: stderr: ""
Jul  8 02:19:04.364: INFO: stdout: "Name:         kubectl-6011\nLabels:       e2e-framework=kubectl\n              e2e-run=056299fd-c67f-45b1-9c38-56a7ea751ee1\n              kubernetes.io/metadata.name=kubectl-6011\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:19:04.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6011" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":339,"completed":80,"skipped":1192,"failed":0}
S
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:19:04.375: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-1743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jul  8 02:19:04.513: INFO: Waiting up to 1m0s for all nodes to be ready
Jul  8 02:20:04.565: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:20:04.578: INFO: Starting informer...
STEP: Starting pods...
Jul  8 02:20:04.803: INFO: Pod1 is running on ip-172-31-6-217.us-east-2.compute.internal. Tainting Node
Jul  8 02:20:07.024: INFO: Pod2 is running on ip-172-31-6-217.us-east-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jul  8 02:20:18.526: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul  8 02:20:38.512: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:20:38.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1743" for this suite.

• [SLOW TEST:94.165 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":339,"completed":81,"skipped":1193,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:20:38.541: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1676
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-1676
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1676
STEP: Creating statefulset with conflicting port in namespace statefulset-1676
STEP: Waiting until pod test-pod will start running in namespace statefulset-1676
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1676
Jul  8 02:20:42.757: INFO: Observed stateful pod in namespace: statefulset-1676, name: ss-0, uid: d341bfc2-663f-4720-b784-af029ad3286f, status phase: Pending. Waiting for statefulset controller to delete.
Jul  8 02:20:43.146: INFO: Observed stateful pod in namespace: statefulset-1676, name: ss-0, uid: d341bfc2-663f-4720-b784-af029ad3286f, status phase: Failed. Waiting for statefulset controller to delete.
Jul  8 02:20:43.168: INFO: Observed stateful pod in namespace: statefulset-1676, name: ss-0, uid: d341bfc2-663f-4720-b784-af029ad3286f, status phase: Failed. Waiting for statefulset controller to delete.
Jul  8 02:20:43.180: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1676
STEP: Removing pod with conflicting port in namespace statefulset-1676
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1676 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul  8 02:20:47.269: INFO: Deleting all statefulset in ns statefulset-1676
Jul  8 02:20:47.272: INFO: Scaling statefulset ss to 0
Jul  8 02:20:57.288: INFO: Waiting for statefulset status.replicas updated to 0
Jul  8 02:20:57.290: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:20:57.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1676" for this suite.

• [SLOW TEST:18.799 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":339,"completed":82,"skipped":1204,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:20:57.344: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-7346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0708 02:20:57.487862      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:25:57.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7346" for this suite.

• [SLOW TEST:300.175 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":339,"completed":83,"skipped":1219,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:25:57.520: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-a9d9a48c-d983-49a3-98fd-e2cfd03ef21a
STEP: Creating a pod to test consume secrets
Jul  8 02:25:57.682: INFO: Waiting up to 5m0s for pod "pod-secrets-3d6ea07a-774c-492e-a997-79bfd09595c3" in namespace "secrets-7931" to be "Succeeded or Failed"
Jul  8 02:25:57.694: INFO: Pod "pod-secrets-3d6ea07a-774c-492e-a997-79bfd09595c3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.310757ms
Jul  8 02:25:59.699: INFO: Pod "pod-secrets-3d6ea07a-774c-492e-a997-79bfd09595c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017440438s
STEP: Saw pod success
Jul  8 02:25:59.699: INFO: Pod "pod-secrets-3d6ea07a-774c-492e-a997-79bfd09595c3" satisfied condition "Succeeded or Failed"
Jul  8 02:25:59.702: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-secrets-3d6ea07a-774c-492e-a997-79bfd09595c3 container secret-volume-test: <nil>
STEP: delete the pod
Jul  8 02:25:59.733: INFO: Waiting for pod pod-secrets-3d6ea07a-774c-492e-a997-79bfd09595c3 to disappear
Jul  8 02:25:59.737: INFO: Pod pod-secrets-3d6ea07a-774c-492e-a997-79bfd09595c3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:25:59.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7931" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":84,"skipped":1227,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:25:59.744: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-7287
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Jul  8 02:25:59.931: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Jul  8 02:26:01.954: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Jul  8 02:26:03.987: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:26:05.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-7287" for this suite.

• [SLOW TEST:6.258 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":339,"completed":85,"skipped":1243,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:26:06.003: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-c79ddb2e-01b6-4ea3-8a69-4d1bb901fe67
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:26:06.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7135" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":339,"completed":86,"skipped":1272,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:26:06.159: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5039
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5039
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  8 02:26:06.295: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul  8 02:26:06.382: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:26:08.386: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:10.389: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:12.388: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:14.388: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:16.389: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:18.386: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:20.388: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:22.388: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:24.391: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:26.393: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul  8 02:26:26.409: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul  8 02:26:26.420: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jul  8 02:26:26.425: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jul  8 02:26:28.448: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jul  8 02:26:28.448: INFO: Breadth first check of 10.42.0.10 on host 172.31.3.228...
Jul  8 02:26:28.450: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.80:9080/dial?request=hostname&protocol=udp&host=10.42.0.10&port=8081&tries=1'] Namespace:pod-network-test-5039 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 02:26:28.450: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:26:28.547: INFO: Waiting for responses: map[]
Jul  8 02:26:28.547: INFO: reached 10.42.0.10 after 0/1 tries
Jul  8 02:26:28.547: INFO: Breadth first check of 10.42.3.79 on host 172.31.6.217...
Jul  8 02:26:28.551: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.80:9080/dial?request=hostname&protocol=udp&host=10.42.3.79&port=8081&tries=1'] Namespace:pod-network-test-5039 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 02:26:28.551: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:26:28.620: INFO: Waiting for responses: map[]
Jul  8 02:26:28.620: INFO: reached 10.42.3.79 after 0/1 tries
Jul  8 02:26:28.620: INFO: Breadth first check of 10.42.1.14 on host 172.31.6.226...
Jul  8 02:26:28.624: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.80:9080/dial?request=hostname&protocol=udp&host=10.42.1.14&port=8081&tries=1'] Namespace:pod-network-test-5039 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 02:26:28.624: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:26:28.687: INFO: Waiting for responses: map[]
Jul  8 02:26:28.687: INFO: reached 10.42.1.14 after 0/1 tries
Jul  8 02:26:28.687: INFO: Breadth first check of 10.42.2.15 on host 172.31.8.165...
Jul  8 02:26:28.690: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.80:9080/dial?request=hostname&protocol=udp&host=10.42.2.15&port=8081&tries=1'] Namespace:pod-network-test-5039 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 02:26:28.690: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:26:28.755: INFO: Waiting for responses: map[]
Jul  8 02:26:28.755: INFO: reached 10.42.2.15 after 0/1 tries
Jul  8 02:26:28.755: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:26:28.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5039" for this suite.

• [SLOW TEST:22.616 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":339,"completed":87,"skipped":1293,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:26:28.776: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4815
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4116
STEP: Creating secret with name secret-test-89be915f-78da-472d-9226-8f4e011f7659
STEP: Creating a pod to test consume secrets
Jul  8 02:26:29.082: INFO: Waiting up to 5m0s for pod "pod-secrets-f8ef8b17-ea8a-4f99-82ed-c1e5a64a1455" in namespace "secrets-4815" to be "Succeeded or Failed"
Jul  8 02:26:29.090: INFO: Pod "pod-secrets-f8ef8b17-ea8a-4f99-82ed-c1e5a64a1455": Phase="Pending", Reason="", readiness=false. Elapsed: 7.992348ms
Jul  8 02:26:31.097: INFO: Pod "pod-secrets-f8ef8b17-ea8a-4f99-82ed-c1e5a64a1455": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015641105s
STEP: Saw pod success
Jul  8 02:26:31.097: INFO: Pod "pod-secrets-f8ef8b17-ea8a-4f99-82ed-c1e5a64a1455" satisfied condition "Succeeded or Failed"
Jul  8 02:26:31.100: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-secrets-f8ef8b17-ea8a-4f99-82ed-c1e5a64a1455 container secret-volume-test: <nil>
STEP: delete the pod
Jul  8 02:26:31.120: INFO: Waiting for pod pod-secrets-f8ef8b17-ea8a-4f99-82ed-c1e5a64a1455 to disappear
Jul  8 02:26:31.123: INFO: Pod pod-secrets-f8ef8b17-ea8a-4f99-82ed-c1e5a64a1455 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:26:31.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4815" for this suite.
STEP: Destroying namespace "secret-namespace-4116" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":339,"completed":88,"skipped":1311,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:26:31.140: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2380
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-136c423b-11cf-46d7-a00a-e6e086e34222
STEP: Creating secret with name s-test-opt-upd-fd45a6cb-e20d-4040-9143-3569c9ee0a64
STEP: Creating the pod
Jul  8 02:26:31.328: INFO: The status of Pod pod-secrets-737b5cb2-fd0a-4a2c-af4f-640acf3fd084 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:26:33.336: INFO: The status of Pod pod-secrets-737b5cb2-fd0a-4a2c-af4f-640acf3fd084 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-136c423b-11cf-46d7-a00a-e6e086e34222
STEP: Updating secret s-test-opt-upd-fd45a6cb-e20d-4040-9143-3569c9ee0a64
STEP: Creating secret with name s-test-opt-create-32430c17-f0fc-4a21-b722-cd78aa5c9858
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:26:35.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2380" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":89,"skipped":1354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:26:35.444: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1681
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-1681
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  8 02:26:35.681: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul  8 02:26:35.759: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:26:37.765: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:39.766: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:41.765: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:43.766: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:45.766: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:47.766: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:49.765: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:51.767: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:53.765: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:26:55.769: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul  8 02:26:55.776: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul  8 02:26:55.795: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jul  8 02:26:55.807: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jul  8 02:26:57.814: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jul  8 02:26:59.849: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jul  8 02:26:59.849: INFO: Breadth first check of 10.42.0.11 on host 172.31.3.228...
Jul  8 02:26:59.851: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.84:9080/dial?request=hostname&protocol=http&host=10.42.0.11&port=8080&tries=1'] Namespace:pod-network-test-1681 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 02:26:59.851: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:26:59.915: INFO: Waiting for responses: map[]
Jul  8 02:26:59.915: INFO: reached 10.42.0.11 after 0/1 tries
Jul  8 02:26:59.915: INFO: Breadth first check of 10.42.3.83 on host 172.31.6.217...
Jul  8 02:26:59.918: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.84:9080/dial?request=hostname&protocol=http&host=10.42.3.83&port=8080&tries=1'] Namespace:pod-network-test-1681 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 02:26:59.918: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:26:59.975: INFO: Waiting for responses: map[]
Jul  8 02:26:59.975: INFO: reached 10.42.3.83 after 0/1 tries
Jul  8 02:26:59.975: INFO: Breadth first check of 10.42.1.15 on host 172.31.6.226...
Jul  8 02:26:59.978: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.84:9080/dial?request=hostname&protocol=http&host=10.42.1.15&port=8080&tries=1'] Namespace:pod-network-test-1681 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 02:26:59.978: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:27:00.042: INFO: Waiting for responses: map[]
Jul  8 02:27:00.042: INFO: reached 10.42.1.15 after 0/1 tries
Jul  8 02:27:00.042: INFO: Breadth first check of 10.42.2.16 on host 172.31.8.165...
Jul  8 02:27:00.045: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.84:9080/dial?request=hostname&protocol=http&host=10.42.2.16&port=8080&tries=1'] Namespace:pod-network-test-1681 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 02:27:00.045: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:27:00.114: INFO: Waiting for responses: map[]
Jul  8 02:27:00.114: INFO: reached 10.42.2.16 after 0/1 tries
Jul  8 02:27:00.114: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:27:00.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1681" for this suite.

• [SLOW TEST:24.683 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":339,"completed":90,"skipped":1404,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:27:00.127: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Jul  8 02:27:00.306: INFO: created test-pod-1
Jul  8 02:27:00.316: INFO: created test-pod-2
Jul  8 02:27:00.337: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:27:00.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1620" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":339,"completed":91,"skipped":1418,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:27:00.438: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul  8 02:27:00.603: INFO: Waiting up to 1m0s for all nodes to be ready
Jul  8 02:28:00.650: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Jul  8 02:28:00.699: INFO: Created pod: pod0-sched-preemption-low-priority
Jul  8 02:28:00.721: INFO: Created pod: pod1-sched-preemption-medium-priority
Jul  8 02:28:00.745: INFO: Created pod: pod2-sched-preemption-medium-priority
Jul  8 02:28:00.782: INFO: Created pod: pod3-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:28:26.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6791" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:86.460 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":339,"completed":92,"skipped":1450,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:28:26.899: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-48
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Jul  8 02:28:27.062: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:28:29.066: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:28:30.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-48" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":339,"completed":93,"skipped":1460,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:28:30.109: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4601
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul  8 02:28:30.292: INFO: Waiting up to 1m0s for all nodes to be ready
Jul  8 02:29:30.373: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Jul  8 02:29:30.394: INFO: Created pod: pod0-sched-preemption-low-priority
Jul  8 02:29:30.438: INFO: Created pod: pod1-sched-preemption-medium-priority
Jul  8 02:29:30.473: INFO: Created pod: pod2-sched-preemption-medium-priority
Jul  8 02:29:30.503: INFO: Created pod: pod3-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:29:38.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4601" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:68.626 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":339,"completed":94,"skipped":1487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:29:38.736: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8391
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:29:59.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8391" for this suite.

• [SLOW TEST:20.505 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":339,"completed":95,"skipped":1518,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:29:59.243: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2543
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 02:30:00.132: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 02:30:03.168: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:03.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2543" for this suite.
STEP: Destroying namespace "webhook-2543-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":339,"completed":96,"skipped":1531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:03.363: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:30:03.554: INFO: The status of Pod busybox-host-aliases6506555c-37c3-43b4-aed9-f45bd7375a83 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:30:05.561: INFO: The status of Pod busybox-host-aliases6506555c-37c3-43b4-aed9-f45bd7375a83 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:05.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4920" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":97,"skipped":1565,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:05.584: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-5301
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:30:06.017: INFO: Checking APIGroup: apiregistration.k8s.io
Jul  8 02:30:06.018: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jul  8 02:30:06.018: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.018: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jul  8 02:30:06.018: INFO: Checking APIGroup: apps
Jul  8 02:30:06.019: INFO: PreferredVersion.GroupVersion: apps/v1
Jul  8 02:30:06.019: INFO: Versions found [{apps/v1 v1}]
Jul  8 02:30:06.019: INFO: apps/v1 matches apps/v1
Jul  8 02:30:06.019: INFO: Checking APIGroup: events.k8s.io
Jul  8 02:30:06.020: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jul  8 02:30:06.020: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.020: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jul  8 02:30:06.020: INFO: Checking APIGroup: authentication.k8s.io
Jul  8 02:30:06.020: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jul  8 02:30:06.020: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.020: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jul  8 02:30:06.020: INFO: Checking APIGroup: authorization.k8s.io
Jul  8 02:30:06.021: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jul  8 02:30:06.021: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.021: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jul  8 02:30:06.021: INFO: Checking APIGroup: autoscaling
Jul  8 02:30:06.022: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jul  8 02:30:06.022: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jul  8 02:30:06.022: INFO: autoscaling/v1 matches autoscaling/v1
Jul  8 02:30:06.022: INFO: Checking APIGroup: batch
Jul  8 02:30:06.022: INFO: PreferredVersion.GroupVersion: batch/v1
Jul  8 02:30:06.022: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jul  8 02:30:06.022: INFO: batch/v1 matches batch/v1
Jul  8 02:30:06.022: INFO: Checking APIGroup: certificates.k8s.io
Jul  8 02:30:06.023: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jul  8 02:30:06.023: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.023: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jul  8 02:30:06.023: INFO: Checking APIGroup: networking.k8s.io
Jul  8 02:30:06.024: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jul  8 02:30:06.024: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.024: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jul  8 02:30:06.024: INFO: Checking APIGroup: extensions
Jul  8 02:30:06.024: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jul  8 02:30:06.024: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jul  8 02:30:06.024: INFO: extensions/v1beta1 matches extensions/v1beta1
Jul  8 02:30:06.024: INFO: Checking APIGroup: policy
Jul  8 02:30:06.025: INFO: PreferredVersion.GroupVersion: policy/v1
Jul  8 02:30:06.025: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jul  8 02:30:06.025: INFO: policy/v1 matches policy/v1
Jul  8 02:30:06.025: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jul  8 02:30:06.026: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jul  8 02:30:06.026: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.026: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jul  8 02:30:06.026: INFO: Checking APIGroup: storage.k8s.io
Jul  8 02:30:06.026: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jul  8 02:30:06.026: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.026: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jul  8 02:30:06.026: INFO: Checking APIGroup: admissionregistration.k8s.io
Jul  8 02:30:06.027: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jul  8 02:30:06.027: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.027: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jul  8 02:30:06.027: INFO: Checking APIGroup: apiextensions.k8s.io
Jul  8 02:30:06.027: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jul  8 02:30:06.027: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.027: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jul  8 02:30:06.027: INFO: Checking APIGroup: scheduling.k8s.io
Jul  8 02:30:06.028: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jul  8 02:30:06.028: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.028: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jul  8 02:30:06.028: INFO: Checking APIGroup: coordination.k8s.io
Jul  8 02:30:06.029: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jul  8 02:30:06.029: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.029: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jul  8 02:30:06.029: INFO: Checking APIGroup: node.k8s.io
Jul  8 02:30:06.029: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jul  8 02:30:06.029: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.029: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jul  8 02:30:06.029: INFO: Checking APIGroup: discovery.k8s.io
Jul  8 02:30:06.030: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jul  8 02:30:06.030: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.030: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jul  8 02:30:06.030: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jul  8 02:30:06.031: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Jul  8 02:30:06.031: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.031: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Jul  8 02:30:06.031: INFO: Checking APIGroup: crd.projectcalico.org
Jul  8 02:30:06.031: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jul  8 02:30:06.031: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jul  8 02:30:06.031: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jul  8 02:30:06.031: INFO: Checking APIGroup: helm.cattle.io
Jul  8 02:30:06.032: INFO: PreferredVersion.GroupVersion: helm.cattle.io/v1
Jul  8 02:30:06.032: INFO: Versions found [{helm.cattle.io/v1 v1}]
Jul  8 02:30:06.032: INFO: helm.cattle.io/v1 matches helm.cattle.io/v1
Jul  8 02:30:06.032: INFO: Checking APIGroup: k3s.cattle.io
Jul  8 02:30:06.033: INFO: PreferredVersion.GroupVersion: k3s.cattle.io/v1
Jul  8 02:30:06.033: INFO: Versions found [{k3s.cattle.io/v1 v1}]
Jul  8 02:30:06.033: INFO: k3s.cattle.io/v1 matches k3s.cattle.io/v1
Jul  8 02:30:06.033: INFO: Checking APIGroup: metrics.k8s.io
Jul  8 02:30:06.033: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jul  8 02:30:06.033: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jul  8 02:30:06.033: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:06.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5301" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":339,"completed":98,"skipped":1569,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:06.044: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Jul  8 02:30:06.200: INFO: Waiting up to 5m0s for pod "var-expansion-04ef2cde-b231-458d-834a-2e0a9d6c6d40" in namespace "var-expansion-6355" to be "Succeeded or Failed"
Jul  8 02:30:06.205: INFO: Pod "var-expansion-04ef2cde-b231-458d-834a-2e0a9d6c6d40": Phase="Pending", Reason="", readiness=false. Elapsed: 5.680216ms
Jul  8 02:30:08.211: INFO: Pod "var-expansion-04ef2cde-b231-458d-834a-2e0a9d6c6d40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011381575s
STEP: Saw pod success
Jul  8 02:30:08.211: INFO: Pod "var-expansion-04ef2cde-b231-458d-834a-2e0a9d6c6d40" satisfied condition "Succeeded or Failed"
Jul  8 02:30:08.213: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod var-expansion-04ef2cde-b231-458d-834a-2e0a9d6c6d40 container dapi-container: <nil>
STEP: delete the pod
Jul  8 02:30:08.231: INFO: Waiting for pod var-expansion-04ef2cde-b231-458d-834a-2e0a9d6c6d40 to disappear
Jul  8 02:30:08.235: INFO: Pod var-expansion-04ef2cde-b231-458d-834a-2e0a9d6c6d40 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:08.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6355" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":339,"completed":99,"skipped":1579,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:08.243: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4871
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:30:08.438: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul  8 02:30:10.488: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:11.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4871" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":339,"completed":100,"skipped":1579,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:11.508: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5817
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:22.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5817" for this suite.

• [SLOW TEST:11.320 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":339,"completed":101,"skipped":1579,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:22.832: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Jul  8 02:30:22.975: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6482 proxy --unix-socket=/tmp/kubectl-proxy-unix097350531/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:23.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6482" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":339,"completed":102,"skipped":1596,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:23.076: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9320
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jul  8 02:30:23.216: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:30:26.713: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:39.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9320" for this suite.

• [SLOW TEST:16.584 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":339,"completed":103,"skipped":1597,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:39.660: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-407
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:43.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-407" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":339,"completed":104,"skipped":1607,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:43.838: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jul  8 02:30:44.019: INFO: The status of Pod annotationupdateb8e2181d-ec94-483d-956f-2654162dd0e6 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:30:46.025: INFO: The status of Pod annotationupdateb8e2181d-ec94-483d-956f-2654162dd0e6 is Running (Ready = true)
Jul  8 02:30:46.547: INFO: Successfully updated pod "annotationupdateb8e2181d-ec94-483d-956f-2654162dd0e6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:50.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9579" for this suite.

• [SLOW TEST:6.751 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":105,"skipped":1607,"failed":0}
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:50.590: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-7429
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jul  8 02:30:50.760: INFO: Waiting up to 5m0s for pod "security-context-f52cea36-bcb3-4116-81f4-7e412d78acac" in namespace "security-context-7429" to be "Succeeded or Failed"
Jul  8 02:30:50.766: INFO: Pod "security-context-f52cea36-bcb3-4116-81f4-7e412d78acac": Phase="Pending", Reason="", readiness=false. Elapsed: 5.761352ms
Jul  8 02:30:52.772: INFO: Pod "security-context-f52cea36-bcb3-4116-81f4-7e412d78acac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012626484s
STEP: Saw pod success
Jul  8 02:30:52.773: INFO: Pod "security-context-f52cea36-bcb3-4116-81f4-7e412d78acac" satisfied condition "Succeeded or Failed"
Jul  8 02:30:52.776: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod security-context-f52cea36-bcb3-4116-81f4-7e412d78acac container test-container: <nil>
STEP: delete the pod
Jul  8 02:30:52.795: INFO: Waiting for pod security-context-f52cea36-bcb3-4116-81f4-7e412d78acac to disappear
Jul  8 02:30:52.797: INFO: Pod security-context-f52cea36-bcb3-4116-81f4-7e412d78acac no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:52.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7429" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":106,"skipped":1607,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:52.814: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-7357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:52.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7357" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":107,"skipped":1642,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:52.976: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8976
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:30:53.118: INFO: Creating deployment "test-recreate-deployment"
Jul  8 02:30:53.123: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul  8 02:30:53.140: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul  8 02:30:55.149: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul  8 02:30:55.151: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul  8 02:30:55.165: INFO: Updating deployment test-recreate-deployment
Jul  8 02:30:55.165: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul  8 02:30:55.303: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8976  a604abd6-b9da-4d01-8623-86192f44c3d1 29276 2 2021-07-08 02:30:53 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-08 02:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-08 02:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e63a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-08 02:30:55 +0000 UTC,LastTransitionTime:2021-07-08 02:30:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-07-08 02:30:55 +0000 UTC,LastTransitionTime:2021-07-08 02:30:53 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul  8 02:30:55.309: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-8976  c6a8ae9a-a330-4235-95b2-5890570be1f1 29275 1 2021-07-08 02:30:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment a604abd6-b9da-4d01-8623-86192f44c3d1 0xc004e90cb0 0xc004e90cb1}] []  [{kube-controller-manager Update apps/v1 2021-07-08 02:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a604abd6-b9da-4d01-8623-86192f44c3d1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e90dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  8 02:30:55.309: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul  8 02:30:55.309: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-8976  19547ffd-9e0e-4a1e-9fb6-5f2270d59d06 29264 2 2021-07-08 02:30:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment a604abd6-b9da-4d01-8623-86192f44c3d1 0xc004e90b17 0xc004e90b18}] []  [{kube-controller-manager Update apps/v1 2021-07-08 02:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a604abd6-b9da-4d01-8623-86192f44c3d1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e90c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  8 02:30:55.315: INFO: Pod "test-recreate-deployment-85d47dcb4-kl7k2" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-kl7k2 test-recreate-deployment-85d47dcb4- deployment-8976  88bdbc84-41f5-40cf-b9c6-4c57852b4196 29274 0 2021-07-08 02:30:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 c6a8ae9a-a330-4235-95b2-5890570be1f1 0xc004e91530 0xc004e91531}] []  [{kube-controller-manager Update v1 2021-07-08 02:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6a8ae9a-a330-4235-95b2-5890570be1f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-08 02:30:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtt8s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtt8s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 02:30:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 02:30:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 02:30:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 02:30:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.217,PodIP:,StartTime:2021-07-08 02:30:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:55.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8976" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":108,"skipped":1647,"failed":0}
SSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:55.325: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-3797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:55.487: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename disruption-2
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-6609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-3797
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:57.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-6609" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:30:57.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3797" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":339,"completed":109,"skipped":1651,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:30:57.719: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9439
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:30:57.961: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f20628ef-1595-41e8-9b8e-c653e9695390", Controller:(*bool)(0xc004f8b07a), BlockOwnerDeletion:(*bool)(0xc004f8b07b)}}
Jul  8 02:30:57.980: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"949e409e-ad88-4a30-8936-7e1713295427", Controller:(*bool)(0xc004f8b4a6), BlockOwnerDeletion:(*bool)(0xc004f8b4a7)}}
Jul  8 02:30:58.026: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"80f8d23c-e8be-4c2c-854e-877dbb997751", Controller:(*bool)(0xc004f8b906), BlockOwnerDeletion:(*bool)(0xc004f8b907)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:31:03.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9439" for this suite.

• [SLOW TEST:5.354 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":339,"completed":110,"skipped":1651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:31:03.074: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-5410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:31:03.229: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5410
I0708 02:31:03.244879      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5410, replica count: 1
I0708 02:31:04.296440      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  8 02:31:04.423: INFO: Created: latency-svc-lm5zt
Jul  8 02:31:04.424: INFO: Got endpoints: latency-svc-lm5zt [27.337057ms]
Jul  8 02:31:04.443: INFO: Created: latency-svc-x222w
Jul  8 02:31:04.456: INFO: Got endpoints: latency-svc-x222w [31.021824ms]
Jul  8 02:31:04.467: INFO: Created: latency-svc-2cq92
Jul  8 02:31:04.472: INFO: Got endpoints: latency-svc-2cq92 [45.929729ms]
Jul  8 02:31:04.500: INFO: Created: latency-svc-rpqf7
Jul  8 02:31:04.511: INFO: Got endpoints: latency-svc-rpqf7 [84.946962ms]
Jul  8 02:31:04.522: INFO: Created: latency-svc-n8k88
Jul  8 02:31:04.528: INFO: Got endpoints: latency-svc-n8k88 [102.866355ms]
Jul  8 02:31:04.536: INFO: Created: latency-svc-rqpl7
Jul  8 02:31:04.541: INFO: Got endpoints: latency-svc-rqpl7 [116.216294ms]
Jul  8 02:31:04.546: INFO: Created: latency-svc-t6wlb
Jul  8 02:31:04.556: INFO: Got endpoints: latency-svc-t6wlb [129.618629ms]
Jul  8 02:31:04.560: INFO: Created: latency-svc-slzxl
Jul  8 02:31:04.573: INFO: Got endpoints: latency-svc-slzxl [148.081941ms]
Jul  8 02:31:04.576: INFO: Created: latency-svc-7nqlx
Jul  8 02:31:04.577: INFO: Got endpoints: latency-svc-7nqlx [151.116989ms]
Jul  8 02:31:04.583: INFO: Created: latency-svc-2l8th
Jul  8 02:31:04.596: INFO: Created: latency-svc-d7mbr
Jul  8 02:31:04.599: INFO: Got endpoints: latency-svc-2l8th [173.681577ms]
Jul  8 02:31:04.601: INFO: Got endpoints: latency-svc-d7mbr [174.899819ms]
Jul  8 02:31:04.606: INFO: Created: latency-svc-wbftr
Jul  8 02:31:04.614: INFO: Got endpoints: latency-svc-wbftr [187.998611ms]
Jul  8 02:31:04.622: INFO: Created: latency-svc-l46cq
Jul  8 02:31:04.629: INFO: Created: latency-svc-4ptrq
Jul  8 02:31:04.629: INFO: Got endpoints: latency-svc-l46cq [202.894715ms]
Jul  8 02:31:04.636: INFO: Got endpoints: latency-svc-4ptrq [209.033723ms]
Jul  8 02:31:04.644: INFO: Created: latency-svc-f4djz
Jul  8 02:31:04.650: INFO: Got endpoints: latency-svc-f4djz [222.616133ms]
Jul  8 02:31:04.659: INFO: Created: latency-svc-6txbs
Jul  8 02:31:04.666: INFO: Got endpoints: latency-svc-6txbs [238.9512ms]
Jul  8 02:31:04.669: INFO: Created: latency-svc-vpb9d
Jul  8 02:31:04.675: INFO: Got endpoints: latency-svc-vpb9d [219.125795ms]
Jul  8 02:31:04.678: INFO: Created: latency-svc-kkchg
Jul  8 02:31:04.688: INFO: Got endpoints: latency-svc-kkchg [216.169397ms]
Jul  8 02:31:04.691: INFO: Created: latency-svc-8d4lj
Jul  8 02:31:04.701: INFO: Got endpoints: latency-svc-8d4lj [190.083575ms]
Jul  8 02:31:04.705: INFO: Created: latency-svc-kjhvt
Jul  8 02:31:04.713: INFO: Got endpoints: latency-svc-kjhvt [185.193195ms]
Jul  8 02:31:04.717: INFO: Created: latency-svc-p4nz2
Jul  8 02:31:04.726: INFO: Got endpoints: latency-svc-p4nz2 [184.144208ms]
Jul  8 02:31:04.734: INFO: Created: latency-svc-c5glx
Jul  8 02:31:04.741: INFO: Got endpoints: latency-svc-c5glx [184.734289ms]
Jul  8 02:31:04.742: INFO: Created: latency-svc-kbzhn
Jul  8 02:31:04.753: INFO: Got endpoints: latency-svc-kbzhn [179.494365ms]
Jul  8 02:31:04.761: INFO: Created: latency-svc-wsmjh
Jul  8 02:31:04.767: INFO: Got endpoints: latency-svc-wsmjh [189.558743ms]
Jul  8 02:31:04.780: INFO: Created: latency-svc-v92q5
Jul  8 02:31:04.790: INFO: Got endpoints: latency-svc-v92q5 [188.511973ms]
Jul  8 02:31:04.795: INFO: Created: latency-svc-nmkb4
Jul  8 02:31:04.809: INFO: Got endpoints: latency-svc-nmkb4 [41.687627ms]
Jul  8 02:31:04.820: INFO: Created: latency-svc-xm96v
Jul  8 02:31:04.825: INFO: Got endpoints: latency-svc-xm96v [225.331487ms]
Jul  8 02:31:04.829: INFO: Created: latency-svc-2rwps
Jul  8 02:31:04.838: INFO: Got endpoints: latency-svc-2rwps [223.583696ms]
Jul  8 02:31:04.842: INFO: Created: latency-svc-g7sj9
Jul  8 02:31:04.852: INFO: Got endpoints: latency-svc-g7sj9 [222.351119ms]
Jul  8 02:31:04.853: INFO: Created: latency-svc-lg8k6
Jul  8 02:31:04.862: INFO: Got endpoints: latency-svc-lg8k6 [225.460285ms]
Jul  8 02:31:04.865: INFO: Created: latency-svc-h57s6
Jul  8 02:31:04.873: INFO: Got endpoints: latency-svc-h57s6 [223.65558ms]
Jul  8 02:31:04.878: INFO: Created: latency-svc-8ckxq
Jul  8 02:31:04.887: INFO: Got endpoints: latency-svc-8ckxq [221.17483ms]
Jul  8 02:31:04.890: INFO: Created: latency-svc-btjxq
Jul  8 02:31:04.896: INFO: Got endpoints: latency-svc-btjxq [221.239515ms]
Jul  8 02:31:04.903: INFO: Created: latency-svc-8sknp
Jul  8 02:31:04.913: INFO: Got endpoints: latency-svc-8sknp [225.443048ms]
Jul  8 02:31:04.917: INFO: Created: latency-svc-h5b7f
Jul  8 02:31:04.927: INFO: Got endpoints: latency-svc-h5b7f [225.442505ms]
Jul  8 02:31:04.936: INFO: Created: latency-svc-mzf5x
Jul  8 02:31:04.946: INFO: Created: latency-svc-s46hm
Jul  8 02:31:04.946: INFO: Got endpoints: latency-svc-mzf5x [233.320352ms]
Jul  8 02:31:04.959: INFO: Got endpoints: latency-svc-s46hm [232.833918ms]
Jul  8 02:31:04.965: INFO: Created: latency-svc-wjzcs
Jul  8 02:31:04.972: INFO: Got endpoints: latency-svc-wjzcs [231.015777ms]
Jul  8 02:31:04.976: INFO: Created: latency-svc-nfg27
Jul  8 02:31:04.984: INFO: Created: latency-svc-69hc6
Jul  8 02:31:04.993: INFO: Got endpoints: latency-svc-nfg27 [239.64915ms]
Jul  8 02:31:04.995: INFO: Got endpoints: latency-svc-69hc6 [205.630647ms]
Jul  8 02:31:05.001: INFO: Created: latency-svc-48vsj
Jul  8 02:31:05.009: INFO: Got endpoints: latency-svc-48vsj [200.713818ms]
Jul  8 02:31:05.015: INFO: Created: latency-svc-kf4hn
Jul  8 02:31:05.023: INFO: Got endpoints: latency-svc-kf4hn [197.953799ms]
Jul  8 02:31:05.027: INFO: Created: latency-svc-dbr6j
Jul  8 02:31:05.033: INFO: Created: latency-svc-t7dkd
Jul  8 02:31:05.046: INFO: Created: latency-svc-27kb4
Jul  8 02:31:05.058: INFO: Created: latency-svc-mzctn
Jul  8 02:31:05.067: INFO: Created: latency-svc-7vp88
Jul  8 02:31:05.076: INFO: Got endpoints: latency-svc-dbr6j [237.766222ms]
Jul  8 02:31:05.080: INFO: Created: latency-svc-svbl9
Jul  8 02:31:05.095: INFO: Created: latency-svc-rv48x
Jul  8 02:31:05.102: INFO: Created: latency-svc-pmcrz
Jul  8 02:31:05.111: INFO: Created: latency-svc-c2gng
Jul  8 02:31:05.121: INFO: Got endpoints: latency-svc-t7dkd [269.159785ms]
Jul  8 02:31:05.124: INFO: Created: latency-svc-p5khf
Jul  8 02:31:05.132: INFO: Created: latency-svc-47pvv
Jul  8 02:31:05.137: INFO: Created: latency-svc-s2js5
Jul  8 02:31:05.145: INFO: Created: latency-svc-nqb47
Jul  8 02:31:05.152: INFO: Created: latency-svc-vpht2
Jul  8 02:31:05.165: INFO: Created: latency-svc-jjq2s
Jul  8 02:31:05.167: INFO: Got endpoints: latency-svc-27kb4 [305.630463ms]
Jul  8 02:31:05.176: INFO: Created: latency-svc-hhgv5
Jul  8 02:31:05.183: INFO: Created: latency-svc-2vzbs
Jul  8 02:31:05.189: INFO: Created: latency-svc-dpj9r
Jul  8 02:31:05.218: INFO: Got endpoints: latency-svc-mzctn [344.806645ms]
Jul  8 02:31:05.229: INFO: Created: latency-svc-g6xdw
Jul  8 02:31:05.265: INFO: Got endpoints: latency-svc-7vp88 [377.919417ms]
Jul  8 02:31:05.278: INFO: Created: latency-svc-4n7rp
Jul  8 02:31:05.317: INFO: Got endpoints: latency-svc-svbl9 [420.162415ms]
Jul  8 02:31:05.327: INFO: Created: latency-svc-5nqpp
Jul  8 02:31:05.366: INFO: Got endpoints: latency-svc-rv48x [452.437216ms]
Jul  8 02:31:05.386: INFO: Created: latency-svc-7pvdt
Jul  8 02:31:05.431: INFO: Got endpoints: latency-svc-pmcrz [504.841636ms]
Jul  8 02:31:05.461: INFO: Created: latency-svc-hmdl6
Jul  8 02:31:05.470: INFO: Got endpoints: latency-svc-c2gng [523.915571ms]
Jul  8 02:31:05.501: INFO: Created: latency-svc-4v4gz
Jul  8 02:31:05.530: INFO: Got endpoints: latency-svc-p5khf [571.261912ms]
Jul  8 02:31:05.551: INFO: Created: latency-svc-rqg9k
Jul  8 02:31:05.569: INFO: Got endpoints: latency-svc-47pvv [597.26328ms]
Jul  8 02:31:05.584: INFO: Created: latency-svc-4sm7x
Jul  8 02:31:05.617: INFO: Got endpoints: latency-svc-s2js5 [624.028848ms]
Jul  8 02:31:05.747: INFO: Created: latency-svc-q2n2w
Jul  8 02:31:05.757: INFO: Got endpoints: latency-svc-vpht2 [748.052425ms]
Jul  8 02:31:05.758: INFO: Got endpoints: latency-svc-nqb47 [762.178842ms]
Jul  8 02:31:05.794: INFO: Got endpoints: latency-svc-jjq2s [770.965152ms]
Jul  8 02:31:05.819: INFO: Got endpoints: latency-svc-hhgv5 [742.991047ms]
Jul  8 02:31:05.823: INFO: Created: latency-svc-xpmz4
Jul  8 02:31:05.829: INFO: Created: latency-svc-cww6z
Jul  8 02:31:05.837: INFO: Created: latency-svc-czhzc
Jul  8 02:31:05.844: INFO: Created: latency-svc-q27h7
Jul  8 02:31:05.869: INFO: Got endpoints: latency-svc-2vzbs [747.500902ms]
Jul  8 02:31:05.877: INFO: Created: latency-svc-rwjwv
Jul  8 02:31:05.919: INFO: Got endpoints: latency-svc-dpj9r [751.871406ms]
Jul  8 02:31:05.929: INFO: Created: latency-svc-llt5f
Jul  8 02:31:05.968: INFO: Got endpoints: latency-svc-g6xdw [749.440041ms]
Jul  8 02:31:05.980: INFO: Created: latency-svc-9jx8r
Jul  8 02:31:06.017: INFO: Got endpoints: latency-svc-4n7rp [751.908719ms]
Jul  8 02:31:06.029: INFO: Created: latency-svc-bxrp8
Jul  8 02:31:06.066: INFO: Got endpoints: latency-svc-5nqpp [749.515995ms]
Jul  8 02:31:06.077: INFO: Created: latency-svc-26gj6
Jul  8 02:31:06.117: INFO: Got endpoints: latency-svc-7pvdt [751.619902ms]
Jul  8 02:31:06.128: INFO: Created: latency-svc-2jt55
Jul  8 02:31:06.168: INFO: Got endpoints: latency-svc-hmdl6 [736.781529ms]
Jul  8 02:31:06.184: INFO: Created: latency-svc-tl5pv
Jul  8 02:31:06.219: INFO: Got endpoints: latency-svc-4v4gz [749.207465ms]
Jul  8 02:31:06.229: INFO: Created: latency-svc-2d272
Jul  8 02:31:06.266: INFO: Got endpoints: latency-svc-rqg9k [736.075244ms]
Jul  8 02:31:06.282: INFO: Created: latency-svc-bs544
Jul  8 02:31:06.318: INFO: Got endpoints: latency-svc-4sm7x [748.356949ms]
Jul  8 02:31:06.328: INFO: Created: latency-svc-4rssb
Jul  8 02:31:06.368: INFO: Got endpoints: latency-svc-q2n2w [751.513478ms]
Jul  8 02:31:06.380: INFO: Created: latency-svc-7fntn
Jul  8 02:31:06.419: INFO: Got endpoints: latency-svc-xpmz4 [661.116737ms]
Jul  8 02:31:06.432: INFO: Created: latency-svc-s5pvw
Jul  8 02:31:06.470: INFO: Got endpoints: latency-svc-cww6z [712.733187ms]
Jul  8 02:31:06.479: INFO: Created: latency-svc-7sncs
Jul  8 02:31:06.517: INFO: Got endpoints: latency-svc-czhzc [722.952674ms]
Jul  8 02:31:06.530: INFO: Created: latency-svc-ll45v
Jul  8 02:31:06.567: INFO: Got endpoints: latency-svc-q27h7 [747.848775ms]
Jul  8 02:31:06.575: INFO: Created: latency-svc-s89br
Jul  8 02:31:06.618: INFO: Got endpoints: latency-svc-rwjwv [749.503785ms]
Jul  8 02:31:06.630: INFO: Created: latency-svc-dvmz6
Jul  8 02:31:06.672: INFO: Got endpoints: latency-svc-llt5f [752.401067ms]
Jul  8 02:31:06.686: INFO: Created: latency-svc-9q22k
Jul  8 02:31:06.724: INFO: Got endpoints: latency-svc-9jx8r [755.558986ms]
Jul  8 02:31:06.741: INFO: Created: latency-svc-wz7px
Jul  8 02:31:06.768: INFO: Got endpoints: latency-svc-bxrp8 [750.365845ms]
Jul  8 02:31:06.780: INFO: Created: latency-svc-5ntmc
Jul  8 02:31:06.820: INFO: Got endpoints: latency-svc-26gj6 [753.966519ms]
Jul  8 02:31:06.835: INFO: Created: latency-svc-ms96c
Jul  8 02:31:06.866: INFO: Got endpoints: latency-svc-2jt55 [748.565676ms]
Jul  8 02:31:06.898: INFO: Created: latency-svc-ts7ss
Jul  8 02:31:06.924: INFO: Got endpoints: latency-svc-tl5pv [755.112124ms]
Jul  8 02:31:06.939: INFO: Created: latency-svc-r9l2k
Jul  8 02:31:06.971: INFO: Got endpoints: latency-svc-2d272 [751.34123ms]
Jul  8 02:31:06.985: INFO: Created: latency-svc-vbdgz
Jul  8 02:31:07.019: INFO: Got endpoints: latency-svc-bs544 [753.120748ms]
Jul  8 02:31:07.030: INFO: Created: latency-svc-nvctz
Jul  8 02:31:07.069: INFO: Got endpoints: latency-svc-4rssb [751.055658ms]
Jul  8 02:31:07.094: INFO: Created: latency-svc-j7zf8
Jul  8 02:31:07.118: INFO: Got endpoints: latency-svc-7fntn [749.806058ms]
Jul  8 02:31:07.130: INFO: Created: latency-svc-xvjl6
Jul  8 02:31:07.165: INFO: Got endpoints: latency-svc-s5pvw [746.120297ms]
Jul  8 02:31:07.179: INFO: Created: latency-svc-h488k
Jul  8 02:31:07.215: INFO: Got endpoints: latency-svc-7sncs [745.217375ms]
Jul  8 02:31:07.228: INFO: Created: latency-svc-h5pvs
Jul  8 02:31:07.275: INFO: Got endpoints: latency-svc-ll45v [757.893474ms]
Jul  8 02:31:07.291: INFO: Created: latency-svc-tvt89
Jul  8 02:31:07.316: INFO: Got endpoints: latency-svc-s89br [749.421301ms]
Jul  8 02:31:07.328: INFO: Created: latency-svc-b4jm8
Jul  8 02:31:07.371: INFO: Got endpoints: latency-svc-dvmz6 [752.17849ms]
Jul  8 02:31:07.381: INFO: Created: latency-svc-b8qqq
Jul  8 02:31:07.416: INFO: Got endpoints: latency-svc-9q22k [744.282674ms]
Jul  8 02:31:07.435: INFO: Created: latency-svc-z9nz6
Jul  8 02:31:07.470: INFO: Got endpoints: latency-svc-wz7px [746.328643ms]
Jul  8 02:31:07.482: INFO: Created: latency-svc-sr6gl
Jul  8 02:31:07.517: INFO: Got endpoints: latency-svc-5ntmc [749.251447ms]
Jul  8 02:31:07.527: INFO: Created: latency-svc-lwb92
Jul  8 02:31:07.569: INFO: Got endpoints: latency-svc-ms96c [748.639955ms]
Jul  8 02:31:07.582: INFO: Created: latency-svc-pksnj
Jul  8 02:31:07.616: INFO: Got endpoints: latency-svc-ts7ss [750.249695ms]
Jul  8 02:31:07.634: INFO: Created: latency-svc-mfhfq
Jul  8 02:31:07.666: INFO: Got endpoints: latency-svc-r9l2k [742.291893ms]
Jul  8 02:31:07.678: INFO: Created: latency-svc-rp2hq
Jul  8 02:31:07.723: INFO: Got endpoints: latency-svc-vbdgz [751.798019ms]
Jul  8 02:31:07.733: INFO: Created: latency-svc-bq6nr
Jul  8 02:31:07.779: INFO: Got endpoints: latency-svc-nvctz [759.462161ms]
Jul  8 02:31:07.799: INFO: Created: latency-svc-8ntzw
Jul  8 02:31:07.823: INFO: Got endpoints: latency-svc-j7zf8 [754.147401ms]
Jul  8 02:31:07.837: INFO: Created: latency-svc-59x7l
Jul  8 02:31:07.871: INFO: Got endpoints: latency-svc-xvjl6 [751.997576ms]
Jul  8 02:31:07.892: INFO: Created: latency-svc-k7lkl
Jul  8 02:31:07.918: INFO: Got endpoints: latency-svc-h488k [752.691588ms]
Jul  8 02:31:07.940: INFO: Created: latency-svc-89jbw
Jul  8 02:31:07.972: INFO: Got endpoints: latency-svc-h5pvs [756.470795ms]
Jul  8 02:31:07.985: INFO: Created: latency-svc-jsdvc
Jul  8 02:31:08.019: INFO: Got endpoints: latency-svc-tvt89 [743.634759ms]
Jul  8 02:31:08.032: INFO: Created: latency-svc-mh6ss
Jul  8 02:31:08.071: INFO: Got endpoints: latency-svc-b4jm8 [754.470903ms]
Jul  8 02:31:08.103: INFO: Created: latency-svc-k5fvf
Jul  8 02:31:08.118: INFO: Got endpoints: latency-svc-b8qqq [747.814307ms]
Jul  8 02:31:08.134: INFO: Created: latency-svc-m9km6
Jul  8 02:31:08.173: INFO: Got endpoints: latency-svc-z9nz6 [756.57621ms]
Jul  8 02:31:08.185: INFO: Created: latency-svc-7qqlj
Jul  8 02:31:08.221: INFO: Got endpoints: latency-svc-sr6gl [750.326247ms]
Jul  8 02:31:08.235: INFO: Created: latency-svc-tzfg5
Jul  8 02:31:08.271: INFO: Got endpoints: latency-svc-lwb92 [754.317488ms]
Jul  8 02:31:08.286: INFO: Created: latency-svc-vrqnh
Jul  8 02:31:08.316: INFO: Got endpoints: latency-svc-pksnj [747.228823ms]
Jul  8 02:31:08.337: INFO: Created: latency-svc-2q24t
Jul  8 02:31:08.368: INFO: Got endpoints: latency-svc-mfhfq [751.163515ms]
Jul  8 02:31:08.382: INFO: Created: latency-svc-rhz9r
Jul  8 02:31:08.415: INFO: Got endpoints: latency-svc-rp2hq [749.141357ms]
Jul  8 02:31:08.438: INFO: Created: latency-svc-r75zm
Jul  8 02:31:08.467: INFO: Got endpoints: latency-svc-bq6nr [744.314583ms]
Jul  8 02:31:08.488: INFO: Created: latency-svc-26sj4
Jul  8 02:31:08.517: INFO: Got endpoints: latency-svc-8ntzw [738.098066ms]
Jul  8 02:31:08.528: INFO: Created: latency-svc-qzk72
Jul  8 02:31:08.574: INFO: Got endpoints: latency-svc-59x7l [750.910184ms]
Jul  8 02:31:08.584: INFO: Created: latency-svc-k8n97
Jul  8 02:31:08.618: INFO: Got endpoints: latency-svc-k7lkl [747.684322ms]
Jul  8 02:31:08.637: INFO: Created: latency-svc-6q4jp
Jul  8 02:31:08.668: INFO: Got endpoints: latency-svc-89jbw [749.420188ms]
Jul  8 02:31:08.699: INFO: Created: latency-svc-8rm9v
Jul  8 02:31:08.716: INFO: Got endpoints: latency-svc-jsdvc [743.727351ms]
Jul  8 02:31:08.725: INFO: Created: latency-svc-97j9h
Jul  8 02:31:08.769: INFO: Got endpoints: latency-svc-mh6ss [750.543695ms]
Jul  8 02:31:08.787: INFO: Created: latency-svc-hsbz8
Jul  8 02:31:08.817: INFO: Got endpoints: latency-svc-k5fvf [746.377114ms]
Jul  8 02:31:08.829: INFO: Created: latency-svc-t7zvd
Jul  8 02:31:08.866: INFO: Got endpoints: latency-svc-m9km6 [747.795858ms]
Jul  8 02:31:08.892: INFO: Created: latency-svc-pkjbj
Jul  8 02:31:08.920: INFO: Got endpoints: latency-svc-7qqlj [746.81557ms]
Jul  8 02:31:08.930: INFO: Created: latency-svc-fnb9r
Jul  8 02:31:08.966: INFO: Got endpoints: latency-svc-tzfg5 [745.29013ms]
Jul  8 02:31:08.977: INFO: Created: latency-svc-zzqlc
Jul  8 02:31:09.018: INFO: Got endpoints: latency-svc-vrqnh [746.355234ms]
Jul  8 02:31:09.030: INFO: Created: latency-svc-dhgkp
Jul  8 02:31:09.066: INFO: Got endpoints: latency-svc-2q24t [749.776128ms]
Jul  8 02:31:09.079: INFO: Created: latency-svc-dnq88
Jul  8 02:31:09.124: INFO: Got endpoints: latency-svc-rhz9r [756.029525ms]
Jul  8 02:31:09.134: INFO: Created: latency-svc-htsvb
Jul  8 02:31:09.187: INFO: Got endpoints: latency-svc-r75zm [771.172655ms]
Jul  8 02:31:09.223: INFO: Created: latency-svc-n2wp9
Jul  8 02:31:09.226: INFO: Got endpoints: latency-svc-26sj4 [758.703573ms]
Jul  8 02:31:09.235: INFO: Created: latency-svc-pck6d
Jul  8 02:31:09.265: INFO: Got endpoints: latency-svc-qzk72 [748.444419ms]
Jul  8 02:31:09.277: INFO: Created: latency-svc-8zprs
Jul  8 02:31:09.316: INFO: Got endpoints: latency-svc-k8n97 [741.994407ms]
Jul  8 02:31:09.327: INFO: Created: latency-svc-mcd8k
Jul  8 02:31:09.369: INFO: Got endpoints: latency-svc-6q4jp [750.555968ms]
Jul  8 02:31:09.383: INFO: Created: latency-svc-l46vd
Jul  8 02:31:09.416: INFO: Got endpoints: latency-svc-8rm9v [748.317023ms]
Jul  8 02:31:09.467: INFO: Created: latency-svc-w546t
Jul  8 02:31:09.475: INFO: Got endpoints: latency-svc-97j9h [759.421732ms]
Jul  8 02:31:09.495: INFO: Created: latency-svc-v9jc9
Jul  8 02:31:09.524: INFO: Got endpoints: latency-svc-hsbz8 [754.482863ms]
Jul  8 02:31:09.557: INFO: Created: latency-svc-qwvxl
Jul  8 02:31:09.566: INFO: Got endpoints: latency-svc-t7zvd [749.3401ms]
Jul  8 02:31:09.578: INFO: Created: latency-svc-cvp9w
Jul  8 02:31:09.617: INFO: Got endpoints: latency-svc-pkjbj [750.90979ms]
Jul  8 02:31:09.627: INFO: Created: latency-svc-vkfv8
Jul  8 02:31:09.666: INFO: Got endpoints: latency-svc-fnb9r [746.449076ms]
Jul  8 02:31:09.678: INFO: Created: latency-svc-z9pz7
Jul  8 02:31:09.721: INFO: Got endpoints: latency-svc-zzqlc [754.684125ms]
Jul  8 02:31:09.730: INFO: Created: latency-svc-w9qd5
Jul  8 02:31:09.767: INFO: Got endpoints: latency-svc-dhgkp [749.239021ms]
Jul  8 02:31:09.782: INFO: Created: latency-svc-mjcjd
Jul  8 02:31:09.817: INFO: Got endpoints: latency-svc-dnq88 [751.350302ms]
Jul  8 02:31:09.834: INFO: Created: latency-svc-4rjxc
Jul  8 02:31:09.868: INFO: Got endpoints: latency-svc-htsvb [744.379343ms]
Jul  8 02:31:09.878: INFO: Created: latency-svc-5299l
Jul  8 02:31:09.918: INFO: Got endpoints: latency-svc-n2wp9 [731.538395ms]
Jul  8 02:31:09.928: INFO: Created: latency-svc-2sn92
Jul  8 02:31:09.967: INFO: Got endpoints: latency-svc-pck6d [740.617551ms]
Jul  8 02:31:09.980: INFO: Created: latency-svc-jxc87
Jul  8 02:31:10.018: INFO: Got endpoints: latency-svc-8zprs [752.742179ms]
Jul  8 02:31:10.029: INFO: Created: latency-svc-9qwhv
Jul  8 02:31:10.067: INFO: Got endpoints: latency-svc-mcd8k [750.644849ms]
Jul  8 02:31:10.084: INFO: Created: latency-svc-g9zrm
Jul  8 02:31:10.119: INFO: Got endpoints: latency-svc-l46vd [750.533785ms]
Jul  8 02:31:10.133: INFO: Created: latency-svc-z2f9l
Jul  8 02:31:10.167: INFO: Got endpoints: latency-svc-w546t [751.147202ms]
Jul  8 02:31:10.177: INFO: Created: latency-svc-6vcdm
Jul  8 02:31:10.218: INFO: Got endpoints: latency-svc-v9jc9 [740.065265ms]
Jul  8 02:31:10.230: INFO: Created: latency-svc-zztpz
Jul  8 02:31:10.267: INFO: Got endpoints: latency-svc-qwvxl [742.952659ms]
Jul  8 02:31:10.282: INFO: Created: latency-svc-j9jn8
Jul  8 02:31:10.322: INFO: Got endpoints: latency-svc-cvp9w [755.79235ms]
Jul  8 02:31:10.341: INFO: Created: latency-svc-wm6j5
Jul  8 02:31:10.367: INFO: Got endpoints: latency-svc-vkfv8 [750.140789ms]
Jul  8 02:31:10.378: INFO: Created: latency-svc-qlhlz
Jul  8 02:31:10.420: INFO: Got endpoints: latency-svc-z9pz7 [753.171059ms]
Jul  8 02:31:10.436: INFO: Created: latency-svc-6w7jk
Jul  8 02:31:10.473: INFO: Got endpoints: latency-svc-w9qd5 [752.914965ms]
Jul  8 02:31:10.495: INFO: Created: latency-svc-4fx76
Jul  8 02:31:10.517: INFO: Got endpoints: latency-svc-mjcjd [749.354562ms]
Jul  8 02:31:10.528: INFO: Created: latency-svc-9g4rw
Jul  8 02:31:10.567: INFO: Got endpoints: latency-svc-4rjxc [749.044148ms]
Jul  8 02:31:10.578: INFO: Created: latency-svc-mqr6z
Jul  8 02:31:10.619: INFO: Got endpoints: latency-svc-5299l [751.10522ms]
Jul  8 02:31:10.637: INFO: Created: latency-svc-pfnmv
Jul  8 02:31:10.668: INFO: Got endpoints: latency-svc-2sn92 [749.819006ms]
Jul  8 02:31:10.689: INFO: Created: latency-svc-bk4nk
Jul  8 02:31:10.717: INFO: Got endpoints: latency-svc-jxc87 [750.260481ms]
Jul  8 02:31:10.728: INFO: Created: latency-svc-dmbhh
Jul  8 02:31:10.766: INFO: Got endpoints: latency-svc-9qwhv [747.392459ms]
Jul  8 02:31:10.784: INFO: Created: latency-svc-zcww2
Jul  8 02:31:10.820: INFO: Got endpoints: latency-svc-g9zrm [752.84435ms]
Jul  8 02:31:10.837: INFO: Created: latency-svc-pflbr
Jul  8 02:31:10.870: INFO: Got endpoints: latency-svc-z2f9l [750.959893ms]
Jul  8 02:31:10.880: INFO: Created: latency-svc-5rs86
Jul  8 02:31:10.918: INFO: Got endpoints: latency-svc-6vcdm [750.120649ms]
Jul  8 02:31:10.938: INFO: Created: latency-svc-lj6fp
Jul  8 02:31:10.968: INFO: Got endpoints: latency-svc-zztpz [750.461503ms]
Jul  8 02:31:10.978: INFO: Created: latency-svc-7djkx
Jul  8 02:31:11.021: INFO: Got endpoints: latency-svc-j9jn8 [753.974746ms]
Jul  8 02:31:11.031: INFO: Created: latency-svc-zktqp
Jul  8 02:31:11.068: INFO: Got endpoints: latency-svc-wm6j5 [746.032846ms]
Jul  8 02:31:11.079: INFO: Created: latency-svc-zz2jn
Jul  8 02:31:11.117: INFO: Got endpoints: latency-svc-qlhlz [749.394571ms]
Jul  8 02:31:11.130: INFO: Created: latency-svc-qvq2f
Jul  8 02:31:11.166: INFO: Got endpoints: latency-svc-6w7jk [746.257512ms]
Jul  8 02:31:11.181: INFO: Created: latency-svc-vwm9k
Jul  8 02:31:11.216: INFO: Got endpoints: latency-svc-4fx76 [742.962552ms]
Jul  8 02:31:11.227: INFO: Created: latency-svc-hbrrf
Jul  8 02:31:11.269: INFO: Got endpoints: latency-svc-9g4rw [751.693254ms]
Jul  8 02:31:11.278: INFO: Created: latency-svc-qlnqm
Jul  8 02:31:11.319: INFO: Got endpoints: latency-svc-mqr6z [751.979733ms]
Jul  8 02:31:11.329: INFO: Created: latency-svc-gssv6
Jul  8 02:31:11.368: INFO: Got endpoints: latency-svc-pfnmv [748.678014ms]
Jul  8 02:31:11.378: INFO: Created: latency-svc-tqqmz
Jul  8 02:31:11.420: INFO: Got endpoints: latency-svc-bk4nk [752.065781ms]
Jul  8 02:31:11.436: INFO: Created: latency-svc-nlmmz
Jul  8 02:31:11.468: INFO: Got endpoints: latency-svc-dmbhh [751.199979ms]
Jul  8 02:31:11.491: INFO: Created: latency-svc-jcx68
Jul  8 02:31:11.517: INFO: Got endpoints: latency-svc-zcww2 [750.677216ms]
Jul  8 02:31:11.531: INFO: Created: latency-svc-dqg4x
Jul  8 02:31:11.568: INFO: Got endpoints: latency-svc-pflbr [748.139103ms]
Jul  8 02:31:11.581: INFO: Created: latency-svc-sbhsc
Jul  8 02:31:11.618: INFO: Got endpoints: latency-svc-5rs86 [747.301913ms]
Jul  8 02:31:11.632: INFO: Created: latency-svc-84sxw
Jul  8 02:31:11.665: INFO: Got endpoints: latency-svc-lj6fp [747.166181ms]
Jul  8 02:31:11.676: INFO: Created: latency-svc-mh2jr
Jul  8 02:31:11.717: INFO: Got endpoints: latency-svc-7djkx [748.694925ms]
Jul  8 02:31:11.732: INFO: Created: latency-svc-dsh9f
Jul  8 02:31:11.772: INFO: Got endpoints: latency-svc-zktqp [751.41316ms]
Jul  8 02:31:11.790: INFO: Created: latency-svc-9fvts
Jul  8 02:31:11.817: INFO: Got endpoints: latency-svc-zz2jn [748.832585ms]
Jul  8 02:31:11.834: INFO: Created: latency-svc-rzpgw
Jul  8 02:31:11.867: INFO: Got endpoints: latency-svc-qvq2f [750.538897ms]
Jul  8 02:31:11.881: INFO: Created: latency-svc-vkcsq
Jul  8 02:31:11.917: INFO: Got endpoints: latency-svc-vwm9k [750.958779ms]
Jul  8 02:31:11.929: INFO: Created: latency-svc-kgk46
Jul  8 02:31:11.970: INFO: Got endpoints: latency-svc-hbrrf [753.425397ms]
Jul  8 02:31:11.980: INFO: Created: latency-svc-8w5wq
Jul  8 02:31:12.018: INFO: Got endpoints: latency-svc-qlnqm [749.449871ms]
Jul  8 02:31:12.032: INFO: Created: latency-svc-5vtq7
Jul  8 02:31:12.067: INFO: Got endpoints: latency-svc-gssv6 [748.210288ms]
Jul  8 02:31:12.079: INFO: Created: latency-svc-4478n
Jul  8 02:31:12.121: INFO: Got endpoints: latency-svc-tqqmz [752.73468ms]
Jul  8 02:31:12.133: INFO: Created: latency-svc-n2n9x
Jul  8 02:31:12.168: INFO: Got endpoints: latency-svc-nlmmz [747.587638ms]
Jul  8 02:31:12.181: INFO: Created: latency-svc-bwcz9
Jul  8 02:31:12.219: INFO: Got endpoints: latency-svc-jcx68 [750.198092ms]
Jul  8 02:31:12.230: INFO: Created: latency-svc-82w8m
Jul  8 02:31:12.266: INFO: Got endpoints: latency-svc-dqg4x [749.243575ms]
Jul  8 02:31:12.318: INFO: Got endpoints: latency-svc-sbhsc [750.222556ms]
Jul  8 02:31:12.369: INFO: Got endpoints: latency-svc-84sxw [751.128471ms]
Jul  8 02:31:12.418: INFO: Got endpoints: latency-svc-mh2jr [753.069809ms]
Jul  8 02:31:12.468: INFO: Got endpoints: latency-svc-dsh9f [751.486067ms]
Jul  8 02:31:12.517: INFO: Got endpoints: latency-svc-9fvts [744.197116ms]
Jul  8 02:31:12.567: INFO: Got endpoints: latency-svc-rzpgw [749.968159ms]
Jul  8 02:31:12.619: INFO: Got endpoints: latency-svc-vkcsq [751.886692ms]
Jul  8 02:31:12.668: INFO: Got endpoints: latency-svc-kgk46 [750.872873ms]
Jul  8 02:31:12.715: INFO: Got endpoints: latency-svc-8w5wq [744.672966ms]
Jul  8 02:31:12.768: INFO: Got endpoints: latency-svc-5vtq7 [749.458382ms]
Jul  8 02:31:12.818: INFO: Got endpoints: latency-svc-4478n [750.171341ms]
Jul  8 02:31:12.876: INFO: Got endpoints: latency-svc-n2n9x [754.852854ms]
Jul  8 02:31:12.918: INFO: Got endpoints: latency-svc-bwcz9 [750.423042ms]
Jul  8 02:31:12.968: INFO: Got endpoints: latency-svc-82w8m [748.77804ms]
Jul  8 02:31:12.968: INFO: Latencies: [31.021824ms 41.687627ms 45.929729ms 84.946962ms 102.866355ms 116.216294ms 129.618629ms 148.081941ms 151.116989ms 173.681577ms 174.899819ms 179.494365ms 184.144208ms 184.734289ms 185.193195ms 187.998611ms 188.511973ms 189.558743ms 190.083575ms 197.953799ms 200.713818ms 202.894715ms 205.630647ms 209.033723ms 216.169397ms 219.125795ms 221.17483ms 221.239515ms 222.351119ms 222.616133ms 223.583696ms 223.65558ms 225.331487ms 225.442505ms 225.443048ms 225.460285ms 231.015777ms 232.833918ms 233.320352ms 237.766222ms 238.9512ms 239.64915ms 269.159785ms 305.630463ms 344.806645ms 377.919417ms 420.162415ms 452.437216ms 504.841636ms 523.915571ms 571.261912ms 597.26328ms 624.028848ms 661.116737ms 712.733187ms 722.952674ms 731.538395ms 736.075244ms 736.781529ms 738.098066ms 740.065265ms 740.617551ms 741.994407ms 742.291893ms 742.952659ms 742.962552ms 742.991047ms 743.634759ms 743.727351ms 744.197116ms 744.282674ms 744.314583ms 744.379343ms 744.672966ms 745.217375ms 745.29013ms 746.032846ms 746.120297ms 746.257512ms 746.328643ms 746.355234ms 746.377114ms 746.449076ms 746.81557ms 747.166181ms 747.228823ms 747.301913ms 747.392459ms 747.500902ms 747.587638ms 747.684322ms 747.795858ms 747.814307ms 747.848775ms 748.052425ms 748.139103ms 748.210288ms 748.317023ms 748.356949ms 748.444419ms 748.565676ms 748.639955ms 748.678014ms 748.694925ms 748.77804ms 748.832585ms 749.044148ms 749.141357ms 749.207465ms 749.239021ms 749.243575ms 749.251447ms 749.3401ms 749.354562ms 749.394571ms 749.420188ms 749.421301ms 749.440041ms 749.449871ms 749.458382ms 749.503785ms 749.515995ms 749.776128ms 749.806058ms 749.819006ms 749.968159ms 750.120649ms 750.140789ms 750.171341ms 750.198092ms 750.222556ms 750.249695ms 750.260481ms 750.326247ms 750.365845ms 750.423042ms 750.461503ms 750.533785ms 750.538897ms 750.543695ms 750.555968ms 750.644849ms 750.677216ms 750.872873ms 750.90979ms 750.910184ms 750.958779ms 750.959893ms 751.055658ms 751.10522ms 751.128471ms 751.147202ms 751.163515ms 751.199979ms 751.34123ms 751.350302ms 751.41316ms 751.486067ms 751.513478ms 751.619902ms 751.693254ms 751.798019ms 751.871406ms 751.886692ms 751.908719ms 751.979733ms 751.997576ms 752.065781ms 752.17849ms 752.401067ms 752.691588ms 752.73468ms 752.742179ms 752.84435ms 752.914965ms 753.069809ms 753.120748ms 753.171059ms 753.425397ms 753.966519ms 753.974746ms 754.147401ms 754.317488ms 754.470903ms 754.482863ms 754.684125ms 754.852854ms 755.112124ms 755.558986ms 755.79235ms 756.029525ms 756.470795ms 756.57621ms 757.893474ms 758.703573ms 759.421732ms 759.462161ms 762.178842ms 770.965152ms 771.172655ms]
Jul  8 02:31:12.968: INFO: 50 %ile: 748.565676ms
Jul  8 02:31:12.968: INFO: 90 %ile: 753.974746ms
Jul  8 02:31:12.968: INFO: 99 %ile: 770.965152ms
Jul  8 02:31:12.968: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:31:12.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5410" for this suite.

• [SLOW TEST:9.909 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":339,"completed":111,"skipped":1673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:31:12.988: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1884
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 02:31:13.142: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b23bc9b7-0f85-40b0-a130-6e152ef77aea" in namespace "projected-1884" to be "Succeeded or Failed"
Jul  8 02:31:13.149: INFO: Pod "downwardapi-volume-b23bc9b7-0f85-40b0-a130-6e152ef77aea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011825ms
Jul  8 02:31:15.155: INFO: Pod "downwardapi-volume-b23bc9b7-0f85-40b0-a130-6e152ef77aea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012075625s
STEP: Saw pod success
Jul  8 02:31:15.155: INFO: Pod "downwardapi-volume-b23bc9b7-0f85-40b0-a130-6e152ef77aea" satisfied condition "Succeeded or Failed"
Jul  8 02:31:15.158: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-b23bc9b7-0f85-40b0-a130-6e152ef77aea container client-container: <nil>
STEP: delete the pod
Jul  8 02:31:15.181: INFO: Waiting for pod downwardapi-volume-b23bc9b7-0f85-40b0-a130-6e152ef77aea to disappear
Jul  8 02:31:15.187: INFO: Pod downwardapi-volume-b23bc9b7-0f85-40b0-a130-6e152ef77aea no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:31:15.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1884" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":112,"skipped":1695,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:31:15.198: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8605
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-1d6d975e-2073-4aac-8486-ebfc3d01eb5d
STEP: Creating a pod to test consume configMaps
Jul  8 02:31:15.369: INFO: Waiting up to 5m0s for pod "pod-configmaps-f9053dd0-b625-4303-8aa7-8eae625e5e81" in namespace "configmap-8605" to be "Succeeded or Failed"
Jul  8 02:31:15.373: INFO: Pod "pod-configmaps-f9053dd0-b625-4303-8aa7-8eae625e5e81": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073846ms
Jul  8 02:31:17.379: INFO: Pod "pod-configmaps-f9053dd0-b625-4303-8aa7-8eae625e5e81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010633695s
STEP: Saw pod success
Jul  8 02:31:17.380: INFO: Pod "pod-configmaps-f9053dd0-b625-4303-8aa7-8eae625e5e81" satisfied condition "Succeeded or Failed"
Jul  8 02:31:17.382: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-configmaps-f9053dd0-b625-4303-8aa7-8eae625e5e81 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  8 02:31:17.401: INFO: Waiting for pod pod-configmaps-f9053dd0-b625-4303-8aa7-8eae625e5e81 to disappear
Jul  8 02:31:17.404: INFO: Pod pod-configmaps-f9053dd0-b625-4303-8aa7-8eae625e5e81 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:31:17.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8605" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":113,"skipped":1700,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:31:17.413: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul  8 02:31:17.567: INFO: Waiting up to 5m0s for pod "pod-c1ae5d88-c8ca-4de9-8f9d-98a1ff493049" in namespace "emptydir-8493" to be "Succeeded or Failed"
Jul  8 02:31:17.572: INFO: Pod "pod-c1ae5d88-c8ca-4de9-8f9d-98a1ff493049": Phase="Pending", Reason="", readiness=false. Elapsed: 5.568261ms
Jul  8 02:31:19.576: INFO: Pod "pod-c1ae5d88-c8ca-4de9-8f9d-98a1ff493049": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009658324s
STEP: Saw pod success
Jul  8 02:31:19.576: INFO: Pod "pod-c1ae5d88-c8ca-4de9-8f9d-98a1ff493049" satisfied condition "Succeeded or Failed"
Jul  8 02:31:19.579: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-c1ae5d88-c8ca-4de9-8f9d-98a1ff493049 container test-container: <nil>
STEP: delete the pod
Jul  8 02:31:19.606: INFO: Waiting for pod pod-c1ae5d88-c8ca-4de9-8f9d-98a1ff493049 to disappear
Jul  8 02:31:19.609: INFO: Pod pod-c1ae5d88-c8ca-4de9-8f9d-98a1ff493049 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:31:19.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8493" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":114,"skipped":1703,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:31:19.622: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5613
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:31:19.813: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul  8 02:31:19.827: INFO: Number of nodes with available pods: 0
Jul  8 02:31:19.827: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul  8 02:31:19.862: INFO: Number of nodes with available pods: 0
Jul  8 02:31:19.862: INFO: Node ip-172-31-6-226.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:31:20.868: INFO: Number of nodes with available pods: 0
Jul  8 02:31:20.868: INFO: Node ip-172-31-6-226.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:31:21.876: INFO: Number of nodes with available pods: 1
Jul  8 02:31:21.876: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul  8 02:31:21.901: INFO: Number of nodes with available pods: 1
Jul  8 02:31:21.901: INFO: Number of running nodes: 0, number of available pods: 1
Jul  8 02:31:22.908: INFO: Number of nodes with available pods: 0
Jul  8 02:31:22.908: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul  8 02:31:22.927: INFO: Number of nodes with available pods: 0
Jul  8 02:31:22.927: INFO: Node ip-172-31-6-226.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:31:23.952: INFO: Number of nodes with available pods: 0
Jul  8 02:31:23.953: INFO: Node ip-172-31-6-226.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:31:24.932: INFO: Number of nodes with available pods: 0
Jul  8 02:31:24.932: INFO: Node ip-172-31-6-226.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:31:25.933: INFO: Number of nodes with available pods: 0
Jul  8 02:31:25.933: INFO: Node ip-172-31-6-226.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:31:26.932: INFO: Number of nodes with available pods: 0
Jul  8 02:31:26.932: INFO: Node ip-172-31-6-226.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:31:27.932: INFO: Number of nodes with available pods: 0
Jul  8 02:31:27.932: INFO: Node ip-172-31-6-226.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:31:28.932: INFO: Number of nodes with available pods: 0
Jul  8 02:31:28.932: INFO: Node ip-172-31-6-226.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:31:29.932: INFO: Number of nodes with available pods: 1
Jul  8 02:31:29.932: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5613, will wait for the garbage collector to delete the pods
Jul  8 02:31:29.996: INFO: Deleting DaemonSet.extensions daemon-set took: 7.247964ms
Jul  8 02:31:30.097: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.724337ms
Jul  8 02:31:37.913: INFO: Number of nodes with available pods: 0
Jul  8 02:31:37.913: INFO: Number of running nodes: 0, number of available pods: 0
Jul  8 02:31:37.916: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31427"},"items":null}

Jul  8 02:31:37.918: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31427"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:31:37.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5613" for this suite.

• [SLOW TEST:18.339 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":339,"completed":115,"skipped":1705,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:31:37.961: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 02:31:38.114: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac018d0c-6013-44e9-b3aa-0f767fdbdca0" in namespace "downward-api-717" to be "Succeeded or Failed"
Jul  8 02:31:38.122: INFO: Pod "downwardapi-volume-ac018d0c-6013-44e9-b3aa-0f767fdbdca0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.633841ms
Jul  8 02:31:40.127: INFO: Pod "downwardapi-volume-ac018d0c-6013-44e9-b3aa-0f767fdbdca0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012774258s
STEP: Saw pod success
Jul  8 02:31:40.127: INFO: Pod "downwardapi-volume-ac018d0c-6013-44e9-b3aa-0f767fdbdca0" satisfied condition "Succeeded or Failed"
Jul  8 02:31:40.130: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-ac018d0c-6013-44e9-b3aa-0f767fdbdca0 container client-container: <nil>
STEP: delete the pod
Jul  8 02:31:40.164: INFO: Waiting for pod downwardapi-volume-ac018d0c-6013-44e9-b3aa-0f767fdbdca0 to disappear
Jul  8 02:31:40.167: INFO: Pod downwardapi-volume-ac018d0c-6013-44e9-b3aa-0f767fdbdca0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:31:40.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-717" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":116,"skipped":1725,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:31:40.178: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8956
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:31:40.336: INFO: Waiting up to 5m0s for pod "busybox-user-65534-0a8e62e6-805d-42f4-bee6-399ae14bbdba" in namespace "security-context-test-8956" to be "Succeeded or Failed"
Jul  8 02:31:40.348: INFO: Pod "busybox-user-65534-0a8e62e6-805d-42f4-bee6-399ae14bbdba": Phase="Pending", Reason="", readiness=false. Elapsed: 11.293826ms
Jul  8 02:31:42.357: INFO: Pod "busybox-user-65534-0a8e62e6-805d-42f4-bee6-399ae14bbdba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020366443s
Jul  8 02:31:42.357: INFO: Pod "busybox-user-65534-0a8e62e6-805d-42f4-bee6-399ae14bbdba" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:31:42.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8956" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":117,"skipped":1746,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:31:42.367: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5496
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:31:42.516: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul  8 02:31:47.527: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  8 02:31:47.527: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul  8 02:31:49.534: INFO: Creating deployment "test-rollover-deployment"
Jul  8 02:31:49.550: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul  8 02:31:51.560: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul  8 02:31:51.567: INFO: Ensure that both replica sets have 1 created replica
Jul  8 02:31:51.572: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul  8 02:31:51.580: INFO: Updating deployment test-rollover-deployment
Jul  8 02:31:51.580: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul  8 02:31:53.596: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul  8 02:31:53.603: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul  8 02:31:53.613: INFO: all replica sets need to contain the pod-template-hash label
Jul  8 02:31:53.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308313, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 02:31:55.627: INFO: all replica sets need to contain the pod-template-hash label
Jul  8 02:31:55.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308313, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 02:31:57.624: INFO: all replica sets need to contain the pod-template-hash label
Jul  8 02:31:57.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308313, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 02:31:59.648: INFO: all replica sets need to contain the pod-template-hash label
Jul  8 02:31:59.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308313, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 02:32:01.635: INFO: all replica sets need to contain the pod-template-hash label
Jul  8 02:32:01.636: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308313, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308309, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 02:32:03.622: INFO: 
Jul  8 02:32:03.622: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul  8 02:32:03.629: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5496  aebbfa6b-0afd-4cd0-b461-3f40c2530809 31651 2 2021-07-08 02:31:49 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-08 02:31:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-08 02:32:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006132298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-08 02:31:49 +0000 UTC,LastTransitionTime:2021-07-08 02:31:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-07-08 02:32:03 +0000 UTC,LastTransitionTime:2021-07-08 02:31:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul  8 02:32:03.632: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-5496  71c3c49c-39fa-46a5-9e5c-8dc4c2028cea 31640 2 2021-07-08 02:31:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment aebbfa6b-0afd-4cd0-b461-3f40c2530809 0xc006132af0 0xc006132af1}] []  [{kube-controller-manager Update apps/v1 2021-07-08 02:32:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aebbfa6b-0afd-4cd0-b461-3f40c2530809\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006132b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul  8 02:32:03.632: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul  8 02:32:03.632: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5496  51d14a05-9585-4759-af20-d48f1501f7a4 31650 2 2021-07-08 02:31:42 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment aebbfa6b-0afd-4cd0-b461-3f40c2530809 0xc0061327c7 0xc0061327c8}] []  [{e2e.test Update apps/v1 2021-07-08 02:31:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-08 02:32:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aebbfa6b-0afd-4cd0-b461-3f40c2530809\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0061328c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  8 02:32:03.632: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-5496  c020e167-766d-4df0-89bb-7dd656133b64 31602 2 2021-07-08 02:31:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment aebbfa6b-0afd-4cd0-b461-3f40c2530809 0xc006132977 0xc006132978}] []  [{kube-controller-manager Update apps/v1 2021-07-08 02:31:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aebbfa6b-0afd-4cd0-b461-3f40c2530809\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006132a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  8 02:32:03.634: INFO: Pod "test-rollover-deployment-98c5f4599-b2x2m" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-b2x2m test-rollover-deployment-98c5f4599- deployment-5496  0b8bc870-14d0-4523-b017-06d614eea2f5 31615 0 2021-07-08 02:31:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/podIP:10.42.3.107/32 cni.projectcalico.org/podIPs:10.42.3.107/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 71c3c49c-39fa-46a5-9e5c-8dc4c2028cea 0xc0061332c0 0xc0061332c1}] []  [{kube-controller-manager Update v1 2021-07-08 02:31:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71c3c49c-39fa-46a5-9e5c-8dc4c2028cea\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 02:31:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 02:31:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-77pp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-77pp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 02:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 02:31:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 02:31:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 02:31:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.217,PodIP:10.42.3.107,StartTime:2021-07-08 02:31:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 02:31:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://066a6b62359783de0463d33cc3d69d04072d4488e29ec6f2a1a6f5a7039e8c54,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:32:03.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5496" for this suite.

• [SLOW TEST:21.275 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":339,"completed":118,"skipped":1759,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:32:03.645: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-8918
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul  8 02:32:03.815: INFO: Waiting up to 1m0s for all nodes to be ready
Jul  8 02:33:03.865: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:33:03.869: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:33:04.041: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jul  8 02:33:04.044: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:33:04.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-939" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:33:04.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8918" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.503 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":339,"completed":119,"skipped":1790,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:33:04.148: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4081
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:33:04.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4081" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":339,"completed":120,"skipped":1802,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:33:04.352: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-8165
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8165 to expose endpoints map[]
Jul  8 02:33:04.555: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jul  8 02:33:05.565: INFO: successfully validated that service multi-endpoint-test in namespace services-8165 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8165
Jul  8 02:33:05.585: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:33:07.593: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8165 to expose endpoints map[pod1:[100]]
Jul  8 02:33:07.602: INFO: successfully validated that service multi-endpoint-test in namespace services-8165 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8165
Jul  8 02:33:07.612: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:33:09.619: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8165 to expose endpoints map[pod1:[100] pod2:[101]]
Jul  8 02:33:09.630: INFO: successfully validated that service multi-endpoint-test in namespace services-8165 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-8165
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8165 to expose endpoints map[pod2:[101]]
Jul  8 02:33:09.685: INFO: successfully validated that service multi-endpoint-test in namespace services-8165 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8165
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8165 to expose endpoints map[]
Jul  8 02:33:09.727: INFO: successfully validated that service multi-endpoint-test in namespace services-8165 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:33:09.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8165" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:5.451 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":339,"completed":121,"skipped":1806,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:33:09.804: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2344
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Jul  8 02:33:09.965: INFO: Waiting up to 5m0s for pod "var-expansion-02afabee-2b1d-4a22-adc7-5910973ee7fd" in namespace "var-expansion-2344" to be "Succeeded or Failed"
Jul  8 02:33:09.969: INFO: Pod "var-expansion-02afabee-2b1d-4a22-adc7-5910973ee7fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.746636ms
Jul  8 02:33:12.178: INFO: Pod "var-expansion-02afabee-2b1d-4a22-adc7-5910973ee7fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.212321341s
STEP: Saw pod success
Jul  8 02:33:12.178: INFO: Pod "var-expansion-02afabee-2b1d-4a22-adc7-5910973ee7fd" satisfied condition "Succeeded or Failed"
Jul  8 02:33:12.189: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod var-expansion-02afabee-2b1d-4a22-adc7-5910973ee7fd container dapi-container: <nil>
STEP: delete the pod
Jul  8 02:33:12.211: INFO: Waiting for pod var-expansion-02afabee-2b1d-4a22-adc7-5910973ee7fd to disappear
Jul  8 02:33:12.214: INFO: Pod var-expansion-02afabee-2b1d-4a22-adc7-5910973ee7fd no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:33:12.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2344" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":339,"completed":122,"skipped":1820,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:33:12.222: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8068
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul  8 02:33:12.376: INFO: Waiting up to 5m0s for pod "downward-api-af16b95b-dd53-4b11-9ba7-3926b7c59bc2" in namespace "downward-api-8068" to be "Succeeded or Failed"
Jul  8 02:33:12.381: INFO: Pod "downward-api-af16b95b-dd53-4b11-9ba7-3926b7c59bc2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146835ms
Jul  8 02:33:14.388: INFO: Pod "downward-api-af16b95b-dd53-4b11-9ba7-3926b7c59bc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011955095s
STEP: Saw pod success
Jul  8 02:33:14.388: INFO: Pod "downward-api-af16b95b-dd53-4b11-9ba7-3926b7c59bc2" satisfied condition "Succeeded or Failed"
Jul  8 02:33:14.391: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downward-api-af16b95b-dd53-4b11-9ba7-3926b7c59bc2 container dapi-container: <nil>
STEP: delete the pod
Jul  8 02:33:14.413: INFO: Waiting for pod downward-api-af16b95b-dd53-4b11-9ba7-3926b7c59bc2 to disappear
Jul  8 02:33:14.417: INFO: Pod downward-api-af16b95b-dd53-4b11-9ba7-3926b7c59bc2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:33:14.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8068" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":339,"completed":123,"skipped":1823,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:33:14.426: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 02:33:14.584: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7b88d67-2214-4b98-ba76-2862c8291286" in namespace "projected-2485" to be "Succeeded or Failed"
Jul  8 02:33:14.590: INFO: Pod "downwardapi-volume-c7b88d67-2214-4b98-ba76-2862c8291286": Phase="Pending", Reason="", readiness=false. Elapsed: 5.750907ms
Jul  8 02:33:16.597: INFO: Pod "downwardapi-volume-c7b88d67-2214-4b98-ba76-2862c8291286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012791109s
STEP: Saw pod success
Jul  8 02:33:16.597: INFO: Pod "downwardapi-volume-c7b88d67-2214-4b98-ba76-2862c8291286" satisfied condition "Succeeded or Failed"
Jul  8 02:33:16.599: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-c7b88d67-2214-4b98-ba76-2862c8291286 container client-container: <nil>
STEP: delete the pod
Jul  8 02:33:16.620: INFO: Waiting for pod downwardapi-volume-c7b88d67-2214-4b98-ba76-2862c8291286 to disappear
Jul  8 02:33:16.624: INFO: Pod downwardapi-volume-c7b88d67-2214-4b98-ba76-2862c8291286 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:33:16.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2485" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":124,"skipped":1830,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:33:16.635: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8221
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul  8 02:33:16.789: INFO: Waiting up to 5m0s for pod "pod-7cf0085f-774e-4282-b832-42320f6c84f2" in namespace "emptydir-8221" to be "Succeeded or Failed"
Jul  8 02:33:16.795: INFO: Pod "pod-7cf0085f-774e-4282-b832-42320f6c84f2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.617828ms
Jul  8 02:33:18.802: INFO: Pod "pod-7cf0085f-774e-4282-b832-42320f6c84f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012884507s
STEP: Saw pod success
Jul  8 02:33:18.802: INFO: Pod "pod-7cf0085f-774e-4282-b832-42320f6c84f2" satisfied condition "Succeeded or Failed"
Jul  8 02:33:18.804: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-7cf0085f-774e-4282-b832-42320f6c84f2 container test-container: <nil>
STEP: delete the pod
Jul  8 02:33:18.823: INFO: Waiting for pod pod-7cf0085f-774e-4282-b832-42320f6c84f2 to disappear
Jul  8 02:33:18.826: INFO: Pod pod-7cf0085f-774e-4282-b832-42320f6c84f2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:33:18.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8221" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":125,"skipped":1851,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:33:18.835: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 02:33:19.275: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul  8 02:33:21.289: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308399, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308399, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308399, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308399, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 02:33:23.295: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308399, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308399, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308399, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761308399, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 02:33:26.308: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:33:26.313: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2450-crds.webhook.example.com via the AdmissionRegistration API
Jul  8 02:33:26.943: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:33:29.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9755" for this suite.
STEP: Destroying namespace "webhook-9755-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.274 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":339,"completed":126,"skipped":1862,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:33:30.124: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6641
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jul  8 02:33:30.353: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul  8 02:33:35.363: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:33:35.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6641" for this suite.

• [SLOW TEST:5.340 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":339,"completed":127,"skipped":1865,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:33:35.461: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-3593c71a-2514-499b-922b-62b21f1262f9
STEP: Creating a pod to test consume secrets
Jul  8 02:33:35.696: INFO: Waiting up to 5m0s for pod "pod-secrets-c5d2de67-edbe-4e62-bc1a-5f2be1c1593b" in namespace "secrets-2252" to be "Succeeded or Failed"
Jul  8 02:33:35.708: INFO: Pod "pod-secrets-c5d2de67-edbe-4e62-bc1a-5f2be1c1593b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.660768ms
Jul  8 02:33:37.714: INFO: Pod "pod-secrets-c5d2de67-edbe-4e62-bc1a-5f2be1c1593b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018242654s
STEP: Saw pod success
Jul  8 02:33:37.714: INFO: Pod "pod-secrets-c5d2de67-edbe-4e62-bc1a-5f2be1c1593b" satisfied condition "Succeeded or Failed"
Jul  8 02:33:37.717: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-secrets-c5d2de67-edbe-4e62-bc1a-5f2be1c1593b container secret-volume-test: <nil>
STEP: delete the pod
Jul  8 02:33:37.737: INFO: Waiting for pod pod-secrets-c5d2de67-edbe-4e62-bc1a-5f2be1c1593b to disappear
Jul  8 02:33:37.740: INFO: Pod pod-secrets-c5d2de67-edbe-4e62-bc1a-5f2be1c1593b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:33:37.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2252" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":128,"skipped":1893,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:33:37.752: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7097
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jul  8 02:33:37.905: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jul  8 02:33:51.606: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:33:55.221: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:34:08.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7097" for this suite.

• [SLOW TEST:31.127 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":339,"completed":129,"skipped":1894,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:34:08.880: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8210
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:34:09.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8210" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":339,"completed":130,"skipped":1912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:34:09.071: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-7e3b2ab3-bf3c-4a80-a4d9-cb21a5b6ef2c
STEP: Creating a pod to test consume secrets
Jul  8 02:34:09.237: INFO: Waiting up to 5m0s for pod "pod-secrets-66070962-3bb3-4a3a-9d5b-6572c8d76914" in namespace "secrets-7753" to be "Succeeded or Failed"
Jul  8 02:34:09.243: INFO: Pod "pod-secrets-66070962-3bb3-4a3a-9d5b-6572c8d76914": Phase="Pending", Reason="", readiness=false. Elapsed: 5.163478ms
Jul  8 02:34:11.248: INFO: Pod "pod-secrets-66070962-3bb3-4a3a-9d5b-6572c8d76914": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010475818s
STEP: Saw pod success
Jul  8 02:34:11.248: INFO: Pod "pod-secrets-66070962-3bb3-4a3a-9d5b-6572c8d76914" satisfied condition "Succeeded or Failed"
Jul  8 02:34:11.251: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-secrets-66070962-3bb3-4a3a-9d5b-6572c8d76914 container secret-volume-test: <nil>
STEP: delete the pod
Jul  8 02:34:11.272: INFO: Waiting for pod pod-secrets-66070962-3bb3-4a3a-9d5b-6572c8d76914 to disappear
Jul  8 02:34:11.275: INFO: Pod pod-secrets-66070962-3bb3-4a3a-9d5b-6572c8d76914 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:34:11.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7753" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":131,"skipped":1944,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:34:11.286: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 02:34:11.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eee20fc8-d530-4c48-8a79-c67db8f85d65" in namespace "projected-9236" to be "Succeeded or Failed"
Jul  8 02:34:11.447: INFO: Pod "downwardapi-volume-eee20fc8-d530-4c48-8a79-c67db8f85d65": Phase="Pending", Reason="", readiness=false. Elapsed: 7.053068ms
Jul  8 02:34:13.454: INFO: Pod "downwardapi-volume-eee20fc8-d530-4c48-8a79-c67db8f85d65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014101704s
STEP: Saw pod success
Jul  8 02:34:13.454: INFO: Pod "downwardapi-volume-eee20fc8-d530-4c48-8a79-c67db8f85d65" satisfied condition "Succeeded or Failed"
Jul  8 02:34:13.456: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-eee20fc8-d530-4c48-8a79-c67db8f85d65 container client-container: <nil>
STEP: delete the pod
Jul  8 02:34:13.476: INFO: Waiting for pod downwardapi-volume-eee20fc8-d530-4c48-8a79-c67db8f85d65 to disappear
Jul  8 02:34:13.481: INFO: Pod downwardapi-volume-eee20fc8-d530-4c48-8a79-c67db8f85d65 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:34:13.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9236" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":132,"skipped":1951,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:34:13.488: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:34:15.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7269" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":339,"completed":133,"skipped":1986,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:34:15.668: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jul  8 02:34:20.346: INFO: Successfully updated pod "adopt-release-m2sjk"
STEP: Checking that the Job readopts the Pod
Jul  8 02:34:20.346: INFO: Waiting up to 15m0s for pod "adopt-release-m2sjk" in namespace "job-6102" to be "adopted"
Jul  8 02:34:20.352: INFO: Pod "adopt-release-m2sjk": Phase="Running", Reason="", readiness=true. Elapsed: 5.295133ms
Jul  8 02:34:22.359: INFO: Pod "adopt-release-m2sjk": Phase="Running", Reason="", readiness=true. Elapsed: 2.012183983s
Jul  8 02:34:22.359: INFO: Pod "adopt-release-m2sjk" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jul  8 02:34:22.872: INFO: Successfully updated pod "adopt-release-m2sjk"
STEP: Checking that the Job releases the Pod
Jul  8 02:34:22.873: INFO: Waiting up to 15m0s for pod "adopt-release-m2sjk" in namespace "job-6102" to be "released"
Jul  8 02:34:22.877: INFO: Pod "adopt-release-m2sjk": Phase="Running", Reason="", readiness=true. Elapsed: 4.79516ms
Jul  8 02:34:24.884: INFO: Pod "adopt-release-m2sjk": Phase="Running", Reason="", readiness=true. Elapsed: 2.011600393s
Jul  8 02:34:24.884: INFO: Pod "adopt-release-m2sjk" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:34:24.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6102" for this suite.

• [SLOW TEST:9.225 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":339,"completed":134,"skipped":2044,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:34:24.895: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  8 02:34:25.089: INFO: Number of nodes with available pods: 0
Jul  8 02:34:25.089: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:26.098: INFO: Number of nodes with available pods: 0
Jul  8 02:34:26.098: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:27.098: INFO: Number of nodes with available pods: 3
Jul  8 02:34:27.098: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:28.100: INFO: Number of nodes with available pods: 3
Jul  8 02:34:28.100: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:29.098: INFO: Number of nodes with available pods: 4
Jul  8 02:34:29.098: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul  8 02:34:29.128: INFO: Number of nodes with available pods: 3
Jul  8 02:34:29.128: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:30.149: INFO: Number of nodes with available pods: 3
Jul  8 02:34:30.149: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:31.136: INFO: Number of nodes with available pods: 3
Jul  8 02:34:31.136: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:32.137: INFO: Number of nodes with available pods: 3
Jul  8 02:34:32.137: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:33.139: INFO: Number of nodes with available pods: 3
Jul  8 02:34:33.139: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:34.138: INFO: Number of nodes with available pods: 3
Jul  8 02:34:34.138: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:35.137: INFO: Number of nodes with available pods: 3
Jul  8 02:34:35.137: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:36.137: INFO: Number of nodes with available pods: 3
Jul  8 02:34:36.137: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:37.137: INFO: Number of nodes with available pods: 3
Jul  8 02:34:37.137: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:38.137: INFO: Number of nodes with available pods: 3
Jul  8 02:34:38.137: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:39.137: INFO: Number of nodes with available pods: 3
Jul  8 02:34:39.137: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 02:34:40.145: INFO: Number of nodes with available pods: 4
Jul  8 02:34:40.145: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9180, will wait for the garbage collector to delete the pods
Jul  8 02:34:40.211: INFO: Deleting DaemonSet.extensions daemon-set took: 7.856882ms
Jul  8 02:34:40.312: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.931487ms
Jul  8 02:34:51.019: INFO: Number of nodes with available pods: 0
Jul  8 02:34:51.019: INFO: Number of running nodes: 0, number of available pods: 0
Jul  8 02:34:51.021: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32806"},"items":null}

Jul  8 02:34:51.023: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32806"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:34:51.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9180" for this suite.

• [SLOW TEST:26.151 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":339,"completed":135,"skipped":2056,"failed":0}
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:34:51.046: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0708 02:34:51.214960      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:36:01.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-263" for this suite.

• [SLOW TEST:70.224 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":339,"completed":136,"skipped":2056,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:36:01.270: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jul  8 02:36:01.444: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:36:03.451: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jul  8 02:36:03.475: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:36:05.484: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jul  8 02:36:05.496: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  8 02:36:05.499: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  8 02:36:07.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  8 02:36:07.507: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  8 02:36:09.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  8 02:36:09.511: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  8 02:36:11.501: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  8 02:36:11.506: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  8 02:36:13.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  8 02:36:13.508: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  8 02:36:15.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  8 02:36:15.507: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  8 02:36:17.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  8 02:36:17.508: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  8 02:36:19.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  8 02:36:19.509: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:36:19.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7390" for this suite.

• [SLOW TEST:18.275 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":339,"completed":137,"skipped":2057,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:36:19.548: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-6044
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jul  8 02:36:19.716: INFO: Waiting up to 1m0s for all nodes to be ready
Jul  8 02:37:19.750: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:37:19.753: INFO: Starting informer...
STEP: Starting pod...
Jul  8 02:37:19.980: INFO: Pod is running on ip-172-31-6-217.us-east-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jul  8 02:37:20.194: INFO: Pod wasn't evicted. Proceeding
Jul  8 02:37:20.194: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jul  8 02:38:35.222: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:38:35.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6044" for this suite.

• [SLOW TEST:135.692 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":339,"completed":138,"skipped":2093,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:38:35.241: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3092
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-3092
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  8 02:38:35.399: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul  8 02:38:35.570: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:38:37.576: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:38:39.577: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:38:41.574: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:38:43.577: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:38:45.576: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:38:47.577: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:38:49.579: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:38:51.579: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:38:53.576: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 02:38:55.577: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul  8 02:38:55.586: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul  8 02:38:55.599: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jul  8 02:38:55.610: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jul  8 02:38:57.700: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jul  8 02:38:57.700: INFO: Going to poll 10.42.0.17 on port 8080 at least 0 times, with a maximum of 46 tries before failing
Jul  8 02:38:57.701: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.0.17:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3092 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 02:38:57.701: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:38:57.768: INFO: Found all 1 expected endpoints: [netserver-0]
Jul  8 02:38:57.768: INFO: Going to poll 10.42.3.129 on port 8080 at least 0 times, with a maximum of 46 tries before failing
Jul  8 02:38:57.778: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.3.129:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3092 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 02:38:57.778: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:38:57.845: INFO: Found all 1 expected endpoints: [netserver-1]
Jul  8 02:38:57.845: INFO: Going to poll 10.42.1.23 on port 8080 at least 0 times, with a maximum of 46 tries before failing
Jul  8 02:38:57.848: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.1.23:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3092 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 02:38:57.848: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:38:57.926: INFO: Found all 1 expected endpoints: [netserver-2]
Jul  8 02:38:57.926: INFO: Going to poll 10.42.2.20 on port 8080 at least 0 times, with a maximum of 46 tries before failing
Jul  8 02:38:57.934: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.2.20:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3092 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 02:38:57.934: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:38:58.016: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:38:58.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3092" for this suite.

• [SLOW TEST:22.791 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":139,"skipped":2094,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:38:58.034: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-3618
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-3618
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3618
Jul  8 02:38:58.209: INFO: Found 0 stateful pods, waiting for 1
Jul  8 02:39:08.225: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul  8 02:39:08.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  8 02:39:08.636: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  8 02:39:08.636: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  8 02:39:08.636: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  8 02:39:08.641: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul  8 02:39:18.656: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  8 02:39:18.656: INFO: Waiting for statefulset status.replicas updated to 0
Jul  8 02:39:18.675: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  8 02:39:18.675: INFO: ss-0  ip-172-31-6-217.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  }]
Jul  8 02:39:18.675: INFO: 
Jul  8 02:39:18.675: INFO: StatefulSet ss has not reached scale 3, at 1
Jul  8 02:39:19.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990735774s
Jul  8 02:39:20.689: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984195155s
Jul  8 02:39:21.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977352453s
Jul  8 02:39:22.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971142922s
Jul  8 02:39:23.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.964320117s
Jul  8 02:39:24.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.958502641s
Jul  8 02:39:25.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.953182095s
Jul  8 02:39:26.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.947482819s
Jul  8 02:39:27.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.834468ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3618
Jul  8 02:39:28.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:39:28.923: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  8 02:39:28.923: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  8 02:39:28.923: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  8 02:39:28.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:39:29.146: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul  8 02:39:29.146: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  8 02:39:29.146: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  8 02:39:29.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:39:29.346: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul  8 02:39:29.346: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  8 02:39:29.346: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  8 02:39:29.350: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 02:39:29.350: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 02:39:29.350: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul  8 02:39:29.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  8 02:39:29.501: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  8 02:39:29.501: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  8 02:39:29.501: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  8 02:39:29.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  8 02:39:29.671: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  8 02:39:29.671: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  8 02:39:29.671: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  8 02:39:29.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  8 02:39:29.823: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  8 02:39:29.823: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  8 02:39:29.823: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  8 02:39:29.823: INFO: Waiting for statefulset status.replicas updated to 0
Jul  8 02:39:29.826: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul  8 02:39:39.873: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  8 02:39:39.873: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul  8 02:39:39.873: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul  8 02:39:39.926: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  8 02:39:39.926: INFO: ss-0  ip-172-31-6-217.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  }]
Jul  8 02:39:39.926: INFO: ss-1  ip-172-31-6-226.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:39.926: INFO: ss-2  ip-172-31-8-165.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:39.926: INFO: 
Jul  8 02:39:39.926: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  8 02:39:40.932: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  8 02:39:40.932: INFO: ss-0  ip-172-31-6-217.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  }]
Jul  8 02:39:40.932: INFO: ss-1  ip-172-31-6-226.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:40.932: INFO: ss-2  ip-172-31-8-165.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:40.932: INFO: 
Jul  8 02:39:40.932: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  8 02:39:41.943: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  8 02:39:41.944: INFO: ss-0  ip-172-31-6-217.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  }]
Jul  8 02:39:41.944: INFO: ss-1  ip-172-31-6-226.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:41.944: INFO: ss-2  ip-172-31-8-165.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:41.944: INFO: 
Jul  8 02:39:41.944: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  8 02:39:42.950: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  8 02:39:42.950: INFO: ss-0  ip-172-31-6-217.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  }]
Jul  8 02:39:42.950: INFO: ss-2  ip-172-31-8-165.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:42.950: INFO: 
Jul  8 02:39:42.950: INFO: StatefulSet ss has not reached scale 0, at 2
Jul  8 02:39:43.956: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  8 02:39:43.956: INFO: ss-0  ip-172-31-6-217.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  }]
Jul  8 02:39:43.956: INFO: ss-2  ip-172-31-8-165.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:43.956: INFO: 
Jul  8 02:39:43.956: INFO: StatefulSet ss has not reached scale 0, at 2
Jul  8 02:39:44.963: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  8 02:39:44.963: INFO: ss-0  ip-172-31-6-217.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  }]
Jul  8 02:39:44.963: INFO: ss-2  ip-172-31-8-165.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:44.963: INFO: 
Jul  8 02:39:44.963: INFO: StatefulSet ss has not reached scale 0, at 2
Jul  8 02:39:45.968: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  8 02:39:45.968: INFO: ss-0  ip-172-31-6-217.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  }]
Jul  8 02:39:45.968: INFO: ss-2  ip-172-31-8-165.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:45.968: INFO: 
Jul  8 02:39:45.968: INFO: StatefulSet ss has not reached scale 0, at 2
Jul  8 02:39:46.974: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  8 02:39:46.974: INFO: ss-0  ip-172-31-6-217.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  }]
Jul  8 02:39:46.974: INFO: ss-2  ip-172-31-8-165.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:46.974: INFO: 
Jul  8 02:39:46.974: INFO: StatefulSet ss has not reached scale 0, at 2
Jul  8 02:39:47.979: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  8 02:39:47.979: INFO: ss-0  ip-172-31-6-217.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:38:58 +0000 UTC  }]
Jul  8 02:39:47.979: INFO: ss-2  ip-172-31-8-165.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:47.979: INFO: 
Jul  8 02:39:47.979: INFO: StatefulSet ss has not reached scale 0, at 2
Jul  8 02:39:48.986: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  8 02:39:48.986: INFO: ss-2  ip-172-31-8-165.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 02:39:18 +0000 UTC  }]
Jul  8 02:39:48.986: INFO: 
Jul  8 02:39:48.986: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3618
Jul  8 02:39:49.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:39:50.104: INFO: rc: 1
Jul  8 02:39:50.104: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jul  8 02:40:00.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:40:00.220: INFO: rc: 1
Jul  8 02:40:00.220: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:40:10.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:40:10.364: INFO: rc: 1
Jul  8 02:40:10.364: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:40:20.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:40:20.471: INFO: rc: 1
Jul  8 02:40:20.471: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:40:30.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:40:30.649: INFO: rc: 1
Jul  8 02:40:30.649: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:40:40.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:40:40.806: INFO: rc: 1
Jul  8 02:40:40.806: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:40:50.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:40:50.901: INFO: rc: 1
Jul  8 02:40:50.901: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:41:00.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:41:01.081: INFO: rc: 1
Jul  8 02:41:01.081: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:41:11.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:41:11.189: INFO: rc: 1
Jul  8 02:41:11.189: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:41:21.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:41:21.439: INFO: rc: 1
Jul  8 02:41:21.439: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:41:31.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:41:31.546: INFO: rc: 1
Jul  8 02:41:31.546: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:41:41.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:41:41.663: INFO: rc: 1
Jul  8 02:41:41.663: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:41:51.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:41:51.773: INFO: rc: 1
Jul  8 02:41:51.773: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:42:01.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:42:01.893: INFO: rc: 1
Jul  8 02:42:01.893: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:42:11.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:42:12.129: INFO: rc: 1
Jul  8 02:42:12.129: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:42:22.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:42:22.257: INFO: rc: 1
Jul  8 02:42:22.257: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:42:32.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:42:32.361: INFO: rc: 1
Jul  8 02:42:32.361: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:42:42.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:42:42.471: INFO: rc: 1
Jul  8 02:42:42.471: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:42:52.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:42:52.648: INFO: rc: 1
Jul  8 02:42:52.648: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:43:02.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:43:02.768: INFO: rc: 1
Jul  8 02:43:02.768: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:43:12.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:43:12.875: INFO: rc: 1
Jul  8 02:43:12.875: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:43:22.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:43:22.966: INFO: rc: 1
Jul  8 02:43:22.967: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:43:32.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:43:33.066: INFO: rc: 1
Jul  8 02:43:33.066: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:43:43.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:43:43.162: INFO: rc: 1
Jul  8 02:43:43.162: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:43:53.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:43:53.251: INFO: rc: 1
Jul  8 02:43:53.251: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:44:03.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:44:03.359: INFO: rc: 1
Jul  8 02:44:03.359: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:44:13.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:44:13.444: INFO: rc: 1
Jul  8 02:44:13.444: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:44:23.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:44:23.543: INFO: rc: 1
Jul  8 02:44:23.543: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:44:33.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:44:33.653: INFO: rc: 1
Jul  8 02:44:33.653: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:44:43.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:44:43.734: INFO: rc: 1
Jul  8 02:44:43.734: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  8 02:44:53.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 02:44:53.831: INFO: rc: 1
Jul  8 02:44:53.831: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Jul  8 02:44:53.831: INFO: Scaling statefulset ss to 0
Jul  8 02:44:53.841: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul  8 02:44:53.843: INFO: Deleting all statefulset in ns statefulset-3618
Jul  8 02:44:53.845: INFO: Scaling statefulset ss to 0
Jul  8 02:44:53.852: INFO: Waiting for statefulset status.replicas updated to 0
Jul  8 02:44:53.854: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:44:53.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3618" for this suite.

• [SLOW TEST:355.873 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":339,"completed":140,"skipped":2114,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:44:53.906: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul  8 02:44:54.046: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  8 02:44:54.062: INFO: Waiting for terminating namespaces to be deleted...
Jul  8 02:44:54.064: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-228.us-east-2.compute.internal before test
Jul  8 02:44:54.071: INFO: etcd-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:35 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container etcd ready: true, restart count 0
Jul  8 02:44:54.072: INFO: helm-install-rke2-canal-zzkr7 from kube-system started at 2021-07-07 23:40:11 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container helm ready: false, restart count 0
Jul  8 02:44:54.072: INFO: helm-install-rke2-coredns-7vm9g from kube-system started at 2021-07-07 23:40:11 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container helm ready: false, restart count 0
Jul  8 02:44:54.072: INFO: helm-install-rke2-kube-proxy-tqnww from kube-system started at 2021-07-07 23:40:11 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container helm ready: false, restart count 0
Jul  8 02:44:54.072: INFO: helm-install-rke2-metrics-server-xbzj4 from kube-system started at 2021-07-07 23:40:54 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container helm ready: false, restart count 0
Jul  8 02:44:54.072: INFO: kube-apiserver-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:47 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul  8 02:44:54.072: INFO: kube-controller-manager-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:57 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul  8 02:44:54.072: INFO: kube-proxy-vvsjd from kube-system started at 2021-07-07 23:40:24 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 02:44:54.072: INFO: kube-scheduler-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:55 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul  8 02:44:54.072: INFO: rke2-canal-kcjjk from kube-system started at 2021-07-07 23:40:25 +0000 UTC (2 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 02:44:54.072: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 02:44:54.072: INFO: rke2-coredns-rke2-coredns-65d668ddf9-bd9pd from kube-system started at 2021-07-07 23:40:52 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container coredns ready: true, restart count 0
Jul  8 02:44:54.072: INFO: rke2-metrics-server-6647ffc866-7z7j7 from kube-system started at 2021-07-07 23:41:00 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container metrics-server ready: true, restart count 0
Jul  8 02:44:54.072: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-84vqn from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 02:44:54.072: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 02:44:54.072: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  8 02:44:54.072: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-217.us-east-2.compute.internal before test
Jul  8 02:44:54.079: INFO: kube-proxy-t4j4c from kube-system started at 2021-07-07 23:44:49 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.079: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 02:44:54.079: INFO: rke2-canal-9wlw7 from kube-system started at 2021-07-07 23:44:49 +0000 UTC (2 container statuses recorded)
Jul  8 02:44:54.079: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 02:44:54.079: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 02:44:54.079: INFO: sonobuoy from sonobuoy started at 2021-07-08 01:44:37 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.079: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  8 02:44:54.079: INFO: sonobuoy-e2e-job-1dfa1a3a74ca4282 from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 02:44:54.079: INFO: 	Container e2e ready: true, restart count 0
Jul  8 02:44:54.079: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 02:44:54.079: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-5s9xh from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 02:44:54.079: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 02:44:54.079: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  8 02:44:54.079: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-226.us-east-2.compute.internal before test
Jul  8 02:44:54.089: INFO: etcd-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:42:57 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.089: INFO: 	Container etcd ready: true, restart count 0
Jul  8 02:44:54.089: INFO: kube-apiserver-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:11 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.089: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul  8 02:44:54.089: INFO: kube-controller-manager-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:22 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.089: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul  8 02:44:54.089: INFO: kube-proxy-87mbg from kube-system started at 2021-07-07 23:43:19 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.089: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 02:44:54.089: INFO: kube-scheduler-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:22 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.089: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul  8 02:44:54.089: INFO: rke2-canal-8n79q from kube-system started at 2021-07-07 23:43:19 +0000 UTC (2 container statuses recorded)
Jul  8 02:44:54.089: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 02:44:54.089: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 02:44:54.089: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-2nxk4 from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 02:44:54.089: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 02:44:54.089: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  8 02:44:54.089: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-8-165.us-east-2.compute.internal before test
Jul  8 02:44:54.098: INFO: etcd-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:41 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.098: INFO: 	Container etcd ready: true, restart count 0
Jul  8 02:44:54.098: INFO: kube-apiserver-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:48 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.098: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul  8 02:44:54.098: INFO: kube-controller-manager-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:59 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.098: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul  8 02:44:54.098: INFO: kube-proxy-p4h8m from kube-system started at 2021-07-07 23:43:56 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.098: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 02:44:54.098: INFO: kube-scheduler-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:58 +0000 UTC (1 container statuses recorded)
Jul  8 02:44:54.098: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul  8 02:44:54.098: INFO: rke2-canal-vnj8b from kube-system started at 2021-07-07 23:43:56 +0000 UTC (2 container statuses recorded)
Jul  8 02:44:54.098: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 02:44:54.098: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 02:44:54.098: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-fndvb from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 02:44:54.098: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 02:44:54.098: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-86b0a050-452b-4d63-a518-19ef731a90fc 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.6.217 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-86b0a050-452b-4d63-a518-19ef731a90fc off the node ip-172-31-6-217.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-86b0a050-452b-4d63-a518-19ef731a90fc
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:49:58.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2622" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.393 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":339,"completed":141,"skipped":2120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:49:58.301: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul  8 02:49:58.500: INFO: Waiting up to 5m0s for pod "pod-4698e425-cfbf-4d29-a639-a20ffd6478cd" in namespace "emptydir-238" to be "Succeeded or Failed"
Jul  8 02:49:58.525: INFO: Pod "pod-4698e425-cfbf-4d29-a639-a20ffd6478cd": Phase="Pending", Reason="", readiness=false. Elapsed: 24.756078ms
Jul  8 02:50:00.533: INFO: Pod "pod-4698e425-cfbf-4d29-a639-a20ffd6478cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032419679s
STEP: Saw pod success
Jul  8 02:50:00.533: INFO: Pod "pod-4698e425-cfbf-4d29-a639-a20ffd6478cd" satisfied condition "Succeeded or Failed"
Jul  8 02:50:00.535: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-4698e425-cfbf-4d29-a639-a20ffd6478cd container test-container: <nil>
STEP: delete the pod
Jul  8 02:50:00.564: INFO: Waiting for pod pod-4698e425-cfbf-4d29-a639-a20ffd6478cd to disappear
Jul  8 02:50:00.566: INFO: Pod pod-4698e425-cfbf-4d29-a639-a20ffd6478cd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:50:00.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-238" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":142,"skipped":2157,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:50:00.576: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6976
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Jul  8 02:50:00.735: INFO: Waiting up to 5m0s for pod "var-expansion-5d82b421-6ad0-4a9c-b16d-71a099127455" in namespace "var-expansion-6976" to be "Succeeded or Failed"
Jul  8 02:50:00.740: INFO: Pod "var-expansion-5d82b421-6ad0-4a9c-b16d-71a099127455": Phase="Pending", Reason="", readiness=false. Elapsed: 4.196902ms
Jul  8 02:50:02.751: INFO: Pod "var-expansion-5d82b421-6ad0-4a9c-b16d-71a099127455": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015766137s
STEP: Saw pod success
Jul  8 02:50:02.751: INFO: Pod "var-expansion-5d82b421-6ad0-4a9c-b16d-71a099127455" satisfied condition "Succeeded or Failed"
Jul  8 02:50:02.754: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod var-expansion-5d82b421-6ad0-4a9c-b16d-71a099127455 container dapi-container: <nil>
STEP: delete the pod
Jul  8 02:50:02.792: INFO: Waiting for pod var-expansion-5d82b421-6ad0-4a9c-b16d-71a099127455 to disappear
Jul  8 02:50:02.796: INFO: Pod var-expansion-5d82b421-6ad0-4a9c-b16d-71a099127455 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:50:02.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6976" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":339,"completed":143,"skipped":2166,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:50:02.817: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-593
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-e002558c-c7bd-474c-a38f-7b2f6e497f7f
STEP: Creating a pod to test consume configMaps
Jul  8 02:50:03.057: INFO: Waiting up to 5m0s for pod "pod-configmaps-87f8b312-1769-4354-afd3-0d89da566e96" in namespace "configmap-593" to be "Succeeded or Failed"
Jul  8 02:50:03.082: INFO: Pod "pod-configmaps-87f8b312-1769-4354-afd3-0d89da566e96": Phase="Pending", Reason="", readiness=false. Elapsed: 23.998536ms
Jul  8 02:50:05.111: INFO: Pod "pod-configmaps-87f8b312-1769-4354-afd3-0d89da566e96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.053702727s
STEP: Saw pod success
Jul  8 02:50:05.111: INFO: Pod "pod-configmaps-87f8b312-1769-4354-afd3-0d89da566e96" satisfied condition "Succeeded or Failed"
Jul  8 02:50:05.114: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-configmaps-87f8b312-1769-4354-afd3-0d89da566e96 container agnhost-container: <nil>
STEP: delete the pod
Jul  8 02:50:05.134: INFO: Waiting for pod pod-configmaps-87f8b312-1769-4354-afd3-0d89da566e96 to disappear
Jul  8 02:50:05.138: INFO: Pod pod-configmaps-87f8b312-1769-4354-afd3-0d89da566e96 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:50:05.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-593" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":144,"skipped":2170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:50:05.156: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8475
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-d557e61d-471c-4fde-8a4c-057c6e2151c3
STEP: Creating a pod to test consume configMaps
Jul  8 02:50:05.315: INFO: Waiting up to 5m0s for pod "pod-configmaps-6aea7984-6f9d-4106-b56e-95252540afa2" in namespace "configmap-8475" to be "Succeeded or Failed"
Jul  8 02:50:05.320: INFO: Pod "pod-configmaps-6aea7984-6f9d-4106-b56e-95252540afa2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.151993ms
Jul  8 02:50:07.331: INFO: Pod "pod-configmaps-6aea7984-6f9d-4106-b56e-95252540afa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01665804s
Jul  8 02:50:09.345: INFO: Pod "pod-configmaps-6aea7984-6f9d-4106-b56e-95252540afa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029981396s
STEP: Saw pod success
Jul  8 02:50:09.345: INFO: Pod "pod-configmaps-6aea7984-6f9d-4106-b56e-95252540afa2" satisfied condition "Succeeded or Failed"
Jul  8 02:50:09.347: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-configmaps-6aea7984-6f9d-4106-b56e-95252540afa2 container agnhost-container: <nil>
STEP: delete the pod
Jul  8 02:50:09.368: INFO: Waiting for pod pod-configmaps-6aea7984-6f9d-4106-b56e-95252540afa2 to disappear
Jul  8 02:50:09.372: INFO: Pod pod-configmaps-6aea7984-6f9d-4106-b56e-95252540afa2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:50:09.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8475" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":145,"skipped":2204,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:50:09.383: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7682
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Jul  8 02:50:09.559: INFO: Waiting up to 5m0s for pod "pod-1b8af3af-5266-4178-b823-9e295a7beda6" in namespace "emptydir-7682" to be "Succeeded or Failed"
Jul  8 02:50:09.568: INFO: Pod "pod-1b8af3af-5266-4178-b823-9e295a7beda6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.566171ms
Jul  8 02:50:11.573: INFO: Pod "pod-1b8af3af-5266-4178-b823-9e295a7beda6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014186286s
STEP: Saw pod success
Jul  8 02:50:11.573: INFO: Pod "pod-1b8af3af-5266-4178-b823-9e295a7beda6" satisfied condition "Succeeded or Failed"
Jul  8 02:50:11.576: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-1b8af3af-5266-4178-b823-9e295a7beda6 container test-container: <nil>
STEP: delete the pod
Jul  8 02:50:11.593: INFO: Waiting for pod pod-1b8af3af-5266-4178-b823-9e295a7beda6 to disappear
Jul  8 02:50:11.596: INFO: Pod pod-1b8af3af-5266-4178-b823-9e295a7beda6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:50:11.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7682" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":146,"skipped":2232,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:50:11.603: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2695
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul  8 02:50:11.752: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2695  96846ee9-28aa-402d-8cef-39507a92b3c4 35769 0 2021-07-08 02:50:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-08 02:50:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 02:50:11.752: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2695  96846ee9-28aa-402d-8cef-39507a92b3c4 35770 0 2021-07-08 02:50:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-08 02:50:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul  8 02:50:11.775: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2695  96846ee9-28aa-402d-8cef-39507a92b3c4 35771 0 2021-07-08 02:50:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-08 02:50:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 02:50:11.775: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2695  96846ee9-28aa-402d-8cef-39507a92b3c4 35772 0 2021-07-08 02:50:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-08 02:50:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:50:11.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2695" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":339,"completed":147,"skipped":2232,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:50:11.786: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1056
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0708 02:50:18.055965      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul  8 02:55:18.062: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:55:18.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1056" for this suite.

• [SLOW TEST:306.289 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":339,"completed":148,"skipped":2259,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:55:18.078: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5654
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jul  8 02:55:18.230: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 02:55:21.727: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:55:34.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5654" for this suite.

• [SLOW TEST:16.188 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":339,"completed":149,"skipped":2262,"failed":0}
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:55:34.266: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-4023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:55:36.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4023" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":339,"completed":150,"skipped":2262,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:55:36.524: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:55:36.695: INFO: The status of Pod server-envvars-7c7a1c2b-8d97-4281-9c94-6caacf778039 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:55:38.703: INFO: The status of Pod server-envvars-7c7a1c2b-8d97-4281-9c94-6caacf778039 is Running (Ready = true)
Jul  8 02:55:38.739: INFO: Waiting up to 5m0s for pod "client-envvars-28b441fe-b280-42a9-8b4d-d3948338b8b2" in namespace "pods-5822" to be "Succeeded or Failed"
Jul  8 02:55:38.749: INFO: Pod "client-envvars-28b441fe-b280-42a9-8b4d-d3948338b8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.208911ms
Jul  8 02:55:40.757: INFO: Pod "client-envvars-28b441fe-b280-42a9-8b4d-d3948338b8b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018003634s
STEP: Saw pod success
Jul  8 02:55:40.757: INFO: Pod "client-envvars-28b441fe-b280-42a9-8b4d-d3948338b8b2" satisfied condition "Succeeded or Failed"
Jul  8 02:55:40.760: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod client-envvars-28b441fe-b280-42a9-8b4d-d3948338b8b2 container env3cont: <nil>
STEP: delete the pod
Jul  8 02:55:40.788: INFO: Waiting for pod client-envvars-28b441fe-b280-42a9-8b4d-d3948338b8b2 to disappear
Jul  8 02:55:40.790: INFO: Pod client-envvars-28b441fe-b280-42a9-8b4d-d3948338b8b2 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:55:40.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5822" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":339,"completed":151,"skipped":2264,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:55:40.813: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5145
STEP: creating service affinity-nodeport in namespace services-5145
STEP: creating replication controller affinity-nodeport in namespace services-5145
I0708 02:55:40.986749      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-5145, replica count: 3
I0708 02:55:44.037973      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  8 02:55:44.060: INFO: Creating new exec pod
Jul  8 02:55:47.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-5145 exec execpod-affinity54wkb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jul  8 02:55:47.531: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jul  8 02:55:47.531: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 02:55:47.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-5145 exec execpod-affinity54wkb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.142.250 80'
Jul  8 02:55:47.685: INFO: stderr: "+ nc -v -t -w 2 10.43.142.250 80\n+ echo hostName\nConnection to 10.43.142.250 80 port [tcp/http] succeeded!\n"
Jul  8 02:55:47.685: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 02:55:47.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-5145 exec execpod-affinity54wkb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.3.228 30993'
Jul  8 02:55:47.839: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.3.228 30993\nConnection to 172.31.3.228 30993 port [tcp/*] succeeded!\n"
Jul  8 02:55:47.839: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 02:55:47.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-5145 exec execpod-affinity54wkb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.6.226 30993'
Jul  8 02:55:47.990: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.6.226 30993\nConnection to 172.31.6.226 30993 port [tcp/*] succeeded!\n"
Jul  8 02:55:47.990: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 02:55:47.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-5145 exec execpod-affinity54wkb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.228:30993/ ; done'
Jul  8 02:55:48.217: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:30993/\n"
Jul  8 02:55:48.218: INFO: stdout: "\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv\naffinity-nodeport-9v4qv"
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Received response from host: affinity-nodeport-9v4qv
Jul  8 02:55:48.218: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5145, will wait for the garbage collector to delete the pods
Jul  8 02:55:48.310: INFO: Deleting ReplicationController affinity-nodeport took: 7.851597ms
Jul  8 02:55:48.411: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.825686ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:56:01.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5145" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:20.311 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":152,"skipped":2274,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:56:01.124: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8858
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jul  8 02:56:01.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 create -f -'
Jul  8 02:56:01.782: INFO: stderr: ""
Jul  8 02:56:01.782: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  8 02:56:01.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  8 02:56:01.882: INFO: stderr: ""
Jul  8 02:56:01.882: INFO: stdout: "update-demo-nautilus-5pdqk update-demo-nautilus-9b669 "
Jul  8 02:56:01.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-5pdqk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  8 02:56:02.024: INFO: stderr: ""
Jul  8 02:56:02.024: INFO: stdout: ""
Jul  8 02:56:02.024: INFO: update-demo-nautilus-5pdqk is created but not running
Jul  8 02:56:07.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  8 02:56:07.124: INFO: stderr: ""
Jul  8 02:56:07.124: INFO: stdout: "update-demo-nautilus-5pdqk update-demo-nautilus-9b669 "
Jul  8 02:56:07.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-5pdqk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  8 02:56:07.208: INFO: stderr: ""
Jul  8 02:56:07.208: INFO: stdout: ""
Jul  8 02:56:07.208: INFO: update-demo-nautilus-5pdqk is created but not running
Jul  8 02:56:12.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  8 02:56:12.299: INFO: stderr: ""
Jul  8 02:56:12.299: INFO: stdout: "update-demo-nautilus-5pdqk update-demo-nautilus-9b669 "
Jul  8 02:56:12.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-5pdqk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  8 02:56:12.382: INFO: stderr: ""
Jul  8 02:56:12.382: INFO: stdout: "true"
Jul  8 02:56:12.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-5pdqk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  8 02:56:12.479: INFO: stderr: ""
Jul  8 02:56:12.479: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul  8 02:56:12.479: INFO: validating pod update-demo-nautilus-5pdqk
Jul  8 02:56:12.485: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  8 02:56:12.485: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  8 02:56:12.485: INFO: update-demo-nautilus-5pdqk is verified up and running
Jul  8 02:56:12.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-9b669 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  8 02:56:12.610: INFO: stderr: ""
Jul  8 02:56:12.610: INFO: stdout: "true"
Jul  8 02:56:12.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-9b669 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  8 02:56:12.742: INFO: stderr: ""
Jul  8 02:56:12.742: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul  8 02:56:12.742: INFO: validating pod update-demo-nautilus-9b669
Jul  8 02:56:12.746: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  8 02:56:12.746: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  8 02:56:12.746: INFO: update-demo-nautilus-9b669 is verified up and running
STEP: scaling down the replication controller
Jul  8 02:56:12.750: INFO: scanned /root for discovery docs: <nil>
Jul  8 02:56:12.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jul  8 02:56:13.861: INFO: stderr: ""
Jul  8 02:56:13.861: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  8 02:56:13.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  8 02:56:13.956: INFO: stderr: ""
Jul  8 02:56:13.956: INFO: stdout: "update-demo-nautilus-5pdqk update-demo-nautilus-9b669 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul  8 02:56:18.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  8 02:56:19.049: INFO: stderr: ""
Jul  8 02:56:19.049: INFO: stdout: "update-demo-nautilus-5pdqk update-demo-nautilus-9b669 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul  8 02:56:24.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  8 02:56:24.145: INFO: stderr: ""
Jul  8 02:56:24.145: INFO: stdout: "update-demo-nautilus-9b669 "
Jul  8 02:56:24.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-9b669 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  8 02:56:24.237: INFO: stderr: ""
Jul  8 02:56:24.237: INFO: stdout: "true"
Jul  8 02:56:24.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-9b669 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  8 02:56:24.326: INFO: stderr: ""
Jul  8 02:56:24.326: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul  8 02:56:24.326: INFO: validating pod update-demo-nautilus-9b669
Jul  8 02:56:24.330: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  8 02:56:24.330: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  8 02:56:24.330: INFO: update-demo-nautilus-9b669 is verified up and running
STEP: scaling up the replication controller
Jul  8 02:56:24.332: INFO: scanned /root for discovery docs: <nil>
Jul  8 02:56:24.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jul  8 02:56:25.461: INFO: stderr: ""
Jul  8 02:56:25.461: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  8 02:56:25.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  8 02:56:25.594: INFO: stderr: ""
Jul  8 02:56:25.594: INFO: stdout: "update-demo-nautilus-9b669 update-demo-nautilus-h6d86 "
Jul  8 02:56:25.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-9b669 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  8 02:56:25.691: INFO: stderr: ""
Jul  8 02:56:25.691: INFO: stdout: "true"
Jul  8 02:56:25.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-9b669 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  8 02:56:25.788: INFO: stderr: ""
Jul  8 02:56:25.788: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul  8 02:56:25.788: INFO: validating pod update-demo-nautilus-9b669
Jul  8 02:56:25.794: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  8 02:56:25.794: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  8 02:56:25.794: INFO: update-demo-nautilus-9b669 is verified up and running
Jul  8 02:56:25.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-h6d86 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  8 02:56:25.947: INFO: stderr: ""
Jul  8 02:56:25.947: INFO: stdout: ""
Jul  8 02:56:25.947: INFO: update-demo-nautilus-h6d86 is created but not running
Jul  8 02:56:30.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  8 02:56:31.146: INFO: stderr: ""
Jul  8 02:56:31.146: INFO: stdout: "update-demo-nautilus-9b669 update-demo-nautilus-h6d86 "
Jul  8 02:56:31.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-9b669 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  8 02:56:31.248: INFO: stderr: ""
Jul  8 02:56:31.248: INFO: stdout: "true"
Jul  8 02:56:31.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-9b669 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  8 02:56:31.342: INFO: stderr: ""
Jul  8 02:56:31.342: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul  8 02:56:31.342: INFO: validating pod update-demo-nautilus-9b669
Jul  8 02:56:31.346: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  8 02:56:31.346: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  8 02:56:31.346: INFO: update-demo-nautilus-9b669 is verified up and running
Jul  8 02:56:31.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-h6d86 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  8 02:56:31.452: INFO: stderr: ""
Jul  8 02:56:31.452: INFO: stdout: "true"
Jul  8 02:56:31.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods update-demo-nautilus-h6d86 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  8 02:56:31.538: INFO: stderr: ""
Jul  8 02:56:31.538: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul  8 02:56:31.538: INFO: validating pod update-demo-nautilus-h6d86
Jul  8 02:56:31.542: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  8 02:56:31.542: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  8 02:56:31.542: INFO: update-demo-nautilus-h6d86 is verified up and running
STEP: using delete to clean up resources
Jul  8 02:56:31.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 delete --grace-period=0 --force -f -'
Jul  8 02:56:31.649: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  8 02:56:31.649: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul  8 02:56:31.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get rc,svc -l name=update-demo --no-headers'
Jul  8 02:56:31.806: INFO: stderr: "No resources found in kubectl-8858 namespace.\n"
Jul  8 02:56:31.806: INFO: stdout: ""
Jul  8 02:56:31.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-8858 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  8 02:56:31.962: INFO: stderr: ""
Jul  8 02:56:31.962: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:56:31.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8858" for this suite.

• [SLOW TEST:30.853 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":339,"completed":153,"skipped":2275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:56:31.977: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5988
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:56:32.137: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul  8 02:56:35.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-5988 --namespace=crd-publish-openapi-5988 create -f -'
Jul  8 02:56:36.397: INFO: stderr: ""
Jul  8 02:56:36.397: INFO: stdout: "e2e-test-crd-publish-openapi-5087-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul  8 02:56:36.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-5988 --namespace=crd-publish-openapi-5988 delete e2e-test-crd-publish-openapi-5087-crds test-cr'
Jul  8 02:56:36.516: INFO: stderr: ""
Jul  8 02:56:36.516: INFO: stdout: "e2e-test-crd-publish-openapi-5087-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul  8 02:56:36.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-5988 --namespace=crd-publish-openapi-5988 apply -f -'
Jul  8 02:56:36.753: INFO: stderr: ""
Jul  8 02:56:36.753: INFO: stdout: "e2e-test-crd-publish-openapi-5087-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul  8 02:56:36.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-5988 --namespace=crd-publish-openapi-5988 delete e2e-test-crd-publish-openapi-5087-crds test-cr'
Jul  8 02:56:36.866: INFO: stderr: ""
Jul  8 02:56:36.866: INFO: stdout: "e2e-test-crd-publish-openapi-5087-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul  8 02:56:36.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-5988 explain e2e-test-crd-publish-openapi-5087-crds'
Jul  8 02:56:37.173: INFO: stderr: ""
Jul  8 02:56:37.173: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5087-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:56:40.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5988" for this suite.

• [SLOW TEST:8.236 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":339,"completed":154,"skipped":2299,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:56:40.214: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8676
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jul  8 02:56:40.373: INFO: The status of Pod annotationupdatea8c7351c-5cef-4933-9029-877ba693a953 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:56:42.402: INFO: The status of Pod annotationupdatea8c7351c-5cef-4933-9029-877ba693a953 is Running (Ready = true)
Jul  8 02:56:42.939: INFO: Successfully updated pod "annotationupdatea8c7351c-5cef-4933-9029-877ba693a953"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:56:46.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8676" for this suite.

• [SLOW TEST:6.766 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":155,"skipped":2314,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:56:46.980: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6468
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-6468/configmap-test-706e14b4-ba29-477b-9bbc-8806fdf58402
STEP: Creating a pod to test consume configMaps
Jul  8 02:56:47.150: INFO: Waiting up to 5m0s for pod "pod-configmaps-8d76d745-948d-4a22-94bf-6dc0e27af1d7" in namespace "configmap-6468" to be "Succeeded or Failed"
Jul  8 02:56:47.155: INFO: Pod "pod-configmaps-8d76d745-948d-4a22-94bf-6dc0e27af1d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.356606ms
Jul  8 02:56:49.168: INFO: Pod "pod-configmaps-8d76d745-948d-4a22-94bf-6dc0e27af1d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017409812s
STEP: Saw pod success
Jul  8 02:56:49.168: INFO: Pod "pod-configmaps-8d76d745-948d-4a22-94bf-6dc0e27af1d7" satisfied condition "Succeeded or Failed"
Jul  8 02:56:49.174: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-configmaps-8d76d745-948d-4a22-94bf-6dc0e27af1d7 container env-test: <nil>
STEP: delete the pod
Jul  8 02:56:49.197: INFO: Waiting for pod pod-configmaps-8d76d745-948d-4a22-94bf-6dc0e27af1d7 to disappear
Jul  8 02:56:49.199: INFO: Pod pod-configmaps-8d76d745-948d-4a22-94bf-6dc0e27af1d7 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:56:49.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6468" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":156,"skipped":2325,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:56:49.212: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9320
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:57:49.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9320" for this suite.

• [SLOW TEST:60.210 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":339,"completed":157,"skipped":2358,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:57:49.422: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 02:57:49.570: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul  8 02:57:49.583: INFO: The status of Pod pod-exec-websocket-2fdbd27f-9a1e-4a3e-9499-960b4e7e48f0 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 02:57:51.590: INFO: The status of Pod pod-exec-websocket-2fdbd27f-9a1e-4a3e-9499-960b4e7e48f0 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 02:57:51.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6533" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":339,"completed":158,"skipped":2377,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 02:57:51.667: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6748
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-09f93670-c36e-4ff2-a0bc-88955434cd24 in namespace container-probe-6748
Jul  8 02:57:53.828: INFO: Started pod liveness-09f93670-c36e-4ff2-a0bc-88955434cd24 in namespace container-probe-6748
STEP: checking the pod's current state and verifying that restartCount is present
Jul  8 02:57:53.831: INFO: Initial restart count of pod liveness-09f93670-c36e-4ff2-a0bc-88955434cd24 is 0
Jul  8 02:58:13.914: INFO: Restart count of pod container-probe-6748/liveness-09f93670-c36e-4ff2-a0bc-88955434cd24 is now 1 (20.083406462s elapsed)
Jul  8 02:58:33.999: INFO: Restart count of pod container-probe-6748/liveness-09f93670-c36e-4ff2-a0bc-88955434cd24 is now 2 (40.168208192s elapsed)
Jul  8 02:58:54.078: INFO: Restart count of pod container-probe-6748/liveness-09f93670-c36e-4ff2-a0bc-88955434cd24 is now 3 (1m0.247503742s elapsed)
Jul  8 02:59:14.160: INFO: Restart count of pod container-probe-6748/liveness-09f93670-c36e-4ff2-a0bc-88955434cd24 is now 4 (1m20.328724724s elapsed)
Jul  8 03:00:14.389: INFO: Restart count of pod container-probe-6748/liveness-09f93670-c36e-4ff2-a0bc-88955434cd24 is now 5 (2m20.557622387s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:00:14.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6748" for this suite.

• [SLOW TEST:142.766 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":339,"completed":159,"skipped":2388,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:00:14.433: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9540
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Jul  8 03:02:15.141: INFO: Successfully updated pod "var-expansion-50b22602-030a-4ffd-97c2-05639c02bad2"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jul  8 03:02:17.157: INFO: Deleting pod "var-expansion-50b22602-030a-4ffd-97c2-05639c02bad2" in namespace "var-expansion-9540"
Jul  8 03:02:17.164: INFO: Wait up to 5m0s for pod "var-expansion-50b22602-030a-4ffd-97c2-05639c02bad2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:02:49.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9540" for this suite.

• [SLOW TEST:154.763 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":339,"completed":160,"skipped":2390,"failed":0}
SS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:02:49.196: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Jul  8 03:02:49.379: INFO: Found Service test-service-dhs8n in namespace services-8235 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jul  8 03:02:49.379: INFO: Service test-service-dhs8n created
STEP: Getting /status
Jul  8 03:02:49.384: INFO: Service test-service-dhs8n has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jul  8 03:02:49.393: INFO: observed Service test-service-dhs8n in namespace services-8235 with annotations: map[] & LoadBalancer: {[]}
Jul  8 03:02:49.393: INFO: Found Service test-service-dhs8n in namespace services-8235 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jul  8 03:02:49.393: INFO: Service test-service-dhs8n has service status patched
STEP: updating the ServiceStatus
Jul  8 03:02:49.403: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jul  8 03:02:49.404: INFO: Observed Service test-service-dhs8n in namespace services-8235 with annotations: map[] & Conditions: {[]}
Jul  8 03:02:49.404: INFO: Observed event: &Service{ObjectMeta:{test-service-dhs8n  services-8235  fe55dc09-44ad-491b-ad34-b564382a4d06 38373 0 2021-07-08 03:02:49 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2021-07-08 03:02:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}},"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}}}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.43.172.41,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,TopologyKeys:[],IPFamilyPolicy:*SingleStack,ClusterIPs:[10.43.172.41],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:nil,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jul  8 03:02:49.404: INFO: Found Service test-service-dhs8n in namespace services-8235 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul  8 03:02:49.404: INFO: Service test-service-dhs8n has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jul  8 03:02:49.428: INFO: observed Service test-service-dhs8n in namespace services-8235 with labels: map[test-service-static:true]
Jul  8 03:02:49.428: INFO: observed Service test-service-dhs8n in namespace services-8235 with labels: map[test-service-static:true]
Jul  8 03:02:49.428: INFO: observed Service test-service-dhs8n in namespace services-8235 with labels: map[test-service-static:true]
Jul  8 03:02:49.428: INFO: Found Service test-service-dhs8n in namespace services-8235 with labels: map[test-service:patched test-service-static:true]
Jul  8 03:02:49.428: INFO: Service test-service-dhs8n patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jul  8 03:02:49.448: INFO: Observed event: ADDED
Jul  8 03:02:49.449: INFO: Observed event: MODIFIED
Jul  8 03:02:49.449: INFO: Observed event: MODIFIED
Jul  8 03:02:49.449: INFO: Observed event: MODIFIED
Jul  8 03:02:49.449: INFO: Found Service test-service-dhs8n in namespace services-8235 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jul  8 03:02:49.449: INFO: Service test-service-dhs8n deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:02:49.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8235" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":339,"completed":161,"skipped":2392,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:02:49.464: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-150a4386-f17d-4460-b521-93a9046c36a4 in namespace container-probe-5167
Jul  8 03:02:51.625: INFO: Started pod busybox-150a4386-f17d-4460-b521-93a9046c36a4 in namespace container-probe-5167
STEP: checking the pod's current state and verifying that restartCount is present
Jul  8 03:02:51.627: INFO: Initial restart count of pod busybox-150a4386-f17d-4460-b521-93a9046c36a4 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:06:52.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5167" for this suite.

• [SLOW TEST:243.196 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":162,"skipped":2425,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:06:52.660: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6227
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul  8 03:06:52.836: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6227  7f6035d6-263c-46f7-864a-be80df570480 38983 0 2021-07-08 03:06:52 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-07-08 03:06:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 03:06:52.836: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6227  7f6035d6-263c-46f7-864a-be80df570480 38984 0 2021-07-08 03:06:52 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-07-08 03:06:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:06:52.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6227" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":339,"completed":163,"skipped":2436,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:06:52.849: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8057
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:06:53.001: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-12b97627-1a27-4ed3-9a97-a1e3751a5734" in namespace "security-context-test-8057" to be "Succeeded or Failed"
Jul  8 03:06:53.005: INFO: Pod "busybox-privileged-false-12b97627-1a27-4ed3-9a97-a1e3751a5734": Phase="Pending", Reason="", readiness=false. Elapsed: 3.748047ms
Jul  8 03:06:55.015: INFO: Pod "busybox-privileged-false-12b97627-1a27-4ed3-9a97-a1e3751a5734": Phase="Running", Reason="", readiness=true. Elapsed: 2.014188576s
Jul  8 03:06:57.022: INFO: Pod "busybox-privileged-false-12b97627-1a27-4ed3-9a97-a1e3751a5734": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020625222s
Jul  8 03:06:57.022: INFO: Pod "busybox-privileged-false-12b97627-1a27-4ed3-9a97-a1e3751a5734" satisfied condition "Succeeded or Failed"
Jul  8 03:06:57.033: INFO: Got logs for pod "busybox-privileged-false-12b97627-1a27-4ed3-9a97-a1e3751a5734": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:06:57.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8057" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":164,"skipped":2461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:06:57.043: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2596
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Jul  8 03:06:57.191: INFO: Waiting up to 5m0s for pod "client-containers-15db3982-3d2e-4e95-8760-55771ddeb868" in namespace "containers-2596" to be "Succeeded or Failed"
Jul  8 03:06:57.196: INFO: Pod "client-containers-15db3982-3d2e-4e95-8760-55771ddeb868": Phase="Pending", Reason="", readiness=false. Elapsed: 4.740681ms
Jul  8 03:06:59.204: INFO: Pod "client-containers-15db3982-3d2e-4e95-8760-55771ddeb868": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012836277s
STEP: Saw pod success
Jul  8 03:06:59.204: INFO: Pod "client-containers-15db3982-3d2e-4e95-8760-55771ddeb868" satisfied condition "Succeeded or Failed"
Jul  8 03:06:59.206: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod client-containers-15db3982-3d2e-4e95-8760-55771ddeb868 container agnhost-container: <nil>
STEP: delete the pod
Jul  8 03:06:59.225: INFO: Waiting for pod client-containers-15db3982-3d2e-4e95-8760-55771ddeb868 to disappear
Jul  8 03:06:59.227: INFO: Pod client-containers-15db3982-3d2e-4e95-8760-55771ddeb868 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:06:59.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2596" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":339,"completed":165,"skipped":2498,"failed":0}
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:06:59.238: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7995
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jul  8 03:06:59.380: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:07:03.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7995" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":339,"completed":166,"skipped":2501,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:07:03.601: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Jul  8 03:07:04.294: INFO: created pod pod-service-account-defaultsa
Jul  8 03:07:04.294: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul  8 03:07:04.310: INFO: created pod pod-service-account-mountsa
Jul  8 03:07:04.311: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul  8 03:07:04.323: INFO: created pod pod-service-account-nomountsa
Jul  8 03:07:04.323: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul  8 03:07:04.333: INFO: created pod pod-service-account-defaultsa-mountspec
Jul  8 03:07:04.333: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul  8 03:07:04.357: INFO: created pod pod-service-account-mountsa-mountspec
Jul  8 03:07:04.358: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul  8 03:07:04.372: INFO: created pod pod-service-account-nomountsa-mountspec
Jul  8 03:07:04.373: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul  8 03:07:04.402: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul  8 03:07:04.402: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul  8 03:07:04.421: INFO: created pod pod-service-account-mountsa-nomountspec
Jul  8 03:07:04.421: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul  8 03:07:04.437: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul  8 03:07:04.437: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:07:04.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3531" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":339,"completed":167,"skipped":2517,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:07:04.482: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:07:04.683: INFO: The status of Pod busybox-scheduling-2978e4a8-475b-4a1d-b37b-6a1ca35f57cf is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:07:06.699: INFO: The status of Pod busybox-scheduling-2978e4a8-475b-4a1d-b37b-6a1ca35f57cf is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:07:08.701: INFO: The status of Pod busybox-scheduling-2978e4a8-475b-4a1d-b37b-6a1ca35f57cf is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:07:08.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2757" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":339,"completed":168,"skipped":2542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:07:08.728: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-351c0f84-0f37-4893-bbd4-f6407b9681ff
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:07:08.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4392" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":339,"completed":169,"skipped":2581,"failed":0}
SSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:07:08.890: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-5177
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5177
STEP: Deleting pre-stop pod
Jul  8 03:07:20.102: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:07:20.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5177" for this suite.

• [SLOW TEST:11.265 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":339,"completed":170,"skipped":2589,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:07:20.156: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jul  8 03:07:20.313: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:07:20.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8331" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":339,"completed":171,"skipped":2598,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:07:20.376: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:07:21.552: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul  8 03:07:23.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761310441, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761310441, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761310441, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761310441, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:07:26.582: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:07:26.588: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:07:29.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-585" for this suite.
STEP: Destroying namespace "webhook-585-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.543 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":339,"completed":172,"skipped":2599,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:07:29.919: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:07:30.178: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul  8 03:07:35.188: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  8 03:07:35.188: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul  8 03:07:35.206: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4222  b3cfbd0c-5f04-47bd-aa28-f0883260cb33 39532 1 2021-07-08 03:07:35 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-07-08 03:07:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00510e1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jul  8 03:07:35.211: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-4222  2d899f7e-ae73-4e22-a518-1daab8e09bc1 39534 1 2021-07-08 03:07:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b3cfbd0c-5f04-47bd-aa28-f0883260cb33 0xc002220617 0xc002220618}] []  [{kube-controller-manager Update apps/v1 2021-07-08 03:07:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3cfbd0c-5f04-47bd-aa28-f0883260cb33\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0022206a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  8 03:07:35.211: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jul  8 03:07:35.211: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4222  70ddd4c8-cafe-4d1c-97de-248c8ea3c7d2 39533 1 2021-07-08 03:07:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment b3cfbd0c-5f04-47bd-aa28-f0883260cb33 0xc0022204c7 0xc0022204c8}] []  [{e2e.test Update apps/v1 2021-07-08 03:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-08 03:07:35 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"b3cfbd0c-5f04-47bd-aa28-f0883260cb33\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0022205a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul  8 03:07:35.221: INFO: Pod "test-cleanup-controller-62ls8" is available:
&Pod{ObjectMeta:{test-cleanup-controller-62ls8 test-cleanup-controller- deployment-4222  2319dafe-b1c8-4b7f-a203-8717dd5da1ef 39502 0 2021-07-08 03:07:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.42.3.161/32 cni.projectcalico.org/podIPs:10.42.3.161/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 70ddd4c8-cafe-4d1c-97de-248c8ea3c7d2 0xc00510e7b7 0xc00510e7b8}] []  [{kube-controller-manager Update v1 2021-07-08 03:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70ddd4c8-cafe-4d1c-97de-248c8ea3c7d2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:07:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rbxrg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbxrg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:07:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:07:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.217,PodIP:10.42.3.161,StartTime:2021-07-08 03:07:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:07:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://99bb2faab2173e22ac3bf75071385df0a0bac1f5a4c30508ed511707cb53850b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:07:35.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4222" for this suite.

• [SLOW TEST:5.366 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":339,"completed":173,"skipped":2619,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:07:35.285: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jul  8 03:07:35.446: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:07:38.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1387" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":339,"completed":174,"skipped":2628,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:07:38.673: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1229
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-894cb67c-f161-4a05-b3be-67c6fc1228bf
STEP: Creating a pod to test consume configMaps
Jul  8 03:07:38.833: INFO: Waiting up to 5m0s for pod "pod-configmaps-43a74f47-e27d-4d15-9dac-cd8bcbce195e" in namespace "configmap-1229" to be "Succeeded or Failed"
Jul  8 03:07:38.836: INFO: Pod "pod-configmaps-43a74f47-e27d-4d15-9dac-cd8bcbce195e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.092786ms
Jul  8 03:07:40.840: INFO: Pod "pod-configmaps-43a74f47-e27d-4d15-9dac-cd8bcbce195e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007263495s
STEP: Saw pod success
Jul  8 03:07:40.840: INFO: Pod "pod-configmaps-43a74f47-e27d-4d15-9dac-cd8bcbce195e" satisfied condition "Succeeded or Failed"
Jul  8 03:07:40.842: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-configmaps-43a74f47-e27d-4d15-9dac-cd8bcbce195e container agnhost-container: <nil>
STEP: delete the pod
Jul  8 03:07:40.863: INFO: Waiting for pod pod-configmaps-43a74f47-e27d-4d15-9dac-cd8bcbce195e to disappear
Jul  8 03:07:40.876: INFO: Pod pod-configmaps-43a74f47-e27d-4d15-9dac-cd8bcbce195e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:07:40.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1229" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":175,"skipped":2632,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:07:40.889: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jul  8 03:07:41.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9326 create -f -'
Jul  8 03:07:41.569: INFO: stderr: ""
Jul  8 03:07:41.569: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  8 03:07:41.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9326 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  8 03:07:41.666: INFO: stderr: ""
Jul  8 03:07:41.666: INFO: stdout: "update-demo-nautilus-7t6nm update-demo-nautilus-8jt56 "
Jul  8 03:07:41.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9326 get pods update-demo-nautilus-7t6nm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  8 03:07:41.744: INFO: stderr: ""
Jul  8 03:07:41.744: INFO: stdout: ""
Jul  8 03:07:41.744: INFO: update-demo-nautilus-7t6nm is created but not running
Jul  8 03:07:46.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9326 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  8 03:07:46.892: INFO: stderr: ""
Jul  8 03:07:46.892: INFO: stdout: "update-demo-nautilus-7t6nm update-demo-nautilus-8jt56 "
Jul  8 03:07:46.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9326 get pods update-demo-nautilus-7t6nm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  8 03:07:46.979: INFO: stderr: ""
Jul  8 03:07:46.979: INFO: stdout: "true"
Jul  8 03:07:46.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9326 get pods update-demo-nautilus-7t6nm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  8 03:07:47.077: INFO: stderr: ""
Jul  8 03:07:47.077: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul  8 03:07:47.077: INFO: validating pod update-demo-nautilus-7t6nm
Jul  8 03:07:47.089: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  8 03:07:47.089: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  8 03:07:47.089: INFO: update-demo-nautilus-7t6nm is verified up and running
Jul  8 03:07:47.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9326 get pods update-demo-nautilus-8jt56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  8 03:07:47.179: INFO: stderr: ""
Jul  8 03:07:47.179: INFO: stdout: "true"
Jul  8 03:07:47.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9326 get pods update-demo-nautilus-8jt56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  8 03:07:47.278: INFO: stderr: ""
Jul  8 03:07:47.278: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul  8 03:07:47.278: INFO: validating pod update-demo-nautilus-8jt56
Jul  8 03:07:47.282: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  8 03:07:47.282: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  8 03:07:47.282: INFO: update-demo-nautilus-8jt56 is verified up and running
STEP: using delete to clean up resources
Jul  8 03:07:47.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9326 delete --grace-period=0 --force -f -'
Jul  8 03:07:47.383: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  8 03:07:47.383: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul  8 03:07:47.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9326 get rc,svc -l name=update-demo --no-headers'
Jul  8 03:07:47.574: INFO: stderr: "No resources found in kubectl-9326 namespace.\n"
Jul  8 03:07:47.574: INFO: stdout: ""
Jul  8 03:07:47.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9326 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  8 03:07:47.727: INFO: stderr: ""
Jul  8 03:07:47.727: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:07:47.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9326" for this suite.

• [SLOW TEST:6.861 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":339,"completed":176,"skipped":2648,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:07:47.751: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:07:58.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4325" for this suite.

• [SLOW TEST:11.216 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":339,"completed":177,"skipped":2670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:07:58.967: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8251
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Jul  8 03:07:59.117: INFO: Waiting up to 5m0s for pod "var-expansion-9184a170-8bff-4abb-957e-7aab720a5191" in namespace "var-expansion-8251" to be "Succeeded or Failed"
Jul  8 03:07:59.128: INFO: Pod "var-expansion-9184a170-8bff-4abb-957e-7aab720a5191": Phase="Pending", Reason="", readiness=false. Elapsed: 11.444656ms
Jul  8 03:08:01.136: INFO: Pod "var-expansion-9184a170-8bff-4abb-957e-7aab720a5191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018714311s
STEP: Saw pod success
Jul  8 03:08:01.136: INFO: Pod "var-expansion-9184a170-8bff-4abb-957e-7aab720a5191" satisfied condition "Succeeded or Failed"
Jul  8 03:08:01.144: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod var-expansion-9184a170-8bff-4abb-957e-7aab720a5191 container dapi-container: <nil>
STEP: delete the pod
Jul  8 03:08:01.183: INFO: Waiting for pod var-expansion-9184a170-8bff-4abb-957e-7aab720a5191 to disappear
Jul  8 03:08:01.189: INFO: Pod var-expansion-9184a170-8bff-4abb-957e-7aab720a5191 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:08:01.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8251" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":339,"completed":178,"skipped":2695,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:08:01.205: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:08:18.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6324" for this suite.

• [SLOW TEST:17.260 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":339,"completed":179,"skipped":2708,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:08:18.467: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-7695
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Jul  8 03:08:18.612: INFO: Major version: 1
STEP: Confirm minor version
Jul  8 03:08:18.612: INFO: cleanMinorVersion: 21
Jul  8 03:08:18.613: INFO: Minor version: 21
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:08:18.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7695" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":339,"completed":180,"skipped":2753,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:08:18.626: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jul  8 03:08:18.776: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  8 03:08:18.776: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  8 03:08:18.787: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  8 03:08:18.787: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  8 03:08:18.801: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  8 03:08:18.801: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  8 03:08:18.871: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  8 03:08:18.871: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  8 03:08:19.768: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul  8 03:08:19.768: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul  8 03:08:20.560: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jul  8 03:08:20.577: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jul  8 03:08:20.578: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0
Jul  8 03:08:20.578: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0
Jul  8 03:08:20.578: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0
Jul  8 03:08:20.578: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0
Jul  8 03:08:20.578: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0
Jul  8 03:08:20.578: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0
Jul  8 03:08:20.578: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0
Jul  8 03:08:20.578: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 0
Jul  8 03:08:20.579: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
Jul  8 03:08:20.579: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
Jul  8 03:08:20.579: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:20.579: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:20.579: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:20.579: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:20.592: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:20.592: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:20.637: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:20.637: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:20.710: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
Jul  8 03:08:20.710: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
Jul  8 03:08:20.726: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
Jul  8 03:08:20.726: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
Jul  8 03:08:21.778: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:21.779: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:21.824: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
STEP: listing Deployments
Jul  8 03:08:21.829: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jul  8 03:08:21.846: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jul  8 03:08:21.863: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul  8 03:08:21.863: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul  8 03:08:21.883: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul  8 03:08:21.907: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul  8 03:08:21.959: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul  8 03:08:21.969: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul  8 03:08:22.887: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul  8 03:08:22.917: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul  8 03:08:22.939: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul  8 03:08:22.971: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul  8 03:08:22.979: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul  8 03:08:24.761: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jul  8 03:08:24.838: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
Jul  8 03:08:24.839: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
Jul  8 03:08:24.839: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
Jul  8 03:08:24.839: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
Jul  8 03:08:24.839: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
Jul  8 03:08:24.839: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 1
Jul  8 03:08:24.839: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:24.839: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:24.839: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:24.839: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:24.840: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 2
Jul  8 03:08:24.840: INFO: observed Deployment test-deployment in namespace deployment-6148 with ReadyReplicas 3
STEP: deleting the Deployment
Jul  8 03:08:24.851: INFO: observed event type MODIFIED
Jul  8 03:08:24.851: INFO: observed event type MODIFIED
Jul  8 03:08:24.851: INFO: observed event type MODIFIED
Jul  8 03:08:24.851: INFO: observed event type MODIFIED
Jul  8 03:08:24.851: INFO: observed event type MODIFIED
Jul  8 03:08:24.851: INFO: observed event type MODIFIED
Jul  8 03:08:24.851: INFO: observed event type MODIFIED
Jul  8 03:08:24.851: INFO: observed event type MODIFIED
Jul  8 03:08:24.852: INFO: observed event type MODIFIED
Jul  8 03:08:24.852: INFO: observed event type MODIFIED
Jul  8 03:08:24.852: INFO: observed event type MODIFIED
Jul  8 03:08:24.852: INFO: observed event type MODIFIED
Jul  8 03:08:24.852: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul  8 03:08:24.874: INFO: Log out all the ReplicaSets if there is no deployment created
Jul  8 03:08:24.880: INFO: ReplicaSet "test-deployment-748588b7cd":
&ReplicaSet{ObjectMeta:{test-deployment-748588b7cd  deployment-6148  c4ec3580-7790-4e70-a9f6-e5e03658492e 40077 4 2021-07-08 03:08:20 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 55a17401-19ef-4beb-8063-8b9fa092c000 0xc00595dc47 0xc00595dc48}] []  [{kube-controller-manager Update apps/v1 2021-07-08 03:08:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55a17401-19ef-4beb-8063-8b9fa092c000\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 748588b7cd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.4.1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00595dcb0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jul  8 03:08:24.883: INFO: ReplicaSet "test-deployment-7b4c744884":
&ReplicaSet{ObjectMeta:{test-deployment-7b4c744884  deployment-6148  5f856914-91fe-44b2-a9be-75490140aab6 39979 3 2021-07-08 03:08:18 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 55a17401-19ef-4beb-8063-8b9fa092c000 0xc00595dd17 0xc00595dd18}] []  [{kube-controller-manager Update apps/v1 2021-07-08 03:08:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55a17401-19ef-4beb-8063-8b9fa092c000\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b4c744884,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00595dd80 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jul  8 03:08:24.886: INFO: ReplicaSet "test-deployment-85d87c6f4b":
&ReplicaSet{ObjectMeta:{test-deployment-85d87c6f4b  deployment-6148  daec0b7a-dc20-4c5b-b13b-32ac3b532178 40068 2 2021-07-08 03:08:21 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 55a17401-19ef-4beb-8063-8b9fa092c000 0xc00595dde7 0xc00595dde8}] []  [{kube-controller-manager Update apps/v1 2021-07-08 03:08:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55a17401-19ef-4beb-8063-8b9fa092c000\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 85d87c6f4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00595de50 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jul  8 03:08:24.890: INFO: pod: "test-deployment-85d87c6f4b-mgwv2":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-mgwv2 test-deployment-85d87c6f4b- deployment-6148  89a67408-c7ee-45a5-8d0c-0832cd8af1b6 40020 0 2021-07-08 03:08:21 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[cni.projectcalico.org/podIP:10.42.3.168/32 cni.projectcalico.org/podIPs:10.42.3.168/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b daec0b7a-dc20-4c5b-b13b-32ac3b532178 0xc002c58697 0xc002c58698}] []  [{kube-controller-manager Update v1 2021-07-08 03:08:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daec0b7a-dc20-4c5b-b13b-32ac3b532178\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:08:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:08:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zzhbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zzhbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:08:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:08:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:08:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:08:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.217,PodIP:10.42.3.168,StartTime:2021-07-08 03:08:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:08:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://076f80f69231ef591290928bbf7a1277ec0899c195d161e9d6ec707e808b34d6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jul  8 03:08:24.891: INFO: pod: "test-deployment-85d87c6f4b-pmdfm":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-pmdfm test-deployment-85d87c6f4b- deployment-6148  a47f8bc9-6aa2-4f53-b94a-123e598c740e 40067 0 2021-07-08 03:08:22 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[cni.projectcalico.org/podIP:10.42.2.28/32 cni.projectcalico.org/podIPs:10.42.2.28/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b daec0b7a-dc20-4c5b-b13b-32ac3b532178 0xc002c588a7 0xc002c588a8}] []  [{kube-controller-manager Update v1 2021-07-08 03:08:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daec0b7a-dc20-4c5b-b13b-32ac3b532178\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:08:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:08:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xwxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xwxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-165.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:08:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:08:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:08:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:08:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.165,PodIP:10.42.2.28,StartTime:2021-07-08 03:08:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:08:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://0c6911d9080cc3fee7abc1a800eab8066bbb953248c92ac004afc9c6177f0ccb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:08:24.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6148" for this suite.

• [SLOW TEST:6.287 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":339,"completed":181,"skipped":2773,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:08:24.913: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8731
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 03:08:25.087: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db3af3ab-9c96-4e52-bb22-9562e065a028" in namespace "projected-8731" to be "Succeeded or Failed"
Jul  8 03:08:25.107: INFO: Pod "downwardapi-volume-db3af3ab-9c96-4e52-bb22-9562e065a028": Phase="Pending", Reason="", readiness=false. Elapsed: 20.194158ms
Jul  8 03:08:27.114: INFO: Pod "downwardapi-volume-db3af3ab-9c96-4e52-bb22-9562e065a028": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026873794s
STEP: Saw pod success
Jul  8 03:08:27.114: INFO: Pod "downwardapi-volume-db3af3ab-9c96-4e52-bb22-9562e065a028" satisfied condition "Succeeded or Failed"
Jul  8 03:08:27.116: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-db3af3ab-9c96-4e52-bb22-9562e065a028 container client-container: <nil>
STEP: delete the pod
Jul  8 03:08:27.146: INFO: Waiting for pod downwardapi-volume-db3af3ab-9c96-4e52-bb22-9562e065a028 to disappear
Jul  8 03:08:27.151: INFO: Pod downwardapi-volume-db3af3ab-9c96-4e52-bb22-9562e065a028 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:08:27.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8731" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":182,"skipped":2789,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:08:27.159: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4688
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:08:27.309: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul  8 03:08:27.322: INFO: The status of Pod pod-logs-websocket-8ae65a46-f31a-4e22-80ec-c032b351aa5d is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:08:29.329: INFO: The status of Pod pod-logs-websocket-8ae65a46-f31a-4e22-80ec-c032b351aa5d is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:08:29.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4688" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":339,"completed":183,"skipped":2794,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:08:29.357: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6849
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0708 03:09:09.570228      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul  8 03:14:09.579: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Jul  8 03:14:09.579: INFO: Deleting pod "simpletest.rc-2ck5q" in namespace "gc-6849"
Jul  8 03:14:09.597: INFO: Deleting pod "simpletest.rc-2vgmw" in namespace "gc-6849"
Jul  8 03:14:09.621: INFO: Deleting pod "simpletest.rc-4rt6f" in namespace "gc-6849"
Jul  8 03:14:09.651: INFO: Deleting pod "simpletest.rc-d6hws" in namespace "gc-6849"
Jul  8 03:14:09.719: INFO: Deleting pod "simpletest.rc-j5s95" in namespace "gc-6849"
Jul  8 03:14:09.779: INFO: Deleting pod "simpletest.rc-nrnmd" in namespace "gc-6849"
Jul  8 03:14:09.817: INFO: Deleting pod "simpletest.rc-nv62p" in namespace "gc-6849"
Jul  8 03:14:09.849: INFO: Deleting pod "simpletest.rc-qqb56" in namespace "gc-6849"
Jul  8 03:14:09.914: INFO: Deleting pod "simpletest.rc-vv8rj" in namespace "gc-6849"
Jul  8 03:14:09.941: INFO: Deleting pod "simpletest.rc-zczdk" in namespace "gc-6849"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:14:09.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6849" for this suite.

• [SLOW TEST:340.702 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":339,"completed":184,"skipped":2801,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:14:10.060: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-870
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1238
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8895
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:14:39.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-870" for this suite.
STEP: Destroying namespace "nsdeletetest-1238" for this suite.
Jul  8 03:14:39.595: INFO: Namespace nsdeletetest-1238 was already deleted
STEP: Destroying namespace "nsdeletetest-8895" for this suite.

• [SLOW TEST:29.540 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":339,"completed":185,"skipped":2811,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:14:39.604: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8389
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:14:39.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8389" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":339,"completed":186,"skipped":2825,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:14:39.785: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3949
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:14:45.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3949" for this suite.

• [SLOW TEST:6.170 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":339,"completed":187,"skipped":2830,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:14:45.955: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-pc6m
STEP: Creating a pod to test atomic-volume-subpath
Jul  8 03:14:46.111: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pc6m" in namespace "subpath-2981" to be "Succeeded or Failed"
Jul  8 03:14:46.118: INFO: Pod "pod-subpath-test-secret-pc6m": Phase="Pending", Reason="", readiness=false. Elapsed: 7.532137ms
Jul  8 03:14:48.125: INFO: Pod "pod-subpath-test-secret-pc6m": Phase="Running", Reason="", readiness=true. Elapsed: 2.01397366s
Jul  8 03:14:50.131: INFO: Pod "pod-subpath-test-secret-pc6m": Phase="Running", Reason="", readiness=true. Elapsed: 4.019960221s
Jul  8 03:14:52.141: INFO: Pod "pod-subpath-test-secret-pc6m": Phase="Running", Reason="", readiness=true. Elapsed: 6.030308848s
Jul  8 03:14:54.151: INFO: Pod "pod-subpath-test-secret-pc6m": Phase="Running", Reason="", readiness=true. Elapsed: 8.039934864s
Jul  8 03:14:56.158: INFO: Pod "pod-subpath-test-secret-pc6m": Phase="Running", Reason="", readiness=true. Elapsed: 10.047046951s
Jul  8 03:14:58.165: INFO: Pod "pod-subpath-test-secret-pc6m": Phase="Running", Reason="", readiness=true. Elapsed: 12.054637652s
Jul  8 03:15:00.179: INFO: Pod "pod-subpath-test-secret-pc6m": Phase="Running", Reason="", readiness=true. Elapsed: 14.067953674s
Jul  8 03:15:02.261: INFO: Pod "pod-subpath-test-secret-pc6m": Phase="Running", Reason="", readiness=true. Elapsed: 16.150130229s
Jul  8 03:15:04.270: INFO: Pod "pod-subpath-test-secret-pc6m": Phase="Running", Reason="", readiness=true. Elapsed: 18.158955628s
Jul  8 03:15:06.277: INFO: Pod "pod-subpath-test-secret-pc6m": Phase="Running", Reason="", readiness=true. Elapsed: 20.166051394s
Jul  8 03:15:08.284: INFO: Pod "pod-subpath-test-secret-pc6m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.173309866s
STEP: Saw pod success
Jul  8 03:15:08.284: INFO: Pod "pod-subpath-test-secret-pc6m" satisfied condition "Succeeded or Failed"
Jul  8 03:15:08.286: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-subpath-test-secret-pc6m container test-container-subpath-secret-pc6m: <nil>
STEP: delete the pod
Jul  8 03:15:08.311: INFO: Waiting for pod pod-subpath-test-secret-pc6m to disappear
Jul  8 03:15:08.316: INFO: Pod pod-subpath-test-secret-pc6m no longer exists
STEP: Deleting pod pod-subpath-test-secret-pc6m
Jul  8 03:15:08.316: INFO: Deleting pod "pod-subpath-test-secret-pc6m" in namespace "subpath-2981"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:15:08.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2981" for this suite.

• [SLOW TEST:22.371 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":339,"completed":188,"skipped":2845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:15:08.327: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5282
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-7f34997f-e4e1-4eeb-90ce-63e69617d77a
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:15:12.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5282" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":189,"skipped":2875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:15:12.534: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul  8 03:15:15.219: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5371 pod-service-account-90fe5930-7e29-4a86-a7ec-d93236a8cc4f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul  8 03:15:15.372: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5371 pod-service-account-90fe5930-7e29-4a86-a7ec-d93236a8cc4f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul  8 03:15:15.536: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5371 pod-service-account-90fe5930-7e29-4a86-a7ec-d93236a8cc4f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:15:15.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5371" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":339,"completed":190,"skipped":2925,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:15:15.714: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4483
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-4483
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jul  8 03:15:15.879: INFO: Found 0 stateful pods, waiting for 3
Jul  8 03:15:25.889: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 03:15:25.889: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 03:15:25.889: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 03:15:25.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-4483 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  8 03:15:26.120: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  8 03:15:26.120: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  8 03:15:26.120: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Jul  8 03:15:36.161: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul  8 03:15:46.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-4483 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 03:15:46.569: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  8 03:15:46.569: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  8 03:15:46.569: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  8 03:16:06.594: INFO: Waiting for StatefulSet statefulset-4483/ss2 to complete update
Jul  8 03:16:06.594: INFO: Waiting for Pod statefulset-4483/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Rolling back to a previous revision
Jul  8 03:16:16.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-4483 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  8 03:16:16.847: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  8 03:16:16.847: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  8 03:16:16.847: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  8 03:16:26.889: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul  8 03:16:36.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-4483 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 03:16:37.133: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  8 03:16:37.133: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  8 03:16:37.133: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  8 03:16:47.160: INFO: Waiting for StatefulSet statefulset-4483/ss2 to complete update
Jul  8 03:16:47.160: INFO: Waiting for Pod statefulset-4483/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Jul  8 03:16:57.178: INFO: Waiting for StatefulSet statefulset-4483/ss2 to complete update
Jul  8 03:16:57.178: INFO: Waiting for Pod statefulset-4483/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul  8 03:17:07.173: INFO: Deleting all statefulset in ns statefulset-4483
Jul  8 03:17:07.177: INFO: Scaling statefulset ss2 to 0
Jul  8 03:17:17.205: INFO: Waiting for statefulset status.replicas updated to 0
Jul  8 03:17:17.207: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:17:17.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4483" for this suite.

• [SLOW TEST:121.546 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":339,"completed":191,"skipped":2936,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:17:17.260: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5351
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul  8 03:17:17.698: INFO: Pod name wrapped-volume-race-b02d5f5a-fa11-4eb4-a4f3-0607c6dcb21c: Found 0 pods out of 5
Jul  8 03:17:22.711: INFO: Pod name wrapped-volume-race-b02d5f5a-fa11-4eb4-a4f3-0607c6dcb21c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b02d5f5a-fa11-4eb4-a4f3-0607c6dcb21c in namespace emptydir-wrapper-5351, will wait for the garbage collector to delete the pods
Jul  8 03:17:32.799: INFO: Deleting ReplicationController wrapped-volume-race-b02d5f5a-fa11-4eb4-a4f3-0607c6dcb21c took: 7.449132ms
Jul  8 03:17:33.100: INFO: Terminating ReplicationController wrapped-volume-race-b02d5f5a-fa11-4eb4-a4f3-0607c6dcb21c pods took: 300.877918ms
STEP: Creating RC which spawns configmap-volume pods
Jul  8 03:17:48.134: INFO: Pod name wrapped-volume-race-fc46aee4-d7d8-4c26-bf56-1ed157ec999c: Found 0 pods out of 5
Jul  8 03:17:53.149: INFO: Pod name wrapped-volume-race-fc46aee4-d7d8-4c26-bf56-1ed157ec999c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fc46aee4-d7d8-4c26-bf56-1ed157ec999c in namespace emptydir-wrapper-5351, will wait for the garbage collector to delete the pods
Jul  8 03:18:13.232: INFO: Deleting ReplicationController wrapped-volume-race-fc46aee4-d7d8-4c26-bf56-1ed157ec999c took: 6.785827ms
Jul  8 03:18:13.332: INFO: Terminating ReplicationController wrapped-volume-race-fc46aee4-d7d8-4c26-bf56-1ed157ec999c pods took: 100.703315ms
STEP: Creating RC which spawns configmap-volume pods
Jul  8 03:18:28.159: INFO: Pod name wrapped-volume-race-346b449d-4fd6-466e-a23c-a5b06b9e2ac4: Found 0 pods out of 5
Jul  8 03:18:33.183: INFO: Pod name wrapped-volume-race-346b449d-4fd6-466e-a23c-a5b06b9e2ac4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-346b449d-4fd6-466e-a23c-a5b06b9e2ac4 in namespace emptydir-wrapper-5351, will wait for the garbage collector to delete the pods
Jul  8 03:18:45.272: INFO: Deleting ReplicationController wrapped-volume-race-346b449d-4fd6-466e-a23c-a5b06b9e2ac4 took: 6.956432ms
Jul  8 03:18:45.372: INFO: Terminating ReplicationController wrapped-volume-race-346b449d-4fd6-466e-a23c-a5b06b9e2ac4 pods took: 100.376954ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:18:58.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5351" for this suite.

• [SLOW TEST:101.075 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":339,"completed":192,"skipped":2941,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:18:58.336: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 03:18:58.505: INFO: Waiting up to 5m0s for pod "downwardapi-volume-255f6d10-b745-4689-a018-d1b8b7529f44" in namespace "projected-6570" to be "Succeeded or Failed"
Jul  8 03:18:58.514: INFO: Pod "downwardapi-volume-255f6d10-b745-4689-a018-d1b8b7529f44": Phase="Pending", Reason="", readiness=false. Elapsed: 8.336109ms
Jul  8 03:19:00.523: INFO: Pod "downwardapi-volume-255f6d10-b745-4689-a018-d1b8b7529f44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018197389s
STEP: Saw pod success
Jul  8 03:19:00.524: INFO: Pod "downwardapi-volume-255f6d10-b745-4689-a018-d1b8b7529f44" satisfied condition "Succeeded or Failed"
Jul  8 03:19:00.527: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-255f6d10-b745-4689-a018-d1b8b7529f44 container client-container: <nil>
STEP: delete the pod
Jul  8 03:19:00.553: INFO: Waiting for pod downwardapi-volume-255f6d10-b745-4689-a018-d1b8b7529f44 to disappear
Jul  8 03:19:00.557: INFO: Pod downwardapi-volume-255f6d10-b745-4689-a018-d1b8b7529f44 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:19:00.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6570" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":193,"skipped":3017,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:19:00.566: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 03:19:00.748: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4bc38145-12c6-477a-995e-fa7625803baa" in namespace "downward-api-8326" to be "Succeeded or Failed"
Jul  8 03:19:00.752: INFO: Pod "downwardapi-volume-4bc38145-12c6-477a-995e-fa7625803baa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.193976ms
Jul  8 03:19:02.759: INFO: Pod "downwardapi-volume-4bc38145-12c6-477a-995e-fa7625803baa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010597879s
STEP: Saw pod success
Jul  8 03:19:02.759: INFO: Pod "downwardapi-volume-4bc38145-12c6-477a-995e-fa7625803baa" satisfied condition "Succeeded or Failed"
Jul  8 03:19:02.761: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-4bc38145-12c6-477a-995e-fa7625803baa container client-container: <nil>
STEP: delete the pod
Jul  8 03:19:02.784: INFO: Waiting for pod downwardapi-volume-4bc38145-12c6-477a-995e-fa7625803baa to disappear
Jul  8 03:19:02.786: INFO: Pod downwardapi-volume-4bc38145-12c6-477a-995e-fa7625803baa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:19:02.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8326" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":194,"skipped":3017,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:19:02.797: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5011
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jul  8 03:19:04.985: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5011 PodName:var-expansion-743f82c7-83fc-413c-a031-f8b68d18ff42 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:19:04.985: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: test for file in mounted path
Jul  8 03:19:05.057: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5011 PodName:var-expansion-743f82c7-83fc-413c-a031-f8b68d18ff42 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:19:05.057: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: updating the annotation value
Jul  8 03:19:05.635: INFO: Successfully updated pod "var-expansion-743f82c7-83fc-413c-a031-f8b68d18ff42"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jul  8 03:19:05.639: INFO: Deleting pod "var-expansion-743f82c7-83fc-413c-a031-f8b68d18ff42" in namespace "var-expansion-5011"
Jul  8 03:19:05.650: INFO: Wait up to 5m0s for pod "var-expansion-743f82c7-83fc-413c-a031-f8b68d18ff42" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:19:49.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5011" for this suite.

• [SLOW TEST:46.879 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":339,"completed":195,"skipped":3022,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:19:49.677: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2360
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-b22937bb-5478-49ba-afc5-a0d5c86b3ea7
STEP: Creating a pod to test consume configMaps
Jul  8 03:19:49.866: INFO: Waiting up to 5m0s for pod "pod-configmaps-377a2ee2-e48f-400d-8ca5-523b5893fc4c" in namespace "configmap-2360" to be "Succeeded or Failed"
Jul  8 03:19:49.877: INFO: Pod "pod-configmaps-377a2ee2-e48f-400d-8ca5-523b5893fc4c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.311086ms
Jul  8 03:19:51.883: INFO: Pod "pod-configmaps-377a2ee2-e48f-400d-8ca5-523b5893fc4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016905635s
STEP: Saw pod success
Jul  8 03:19:51.883: INFO: Pod "pod-configmaps-377a2ee2-e48f-400d-8ca5-523b5893fc4c" satisfied condition "Succeeded or Failed"
Jul  8 03:19:51.889: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-configmaps-377a2ee2-e48f-400d-8ca5-523b5893fc4c container agnhost-container: <nil>
STEP: delete the pod
Jul  8 03:19:52.243: INFO: Waiting for pod pod-configmaps-377a2ee2-e48f-400d-8ca5-523b5893fc4c to disappear
Jul  8 03:19:52.246: INFO: Pod pod-configmaps-377a2ee2-e48f-400d-8ca5-523b5893fc4c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:19:52.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2360" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":196,"skipped":3027,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:19:52.256: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:19:52.406: INFO: Creating deployment "webserver-deployment"
Jul  8 03:19:52.411: INFO: Waiting for observed generation 1
Jul  8 03:19:54.423: INFO: Waiting for all required pods to come up
Jul  8 03:19:54.426: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul  8 03:19:56.438: INFO: Waiting for deployment "webserver-deployment" to complete
Jul  8 03:19:56.443: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul  8 03:19:56.450: INFO: Updating deployment webserver-deployment
Jul  8 03:19:56.450: INFO: Waiting for observed generation 2
Jul  8 03:19:58.461: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul  8 03:19:58.463: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul  8 03:19:58.469: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul  8 03:19:58.475: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul  8 03:19:58.475: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul  8 03:19:58.477: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul  8 03:19:58.481: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul  8 03:19:58.481: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul  8 03:19:58.487: INFO: Updating deployment webserver-deployment
Jul  8 03:19:58.487: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul  8 03:19:58.511: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul  8 03:19:58.516: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul  8 03:19:58.627: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5622  1304c265-01d9-4a87-b31b-51b8a2e7a4ef 43969 3 2021-07-08 03:19:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-08 03:19:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-08 03:19:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ffe5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-07-08 03:19:56 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-08 03:19:58 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jul  8 03:19:58.662: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-5622  44f4262c-67b9-47c0-a208-b0943b39bbbb 43963 3 2021-07-08 03:19:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1304c265-01d9-4a87-b31b-51b8a2e7a4ef 0xc004ffe9b7 0xc004ffe9b8}] []  [{kube-controller-manager Update apps/v1 2021-07-08 03:19:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1304c265-01d9-4a87-b31b-51b8a2e7a4ef\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ffea48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  8 03:19:58.662: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul  8 03:19:58.662: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-5622  b8023c73-aea9-4365-883c-411e8519da72 43960 3 2021-07-08 03:19:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1304c265-01d9-4a87-b31b-51b8a2e7a4ef 0xc004ffeaa7 0xc004ffeaa8}] []  [{kube-controller-manager Update apps/v1 2021-07-08 03:19:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1304c265-01d9-4a87-b31b-51b8a2e7a4ef\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ffeb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jul  8 03:19:58.715: INFO: Pod "webserver-deployment-795d758f88-72hl6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-72hl6 webserver-deployment-795d758f88- deployment-5622  0b30abcd-5057-4ca2-8ce8-11b0dd244cd5 43953 0 2021-07-08 03:19:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.42.3.193/32 cni.projectcalico.org/podIPs:10.42.3.193/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 44f4262c-67b9-47c0-a208-b0943b39bbbb 0xc004ffefc7 0xc004ffefc8}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f4262c-67b9-47c0-a208-b0943b39bbbb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:19:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.193\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8hrx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8hrx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.217,PodIP:10.42.3.193,StartTime:2021-07-08 03:19:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.193,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.715: INFO: Pod "webserver-deployment-795d758f88-fn5br" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fn5br webserver-deployment-795d758f88- deployment-5622  72f19d7e-6c49-4783-93bf-cbeec4f568ee 43998 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 44f4262c-67b9-47c0-a208-b0943b39bbbb 0xc004fff220 0xc004fff221}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f4262c-67b9-47c0-a208-b0943b39bbbb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xl5lt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xl5lt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.716: INFO: Pod "webserver-deployment-795d758f88-q984g" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-q984g webserver-deployment-795d758f88- deployment-5622  6b84277d-98b1-42cf-9256-35e9e4748215 43947 0 2021-07-08 03:19:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.42.0.24/32 cni.projectcalico.org/podIPs:10.42.0.24/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 44f4262c-67b9-47c0-a208-b0943b39bbbb 0xc004fff4f0 0xc004fff4f1}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f4262c-67b9-47c0-a208-b0943b39bbbb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:19:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vtzt6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vtzt6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-228.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.228,PodIP:10.42.0.24,StartTime:2021-07-08 03:19:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.716: INFO: Pod "webserver-deployment-795d758f88-rdbc4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rdbc4 webserver-deployment-795d758f88- deployment-5622  55be19b6-8d99-454b-bb74-8b987391e7b8 43957 0 2021-07-08 03:19:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.42.1.58/32 cni.projectcalico.org/podIPs:10.42.1.58/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 44f4262c-67b9-47c0-a208-b0943b39bbbb 0xc004fff760 0xc004fff761}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f4262c-67b9-47c0-a208-b0943b39bbbb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:19:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vn8k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vn8k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.226,PodIP:10.42.1.58,StartTime:2021-07-08 03:19:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.716: INFO: Pod "webserver-deployment-795d758f88-rgls6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rgls6 webserver-deployment-795d758f88- deployment-5622  84bbc770-6aa7-41e5-91a5-e1575947d8a2 43997 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 44f4262c-67b9-47c0-a208-b0943b39bbbb 0xc004fffaf0 0xc004fffaf1}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f4262c-67b9-47c0-a208-b0943b39bbbb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntntd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntntd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.716: INFO: Pod "webserver-deployment-795d758f88-sgwp9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-sgwp9 webserver-deployment-795d758f88- deployment-5622  08e36a4b-2221-4a95-b2e0-8cc392d62459 43946 0 2021-07-08 03:19:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.42.2.37/32 cni.projectcalico.org/podIPs:10.42.2.37/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 44f4262c-67b9-47c0-a208-b0943b39bbbb 0xc004fffd60 0xc004fffd61}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f4262c-67b9-47c0-a208-b0943b39bbbb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:19:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gq52r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gq52r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-165.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.165,PodIP:10.42.2.37,StartTime:2021-07-08 03:19:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.716: INFO: Pod "webserver-deployment-795d758f88-skmq5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-skmq5 webserver-deployment-795d758f88- deployment-5622  958ec3d9-1284-4810-b842-930a7d1c0668 43991 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 44f4262c-67b9-47c0-a208-b0943b39bbbb 0xc003a84090 0xc003a84091}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f4262c-67b9-47c0-a208-b0943b39bbbb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x99xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x99xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.716: INFO: Pod "webserver-deployment-795d758f88-t7s67" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-t7s67 webserver-deployment-795d758f88- deployment-5622  eb228c22-1c41-4348-bb69-0b9e5ee6a727 43999 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 44f4262c-67b9-47c0-a208-b0943b39bbbb 0xc003a841d7 0xc003a841d8}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f4262c-67b9-47c0-a208-b0943b39bbbb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gn4t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gn4t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.717: INFO: Pod "webserver-deployment-795d758f88-txflr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-txflr webserver-deployment-795d758f88- deployment-5622  e5f4fade-3a06-4f86-97d6-41a540ca3cf3 44000 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 44f4262c-67b9-47c0-a208-b0943b39bbbb 0xc003a84327 0xc003a84328}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f4262c-67b9-47c0-a208-b0943b39bbbb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kk4j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kk4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.717: INFO: Pod "webserver-deployment-795d758f88-v4mtq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-v4mtq webserver-deployment-795d758f88- deployment-5622  8519a8c1-3720-41df-946c-a1b22cf5e952 43976 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 44f4262c-67b9-47c0-a208-b0943b39bbbb 0xc003a84477 0xc003a84478}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f4262c-67b9-47c0-a208-b0943b39bbbb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdq5b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdq5b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-165.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.717: INFO: Pod "webserver-deployment-795d758f88-w7gpq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-w7gpq webserver-deployment-795d758f88- deployment-5622  c8632f96-f189-42c1-a976-2e244d389f68 44001 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 44f4262c-67b9-47c0-a208-b0943b39bbbb 0xc003a845e0 0xc003a845e1}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f4262c-67b9-47c0-a208-b0943b39bbbb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9v27b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9v27b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.717: INFO: Pod "webserver-deployment-795d758f88-zzh5v" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zzh5v webserver-deployment-795d758f88- deployment-5622  dc1fc6a3-2560-44ee-bc0d-2d7649b8ae5f 43950 0 2021-07-08 03:19:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.42.3.192/32 cni.projectcalico.org/podIPs:10.42.3.192/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 44f4262c-67b9-47c0-a208-b0943b39bbbb 0xc003a84747 0xc003a84748}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f4262c-67b9-47c0-a208-b0943b39bbbb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:19:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfnvv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfnvv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.217,PodIP:10.42.3.192,StartTime:2021-07-08 03:19:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.717: INFO: Pod "webserver-deployment-847dcfb7fb-2gl2j" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2gl2j webserver-deployment-847dcfb7fb- deployment-5622  529d9f24-78b4-4c6d-b4e4-ec8fbb01ff23 43827 0 2021-07-08 03:19:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.42.1.56/32 cni.projectcalico.org/podIPs:10.42.1.56/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a84990 0xc003a84991}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:19:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:19:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8smf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8smf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.226,PodIP:10.42.1.56,StartTime:2021-07-08 03:19:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:19:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://43256b1a6706ba69c84adeca1ef1ee503e71f585e7b6a22864fd1cb819bc72c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.717: INFO: Pod "webserver-deployment-847dcfb7fb-5f45z" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-5f45z webserver-deployment-847dcfb7fb- deployment-5622  65961692-66a8-458c-a580-683ea94bce38 43985 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a84b70 0xc003a84b71}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xbtfk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xbtfk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.717: INFO: Pod "webserver-deployment-847dcfb7fb-7dmgd" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-7dmgd webserver-deployment-847dcfb7fb- deployment-5622  cb8b2ce8-9ed8-4f12-8bcb-08fac70f64d1 43993 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a84cc0 0xc003a84cc1}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8zvpj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8zvpj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.718: INFO: Pod "webserver-deployment-847dcfb7fb-8zmsp" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8zmsp webserver-deployment-847dcfb7fb- deployment-5622  651a42d0-0774-4dbc-8794-d0c12fa77c1e 43992 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a84df7 0xc003a84df8}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xw4xx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xw4xx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.718: INFO: Pod "webserver-deployment-847dcfb7fb-b5jbp" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-b5jbp webserver-deployment-847dcfb7fb- deployment-5622  f9df18b0-f69a-4dc8-9e04-0db296143fe7 43981 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a84f37 0xc003a84f38}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4rq5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rq5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.718: INFO: Pod "webserver-deployment-847dcfb7fb-bkbsh" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-bkbsh webserver-deployment-847dcfb7fb- deployment-5622  5a8a705a-bd49-4646-8b2e-e6e0267cd71f 43990 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a85090 0xc003a85091}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jq6l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jq6l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.718: INFO: Pod "webserver-deployment-847dcfb7fb-c5t85" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-c5t85 webserver-deployment-847dcfb7fb- deployment-5622  48768ce9-18af-4da5-88ee-967e3750989c 43807 0 2021-07-08 03:19:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.42.2.36/32 cni.projectcalico.org/podIPs:10.42.2.36/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a851e7 0xc003a851e8}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:19:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:19:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t7zz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t7zz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-165.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.165,PodIP:10.42.2.36,StartTime:2021-07-08 03:19:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:19:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://5e7b4d66491290feba2a12d69adebef2fa5f256c8d2c2e209b1e8333c72ddeb8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.718: INFO: Pod "webserver-deployment-847dcfb7fb-cp8r8" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-cp8r8 webserver-deployment-847dcfb7fb- deployment-5622  2dfe29f8-00ad-48a0-aeff-a068dc4db767 43996 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a853d0 0xc003a853d1}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmk9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmk9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.718: INFO: Pod "webserver-deployment-847dcfb7fb-kzbff" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-kzbff webserver-deployment-847dcfb7fb- deployment-5622  320fc59c-0ff0-4add-9328-5836b36f2e73 43805 0 2021-07-08 03:19:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.42.0.22/32 cni.projectcalico.org/podIPs:10.42.0.22/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a85527 0xc003a85528}] []  [{calico Update v1 2021-07-08 03:19:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-07-08 03:19:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-08 03:19:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5p89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5p89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-228.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.228,PodIP:10.42.0.22,StartTime:2021-07-08 03:19:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:19:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://76bcb168108670c122db52c23951a41b7dcd165e7cae741d156a0f1a9f2e5afc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.718: INFO: Pod "webserver-deployment-847dcfb7fb-l4878" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-l4878 webserver-deployment-847dcfb7fb- deployment-5622  aac7933d-9365-40f3-9b31-2594648860a4 43850 0 2021-07-08 03:19:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.42.0.23/32 cni.projectcalico.org/podIPs:10.42.0.23/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a85720 0xc003a85721}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:19:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:19:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sw8lg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sw8lg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-228.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.228,PodIP:10.42.0.23,StartTime:2021-07-08 03:19:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:19:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://bfbf3ef8414ace4825760c463f9d77ae0878496191bcaf54774264e74887aa36,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.719: INFO: Pod "webserver-deployment-847dcfb7fb-l5d4r" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-l5d4r webserver-deployment-847dcfb7fb- deployment-5622  0686d5a4-f290-4772-b706-b3406520d823 43986 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a85900 0xc003a85901}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rq9bq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rq9bq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.720: INFO: Pod "webserver-deployment-847dcfb7fb-mspdd" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mspdd webserver-deployment-847dcfb7fb- deployment-5622  b93f9435-7df7-4885-8c59-9cd41b6d1b69 43821 0 2021-07-08 03:19:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.42.3.190/32 cni.projectcalico.org/podIPs:10.42.3.190/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a85a90 0xc003a85a91}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:19:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:19:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-46jhm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-46jhm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.217,PodIP:10.42.3.190,StartTime:2021-07-08 03:19:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:19:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://1ef9d28cf112296eeae3633846192744ce1dbc90b9e1aeaa62d2df8ad49cefbd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.720: INFO: Pod "webserver-deployment-847dcfb7fb-nxwrc" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-nxwrc webserver-deployment-847dcfb7fb- deployment-5622  e8120412-fbf8-43b3-b99d-0cf6bfb6cbef 43815 0 2021-07-08 03:19:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.42.3.191/32 cni.projectcalico.org/podIPs:10.42.3.191/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a85c90 0xc003a85c91}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:19:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:19:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8llxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8llxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.217,PodIP:10.42.3.191,StartTime:2021-07-08 03:19:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:19:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://7f49466489396978453e2f936f67615483a1745dce374e78deaa616f2b6eb807,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.720: INFO: Pod "webserver-deployment-847dcfb7fb-pwp7w" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-pwp7w webserver-deployment-847dcfb7fb- deployment-5622  aed77436-339f-4266-b006-f6766d359c8b 43833 0 2021-07-08 03:19:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.42.1.57/32 cni.projectcalico.org/podIPs:10.42.1.57/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc003a85ea0 0xc003a85ea1}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:19:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:19:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2p86r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2p86r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.226,PodIP:10.42.1.57,StartTime:2021-07-08 03:19:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:19:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://f649073f3be997449c5b91ece08e4c11d0f108690ca9a4b8ee3ad394d990ce55,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.721: INFO: Pod "webserver-deployment-847dcfb7fb-qn6gt" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-qn6gt webserver-deployment-847dcfb7fb- deployment-5622  654af074-5cbd-4011-afc0-63f76476c8c4 43995 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc007b26090 0xc007b26091}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6sfr7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6sfr7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.721: INFO: Pod "webserver-deployment-847dcfb7fb-qqz89" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-qqz89 webserver-deployment-847dcfb7fb- deployment-5622  68fc8b6b-c4dc-4bfb-99e2-11bd639108ec 43811 0 2021-07-08 03:19:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.42.2.35/32 cni.projectcalico.org/podIPs:10.42.2.35/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc007b26207 0xc007b26208}] []  [{calico Update v1 2021-07-08 03:19:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-07-08 03:19:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-08 03:19:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xgzgp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xgzgp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-165.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.165,PodIP:10.42.2.35,StartTime:2021-07-08 03:19:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:19:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://83cda2702128ce9ef598dba566b08cc5e580962402e139cc1baa6b5b3aecb257,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.721: INFO: Pod "webserver-deployment-847dcfb7fb-rjmnb" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-rjmnb webserver-deployment-847dcfb7fb- deployment-5622  c1d1c202-b2c5-4e9f-b293-fe18b3c5d114 43980 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc007b263f0 0xc007b263f1}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h9sb5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h9sb5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-228.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.721: INFO: Pod "webserver-deployment-847dcfb7fb-tjglq" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tjglq webserver-deployment-847dcfb7fb- deployment-5622  83c25f08-a848-44ec-8309-429e9cf82d1d 43994 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc007b266b0 0xc007b266b1}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6hsrt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6hsrt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.217,PodIP:,StartTime:2021-07-08 03:19:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.722: INFO: Pod "webserver-deployment-847dcfb7fb-wrdsn" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wrdsn webserver-deployment-847dcfb7fb- deployment-5622  cceefede-f6fe-4435-bb4f-a7d6b1b63c7e 43988 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc007b26877 0xc007b26878}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xsk55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xsk55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.226,PodIP:,StartTime:2021-07-08 03:19:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:19:58.722: INFO: Pod "webserver-deployment-847dcfb7fb-wswch" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wswch webserver-deployment-847dcfb7fb- deployment-5622  a2996308-c1fb-41e1-ae90-a114cba032cd 43987 0 2021-07-08 03:19:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b8023c73-aea9-4365-883c-411e8519da72 0xc007b26a47 0xc007b26a48}] []  [{kube-controller-manager Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8023c73-aea9-4365-883c-411e8519da72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-08 03:19:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7zp92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7zp92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-165.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:19:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.165,PodIP:,StartTime:2021-07-08 03:19:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:19:58.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5622" for this suite.

• [SLOW TEST:6.561 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":339,"completed":197,"skipped":3031,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:19:58.817: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8658
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-8658
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8658
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8658
Jul  8 03:19:59.058: INFO: Found 0 stateful pods, waiting for 1
Jul  8 03:20:09.071: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul  8 03:20:09.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-8658 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  8 03:20:09.610: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  8 03:20:09.610: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  8 03:20:09.610: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  8 03:20:09.614: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul  8 03:20:19.629: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  8 03:20:19.629: INFO: Waiting for statefulset status.replicas updated to 0
Jul  8 03:20:19.650: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999705s
Jul  8 03:20:20.658: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991011905s
Jul  8 03:20:21.664: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.982919909s
Jul  8 03:20:22.671: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.976894816s
Jul  8 03:20:23.678: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.970710801s
Jul  8 03:20:24.683: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.962393849s
Jul  8 03:20:25.689: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.958407643s
Jul  8 03:20:26.694: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.952093183s
Jul  8 03:20:27.699: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.947270262s
Jul  8 03:20:28.704: INFO: Verifying statefulset ss doesn't scale past 1 for another 942.238481ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8658
Jul  8 03:20:29.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-8658 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 03:20:29.909: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  8 03:20:29.909: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  8 03:20:29.909: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  8 03:20:29.912: INFO: Found 1 stateful pods, waiting for 3
Jul  8 03:20:39.923: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 03:20:39.923: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  8 03:20:39.923: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul  8 03:20:39.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-8658 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  8 03:20:40.160: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  8 03:20:40.160: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  8 03:20:40.160: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  8 03:20:40.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-8658 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  8 03:20:40.370: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  8 03:20:40.370: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  8 03:20:40.370: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  8 03:20:40.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-8658 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  8 03:20:40.551: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  8 03:20:40.551: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  8 03:20:40.551: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  8 03:20:40.551: INFO: Waiting for statefulset status.replicas updated to 0
Jul  8 03:20:40.555: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul  8 03:20:50.572: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  8 03:20:50.572: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul  8 03:20:50.572: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul  8 03:20:50.590: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999811s
Jul  8 03:20:51.599: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988534764s
Jul  8 03:20:52.606: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978198882s
Jul  8 03:20:53.613: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97212713s
Jul  8 03:20:54.622: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965341361s
Jul  8 03:20:55.628: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.956362269s
Jul  8 03:20:56.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.950263541s
Jul  8 03:20:57.640: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.944040716s
Jul  8 03:20:58.649: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.938173183s
Jul  8 03:20:59.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 929.485328ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8658
Jul  8 03:21:00.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-8658 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 03:21:00.836: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  8 03:21:00.836: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  8 03:21:00.836: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  8 03:21:00.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-8658 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 03:21:00.987: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  8 03:21:00.987: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  8 03:21:00.987: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  8 03:21:00.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=statefulset-8658 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  8 03:21:01.151: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  8 03:21:01.151: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  8 03:21:01.151: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  8 03:21:01.151: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul  8 03:21:21.198: INFO: Deleting all statefulset in ns statefulset-8658
Jul  8 03:21:21.200: INFO: Scaling statefulset ss to 0
Jul  8 03:21:21.209: INFO: Waiting for statefulset status.replicas updated to 0
Jul  8 03:21:21.211: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:21:21.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8658" for this suite.

• [SLOW TEST:82.448 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":339,"completed":198,"skipped":3035,"failed":0}
SSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:21:21.266: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-706
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:21:21.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-706" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":339,"completed":199,"skipped":3040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:21:21.480: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul  8 03:21:21.626: INFO: Waiting up to 5m0s for pod "downward-api-a1f06d76-38fe-4b8f-a416-da853ffecb0e" in namespace "downward-api-1861" to be "Succeeded or Failed"
Jul  8 03:21:21.642: INFO: Pod "downward-api-a1f06d76-38fe-4b8f-a416-da853ffecb0e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.773334ms
Jul  8 03:21:23.649: INFO: Pod "downward-api-a1f06d76-38fe-4b8f-a416-da853ffecb0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022254736s
STEP: Saw pod success
Jul  8 03:21:23.649: INFO: Pod "downward-api-a1f06d76-38fe-4b8f-a416-da853ffecb0e" satisfied condition "Succeeded or Failed"
Jul  8 03:21:23.651: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downward-api-a1f06d76-38fe-4b8f-a416-da853ffecb0e container dapi-container: <nil>
STEP: delete the pod
Jul  8 03:21:23.679: INFO: Waiting for pod downward-api-a1f06d76-38fe-4b8f-a416-da853ffecb0e to disappear
Jul  8 03:21:23.682: INFO: Pod downward-api-a1f06d76-38fe-4b8f-a416-da853ffecb0e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:21:23.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1861" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":339,"completed":200,"skipped":3073,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:21:23.692: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9021
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 03:21:23.850: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa981965-659e-4603-860d-cb7ed125530a" in namespace "downward-api-9021" to be "Succeeded or Failed"
Jul  8 03:21:23.853: INFO: Pod "downwardapi-volume-aa981965-659e-4603-860d-cb7ed125530a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.804678ms
Jul  8 03:21:25.858: INFO: Pod "downwardapi-volume-aa981965-659e-4603-860d-cb7ed125530a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00810327s
STEP: Saw pod success
Jul  8 03:21:25.858: INFO: Pod "downwardapi-volume-aa981965-659e-4603-860d-cb7ed125530a" satisfied condition "Succeeded or Failed"
Jul  8 03:21:25.861: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-aa981965-659e-4603-860d-cb7ed125530a container client-container: <nil>
STEP: delete the pod
Jul  8 03:21:25.882: INFO: Waiting for pod downwardapi-volume-aa981965-659e-4603-860d-cb7ed125530a to disappear
Jul  8 03:21:25.885: INFO: Pod downwardapi-volume-aa981965-659e-4603-860d-cb7ed125530a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:21:25.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9021" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":201,"skipped":3081,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:21:25.894: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6790
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 03:21:26.051: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bcb0e486-d035-4151-9d9e-ee44956e2464" in namespace "downward-api-6790" to be "Succeeded or Failed"
Jul  8 03:21:26.065: INFO: Pod "downwardapi-volume-bcb0e486-d035-4151-9d9e-ee44956e2464": Phase="Pending", Reason="", readiness=false. Elapsed: 13.79395ms
Jul  8 03:21:28.080: INFO: Pod "downwardapi-volume-bcb0e486-d035-4151-9d9e-ee44956e2464": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029257266s
STEP: Saw pod success
Jul  8 03:21:28.080: INFO: Pod "downwardapi-volume-bcb0e486-d035-4151-9d9e-ee44956e2464" satisfied condition "Succeeded or Failed"
Jul  8 03:21:28.083: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-bcb0e486-d035-4151-9d9e-ee44956e2464 container client-container: <nil>
STEP: delete the pod
Jul  8 03:21:28.102: INFO: Waiting for pod downwardapi-volume-bcb0e486-d035-4151-9d9e-ee44956e2464 to disappear
Jul  8 03:21:28.104: INFO: Pod downwardapi-volume-bcb0e486-d035-4151-9d9e-ee44956e2464 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:21:28.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6790" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":202,"skipped":3096,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:21:28.113: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1549
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 03:21:28.260: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c8c56ec-f08f-4945-b489-d87487e3039d" in namespace "downward-api-1549" to be "Succeeded or Failed"
Jul  8 03:21:28.265: INFO: Pod "downwardapi-volume-3c8c56ec-f08f-4945-b489-d87487e3039d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.04094ms
Jul  8 03:21:30.271: INFO: Pod "downwardapi-volume-3c8c56ec-f08f-4945-b489-d87487e3039d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010838632s
STEP: Saw pod success
Jul  8 03:21:30.271: INFO: Pod "downwardapi-volume-3c8c56ec-f08f-4945-b489-d87487e3039d" satisfied condition "Succeeded or Failed"
Jul  8 03:21:30.273: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-3c8c56ec-f08f-4945-b489-d87487e3039d container client-container: <nil>
STEP: delete the pod
Jul  8 03:21:30.293: INFO: Waiting for pod downwardapi-volume-3c8c56ec-f08f-4945-b489-d87487e3039d to disappear
Jul  8 03:21:30.296: INFO: Pod downwardapi-volume-3c8c56ec-f08f-4945-b489-d87487e3039d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:21:30.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1549" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":203,"skipped":3163,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:21:30.304: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9838
Jul  8 03:21:30.464: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:21:32.472: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jul  8 03:21:32.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-9838 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul  8 03:21:32.683: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul  8 03:21:32.683: INFO: stdout: "iptables"
Jul  8 03:21:32.683: INFO: proxyMode: iptables
Jul  8 03:21:32.697: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul  8 03:21:32.704: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-9838
STEP: creating replication controller affinity-nodeport-timeout in namespace services-9838
I0708 03:21:32.745323      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9838, replica count: 3
I0708 03:21:35.796268      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  8 03:21:35.805: INFO: Creating new exec pod
Jul  8 03:21:38.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-9838 exec execpod-affinityzt2lr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jul  8 03:21:39.038: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-timeout 80\n+ echo hostName\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jul  8 03:21:39.038: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 03:21:39.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-9838 exec execpod-affinityzt2lr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.228.6 80'
Jul  8 03:21:39.242: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.228.6 80\nConnection to 10.43.228.6 80 port [tcp/http] succeeded!\n"
Jul  8 03:21:39.242: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 03:21:39.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-9838 exec execpod-affinityzt2lr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.8.165 32181'
Jul  8 03:21:39.530: INFO: stderr: "+ nc -v -t -w 2 172.31.8.165 32181\n+ echo hostName\nConnection to 172.31.8.165 32181 port [tcp/*] succeeded!\n"
Jul  8 03:21:39.530: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 03:21:39.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-9838 exec execpod-affinityzt2lr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.3.228 32181'
Jul  8 03:21:39.685: INFO: stderr: "+ nc -v -t -w 2 172.31.3.228 32181\n+ echo hostName\nConnection to 172.31.3.228 32181 port [tcp/*] succeeded!\n"
Jul  8 03:21:39.685: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 03:21:39.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-9838 exec execpod-affinityzt2lr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.228:32181/ ; done'
Jul  8 03:21:39.917: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n"
Jul  8 03:21:39.918: INFO: stdout: "\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6\naffinity-nodeport-timeout-pqnk6"
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Received response from host: affinity-nodeport-timeout-pqnk6
Jul  8 03:21:39.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-9838 exec execpod-affinityzt2lr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.3.228:32181/'
Jul  8 03:21:40.084: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n"
Jul  8 03:21:40.084: INFO: stdout: "affinity-nodeport-timeout-pqnk6"
Jul  8 03:22:00.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-9838 exec execpod-affinityzt2lr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.3.228:32181/'
Jul  8 03:22:00.275: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.3.228:32181/\n"
Jul  8 03:22:00.275: INFO: stdout: "affinity-nodeport-timeout-5r8rc"
Jul  8 03:22:00.275: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9838, will wait for the garbage collector to delete the pods
Jul  8 03:22:00.378: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 13.384115ms
Jul  8 03:22:00.478: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.919395ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:22:11.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9838" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:40.736 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":204,"skipped":3175,"failed":0}
SSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:22:11.043: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-6857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jul  8 03:22:13.295: INFO: running pods: 0 < 3
Jul  8 03:22:15.299: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:22:17.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6857" for this suite.

• [SLOW TEST:6.270 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":339,"completed":205,"skipped":3178,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:22:17.314: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:22:33.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1864" for this suite.

• [SLOW TEST:16.326 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":339,"completed":206,"skipped":3183,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:22:33.642: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-9896
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:22:35.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9896" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":207,"skipped":3197,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:22:35.828: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5317
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:22:36.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5317" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":208,"skipped":3228,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:22:36.029: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8530
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:22:36.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8530" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":339,"completed":209,"skipped":3248,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:22:36.187: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4605
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:22:36.330: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:22:37.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4605" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":339,"completed":210,"skipped":3255,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:22:37.395: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1982
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:22:37.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1982" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":339,"completed":211,"skipped":3286,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:22:37.624: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5270
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-2e6c3910-8f19-4589-914e-6bd0a447e9ae in namespace container-probe-5270
Jul  8 03:22:39.826: INFO: Started pod liveness-2e6c3910-8f19-4589-914e-6bd0a447e9ae in namespace container-probe-5270
STEP: checking the pod's current state and verifying that restartCount is present
Jul  8 03:22:39.832: INFO: Initial restart count of pod liveness-2e6c3910-8f19-4589-914e-6bd0a447e9ae is 0
Jul  8 03:22:59.911: INFO: Restart count of pod container-probe-5270/liveness-2e6c3910-8f19-4589-914e-6bd0a447e9ae is now 1 (20.079306947s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:22:59.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5270" for this suite.

• [SLOW TEST:22.319 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":212,"skipped":3304,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:22:59.944: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9711
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7185
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-193
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:23:06.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9711" for this suite.
STEP: Destroying namespace "nsdeletetest-7185" for this suite.
Jul  8 03:23:06.423: INFO: Namespace nsdeletetest-7185 was already deleted
STEP: Destroying namespace "nsdeletetest-193" for this suite.

• [SLOW TEST:6.484 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":339,"completed":213,"skipped":3320,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:23:06.429: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3161
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-485e181e-3da0-4796-bd3d-748cb7294967
STEP: Creating the pod
Jul  8 03:23:06.601: INFO: The status of Pod pod-configmaps-6b37a7c2-eeee-4202-9e50-c25672aa2055 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:23:08.608: INFO: The status of Pod pod-configmaps-6b37a7c2-eeee-4202-9e50-c25672aa2055 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-485e181e-3da0-4796-bd3d-748cb7294967
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:23:10.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3161" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":214,"skipped":3334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:23:10.651: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9790
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:23:11.221: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:23:14.247: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:23:14.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9790" for this suite.
STEP: Destroying namespace "webhook-9790-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":339,"completed":215,"skipped":3385,"failed":0}

------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:23:14.454: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4456
STEP: creating service affinity-clusterip-transition in namespace services-4456
STEP: creating replication controller affinity-clusterip-transition in namespace services-4456
I0708 03:23:14.654192      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-4456, replica count: 3
I0708 03:23:17.706276      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  8 03:23:17.715: INFO: Creating new exec pod
Jul  8 03:23:20.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-4456 exec execpod-affinitykqgdr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jul  8 03:23:20.904: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jul  8 03:23:20.904: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 03:23:20.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-4456 exec execpod-affinitykqgdr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.145.218 80'
Jul  8 03:23:21.074: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.145.218 80\nConnection to 10.43.145.218 80 port [tcp/http] succeeded!\n"
Jul  8 03:23:21.074: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  8 03:23:21.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-4456 exec execpod-affinitykqgdr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.145.218:80/ ; done'
Jul  8 03:23:21.348: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n"
Jul  8 03:23:21.348: INFO: stdout: "\naffinity-clusterip-transition-kllsn\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-dsc5k\naffinity-clusterip-transition-kllsn\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-kllsn\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-kllsn\naffinity-clusterip-transition-kllsn\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-dsc5k"
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-kllsn
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-dsc5k
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-kllsn
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-kllsn
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-kllsn
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-kllsn
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.348: INFO: Received response from host: affinity-clusterip-transition-dsc5k
Jul  8 03:23:21.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-4456 exec execpod-affinitykqgdr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.145.218:80/ ; done'
Jul  8 03:23:21.597: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.145.218:80/\n"
Jul  8 03:23:21.597: INFO: stdout: "\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv\naffinity-clusterip-transition-2gjwv"
Jul  8 03:23:21.597: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.597: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.597: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.597: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.597: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.597: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.597: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.597: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.597: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.598: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.598: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.598: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.598: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.598: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.598: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.598: INFO: Received response from host: affinity-clusterip-transition-2gjwv
Jul  8 03:23:21.598: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4456, will wait for the garbage collector to delete the pods
Jul  8 03:23:21.680: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.076899ms
Jul  8 03:23:21.781: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.771845ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:23:38.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4456" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:23.593 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":216,"skipped":3385,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:23:38.051: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8386
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:23:49.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8386" for this suite.

• [SLOW TEST:11.289 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":339,"completed":217,"skipped":3396,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:23:49.344: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul  8 03:23:49.489: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  8 03:23:49.495: INFO: Waiting for terminating namespaces to be deleted...
Jul  8 03:23:49.497: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-228.us-east-2.compute.internal before test
Jul  8 03:23:49.503: INFO: etcd-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:35 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.503: INFO: 	Container etcd ready: true, restart count 0
Jul  8 03:23:49.503: INFO: helm-install-rke2-canal-zzkr7 from kube-system started at 2021-07-07 23:40:11 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.504: INFO: 	Container helm ready: false, restart count 0
Jul  8 03:23:49.504: INFO: helm-install-rke2-coredns-7vm9g from kube-system started at 2021-07-07 23:40:11 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.504: INFO: 	Container helm ready: false, restart count 0
Jul  8 03:23:49.504: INFO: helm-install-rke2-kube-proxy-tqnww from kube-system started at 2021-07-07 23:40:11 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.504: INFO: 	Container helm ready: false, restart count 0
Jul  8 03:23:49.504: INFO: helm-install-rke2-metrics-server-xbzj4 from kube-system started at 2021-07-07 23:40:54 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.504: INFO: 	Container helm ready: false, restart count 0
Jul  8 03:23:49.504: INFO: kube-apiserver-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:47 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.504: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul  8 03:23:49.504: INFO: kube-controller-manager-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:57 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.504: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul  8 03:23:49.504: INFO: kube-proxy-vvsjd from kube-system started at 2021-07-07 23:40:24 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.504: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 03:23:49.504: INFO: kube-scheduler-ip-172-31-3-228.us-east-2.compute.internal from kube-system started at 2021-07-07 23:39:55 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.504: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul  8 03:23:49.504: INFO: rke2-canal-kcjjk from kube-system started at 2021-07-07 23:40:25 +0000 UTC (2 container statuses recorded)
Jul  8 03:23:49.504: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 03:23:49.504: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 03:23:49.504: INFO: rke2-coredns-rke2-coredns-65d668ddf9-bd9pd from kube-system started at 2021-07-07 23:40:52 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.504: INFO: 	Container coredns ready: true, restart count 0
Jul  8 03:23:49.504: INFO: rke2-metrics-server-6647ffc866-7z7j7 from kube-system started at 2021-07-07 23:41:00 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.504: INFO: 	Container metrics-server ready: true, restart count 0
Jul  8 03:23:49.504: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-84vqn from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 03:23:49.504: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Jul  8 03:23:49.504: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  8 03:23:49.504: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-217.us-east-2.compute.internal before test
Jul  8 03:23:49.510: INFO: kube-proxy-t4j4c from kube-system started at 2021-07-07 23:44:49 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.510: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 03:23:49.510: INFO: rke2-canal-9wlw7 from kube-system started at 2021-07-07 23:44:49 +0000 UTC (2 container statuses recorded)
Jul  8 03:23:49.510: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 03:23:49.510: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 03:23:49.510: INFO: sonobuoy from sonobuoy started at 2021-07-08 01:44:37 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.510: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  8 03:23:49.510: INFO: sonobuoy-e2e-job-1dfa1a3a74ca4282 from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 03:23:49.510: INFO: 	Container e2e ready: true, restart count 0
Jul  8 03:23:49.510: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  8 03:23:49.510: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-5s9xh from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 03:23:49.511: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Jul  8 03:23:49.511: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  8 03:23:49.511: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-6-226.us-east-2.compute.internal before test
Jul  8 03:23:49.516: INFO: etcd-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:42:57 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.516: INFO: 	Container etcd ready: true, restart count 0
Jul  8 03:23:49.516: INFO: kube-apiserver-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:11 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.516: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul  8 03:23:49.516: INFO: kube-controller-manager-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:22 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.516: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul  8 03:23:49.516: INFO: kube-proxy-87mbg from kube-system started at 2021-07-07 23:43:19 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.516: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 03:23:49.516: INFO: kube-scheduler-ip-172-31-6-226.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:22 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.516: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul  8 03:23:49.516: INFO: rke2-canal-8n79q from kube-system started at 2021-07-07 23:43:19 +0000 UTC (2 container statuses recorded)
Jul  8 03:23:49.516: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 03:23:49.516: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 03:23:49.516: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-2nxk4 from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 03:23:49.516: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Jul  8 03:23:49.516: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  8 03:23:49.516: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-8-165.us-east-2.compute.internal before test
Jul  8 03:23:49.523: INFO: etcd-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:41 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.523: INFO: 	Container etcd ready: true, restart count 0
Jul  8 03:23:49.523: INFO: kube-apiserver-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:48 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.523: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul  8 03:23:49.523: INFO: kube-controller-manager-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:59 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.523: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul  8 03:23:49.523: INFO: kube-proxy-p4h8m from kube-system started at 2021-07-07 23:43:56 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.523: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  8 03:23:49.523: INFO: kube-scheduler-ip-172-31-8-165.us-east-2.compute.internal from kube-system started at 2021-07-07 23:43:58 +0000 UTC (1 container statuses recorded)
Jul  8 03:23:49.523: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul  8 03:23:49.523: INFO: rke2-canal-vnj8b from kube-system started at 2021-07-07 23:43:56 +0000 UTC (2 container statuses recorded)
Jul  8 03:23:49.523: INFO: 	Container calico-node ready: true, restart count 0
Jul  8 03:23:49.523: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  8 03:23:49.523: INFO: sonobuoy-systemd-logs-daemon-set-ffd2732b62e449b2-fndvb from sonobuoy started at 2021-07-08 01:44:44 +0000 UTC (2 container statuses recorded)
Jul  8 03:23:49.524: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Jul  8 03:23:49.524: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.168fb2d7986f75a8], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:23:50.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8822" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":339,"completed":218,"skipped":3445,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:23:50.563: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-960
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-fd684524-8b21-4e9b-b5db-a8a3d2bdb596
STEP: Creating a pod to test consume secrets
Jul  8 03:23:50.718: INFO: Waiting up to 5m0s for pod "pod-secrets-c73a6859-8da1-4160-b2e7-3a137255d4f3" in namespace "secrets-960" to be "Succeeded or Failed"
Jul  8 03:23:50.729: INFO: Pod "pod-secrets-c73a6859-8da1-4160-b2e7-3a137255d4f3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.14427ms
Jul  8 03:23:52.736: INFO: Pod "pod-secrets-c73a6859-8da1-4160-b2e7-3a137255d4f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018744811s
STEP: Saw pod success
Jul  8 03:23:52.736: INFO: Pod "pod-secrets-c73a6859-8da1-4160-b2e7-3a137255d4f3" satisfied condition "Succeeded or Failed"
Jul  8 03:23:52.739: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-secrets-c73a6859-8da1-4160-b2e7-3a137255d4f3 container secret-volume-test: <nil>
STEP: delete the pod
Jul  8 03:23:52.757: INFO: Waiting for pod pod-secrets-c73a6859-8da1-4160-b2e7-3a137255d4f3 to disappear
Jul  8 03:23:52.759: INFO: Pod pod-secrets-c73a6859-8da1-4160-b2e7-3a137255d4f3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:23:52.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-960" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":219,"skipped":3451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:23:52.768: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul  8 03:23:52.931: INFO: Waiting up to 5m0s for pod "pod-e90808e7-5b21-4df0-bb6f-15cb3e8d180b" in namespace "emptydir-2846" to be "Succeeded or Failed"
Jul  8 03:23:52.935: INFO: Pod "pod-e90808e7-5b21-4df0-bb6f-15cb3e8d180b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.55371ms
Jul  8 03:23:54.942: INFO: Pod "pod-e90808e7-5b21-4df0-bb6f-15cb3e8d180b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011529729s
STEP: Saw pod success
Jul  8 03:23:54.943: INFO: Pod "pod-e90808e7-5b21-4df0-bb6f-15cb3e8d180b" satisfied condition "Succeeded or Failed"
Jul  8 03:23:54.945: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-e90808e7-5b21-4df0-bb6f-15cb3e8d180b container test-container: <nil>
STEP: delete the pod
Jul  8 03:23:54.965: INFO: Waiting for pod pod-e90808e7-5b21-4df0-bb6f-15cb3e8d180b to disappear
Jul  8 03:23:54.967: INFO: Pod pod-e90808e7-5b21-4df0-bb6f-15cb3e8d180b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:23:54.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2846" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":220,"skipped":3482,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:23:54.975: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6660
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:23:55.120: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:24:02.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6660" for this suite.

• [SLOW TEST:7.580 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":339,"completed":221,"skipped":3488,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:24:02.556: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9944
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9944.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9944.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  8 03:24:06.817: INFO: DNS probes using dns-test-e075cc22-7b98-4ee8-a5dd-92d587c64d4c succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9944.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9944.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  8 03:24:08.934: INFO: File wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local from pod  dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  8 03:24:08.936: INFO: File jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local from pod  dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  8 03:24:08.936: INFO: Lookups using dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b failed for: [wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local]

Jul  8 03:24:13.941: INFO: File wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local from pod  dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  8 03:24:13.944: INFO: File jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local from pod  dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  8 03:24:13.944: INFO: Lookups using dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b failed for: [wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local]

Jul  8 03:24:18.941: INFO: File wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local from pod  dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  8 03:24:18.944: INFO: File jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local from pod  dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  8 03:24:18.944: INFO: Lookups using dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b failed for: [wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local]

Jul  8 03:24:23.940: INFO: File wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local from pod  dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  8 03:24:23.943: INFO: File jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local from pod  dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  8 03:24:23.943: INFO: Lookups using dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b failed for: [wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local]

Jul  8 03:24:28.941: INFO: File wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local from pod  dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  8 03:24:28.943: INFO: File jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local from pod  dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  8 03:24:28.943: INFO: Lookups using dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b failed for: [wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local]

Jul  8 03:24:33.942: INFO: File wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local from pod  dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  8 03:24:33.945: INFO: File jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local from pod  dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  8 03:24:33.945: INFO: Lookups using dns-9944/dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b failed for: [wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local]

Jul  8 03:24:38.952: INFO: DNS probes using dns-test-31cd8142-f6ff-4e9a-b78c-fcdb7320292b succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9944.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9944.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9944.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9944.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  8 03:24:41.051: INFO: DNS probes using dns-test-4c8fed65-ab61-4407-b6ae-ec62b95f318b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:24:41.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9944" for this suite.

• [SLOW TEST:38.555 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":339,"completed":222,"skipped":3496,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:24:41.113: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:24:41.560: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:24:44.582: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:24:44.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6472" for this suite.
STEP: Destroying namespace "webhook-6472-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":339,"completed":223,"skipped":3496,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:24:44.788: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-4589/secret-test-8d1639a9-2c6a-48f7-8089-df6f46bb9abf
STEP: Creating a pod to test consume secrets
Jul  8 03:24:44.944: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e565f33-19f5-4e9a-80d3-ee24f5cbb981" in namespace "secrets-4589" to be "Succeeded or Failed"
Jul  8 03:24:44.951: INFO: Pod "pod-configmaps-6e565f33-19f5-4e9a-80d3-ee24f5cbb981": Phase="Pending", Reason="", readiness=false. Elapsed: 6.717392ms
Jul  8 03:24:46.957: INFO: Pod "pod-configmaps-6e565f33-19f5-4e9a-80d3-ee24f5cbb981": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012750673s
STEP: Saw pod success
Jul  8 03:24:46.957: INFO: Pod "pod-configmaps-6e565f33-19f5-4e9a-80d3-ee24f5cbb981" satisfied condition "Succeeded or Failed"
Jul  8 03:24:46.959: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-configmaps-6e565f33-19f5-4e9a-80d3-ee24f5cbb981 container env-test: <nil>
STEP: delete the pod
Jul  8 03:24:46.980: INFO: Waiting for pod pod-configmaps-6e565f33-19f5-4e9a-80d3-ee24f5cbb981 to disappear
Jul  8 03:24:46.987: INFO: Pod pod-configmaps-6e565f33-19f5-4e9a-80d3-ee24f5cbb981 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:24:46.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4589" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":224,"skipped":3507,"failed":0}

------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:24:46.997: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-18a67a79-d8b3-4950-a6dd-bc2877c4651e in namespace container-probe-1192
Jul  8 03:24:49.162: INFO: Started pod busybox-18a67a79-d8b3-4950-a6dd-bc2877c4651e in namespace container-probe-1192
STEP: checking the pod's current state and verifying that restartCount is present
Jul  8 03:24:49.165: INFO: Initial restart count of pod busybox-18a67a79-d8b3-4950-a6dd-bc2877c4651e is 0
Jul  8 03:25:39.357: INFO: Restart count of pod container-probe-1192/busybox-18a67a79-d8b3-4950-a6dd-bc2877c4651e is now 1 (50.191978533s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:25:39.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1192" for this suite.

• [SLOW TEST:52.425 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":225,"skipped":3507,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:25:39.422: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 03:25:39.639: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11f7c32e-d2de-4722-b49b-32b7e001f5ce" in namespace "projected-413" to be "Succeeded or Failed"
Jul  8 03:25:39.649: INFO: Pod "downwardapi-volume-11f7c32e-d2de-4722-b49b-32b7e001f5ce": Phase="Pending", Reason="", readiness=false. Elapsed: 9.940802ms
Jul  8 03:25:41.656: INFO: Pod "downwardapi-volume-11f7c32e-d2de-4722-b49b-32b7e001f5ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016587776s
STEP: Saw pod success
Jul  8 03:25:41.656: INFO: Pod "downwardapi-volume-11f7c32e-d2de-4722-b49b-32b7e001f5ce" satisfied condition "Succeeded or Failed"
Jul  8 03:25:41.666: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-11f7c32e-d2de-4722-b49b-32b7e001f5ce container client-container: <nil>
STEP: delete the pod
Jul  8 03:25:41.692: INFO: Waiting for pod downwardapi-volume-11f7c32e-d2de-4722-b49b-32b7e001f5ce to disappear
Jul  8 03:25:41.694: INFO: Pod downwardapi-volume-11f7c32e-d2de-4722-b49b-32b7e001f5ce no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:25:41.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-413" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":339,"completed":226,"skipped":3529,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:25:41.702: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-1b2cfca0-df77-4425-a78f-2540f4bbd6a9
STEP: Creating a pod to test consume configMaps
Jul  8 03:25:41.872: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-16cf85b1-c662-4cbd-b47c-9d3965f7768e" in namespace "projected-9127" to be "Succeeded or Failed"
Jul  8 03:25:41.882: INFO: Pod "pod-projected-configmaps-16cf85b1-c662-4cbd-b47c-9d3965f7768e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.453287ms
Jul  8 03:25:43.890: INFO: Pod "pod-projected-configmaps-16cf85b1-c662-4cbd-b47c-9d3965f7768e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017952359s
STEP: Saw pod success
Jul  8 03:25:43.890: INFO: Pod "pod-projected-configmaps-16cf85b1-c662-4cbd-b47c-9d3965f7768e" satisfied condition "Succeeded or Failed"
Jul  8 03:25:43.892: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-projected-configmaps-16cf85b1-c662-4cbd-b47c-9d3965f7768e container agnhost-container: <nil>
STEP: delete the pod
Jul  8 03:25:43.912: INFO: Waiting for pod pod-projected-configmaps-16cf85b1-c662-4cbd-b47c-9d3965f7768e to disappear
Jul  8 03:25:43.914: INFO: Pod pod-projected-configmaps-16cf85b1-c662-4cbd-b47c-9d3965f7768e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:25:43.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9127" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":227,"skipped":3533,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:25:43.922: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4984
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:25:44.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4984" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":339,"completed":228,"skipped":3553,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:25:44.131: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5563
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:25:44.812: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:25:47.835: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:25:47.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5563" for this suite.
STEP: Destroying namespace "webhook-5563-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":339,"completed":229,"skipped":3565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:25:47.997: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1476
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1476
STEP: creating replication controller externalsvc in namespace services-1476
I0708 03:25:48.209900      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1476, replica count: 2
I0708 03:25:51.260699      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jul  8 03:25:51.316: INFO: Creating new exec pod
Jul  8 03:25:53.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-1476 exec execpodswh7b -- /bin/sh -x -c nslookup nodeport-service.services-1476.svc.cluster.local'
Jul  8 03:25:53.545: INFO: stderr: "+ nslookup nodeport-service.services-1476.svc.cluster.local\n"
Jul  8 03:25:53.545: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nnodeport-service.services-1476.svc.cluster.local\tcanonical name = externalsvc.services-1476.svc.cluster.local.\nName:\texternalsvc.services-1476.svc.cluster.local\nAddress: 10.43.27.104\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1476, will wait for the garbage collector to delete the pods
Jul  8 03:25:53.604: INFO: Deleting ReplicationController externalsvc took: 5.41327ms
Jul  8 03:25:53.705: INFO: Terminating ReplicationController externalsvc pods took: 101.070557ms
Jul  8 03:26:01.111: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:26:01.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1476" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:13.207 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":339,"completed":230,"skipped":3598,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:26:01.205: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6329
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:26:02.663: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul  8 03:26:04.681: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311562, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311562, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311562, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311562, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:26:07.702: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jul  8 03:26:09.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=webhook-6329 attach --namespace=webhook-6329 to-be-attached-pod -i -c=container1'
Jul  8 03:26:09.837: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:26:09.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6329" for this suite.
STEP: Destroying namespace "webhook-6329-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.727 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":339,"completed":231,"skipped":3601,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:26:09.932: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8165
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:26:10.107: INFO: created pod
Jul  8 03:26:10.107: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8165" to be "Succeeded or Failed"
Jul  8 03:26:10.114: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.078388ms
Jul  8 03:26:12.122: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015042143s
STEP: Saw pod success
Jul  8 03:26:12.122: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jul  8 03:26:42.122: INFO: polling logs
Jul  8 03:26:42.139: INFO: Pod logs: 
2021/07/08 03:26:10 OK: Got token
2021/07/08 03:26:10 validating with in-cluster discovery
2021/07/08 03:26:10 OK: got issuer https://kubernetes.default.svc.cluster.local
2021/07/08 03:26:10 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8165:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1625715370, NotBefore:1625714770, IssuedAt:1625714770, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8165", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cfba1fd0-7050-41fb-86e3-af50eedab48d"}}}
2021/07/08 03:26:10 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2021/07/08 03:26:10 OK: Validated signature on JWT
2021/07/08 03:26:10 OK: Got valid claims from token!
2021/07/08 03:26:10 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8165:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1625715370, NotBefore:1625714770, IssuedAt:1625714770, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8165", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cfba1fd0-7050-41fb-86e3-af50eedab48d"}}}

Jul  8 03:26:42.139: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:26:42.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8165" for this suite.

• [SLOW TEST:32.232 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":339,"completed":232,"skipped":3605,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:26:42.164: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1514
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jul  8 03:26:42.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-5505 run e2e-test-httpd-pod --restart=Never --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Jul  8 03:26:42.417: INFO: stderr: ""
Jul  8 03:26:42.418: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1518
Jul  8 03:26:42.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-5505 delete pods e2e-test-httpd-pod'
Jul  8 03:26:58.515: INFO: stderr: ""
Jul  8 03:26:58.515: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:26:58.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5505" for this suite.

• [SLOW TEST:16.364 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":339,"completed":233,"skipped":3611,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:26:58.529: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7261
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-66908663-4d2e-457e-8351-78599c34d3f5
STEP: Creating a pod to test consume secrets
Jul  8 03:26:58.690: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8ba150a5-7c0a-437f-ac86-818c6b96aa74" in namespace "projected-7261" to be "Succeeded or Failed"
Jul  8 03:26:58.695: INFO: Pod "pod-projected-secrets-8ba150a5-7c0a-437f-ac86-818c6b96aa74": Phase="Pending", Reason="", readiness=false. Elapsed: 4.420798ms
Jul  8 03:27:00.703: INFO: Pod "pod-projected-secrets-8ba150a5-7c0a-437f-ac86-818c6b96aa74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012194367s
STEP: Saw pod success
Jul  8 03:27:00.703: INFO: Pod "pod-projected-secrets-8ba150a5-7c0a-437f-ac86-818c6b96aa74" satisfied condition "Succeeded or Failed"
Jul  8 03:27:00.705: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-projected-secrets-8ba150a5-7c0a-437f-ac86-818c6b96aa74 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  8 03:27:00.727: INFO: Waiting for pod pod-projected-secrets-8ba150a5-7c0a-437f-ac86-818c6b96aa74 to disappear
Jul  8 03:27:00.729: INFO: Pod pod-projected-secrets-8ba150a5-7c0a-437f-ac86-818c6b96aa74 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:27:00.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7261" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":234,"skipped":3619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:27:00.741: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul  8 03:27:00.893: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:27:08.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9107" for this suite.

• [SLOW TEST:7.793 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":339,"completed":235,"skipped":3663,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:27:08.535: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9559
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:27:08.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9559 version'
Jul  8 03:27:08.819: INFO: stderr: ""
Jul  8 03:27:08.819: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.2\", GitCommit:\"092fbfbf53427de67cac1e9fa54aaa09a28371d7\", GitTreeState:\"clean\", BuildDate:\"2021-06-16T12:59:11Z\", GoVersion:\"go1.16.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.2+rke2r1\", GitCommit:\"5e58841cce77d4bc13713ad2b91fa0d961e69192\", GitTreeState:\"clean\", BuildDate:\"2021-06-24T00:52:42Z\", GoVersion:\"go1.16.4b7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:27:08.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9559" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":339,"completed":236,"skipped":3667,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:27:08.837: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-9938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jul  8 03:27:09.020: INFO: Waiting up to 5m0s for pod "security-context-02053ca1-7ea7-468b-b1a0-7e9d69306fe3" in namespace "security-context-9938" to be "Succeeded or Failed"
Jul  8 03:27:09.022: INFO: Pod "security-context-02053ca1-7ea7-468b-b1a0-7e9d69306fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.328622ms
Jul  8 03:27:11.033: INFO: Pod "security-context-02053ca1-7ea7-468b-b1a0-7e9d69306fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013033328s
Jul  8 03:27:13.042: INFO: Pod "security-context-02053ca1-7ea7-468b-b1a0-7e9d69306fe3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022280526s
STEP: Saw pod success
Jul  8 03:27:13.042: INFO: Pod "security-context-02053ca1-7ea7-468b-b1a0-7e9d69306fe3" satisfied condition "Succeeded or Failed"
Jul  8 03:27:13.045: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod security-context-02053ca1-7ea7-468b-b1a0-7e9d69306fe3 container test-container: <nil>
STEP: delete the pod
Jul  8 03:27:13.075: INFO: Waiting for pod security-context-02053ca1-7ea7-468b-b1a0-7e9d69306fe3 to disappear
Jul  8 03:27:13.077: INFO: Pod security-context-02053ca1-7ea7-468b-b1a0-7e9d69306fe3 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:27:13.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9938" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":237,"skipped":3741,"failed":0}

------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:27:13.090: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-8109
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:27:13.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8109" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":339,"completed":238,"skipped":3741,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:27:13.275: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-hf24
STEP: Creating a pod to test atomic-volume-subpath
Jul  8 03:27:13.434: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hf24" in namespace "subpath-8078" to be "Succeeded or Failed"
Jul  8 03:27:13.443: INFO: Pod "pod-subpath-test-configmap-hf24": Phase="Pending", Reason="", readiness=false. Elapsed: 8.196851ms
Jul  8 03:27:15.465: INFO: Pod "pod-subpath-test-configmap-hf24": Phase="Running", Reason="", readiness=true. Elapsed: 2.03061677s
Jul  8 03:27:17.471: INFO: Pod "pod-subpath-test-configmap-hf24": Phase="Running", Reason="", readiness=true. Elapsed: 4.036729166s
Jul  8 03:27:19.477: INFO: Pod "pod-subpath-test-configmap-hf24": Phase="Running", Reason="", readiness=true. Elapsed: 6.042889956s
Jul  8 03:27:21.484: INFO: Pod "pod-subpath-test-configmap-hf24": Phase="Running", Reason="", readiness=true. Elapsed: 8.049624474s
Jul  8 03:27:23.492: INFO: Pod "pod-subpath-test-configmap-hf24": Phase="Running", Reason="", readiness=true. Elapsed: 10.057873468s
Jul  8 03:27:25.500: INFO: Pod "pod-subpath-test-configmap-hf24": Phase="Running", Reason="", readiness=true. Elapsed: 12.065379025s
Jul  8 03:27:27.505: INFO: Pod "pod-subpath-test-configmap-hf24": Phase="Running", Reason="", readiness=true. Elapsed: 14.070919274s
Jul  8 03:27:29.514: INFO: Pod "pod-subpath-test-configmap-hf24": Phase="Running", Reason="", readiness=true. Elapsed: 16.079246505s
Jul  8 03:27:31.522: INFO: Pod "pod-subpath-test-configmap-hf24": Phase="Running", Reason="", readiness=true. Elapsed: 18.08744441s
Jul  8 03:27:33.528: INFO: Pod "pod-subpath-test-configmap-hf24": Phase="Running", Reason="", readiness=true. Elapsed: 20.093785093s
Jul  8 03:27:35.535: INFO: Pod "pod-subpath-test-configmap-hf24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.100125594s
STEP: Saw pod success
Jul  8 03:27:35.535: INFO: Pod "pod-subpath-test-configmap-hf24" satisfied condition "Succeeded or Failed"
Jul  8 03:27:35.537: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-subpath-test-configmap-hf24 container test-container-subpath-configmap-hf24: <nil>
STEP: delete the pod
Jul  8 03:27:35.556: INFO: Waiting for pod pod-subpath-test-configmap-hf24 to disappear
Jul  8 03:27:35.560: INFO: Pod pod-subpath-test-configmap-hf24 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hf24
Jul  8 03:27:35.560: INFO: Deleting pod "pod-subpath-test-configmap-hf24" in namespace "subpath-8078"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:27:35.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8078" for this suite.

• [SLOW TEST:22.296 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":339,"completed":239,"skipped":3756,"failed":0}
SSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:27:35.571: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename hostport
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-7292
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jul  8 03:27:35.730: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:27:37.735: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.6.226 on the node which pod1 resides and expect scheduled
Jul  8 03:27:37.745: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:27:39.752: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.6.226 but use UDP protocol on the node which pod2 resides
Jul  8 03:27:39.762: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:27:41.769: INFO: The status of Pod pod3 is Running (Ready = true)
Jul  8 03:27:41.780: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:27:43.786: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jul  8 03:27:43.788: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.6.226 http://127.0.0.1:54323/hostname] Namespace:hostport-7292 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:27:43.789: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.6.226, port: 54323
Jul  8 03:27:43.872: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.6.226:54323/hostname] Namespace:hostport-7292 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:27:43.872: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.6.226, port: 54323 UDP
Jul  8 03:27:43.949: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.6.226 54323] Namespace:hostport-7292 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:27:43.949: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:27:49.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-7292" for this suite.

• [SLOW TEST:13.483 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":339,"completed":240,"skipped":3764,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:27:49.054: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:27:49.686: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:27:52.711: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:27:52.716: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9417-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:27:55.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1764" for this suite.
STEP: Destroying namespace "webhook-1764-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.887 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":339,"completed":241,"skipped":3766,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:27:55.943: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2888
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:27:59.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2888" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":339,"completed":242,"skipped":3767,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:27:59.236: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3796
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul  8 03:27:59.382: INFO: Waiting up to 5m0s for pod "pod-38067452-17c5-43e3-887d-1c9b04b6a6ac" in namespace "emptydir-3796" to be "Succeeded or Failed"
Jul  8 03:27:59.386: INFO: Pod "pod-38067452-17c5-43e3-887d-1c9b04b6a6ac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.925558ms
Jul  8 03:28:01.393: INFO: Pod "pod-38067452-17c5-43e3-887d-1c9b04b6a6ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010193826s
STEP: Saw pod success
Jul  8 03:28:01.393: INFO: Pod "pod-38067452-17c5-43e3-887d-1c9b04b6a6ac" satisfied condition "Succeeded or Failed"
Jul  8 03:28:01.395: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-38067452-17c5-43e3-887d-1c9b04b6a6ac container test-container: <nil>
STEP: delete the pod
Jul  8 03:28:01.420: INFO: Waiting for pod pod-38067452-17c5-43e3-887d-1c9b04b6a6ac to disappear
Jul  8 03:28:01.423: INFO: Pod pod-38067452-17c5-43e3-887d-1c9b04b6a6ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:28:01.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3796" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":243,"skipped":3777,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:28:01.434: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9063
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-ee19812e-2fbd-4aa4-94e1-24796ef449c8
STEP: Creating a pod to test consume configMaps
Jul  8 03:28:01.590: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d887e830-ecd7-4c8b-9caf-727556e40e89" in namespace "projected-9063" to be "Succeeded or Failed"
Jul  8 03:28:01.599: INFO: Pod "pod-projected-configmaps-d887e830-ecd7-4c8b-9caf-727556e40e89": Phase="Pending", Reason="", readiness=false. Elapsed: 8.769599ms
Jul  8 03:28:03.610: INFO: Pod "pod-projected-configmaps-d887e830-ecd7-4c8b-9caf-727556e40e89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0201541s
STEP: Saw pod success
Jul  8 03:28:03.610: INFO: Pod "pod-projected-configmaps-d887e830-ecd7-4c8b-9caf-727556e40e89" satisfied condition "Succeeded or Failed"
Jul  8 03:28:03.613: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-projected-configmaps-d887e830-ecd7-4c8b-9caf-727556e40e89 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  8 03:28:03.654: INFO: Waiting for pod pod-projected-configmaps-d887e830-ecd7-4c8b-9caf-727556e40e89 to disappear
Jul  8 03:28:03.659: INFO: Pod pod-projected-configmaps-d887e830-ecd7-4c8b-9caf-727556e40e89 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:28:03.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9063" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":244,"skipped":3784,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:28:03.677: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4141
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jul  8 03:28:03.826: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:28:23.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4141" for this suite.

• [SLOW TEST:19.598 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":339,"completed":245,"skipped":3792,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:28:23.278: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-3648
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:28:23.424: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Creating first CR 
Jul  8 03:28:25.977: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-08T03:28:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-08T03:28:25Z]] name:name1 resourceVersion:47949 uid:0cde2d56-d064-4f20-8e59-558e8d1a6703] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jul  8 03:28:35.999: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-08T03:28:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-08T03:28:35Z]] name:name2 resourceVersion:47981 uid:8cbda38c-e474-47e9-ab68-32c736425a85] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jul  8 03:28:46.021: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-08T03:28:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-08T03:28:46Z]] name:name1 resourceVersion:48004 uid:0cde2d56-d064-4f20-8e59-558e8d1a6703] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jul  8 03:28:56.049: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-08T03:28:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-08T03:28:56Z]] name:name2 resourceVersion:48027 uid:8cbda38c-e474-47e9-ab68-32c736425a85] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jul  8 03:29:06.069: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-08T03:28:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-08T03:28:46Z]] name:name1 resourceVersion:48050 uid:0cde2d56-d064-4f20-8e59-558e8d1a6703] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jul  8 03:29:16.091: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-08T03:28:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-08T03:28:56Z]] name:name2 resourceVersion:48073 uid:8cbda38c-e474-47e9-ab68-32c736425a85] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:29:26.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-3648" for this suite.

• [SLOW TEST:63.350 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":339,"completed":246,"skipped":3794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:29:26.628: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-936
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-83a305ef-6d64-4c87-836b-1f394b1e262d
STEP: Creating a pod to test consume configMaps
Jul  8 03:29:26.798: INFO: Waiting up to 5m0s for pod "pod-configmaps-e38a9a00-9963-4bdb-84c3-b1cc34fdd90f" in namespace "configmap-936" to be "Succeeded or Failed"
Jul  8 03:29:26.803: INFO: Pod "pod-configmaps-e38a9a00-9963-4bdb-84c3-b1cc34fdd90f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.336317ms
Jul  8 03:29:28.810: INFO: Pod "pod-configmaps-e38a9a00-9963-4bdb-84c3-b1cc34fdd90f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011906381s
STEP: Saw pod success
Jul  8 03:29:28.810: INFO: Pod "pod-configmaps-e38a9a00-9963-4bdb-84c3-b1cc34fdd90f" satisfied condition "Succeeded or Failed"
Jul  8 03:29:28.812: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-configmaps-e38a9a00-9963-4bdb-84c3-b1cc34fdd90f container agnhost-container: <nil>
STEP: delete the pod
Jul  8 03:29:28.830: INFO: Waiting for pod pod-configmaps-e38a9a00-9963-4bdb-84c3-b1cc34fdd90f to disappear
Jul  8 03:29:28.832: INFO: Pod pod-configmaps-e38a9a00-9963-4bdb-84c3-b1cc34fdd90f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:29:28.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-936" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":247,"skipped":3830,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:29:28.844: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:29:29.330: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:29:32.352: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:29:32.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8843" for this suite.
STEP: Destroying namespace "webhook-8843-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":339,"completed":248,"skipped":3854,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:29:32.470: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:29:33.095: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:29:36.120: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:29:36.124: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:29:39.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2319" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.913 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":339,"completed":249,"skipped":3858,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:29:39.383: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-241
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jul  8 03:29:39.622: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:29:41.628: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jul  8 03:29:41.641: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:29:43.647: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul  8 03:29:43.661: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  8 03:29:43.665: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  8 03:29:45.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  8 03:29:45.672: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  8 03:29:47.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  8 03:29:47.672: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:29:47.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-241" for this suite.

• [SLOW TEST:8.299 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":339,"completed":250,"skipped":3866,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:29:47.684: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-700
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:29:47.833: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul  8 03:29:52.844: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jul  8 03:29:52.853: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jul  8 03:29:52.864: INFO: observed ReplicaSet test-rs in namespace replicaset-700 with ReadyReplicas 1, AvailableReplicas 1
Jul  8 03:29:52.927: INFO: observed ReplicaSet test-rs in namespace replicaset-700 with ReadyReplicas 1, AvailableReplicas 1
Jul  8 03:29:52.947: INFO: observed ReplicaSet test-rs in namespace replicaset-700 with ReadyReplicas 1, AvailableReplicas 1
Jul  8 03:29:52.969: INFO: observed ReplicaSet test-rs in namespace replicaset-700 with ReadyReplicas 1, AvailableReplicas 1
Jul  8 03:29:54.272: INFO: observed ReplicaSet test-rs in namespace replicaset-700 with ReadyReplicas 2, AvailableReplicas 2
Jul  8 03:29:54.672: INFO: observed Replicaset test-rs in namespace replicaset-700 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:29:54.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-700" for this suite.

• [SLOW TEST:7.002 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":339,"completed":251,"skipped":3883,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:29:54.686: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  8 03:29:56.851: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:29:56.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7650" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":252,"skipped":3897,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:29:56.885: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4449
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:29:57.602: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:30:00.627: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jul  8 03:30:00.648: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:30:00.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4449" for this suite.
STEP: Destroying namespace "webhook-4449-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":339,"completed":253,"skipped":3968,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:30:00.745: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-2263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jul  8 03:30:00.924: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Jul  8 03:30:01.469: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul  8 03:30:03.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 03:30:05.542: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 03:30:07.540: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 03:30:09.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 03:30:11.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 03:30:13.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 03:30:15.542: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761311801, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  8 03:30:18.467: INFO: Waited 916.914303ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jul  8 03:30:18.767: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:30:19.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2263" for this suite.

• [SLOW TEST:18.925 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":339,"completed":254,"skipped":3982,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:30:19.670: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:30:19.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-987" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":339,"completed":255,"skipped":3994,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:30:19.892: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-3ba116ac-0791-4c08-866d-f7b295184408-5909
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:30:20.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8122" for this suite.
STEP: Destroying namespace "nspatchtest-3ba116ac-0791-4c08-866d-f7b295184408-5909" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":339,"completed":256,"skipped":3996,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:30:20.189: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-484
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Jul  8 03:30:20.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-484 api-versions'
Jul  8 03:30:20.409: INFO: stderr: ""
Jul  8 03:30:20.409: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nhelm.cattle.io/v1\nk3s.cattle.io/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:30:20.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-484" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":339,"completed":257,"skipped":4041,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:30:20.422: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jul  8 03:30:20.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6159 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Jul  8 03:30:20.968: INFO: stderr: ""
Jul  8 03:30:20.968: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jul  8 03:30:20.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6159 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Jul  8 03:30:21.282: INFO: stderr: ""
Jul  8 03:30:21.282: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jul  8 03:30:21.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6159 delete pods e2e-test-httpd-pod'
Jul  8 03:30:28.514: INFO: stderr: ""
Jul  8 03:30:28.514: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:30:28.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6159" for this suite.

• [SLOW TEST:8.104 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:903
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":339,"completed":258,"skipped":4073,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:30:28.526: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1379
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:30:28.671: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul  8 03:30:28.678: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul  8 03:30:33.689: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  8 03:30:33.689: INFO: Creating deployment "test-rolling-update-deployment"
Jul  8 03:30:33.695: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul  8 03:30:33.705: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul  8 03:30:35.714: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul  8 03:30:35.716: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul  8 03:30:35.722: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1379  14684ede-add7-4a9f-b5b7-8413564dee4f 48919 1 2021-07-08 03:30:33 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-07-08 03:30:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-08 03:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f6b128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-08 03:30:33 +0000 UTC,LastTransitionTime:2021-07-08 03:30:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-07-08 03:30:34 +0000 UTC,LastTransitionTime:2021-07-08 03:30:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul  8 03:30:35.725: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-1379  6fb51251-1316-40d4-a5ab-fb48cd91070f 48908 1 2021-07-08 03:30:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 14684ede-add7-4a9f-b5b7-8413564dee4f 0xc002f6b5e7 0xc002f6b5e8}] []  [{kube-controller-manager Update apps/v1 2021-07-08 03:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14684ede-add7-4a9f-b5b7-8413564dee4f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f6b6f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul  8 03:30:35.725: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul  8 03:30:35.725: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1379  a94ba5d1-6029-464f-9f52-1ffb23f50aad 48917 2 2021-07-08 03:30:28 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 14684ede-add7-4a9f-b5b7-8413564dee4f 0xc002f6b4d7 0xc002f6b4d8}] []  [{e2e.test Update apps/v1 2021-07-08 03:30:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-08 03:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14684ede-add7-4a9f-b5b7-8413564dee4f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002f6b578 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  8 03:30:35.727: INFO: Pod "test-rolling-update-deployment-585b757574-hmrpk" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-hmrpk test-rolling-update-deployment-585b757574- deployment-1379  2cc903b0-6d1d-462e-9bdf-94115232dbb5 48907 0 2021-07-08 03:30:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/podIP:10.42.3.247/32 cni.projectcalico.org/podIPs:10.42.3.247/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 6fb51251-1316-40d4-a5ab-fb48cd91070f 0xc002f6bb97 0xc002f6bb98}] []  [{kube-controller-manager Update v1 2021-07-08 03:30:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fb51251-1316-40d4-a5ab-fb48cd91070f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-08 03:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-08 03:30:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-66q9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-66q9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:30:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:30:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:30:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:30:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.217,PodIP:10.42.3.247,StartTime:2021-07-08 03:30:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:30:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://17b30e1210dc5c29f034ddabceb5dc2e76144415431285d8749ba163b2c29a1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:30:35.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1379" for this suite.

• [SLOW TEST:7.210 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":259,"skipped":4085,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:30:35.736: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6868
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:30:36.200: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:30:39.219: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:30:39.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6868" for this suite.
STEP: Destroying namespace "webhook-6868-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":339,"completed":260,"skipped":4092,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:30:39.545: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3250
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-3250
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  8 03:30:39.692: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul  8 03:30:39.782: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:30:41.789: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 03:30:43.789: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 03:30:45.789: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 03:30:47.788: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 03:30:49.790: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 03:30:51.789: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 03:30:53.789: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 03:30:55.791: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 03:30:57.787: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  8 03:30:59.795: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul  8 03:30:59.813: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul  8 03:30:59.821: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jul  8 03:30:59.825: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jul  8 03:31:01.869: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jul  8 03:31:01.869: INFO: Going to poll 10.42.0.25 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jul  8 03:31:01.881: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.0.25 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3250 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:31:01.881: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:31:02.976: INFO: Found all 1 expected endpoints: [netserver-0]
Jul  8 03:31:02.976: INFO: Going to poll 10.42.3.249 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jul  8 03:31:02.983: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.3.249 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3250 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:31:02.983: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:31:04.045: INFO: Found all 1 expected endpoints: [netserver-1]
Jul  8 03:31:04.045: INFO: Going to poll 10.42.1.67 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jul  8 03:31:04.055: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.1.67 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3250 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:31:04.055: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:31:05.140: INFO: Found all 1 expected endpoints: [netserver-2]
Jul  8 03:31:05.140: INFO: Going to poll 10.42.2.46 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jul  8 03:31:05.145: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.2.46 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3250 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:31:05.145: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:31:06.215: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:31:06.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3250" for this suite.

• [SLOW TEST:26.691 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":261,"skipped":4104,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:31:06.237: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Jul  8 03:31:06.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-2084 cluster-info'
Jul  8 03:31:06.474: INFO: stderr: ""
Jul  8 03:31:06.474: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.43.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:31:06.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2084" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":339,"completed":262,"skipped":4106,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:31:06.483: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4422
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul  8 03:31:06.632: INFO: Waiting up to 5m0s for pod "downward-api-1b7e594b-192c-4908-b38b-2fa2bb561032" in namespace "downward-api-4422" to be "Succeeded or Failed"
Jul  8 03:31:06.644: INFO: Pod "downward-api-1b7e594b-192c-4908-b38b-2fa2bb561032": Phase="Pending", Reason="", readiness=false. Elapsed: 12.678193ms
Jul  8 03:31:08.651: INFO: Pod "downward-api-1b7e594b-192c-4908-b38b-2fa2bb561032": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019225682s
STEP: Saw pod success
Jul  8 03:31:08.651: INFO: Pod "downward-api-1b7e594b-192c-4908-b38b-2fa2bb561032" satisfied condition "Succeeded or Failed"
Jul  8 03:31:08.653: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downward-api-1b7e594b-192c-4908-b38b-2fa2bb561032 container dapi-container: <nil>
STEP: delete the pod
Jul  8 03:31:08.673: INFO: Waiting for pod downward-api-1b7e594b-192c-4908-b38b-2fa2bb561032 to disappear
Jul  8 03:31:08.677: INFO: Pod downward-api-1b7e594b-192c-4908-b38b-2fa2bb561032 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:31:08.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4422" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":339,"completed":263,"skipped":4112,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:31:08.686: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-70ce6fea-d40b-4254-933d-375f2f517151
STEP: Creating a pod to test consume configMaps
Jul  8 03:31:08.843: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-10becc5e-bb84-41eb-9f97-ed6ff17ef5f6" in namespace "projected-8730" to be "Succeeded or Failed"
Jul  8 03:31:08.848: INFO: Pod "pod-projected-configmaps-10becc5e-bb84-41eb-9f97-ed6ff17ef5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.456951ms
Jul  8 03:31:10.856: INFO: Pod "pod-projected-configmaps-10becc5e-bb84-41eb-9f97-ed6ff17ef5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013153346s
Jul  8 03:31:12.863: INFO: Pod "pod-projected-configmaps-10becc5e-bb84-41eb-9f97-ed6ff17ef5f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019769395s
STEP: Saw pod success
Jul  8 03:31:12.863: INFO: Pod "pod-projected-configmaps-10becc5e-bb84-41eb-9f97-ed6ff17ef5f6" satisfied condition "Succeeded or Failed"
Jul  8 03:31:12.866: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-projected-configmaps-10becc5e-bb84-41eb-9f97-ed6ff17ef5f6 container agnhost-container: <nil>
STEP: delete the pod
Jul  8 03:31:12.887: INFO: Waiting for pod pod-projected-configmaps-10becc5e-bb84-41eb-9f97-ed6ff17ef5f6 to disappear
Jul  8 03:31:12.889: INFO: Pod pod-projected-configmaps-10becc5e-bb84-41eb-9f97-ed6ff17ef5f6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:31:12.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8730" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":264,"skipped":4121,"failed":0}
S
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:31:12.898: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jul  8 03:31:13.063: INFO: observed Pod pod-test in namespace pods-5585 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jul  8 03:31:13.067: INFO: observed Pod pod-test in namespace pods-5585 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC  }]
Jul  8 03:31:13.079: INFO: observed Pod pod-test in namespace pods-5585 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC  }]
Jul  8 03:31:13.505: INFO: observed Pod pod-test in namespace pods-5585 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC  }]
Jul  8 03:31:13.882: INFO: Found Pod pod-test in namespace pods-5585 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-08 03:31:13 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jul  8 03:31:13.910: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jul  8 03:31:13.961: INFO: observed event type ADDED
Jul  8 03:31:13.964: INFO: observed event type MODIFIED
Jul  8 03:31:13.970: INFO: observed event type MODIFIED
Jul  8 03:31:13.971: INFO: observed event type MODIFIED
Jul  8 03:31:13.971: INFO: observed event type MODIFIED
Jul  8 03:31:13.972: INFO: observed event type MODIFIED
Jul  8 03:31:13.972: INFO: observed event type MODIFIED
Jul  8 03:31:13.972: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:31:13.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5585" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":339,"completed":265,"skipped":4122,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:31:13.985: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5613
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul  8 03:31:16.157: INFO: &Pod{ObjectMeta:{send-events-28a6b6fc-f1cf-40d9-bcc1-52ef805216c3  events-5613  a610f14b-5efa-47d4-8009-d08541babb86 49334 0 2021-07-08 03:31:14 +0000 UTC <nil> <nil> map[name:foo time:122438496] map[cni.projectcalico.org/podIP:10.42.3.254/32 cni.projectcalico.org/podIPs:10.42.3.254/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{calico Update v1 2021-07-08 03:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {e2e.test Update v1 2021-07-08 03:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-08 03:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wxrnd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wxrnd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:31:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:31:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.217,PodIP:10.42.3.254,StartTime:2021-07-08 03:31:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:31:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://2cc7269ba21807634d43ef1b0eeb3400be7c106e5cbfe5dba1587fa6076e6f5e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.254,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jul  8 03:31:18.166: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul  8 03:31:20.176: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:31:20.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5613" for this suite.

• [SLOW TEST:6.226 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":339,"completed":266,"skipped":4134,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:31:20.211: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6693
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-8cef6efe-f841-49c4-b382-6d17e90f4d88
STEP: Creating a pod to test consume secrets
Jul  8 03:31:20.378: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1269b256-6e4b-49d7-92be-2d67ddbc054f" in namespace "projected-6693" to be "Succeeded or Failed"
Jul  8 03:31:20.385: INFO: Pod "pod-projected-secrets-1269b256-6e4b-49d7-92be-2d67ddbc054f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.685488ms
Jul  8 03:31:22.393: INFO: Pod "pod-projected-secrets-1269b256-6e4b-49d7-92be-2d67ddbc054f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01521018s
STEP: Saw pod success
Jul  8 03:31:22.393: INFO: Pod "pod-projected-secrets-1269b256-6e4b-49d7-92be-2d67ddbc054f" satisfied condition "Succeeded or Failed"
Jul  8 03:31:22.398: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-projected-secrets-1269b256-6e4b-49d7-92be-2d67ddbc054f container secret-volume-test: <nil>
STEP: delete the pod
Jul  8 03:31:22.420: INFO: Waiting for pod pod-projected-secrets-1269b256-6e4b-49d7-92be-2d67ddbc054f to disappear
Jul  8 03:31:22.423: INFO: Pod pod-projected-secrets-1269b256-6e4b-49d7-92be-2d67ddbc054f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:31:22.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6693" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":267,"skipped":4154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:31:22.440: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9689
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:31:23.134: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:31:26.164: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:31:37.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9689" for this suite.
STEP: Destroying namespace "webhook-9689-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.940 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":339,"completed":268,"skipped":4179,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:31:37.381: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:31:50.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2559" for this suite.

• [SLOW TEST:13.253 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":339,"completed":269,"skipped":4201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:31:50.635: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-827
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-35a86a73-c0c1-42f7-b351-58f3fc773b84
STEP: Creating configMap with name cm-test-opt-upd-ec43189f-b180-4bff-bdf6-9227830fe2c8
STEP: Creating the pod
Jul  8 03:31:50.823: INFO: The status of Pod pod-configmaps-303283a0-28c4-4c4a-b1aa-bb224f1ff207 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:31:52.830: INFO: The status of Pod pod-configmaps-303283a0-28c4-4c4a-b1aa-bb224f1ff207 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-35a86a73-c0c1-42f7-b351-58f3fc773b84
STEP: Updating configmap cm-test-opt-upd-ec43189f-b180-4bff-bdf6-9227830fe2c8
STEP: Creating configMap with name cm-test-opt-create-1bb391f2-4d09-4f2c-8916-ab5cea3c2d65
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:31:54.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-827" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":270,"skipped":4236,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:31:54.895: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6616
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:31:57.075: INFO: Deleting pod "var-expansion-c6cb750a-f34c-4ddc-a3be-0f6271f74539" in namespace "var-expansion-6616"
Jul  8 03:31:57.082: INFO: Wait up to 5m0s for pod "var-expansion-c6cb750a-f34c-4ddc-a3be-0f6271f74539" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:32:09.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6616" for this suite.

• [SLOW TEST:14.207 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":339,"completed":271,"skipped":4261,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:32:09.103: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4117
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:32:09.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4117" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":339,"completed":272,"skipped":4292,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:32:09.302: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8612
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8612
I0708 03:32:09.499877      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8612, replica count: 2
Jul  8 03:32:12.551: INFO: Creating new exec pod
I0708 03:32:12.551287      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  8 03:32:15.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-8612 exec execpodmzg5s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul  8 03:32:15.800: INFO: stderr: "+ echo hostName+ nc -v -t -w 2 externalname-service 80\n\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul  8 03:32:15.800: INFO: stdout: "externalname-service-jz6g9"
Jul  8 03:32:15.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-8612 exec execpodmzg5s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.194.111 80'
Jul  8 03:32:15.983: INFO: stderr: "+ nc -v -t -w 2 10.43.194.111 80\n+ echo hostName\nConnection to 10.43.194.111 80 port [tcp/http] succeeded!\n"
Jul  8 03:32:15.983: INFO: stdout: ""
Jul  8 03:32:16.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-8612 exec execpodmzg5s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.194.111 80'
Jul  8 03:32:17.139: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.194.111 80\nConnection to 10.43.194.111 80 port [tcp/http] succeeded!\n"
Jul  8 03:32:17.139: INFO: stdout: ""
Jul  8 03:32:17.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-8612 exec execpodmzg5s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.194.111 80'
Jul  8 03:32:18.272: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.194.111 80\nConnection to 10.43.194.111 80 port [tcp/http] succeeded!\n"
Jul  8 03:32:18.272: INFO: stdout: "externalname-service-b544q"
Jul  8 03:32:18.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-8612 exec execpodmzg5s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.6.217 32735'
Jul  8 03:32:18.464: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.6.217 32735\nConnection to 172.31.6.217 32735 port [tcp/*] succeeded!\n"
Jul  8 03:32:18.464: INFO: stdout: "externalname-service-jz6g9"
Jul  8 03:32:18.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-8612 exec execpodmzg5s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.8.165 32735'
Jul  8 03:32:18.627: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.8.165 32735\nConnection to 172.31.8.165 32735 port [tcp/*] succeeded!\n"
Jul  8 03:32:18.627: INFO: stdout: "externalname-service-jz6g9"
Jul  8 03:32:18.627: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:32:18.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8612" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:9.395 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":339,"completed":273,"skipped":4300,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:32:18.698: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1219
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Jul  8 03:32:18.854: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-1219 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:32:18.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1219" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":339,"completed":274,"skipped":4303,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:32:18.933: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:32:19.336: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:32:22.360: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:32:22.365: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4355-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:32:25.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5925" for this suite.
STEP: Destroying namespace "webhook-5925-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.678 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":339,"completed":275,"skipped":4360,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:32:25.612: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2703
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jul  8 03:32:25.795: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:32:27.799: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jul  8 03:32:27.810: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:32:29.816: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jul  8 03:32:29.826: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  8 03:32:29.835: INFO: Pod pod-with-prestop-http-hook still exists
Jul  8 03:32:31.840: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  8 03:32:31.847: INFO: Pod pod-with-prestop-http-hook still exists
Jul  8 03:32:33.840: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  8 03:32:33.846: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:32:33.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2703" for this suite.

• [SLOW TEST:8.249 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":339,"completed":276,"skipped":4403,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:32:33.863: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7929
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:33:02.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7929" for this suite.

• [SLOW TEST:28.263 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":339,"completed":277,"skipped":4412,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:33:02.126: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-9130
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jul  8 03:33:02.312: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jul  8 03:33:02.323: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul  8 03:33:02.324: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jul  8 03:33:02.339: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul  8 03:33:02.339: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jul  8 03:33:02.359: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jul  8 03:33:02.359: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jul  8 03:33:09.408: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:33:09.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9130" for this suite.

• [SLOW TEST:7.311 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":339,"completed":278,"skipped":4421,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:33:09.439: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4698
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-f6349a27-65de-4991-9260-6c2043366f60
STEP: Creating a pod to test consume configMaps
Jul  8 03:33:09.591: INFO: Waiting up to 5m0s for pod "pod-configmaps-db79ca5a-2258-460b-a3d9-5010c2f6e01f" in namespace "configmap-4698" to be "Succeeded or Failed"
Jul  8 03:33:09.604: INFO: Pod "pod-configmaps-db79ca5a-2258-460b-a3d9-5010c2f6e01f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.118944ms
Jul  8 03:33:11.612: INFO: Pod "pod-configmaps-db79ca5a-2258-460b-a3d9-5010c2f6e01f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020181261s
STEP: Saw pod success
Jul  8 03:33:11.612: INFO: Pod "pod-configmaps-db79ca5a-2258-460b-a3d9-5010c2f6e01f" satisfied condition "Succeeded or Failed"
Jul  8 03:33:11.614: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-configmaps-db79ca5a-2258-460b-a3d9-5010c2f6e01f container agnhost-container: <nil>
STEP: delete the pod
Jul  8 03:33:11.632: INFO: Waiting for pod pod-configmaps-db79ca5a-2258-460b-a3d9-5010c2f6e01f to disappear
Jul  8 03:33:11.636: INFO: Pod pod-configmaps-db79ca5a-2258-460b-a3d9-5010c2f6e01f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:33:11.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4698" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":279,"skipped":4428,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:33:11.645: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-3148
STEP: creating replication controller nodeport-test in namespace services-3148
I0708 03:33:11.848077      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-3148, replica count: 2
I0708 03:33:14.899916      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  8 03:33:14.899: INFO: Creating new exec pod
Jul  8 03:33:17.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-3148 exec execpodwcm42 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul  8 03:33:18.142: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul  8 03:33:18.142: INFO: stdout: ""
Jul  8 03:33:19.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-3148 exec execpodwcm42 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul  8 03:33:19.313: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul  8 03:33:19.313: INFO: stdout: ""
Jul  8 03:33:20.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-3148 exec execpodwcm42 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul  8 03:33:20.307: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul  8 03:33:20.307: INFO: stdout: "nodeport-test-vsjlv"
Jul  8 03:33:20.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-3148 exec execpodwcm42 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.117.232 80'
Jul  8 03:33:20.453: INFO: stderr: "+ nc -v -t -w 2 10.43.117.232 80\n+ echo hostName\nConnection to 10.43.117.232 80 port [tcp/http] succeeded!\n"
Jul  8 03:33:20.453: INFO: stdout: "nodeport-test-7f762"
Jul  8 03:33:20.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-3148 exec execpodwcm42 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.3.228 31844'
Jul  8 03:33:20.622: INFO: stderr: "+ nc -v -t -w 2 172.31.3.228 31844\n+ echo hostName\nConnection to 172.31.3.228 31844 port [tcp/*] succeeded!\n"
Jul  8 03:33:20.622: INFO: stdout: "nodeport-test-vsjlv"
Jul  8 03:33:20.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-3148 exec execpodwcm42 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.8.165 31844'
Jul  8 03:33:20.775: INFO: stderr: "+ nc -v -t -w 2 172.31.8.165 31844\n+ echo hostName\nConnection to 172.31.8.165 31844 port [tcp/*] succeeded!\n"
Jul  8 03:33:20.775: INFO: stdout: ""
Jul  8 03:33:21.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-3148 exec execpodwcm42 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.8.165 31844'
Jul  8 03:33:21.927: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.8.165 31844\nConnection to 172.31.8.165 31844 port [tcp/*] succeeded!\n"
Jul  8 03:33:21.927: INFO: stdout: "nodeport-test-7f762"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:33:21.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3148" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:10.300 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":339,"completed":280,"skipped":4432,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:33:21.945: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-2527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Jul  8 03:33:22.103: INFO: created test-podtemplate-1
Jul  8 03:33:22.107: INFO: created test-podtemplate-2
Jul  8 03:33:22.112: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jul  8 03:33:22.114: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jul  8 03:33:22.129: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:33:22.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2527" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":339,"completed":281,"skipped":4444,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:33:22.152: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:33:22.730: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:33:25.758: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:33:37.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3906" for this suite.
STEP: Destroying namespace "webhook-3906-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.847 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":339,"completed":282,"skipped":4461,"failed":0}
SS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:33:38.005: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2396
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jul  8 03:33:38.220: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:33:40.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2396" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":339,"completed":283,"skipped":4463,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:33:40.274: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9403
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9403
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9403
STEP: creating replication controller externalsvc in namespace services-9403
I0708 03:33:40.481399      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9403, replica count: 2
I0708 03:33:43.531733      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jul  8 03:33:43.571: INFO: Creating new exec pod
Jul  8 03:33:45.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-9403 exec execpodf6wms -- /bin/sh -x -c nslookup clusterip-service.services-9403.svc.cluster.local'
Jul  8 03:33:45.992: INFO: stderr: "+ nslookup clusterip-service.services-9403.svc.cluster.local\n"
Jul  8 03:33:45.992: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nclusterip-service.services-9403.svc.cluster.local\tcanonical name = externalsvc.services-9403.svc.cluster.local.\nName:\texternalsvc.services-9403.svc.cluster.local\nAddress: 10.43.128.128\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9403, will wait for the garbage collector to delete the pods
Jul  8 03:33:46.063: INFO: Deleting ReplicationController externalsvc took: 8.501885ms
Jul  8 03:33:46.164: INFO: Terminating ReplicationController externalsvc pods took: 100.592351ms
Jul  8 03:33:58.627: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:33:58.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9403" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:18.395 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":339,"completed":284,"skipped":4473,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:33:58.669: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul  8 03:33:58.822: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9285  cb9929f1-0685-4047-85c4-cb74007e0c8e 50679 0 2021-07-08 03:33:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-08 03:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 03:33:58.823: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9285  cb9929f1-0685-4047-85c4-cb74007e0c8e 50679 0 2021-07-08 03:33:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-08 03:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul  8 03:34:08.835: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9285  cb9929f1-0685-4047-85c4-cb74007e0c8e 50729 0 2021-07-08 03:33:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-08 03:34:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 03:34:08.835: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9285  cb9929f1-0685-4047-85c4-cb74007e0c8e 50729 0 2021-07-08 03:33:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-08 03:34:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul  8 03:34:18.846: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9285  cb9929f1-0685-4047-85c4-cb74007e0c8e 50752 0 2021-07-08 03:33:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-08 03:34:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 03:34:18.847: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9285  cb9929f1-0685-4047-85c4-cb74007e0c8e 50752 0 2021-07-08 03:33:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-08 03:34:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul  8 03:34:28.855: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9285  cb9929f1-0685-4047-85c4-cb74007e0c8e 50775 0 2021-07-08 03:33:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-08 03:34:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 03:34:28.856: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9285  cb9929f1-0685-4047-85c4-cb74007e0c8e 50775 0 2021-07-08 03:33:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-08 03:34:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul  8 03:34:38.866: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9285  95b44cd8-8337-40c8-9c27-a363a448dccc 50798 0 2021-07-08 03:34:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-08 03:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 03:34:38.866: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9285  95b44cd8-8337-40c8-9c27-a363a448dccc 50798 0 2021-07-08 03:34:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-08 03:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul  8 03:34:48.883: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9285  95b44cd8-8337-40c8-9c27-a363a448dccc 50822 0 2021-07-08 03:34:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-08 03:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  8 03:34:48.883: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9285  95b44cd8-8337-40c8-9c27-a363a448dccc 50822 0 2021-07-08 03:34:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-08 03:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:34:58.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9285" for this suite.

• [SLOW TEST:60.228 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":339,"completed":285,"skipped":4489,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:34:58.898: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6782
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-5c581338-e659-4d96-83bf-d03ee4816229
STEP: Creating configMap with name cm-test-opt-upd-067c4bfb-e1fd-483c-8756-c76d28208559
STEP: Creating the pod
Jul  8 03:34:59.075: INFO: The status of Pod pod-projected-configmaps-0f4a7843-aedc-4cb5-bf4f-d14325cfb8d1 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:35:01.082: INFO: The status of Pod pod-projected-configmaps-0f4a7843-aedc-4cb5-bf4f-d14325cfb8d1 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-5c581338-e659-4d96-83bf-d03ee4816229
STEP: Updating configmap cm-test-opt-upd-067c4bfb-e1fd-483c-8756-c76d28208559
STEP: Creating configMap with name cm-test-opt-create-8adf3cbd-ebbe-4c47-9fa8-1caebd322da4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:35:03.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6782" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":286,"skipped":4506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:35:03.167: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-507
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:35:03.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-507" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":339,"completed":287,"skipped":4534,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:35:03.346: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul  8 03:35:03.515: INFO: Waiting up to 5m0s for pod "pod-a2618303-0453-4901-8efa-38d799c0a21e" in namespace "emptydir-5325" to be "Succeeded or Failed"
Jul  8 03:35:03.521: INFO: Pod "pod-a2618303-0453-4901-8efa-38d799c0a21e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274941ms
Jul  8 03:35:05.527: INFO: Pod "pod-a2618303-0453-4901-8efa-38d799c0a21e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011860287s
STEP: Saw pod success
Jul  8 03:35:05.527: INFO: Pod "pod-a2618303-0453-4901-8efa-38d799c0a21e" satisfied condition "Succeeded or Failed"
Jul  8 03:35:05.529: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-a2618303-0453-4901-8efa-38d799c0a21e container test-container: <nil>
STEP: delete the pod
Jul  8 03:35:05.555: INFO: Waiting for pod pod-a2618303-0453-4901-8efa-38d799c0a21e to disappear
Jul  8 03:35:05.559: INFO: Pod pod-a2618303-0453-4901-8efa-38d799c0a21e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:35:05.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5325" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":288,"skipped":4534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:35:05.571: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5420
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5420
I0708 03:35:05.764900      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5420, replica count: 2
I0708 03:35:08.816711      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  8 03:35:08.816: INFO: Creating new exec pod
Jul  8 03:35:11.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-5420 exec execpodwjwlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul  8 03:35:12.029: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul  8 03:35:12.029: INFO: stdout: "externalname-service-lk8vm"
Jul  8 03:35:12.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-5420 exec execpodwjwlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.37.74 80'
Jul  8 03:35:12.197: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.37.74 80\nConnection to 10.43.37.74 80 port [tcp/http] succeeded!\n"
Jul  8 03:35:12.197: INFO: stdout: ""
Jul  8 03:35:13.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-5420 exec execpodwjwlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.37.74 80'
Jul  8 03:35:13.383: INFO: stderr: "+ nc -v -t -w 2 10.43.37.74 80\n+ echo hostName\nConnection to 10.43.37.74 80 port [tcp/http] succeeded!\n"
Jul  8 03:35:13.383: INFO: stdout: ""
Jul  8 03:35:14.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-5420 exec execpodwjwlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.37.74 80'
Jul  8 03:35:14.355: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.37.74 80\nConnection to 10.43.37.74 80 port [tcp/http] succeeded!\n"
Jul  8 03:35:14.355: INFO: stdout: ""
Jul  8 03:35:15.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=services-5420 exec execpodwjwlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.37.74 80'
Jul  8 03:35:15.383: INFO: stderr: "+ nc -v -t -w 2 10.43.37.74 80\nConnection to 10.43.37.74 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jul  8 03:35:15.383: INFO: stdout: "externalname-service-wp4f9"
Jul  8 03:35:15.383: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:35:15.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5420" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:9.950 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":339,"completed":289,"skipped":4557,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:35:15.521: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-4564
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-4564
Jul  8 03:35:15.755: INFO: Found 0 stateful pods, waiting for 1
Jul  8 03:35:25.774: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul  8 03:35:25.815: INFO: Deleting all statefulset in ns statefulset-4564
Jul  8 03:35:25.825: INFO: Scaling statefulset ss to 0
Jul  8 03:35:55.849: INFO: Waiting for statefulset status.replicas updated to 0
Jul  8 03:35:55.851: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:35:55.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4564" for this suite.

• [SLOW TEST:40.379 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":339,"completed":290,"skipped":4561,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:35:55.901: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9212
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jul  8 03:35:56.078: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:35:58.085: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul  8 03:35:59.103: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:36:00.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9212" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":339,"completed":291,"skipped":4563,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:36:00.145: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1976
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-f0728cbe-aa57-4adc-880e-4661b1681121
STEP: Creating a pod to test consume configMaps
Jul  8 03:36:00.312: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-496055a0-957a-4846-af80-890fc84b2827" in namespace "projected-1976" to be "Succeeded or Failed"
Jul  8 03:36:00.317: INFO: Pod "pod-projected-configmaps-496055a0-957a-4846-af80-890fc84b2827": Phase="Pending", Reason="", readiness=false. Elapsed: 5.088475ms
Jul  8 03:36:02.324: INFO: Pod "pod-projected-configmaps-496055a0-957a-4846-af80-890fc84b2827": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011597225s
STEP: Saw pod success
Jul  8 03:36:02.324: INFO: Pod "pod-projected-configmaps-496055a0-957a-4846-af80-890fc84b2827" satisfied condition "Succeeded or Failed"
Jul  8 03:36:02.326: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-projected-configmaps-496055a0-957a-4846-af80-890fc84b2827 container agnhost-container: <nil>
STEP: delete the pod
Jul  8 03:36:02.351: INFO: Waiting for pod pod-projected-configmaps-496055a0-957a-4846-af80-890fc84b2827 to disappear
Jul  8 03:36:02.353: INFO: Pod pod-projected-configmaps-496055a0-957a-4846-af80-890fc84b2827 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:36:02.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1976" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":292,"skipped":4568,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:36:02.361: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0708 03:36:12.680342      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul  8 03:41:12.686: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Jul  8 03:41:12.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdx7r" in namespace "gc-2803"
Jul  8 03:41:12.724: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcwx2" in namespace "gc-2803"
Jul  8 03:41:12.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-fssdm" in namespace "gc-2803"
Jul  8 03:41:12.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-gj7cb" in namespace "gc-2803"
Jul  8 03:41:12.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-gskq8" in namespace "gc-2803"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:41:12.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2803" for this suite.

• [SLOW TEST:310.462 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":339,"completed":293,"skipped":4572,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:41:12.824: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2855
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul  8 03:41:12.995: INFO: Waiting up to 5m0s for pod "downward-api-966da1c3-1331-47c9-8c98-76db1399a3ed" in namespace "downward-api-2855" to be "Succeeded or Failed"
Jul  8 03:41:13.005: INFO: Pod "downward-api-966da1c3-1331-47c9-8c98-76db1399a3ed": Phase="Pending", Reason="", readiness=false. Elapsed: 9.138029ms
Jul  8 03:41:15.013: INFO: Pod "downward-api-966da1c3-1331-47c9-8c98-76db1399a3ed": Phase="Running", Reason="", readiness=true. Elapsed: 2.017094865s
Jul  8 03:41:17.020: INFO: Pod "downward-api-966da1c3-1331-47c9-8c98-76db1399a3ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023999194s
STEP: Saw pod success
Jul  8 03:41:17.020: INFO: Pod "downward-api-966da1c3-1331-47c9-8c98-76db1399a3ed" satisfied condition "Succeeded or Failed"
Jul  8 03:41:17.022: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downward-api-966da1c3-1331-47c9-8c98-76db1399a3ed container dapi-container: <nil>
STEP: delete the pod
Jul  8 03:41:17.048: INFO: Waiting for pod downward-api-966da1c3-1331-47c9-8c98-76db1399a3ed to disappear
Jul  8 03:41:17.050: INFO: Pod downward-api-966da1c3-1331-47c9-8c98-76db1399a3ed no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:41:17.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2855" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":339,"completed":294,"skipped":4576,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:41:17.062: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7613
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Jul  8 03:41:17.217: INFO: Waiting up to 5m0s for pod "test-pod-b54c6800-3a79-4a58-902c-f044d7308650" in namespace "svcaccounts-7613" to be "Succeeded or Failed"
Jul  8 03:41:17.224: INFO: Pod "test-pod-b54c6800-3a79-4a58-902c-f044d7308650": Phase="Pending", Reason="", readiness=false. Elapsed: 6.852134ms
Jul  8 03:41:19.231: INFO: Pod "test-pod-b54c6800-3a79-4a58-902c-f044d7308650": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013859939s
STEP: Saw pod success
Jul  8 03:41:19.231: INFO: Pod "test-pod-b54c6800-3a79-4a58-902c-f044d7308650" satisfied condition "Succeeded or Failed"
Jul  8 03:41:19.233: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod test-pod-b54c6800-3a79-4a58-902c-f044d7308650 container agnhost-container: <nil>
STEP: delete the pod
Jul  8 03:41:19.248: INFO: Waiting for pod test-pod-b54c6800-3a79-4a58-902c-f044d7308650 to disappear
Jul  8 03:41:19.251: INFO: Pod test-pod-b54c6800-3a79-4a58-902c-f044d7308650 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:41:19.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7613" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":339,"completed":295,"skipped":4619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:41:19.260: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:41:24.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9538" for this suite.

• [SLOW TEST:5.508 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":339,"completed":296,"skipped":4645,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:41:24.768: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:41:24.932: INFO: Create a RollingUpdate DaemonSet
Jul  8 03:41:24.936: INFO: Check that daemon pods launch on every node of the cluster
Jul  8 03:41:24.944: INFO: Number of nodes with available pods: 0
Jul  8 03:41:24.944: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 03:41:25.963: INFO: Number of nodes with available pods: 0
Jul  8 03:41:25.963: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 03:41:26.953: INFO: Number of nodes with available pods: 4
Jul  8 03:41:26.953: INFO: Number of running nodes: 4, number of available pods: 4
Jul  8 03:41:26.953: INFO: Update the DaemonSet to trigger a rollout
Jul  8 03:41:26.961: INFO: Updating DaemonSet daemon-set
Jul  8 03:41:35.980: INFO: Roll back the DaemonSet before rollout is complete
Jul  8 03:41:35.988: INFO: Updating DaemonSet daemon-set
Jul  8 03:41:35.989: INFO: Make sure DaemonSet rollback is complete
Jul  8 03:41:35.993: INFO: Wrong image for pod: daemon-set-ttwwb. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Jul  8 03:41:35.993: INFO: Pod daemon-set-ttwwb is not available
Jul  8 03:41:46.004: INFO: Pod daemon-set-jcm45 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4960, will wait for the garbage collector to delete the pods
Jul  8 03:41:46.077: INFO: Deleting DaemonSet.extensions daemon-set took: 7.37983ms
Jul  8 03:41:46.178: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.565704ms
Jul  8 03:41:58.593: INFO: Number of nodes with available pods: 0
Jul  8 03:41:58.593: INFO: Number of running nodes: 0, number of available pods: 0
Jul  8 03:41:58.596: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52717"},"items":null}

Jul  8 03:41:58.597: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52717"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:41:58.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4960" for this suite.

• [SLOW TEST:33.849 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":339,"completed":297,"skipped":4653,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:41:58.617: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:41:59.456: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:42:02.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:02.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7476" for this suite.
STEP: Destroying namespace "webhook-7476-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":339,"completed":298,"skipped":4653,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:02.679: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3022
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  8 03:42:04.876: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:04.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3022" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":339,"completed":299,"skipped":4661,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:04.899: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3854
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul  8 03:42:05.050: INFO: Waiting up to 5m0s for pod "pod-b7d89e3c-cb8d-4209-95e5-415e445ae886" in namespace "emptydir-3854" to be "Succeeded or Failed"
Jul  8 03:42:05.057: INFO: Pod "pod-b7d89e3c-cb8d-4209-95e5-415e445ae886": Phase="Pending", Reason="", readiness=false. Elapsed: 6.851434ms
Jul  8 03:42:07.072: INFO: Pod "pod-b7d89e3c-cb8d-4209-95e5-415e445ae886": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022004737s
STEP: Saw pod success
Jul  8 03:42:07.072: INFO: Pod "pod-b7d89e3c-cb8d-4209-95e5-415e445ae886" satisfied condition "Succeeded or Failed"
Jul  8 03:42:07.074: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-b7d89e3c-cb8d-4209-95e5-415e445ae886 container test-container: <nil>
STEP: delete the pod
Jul  8 03:42:07.095: INFO: Waiting for pod pod-b7d89e3c-cb8d-4209-95e5-415e445ae886 to disappear
Jul  8 03:42:07.102: INFO: Pod pod-b7d89e3c-cb8d-4209-95e5-415e445ae886 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:07.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3854" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":300,"skipped":4661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:07.111: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-a69ad39d-6280-4351-a203-72ac449c75a1
STEP: Creating a pod to test consume configMaps
Jul  8 03:42:07.267: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b175fbf1-e208-4f4c-9727-c2669fc31ab5" in namespace "projected-6247" to be "Succeeded or Failed"
Jul  8 03:42:07.276: INFO: Pod "pod-projected-configmaps-b175fbf1-e208-4f4c-9727-c2669fc31ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.844927ms
Jul  8 03:42:09.282: INFO: Pod "pod-projected-configmaps-b175fbf1-e208-4f4c-9727-c2669fc31ab5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015515573s
STEP: Saw pod success
Jul  8 03:42:09.282: INFO: Pod "pod-projected-configmaps-b175fbf1-e208-4f4c-9727-c2669fc31ab5" satisfied condition "Succeeded or Failed"
Jul  8 03:42:09.284: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-projected-configmaps-b175fbf1-e208-4f4c-9727-c2669fc31ab5 container agnhost-container: <nil>
STEP: delete the pod
Jul  8 03:42:09.303: INFO: Waiting for pod pod-projected-configmaps-b175fbf1-e208-4f4c-9727-c2669fc31ab5 to disappear
Jul  8 03:42:09.305: INFO: Pod pod-projected-configmaps-b175fbf1-e208-4f4c-9727-c2669fc31ab5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:09.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6247" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":301,"skipped":4695,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:09.313: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-75
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jul  8 03:42:09.461: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:27.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-75" for this suite.

• [SLOW TEST:17.833 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":339,"completed":302,"skipped":4705,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:27.148: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2595
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 03:42:27.304: INFO: Waiting up to 5m0s for pod "downwardapi-volume-616473f1-114f-423c-8ac3-8cd9a5135619" in namespace "downward-api-2595" to be "Succeeded or Failed"
Jul  8 03:42:27.307: INFO: Pod "downwardapi-volume-616473f1-114f-423c-8ac3-8cd9a5135619": Phase="Pending", Reason="", readiness=false. Elapsed: 2.914343ms
Jul  8 03:42:29.317: INFO: Pod "downwardapi-volume-616473f1-114f-423c-8ac3-8cd9a5135619": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013847397s
STEP: Saw pod success
Jul  8 03:42:29.317: INFO: Pod "downwardapi-volume-616473f1-114f-423c-8ac3-8cd9a5135619" satisfied condition "Succeeded or Failed"
Jul  8 03:42:29.320: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-616473f1-114f-423c-8ac3-8cd9a5135619 container client-container: <nil>
STEP: delete the pod
Jul  8 03:42:29.341: INFO: Waiting for pod downwardapi-volume-616473f1-114f-423c-8ac3-8cd9a5135619 to disappear
Jul  8 03:42:29.350: INFO: Pod downwardapi-volume-616473f1-114f-423c-8ac3-8cd9a5135619 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:29.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2595" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":303,"skipped":4805,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:29.359: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-6660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:149
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul  8 03:42:29.536: INFO: starting watch
STEP: patching
STEP: updating
Jul  8 03:42:29.546: INFO: waiting for watch events with expected annotations
Jul  8 03:42:29.546: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:29.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6660" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":339,"completed":304,"skipped":4816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:29.581: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-6966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul  8 03:42:30.512: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  8 03:42:33.537: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:42:33.542: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:36.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6966" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.334 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":339,"completed":305,"skipped":4838,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:36.916: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1875
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul  8 03:42:37.163: INFO: Waiting up to 5m0s for pod "pod-1bb87944-ce76-4d35-9dab-ced537efe0bb" in namespace "emptydir-1875" to be "Succeeded or Failed"
Jul  8 03:42:37.172: INFO: Pod "pod-1bb87944-ce76-4d35-9dab-ced537efe0bb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.443464ms
Jul  8 03:42:39.179: INFO: Pod "pod-1bb87944-ce76-4d35-9dab-ced537efe0bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016565434s
STEP: Saw pod success
Jul  8 03:42:39.179: INFO: Pod "pod-1bb87944-ce76-4d35-9dab-ced537efe0bb" satisfied condition "Succeeded or Failed"
Jul  8 03:42:39.181: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-1bb87944-ce76-4d35-9dab-ced537efe0bb container test-container: <nil>
STEP: delete the pod
Jul  8 03:42:39.206: INFO: Waiting for pod pod-1bb87944-ce76-4d35-9dab-ced537efe0bb to disappear
Jul  8 03:42:39.210: INFO: Pod pod-1bb87944-ce76-4d35-9dab-ced537efe0bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:39.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1875" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":306,"skipped":4854,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:39.220: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:42:39.359: INFO: Creating simple deployment test-new-deployment
Jul  8 03:42:39.373: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul  8 03:42:41.432: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8024  660e4b58-bf18-4f18-953d-f973d4e3c9a0 53227 3 2021-07-08 03:42:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-07-08 03:42:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-08 03:42:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005277dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-08 03:42:40 +0000 UTC,LastTransitionTime:2021-07-08 03:42:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-07-08 03:42:40 +0000 UTC,LastTransitionTime:2021-07-08 03:42:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul  8 03:42:41.446: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-8024  5d7f1aff-8f2f-4c36-b48a-7bcbfff3a9f0 53229 3 2021-07-08 03:42:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 660e4b58-bf18-4f18-953d-f973d4e3c9a0 0xc00529e287 0xc00529e288}] []  [{kube-controller-manager Update apps/v1 2021-07-08 03:42:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"660e4b58-bf18-4f18-953d-f973d4e3c9a0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00529e308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul  8 03:42:41.457: INFO: Pod "test-new-deployment-847dcfb7fb-ccgsr" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-ccgsr test-new-deployment-847dcfb7fb- deployment-8024  527ccc4e-e145-410e-8e96-a11e7b422c5e 53220 0 2021-07-08 03:42:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.42.3.41/32 cni.projectcalico.org/podIPs:10.42.3.41/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 5d7f1aff-8f2f-4c36-b48a-7bcbfff3a9f0 0xc00529e757 0xc00529e758}] []  [{calico Update v1 2021-07-08 03:42:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-07-08 03:42:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d7f1aff-8f2f-4c36-b48a-7bcbfff3a9f0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-08 03:42:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4p79v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4p79v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-217.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:42:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:42:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:42:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:42:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.217,PodIP:10.42.3.41,StartTime:2021-07-08 03:42:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-08 03:42:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://e262886c0c5290c7d95ec32bd930c2fd194a176ad4c54a9358beaecb37734854,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:42:41.457: INFO: Pod "test-new-deployment-847dcfb7fb-jxpw9" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-jxpw9 test-new-deployment-847dcfb7fb- deployment-8024  90367bf9-687f-4a26-8f29-8b4b929af008 53230 0 2021-07-08 03:42:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 5d7f1aff-8f2f-4c36-b48a-7bcbfff3a9f0 0xc00529e9a0 0xc00529e9a1}] []  [{kube-controller-manager Update v1 2021-07-08 03:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d7f1aff-8f2f-4c36-b48a-7bcbfff3a9f0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9rl98,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9rl98,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-08 03:42:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:41.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8024" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":339,"completed":307,"skipped":4857,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:41.476: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1684
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:42:41.641: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jul  8 03:42:45.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 --namespace=crd-publish-openapi-1684 create -f -'
Jul  8 03:42:45.827: INFO: stderr: ""
Jul  8 03:42:45.827: INFO: stdout: "e2e-test-crd-publish-openapi-8920-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul  8 03:42:45.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 --namespace=crd-publish-openapi-1684 delete e2e-test-crd-publish-openapi-8920-crds test-foo'
Jul  8 03:42:45.951: INFO: stderr: ""
Jul  8 03:42:45.951: INFO: stdout: "e2e-test-crd-publish-openapi-8920-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul  8 03:42:45.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 --namespace=crd-publish-openapi-1684 apply -f -'
Jul  8 03:42:46.231: INFO: stderr: ""
Jul  8 03:42:46.231: INFO: stdout: "e2e-test-crd-publish-openapi-8920-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul  8 03:42:46.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 --namespace=crd-publish-openapi-1684 delete e2e-test-crd-publish-openapi-8920-crds test-foo'
Jul  8 03:42:46.352: INFO: stderr: ""
Jul  8 03:42:46.352: INFO: stdout: "e2e-test-crd-publish-openapi-8920-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jul  8 03:42:46.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 --namespace=crd-publish-openapi-1684 create -f -'
Jul  8 03:42:46.681: INFO: rc: 1
Jul  8 03:42:46.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 --namespace=crd-publish-openapi-1684 apply -f -'
Jul  8 03:42:47.128: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jul  8 03:42:47.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 --namespace=crd-publish-openapi-1684 create -f -'
Jul  8 03:42:47.399: INFO: rc: 1
Jul  8 03:42:47.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 --namespace=crd-publish-openapi-1684 apply -f -'
Jul  8 03:42:47.619: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jul  8 03:42:47.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 explain e2e-test-crd-publish-openapi-8920-crds'
Jul  8 03:42:47.907: INFO: stderr: ""
Jul  8 03:42:47.907: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8920-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jul  8 03:42:47.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 explain e2e-test-crd-publish-openapi-8920-crds.metadata'
Jul  8 03:42:48.199: INFO: stderr: ""
Jul  8 03:42:48.199: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8920-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul  8 03:42:48.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 explain e2e-test-crd-publish-openapi-8920-crds.spec'
Jul  8 03:42:48.973: INFO: stderr: ""
Jul  8 03:42:48.973: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8920-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul  8 03:42:48.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 explain e2e-test-crd-publish-openapi-8920-crds.spec.bars'
Jul  8 03:42:49.249: INFO: stderr: ""
Jul  8 03:42:49.249: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8920-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jul  8 03:42:49.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-1684 explain e2e-test-crd-publish-openapi-8920-crds.spec.bars2'
Jul  8 03:42:49.472: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:52.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1684" for this suite.

• [SLOW TEST:11.495 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":339,"completed":308,"skipped":4868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:52.971: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  8 03:42:55.145: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:55.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7823" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":309,"skipped":4899,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:55.177: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7161
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-e928bcc2-82db-4fa7-b5d8-4ba941395b25
STEP: Creating a pod to test consume secrets
Jul  8 03:42:55.346: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ad2283ee-ab5a-4952-8b14-e7261db5c2ff" in namespace "projected-7161" to be "Succeeded or Failed"
Jul  8 03:42:55.358: INFO: Pod "pod-projected-secrets-ad2283ee-ab5a-4952-8b14-e7261db5c2ff": Phase="Pending", Reason="", readiness=false. Elapsed: 12.730284ms
Jul  8 03:42:57.366: INFO: Pod "pod-projected-secrets-ad2283ee-ab5a-4952-8b14-e7261db5c2ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020178781s
STEP: Saw pod success
Jul  8 03:42:57.366: INFO: Pod "pod-projected-secrets-ad2283ee-ab5a-4952-8b14-e7261db5c2ff" satisfied condition "Succeeded or Failed"
Jul  8 03:42:57.368: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-projected-secrets-ad2283ee-ab5a-4952-8b14-e7261db5c2ff container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  8 03:42:57.388: INFO: Waiting for pod pod-projected-secrets-ad2283ee-ab5a-4952-8b14-e7261db5c2ff to disappear
Jul  8 03:42:57.390: INFO: Pod pod-projected-secrets-ad2283ee-ab5a-4952-8b14-e7261db5c2ff no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:42:57.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7161" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":310,"skipped":4911,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:42:57.399: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-x7fh
STEP: Creating a pod to test atomic-volume-subpath
Jul  8 03:42:57.566: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-x7fh" in namespace "subpath-4371" to be "Succeeded or Failed"
Jul  8 03:42:57.570: INFO: Pod "pod-subpath-test-downwardapi-x7fh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.82931ms
Jul  8 03:42:59.576: INFO: Pod "pod-subpath-test-downwardapi-x7fh": Phase="Running", Reason="", readiness=true. Elapsed: 2.009982053s
Jul  8 03:43:01.582: INFO: Pod "pod-subpath-test-downwardapi-x7fh": Phase="Running", Reason="", readiness=true. Elapsed: 4.016157357s
Jul  8 03:43:03.589: INFO: Pod "pod-subpath-test-downwardapi-x7fh": Phase="Running", Reason="", readiness=true. Elapsed: 6.023288796s
Jul  8 03:43:05.596: INFO: Pod "pod-subpath-test-downwardapi-x7fh": Phase="Running", Reason="", readiness=true. Elapsed: 8.030076199s
Jul  8 03:43:07.603: INFO: Pod "pod-subpath-test-downwardapi-x7fh": Phase="Running", Reason="", readiness=true. Elapsed: 10.037312029s
Jul  8 03:43:09.611: INFO: Pod "pod-subpath-test-downwardapi-x7fh": Phase="Running", Reason="", readiness=true. Elapsed: 12.044574333s
Jul  8 03:43:11.617: INFO: Pod "pod-subpath-test-downwardapi-x7fh": Phase="Running", Reason="", readiness=true. Elapsed: 14.050750992s
Jul  8 03:43:13.627: INFO: Pod "pod-subpath-test-downwardapi-x7fh": Phase="Running", Reason="", readiness=true. Elapsed: 16.060508981s
Jul  8 03:43:15.634: INFO: Pod "pod-subpath-test-downwardapi-x7fh": Phase="Running", Reason="", readiness=true. Elapsed: 18.067999312s
Jul  8 03:43:17.642: INFO: Pod "pod-subpath-test-downwardapi-x7fh": Phase="Running", Reason="", readiness=true. Elapsed: 20.07639753s
Jul  8 03:43:19.650: INFO: Pod "pod-subpath-test-downwardapi-x7fh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.084301938s
STEP: Saw pod success
Jul  8 03:43:19.651: INFO: Pod "pod-subpath-test-downwardapi-x7fh" satisfied condition "Succeeded or Failed"
Jul  8 03:43:19.653: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-subpath-test-downwardapi-x7fh container test-container-subpath-downwardapi-x7fh: <nil>
STEP: delete the pod
Jul  8 03:43:19.673: INFO: Waiting for pod pod-subpath-test-downwardapi-x7fh to disappear
Jul  8 03:43:19.676: INFO: Pod pod-subpath-test-downwardapi-x7fh no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-x7fh
Jul  8 03:43:19.676: INFO: Deleting pod "pod-subpath-test-downwardapi-x7fh" in namespace "subpath-4371"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:43:19.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4371" for this suite.

• [SLOW TEST:22.290 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":339,"completed":311,"skipped":4916,"failed":0}
SS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:43:19.689: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-5864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jul  8 03:43:19.854: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:43:21.860: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:43:23.862: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jul  8 03:43:23.888: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:43:25.895: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul  8 03:43:25.897: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5864 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:43:25.897: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:43:25.977: INFO: Exec stderr: ""
Jul  8 03:43:25.977: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5864 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:43:25.977: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:43:26.092: INFO: Exec stderr: ""
Jul  8 03:43:26.093: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5864 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:43:26.093: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:43:26.189: INFO: Exec stderr: ""
Jul  8 03:43:26.189: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5864 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:43:26.189: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:43:26.280: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul  8 03:43:26.280: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5864 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:43:26.280: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:43:26.366: INFO: Exec stderr: ""
Jul  8 03:43:26.366: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5864 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:43:26.366: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:43:26.436: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul  8 03:43:26.436: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5864 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:43:26.436: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:43:26.508: INFO: Exec stderr: ""
Jul  8 03:43:26.508: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5864 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:43:26.508: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:43:26.575: INFO: Exec stderr: ""
Jul  8 03:43:26.575: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5864 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:43:26.575: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:43:26.651: INFO: Exec stderr: ""
Jul  8 03:43:26.651: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5864 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:43:26.651: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:43:26.730: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:43:26.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5864" for this suite.

• [SLOW TEST:7.054 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":312,"skipped":4918,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:43:26.743: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-cb86a58e-426c-4737-91a5-6c83c6098f50
STEP: Creating a pod to test consume secrets
Jul  8 03:43:26.899: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6d47076e-12d3-44c8-b7e1-909c02e1c3b1" in namespace "projected-3702" to be "Succeeded or Failed"
Jul  8 03:43:26.903: INFO: Pod "pod-projected-secrets-6d47076e-12d3-44c8-b7e1-909c02e1c3b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278065ms
Jul  8 03:43:29.024: INFO: Pod "pod-projected-secrets-6d47076e-12d3-44c8-b7e1-909c02e1c3b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.125809668s
STEP: Saw pod success
Jul  8 03:43:29.025: INFO: Pod "pod-projected-secrets-6d47076e-12d3-44c8-b7e1-909c02e1c3b1" satisfied condition "Succeeded or Failed"
Jul  8 03:43:29.027: INFO: Trying to get logs from node ip-172-31-8-165.us-east-2.compute.internal pod pod-projected-secrets-6d47076e-12d3-44c8-b7e1-909c02e1c3b1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  8 03:43:29.129: INFO: Waiting for pod pod-projected-secrets-6d47076e-12d3-44c8-b7e1-909c02e1c3b1 to disappear
Jul  8 03:43:29.132: INFO: Pod pod-projected-secrets-6d47076e-12d3-44c8-b7e1-909c02e1c3b1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:43:29.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3702" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":313,"skipped":4959,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:43:29.141: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:43:36.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6220" for this suite.

• [SLOW TEST:7.169 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":339,"completed":314,"skipped":4959,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:43:36.309: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Jul  8 03:43:36.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-1361 create -f -'
Jul  8 03:43:36.985: INFO: stderr: ""
Jul  8 03:43:36.985: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jul  8 03:43:36.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-1361 diff -f -'
Jul  8 03:43:37.317: INFO: rc: 1
Jul  8 03:43:37.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-1361 delete -f -'
Jul  8 03:43:37.432: INFO: stderr: ""
Jul  8 03:43:37.432: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:43:37.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1361" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":339,"completed":315,"skipped":4963,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:43:37.444: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:43:41.622: INFO: Deleting pod "var-expansion-567bc6e9-07c1-4d39-9d5d-54a25132e135" in namespace "var-expansion-7301"
Jul  8 03:43:41.631: INFO: Wait up to 5m0s for pod "var-expansion-567bc6e9-07c1-4d39-9d5d-54a25132e135" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:43:49.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7301" for this suite.

• [SLOW TEST:12.215 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":339,"completed":316,"skipped":4965,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:43:49.660: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6476, will wait for the garbage collector to delete the pods
Jul  8 03:43:53.877: INFO: Deleting Job.batch foo took: 5.416524ms
Jul  8 03:43:53.977: INFO: Terminating Job.batch foo pods took: 100.305002ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:44:28.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6476" for this suite.

• [SLOW TEST:38.944 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":339,"completed":317,"skipped":4983,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:44:28.604: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-f4d5fde2-3e3f-4c57-bc5c-6f34831e54b7
STEP: Creating a pod to test consume secrets
Jul  8 03:44:28.797: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6c0bff8a-6f7c-4521-aa4d-e8be2c30e66e" in namespace "projected-8553" to be "Succeeded or Failed"
Jul  8 03:44:28.802: INFO: Pod "pod-projected-secrets-6c0bff8a-6f7c-4521-aa4d-e8be2c30e66e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.140674ms
Jul  8 03:44:30.808: INFO: Pod "pod-projected-secrets-6c0bff8a-6f7c-4521-aa4d-e8be2c30e66e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011826329s
Jul  8 03:44:32.815: INFO: Pod "pod-projected-secrets-6c0bff8a-6f7c-4521-aa4d-e8be2c30e66e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018559674s
STEP: Saw pod success
Jul  8 03:44:32.815: INFO: Pod "pod-projected-secrets-6c0bff8a-6f7c-4521-aa4d-e8be2c30e66e" satisfied condition "Succeeded or Failed"
Jul  8 03:44:32.817: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-projected-secrets-6c0bff8a-6f7c-4521-aa4d-e8be2c30e66e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  8 03:44:32.836: INFO: Waiting for pod pod-projected-secrets-6c0bff8a-6f7c-4521-aa4d-e8be2c30e66e to disappear
Jul  8 03:44:32.840: INFO: Pod pod-projected-secrets-6c0bff8a-6f7c-4521-aa4d-e8be2c30e66e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:44:32.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8553" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":318,"skipped":4984,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:44:32.850: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9540
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jul  8 03:44:32.996: INFO: namespace kubectl-9540
Jul  8 03:44:32.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9540 create -f -'
Jul  8 03:44:33.252: INFO: stderr: ""
Jul  8 03:44:33.252: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul  8 03:44:34.261: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  8 03:44:34.261: INFO: Found 1 / 1
Jul  8 03:44:34.261: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  8 03:44:34.264: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  8 03:44:34.264: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  8 03:44:34.264: INFO: wait on agnhost-primary startup in kubectl-9540 
Jul  8 03:44:34.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9540 logs agnhost-primary-mb9z8 agnhost-primary'
Jul  8 03:44:34.348: INFO: stderr: ""
Jul  8 03:44:34.348: INFO: stdout: "Paused\n"
STEP: exposing RC
Jul  8 03:44:34.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9540 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jul  8 03:44:34.454: INFO: stderr: ""
Jul  8 03:44:34.454: INFO: stdout: "service/rm2 exposed\n"
Jul  8 03:44:34.463: INFO: Service rm2 in namespace kubectl-9540 found.
STEP: exposing service
Jul  8 03:44:36.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-9540 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jul  8 03:44:36.622: INFO: stderr: ""
Jul  8 03:44:36.622: INFO: stdout: "service/rm3 exposed\n"
Jul  8 03:44:36.646: INFO: Service rm3 in namespace kubectl-9540 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:44:38.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9540" for this suite.

• [SLOW TEST:5.835 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1223
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":339,"completed":319,"skipped":5009,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:44:38.685: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-d1ad9760-ac2f-4b5d-92fe-6c811806a530
Jul  8 03:44:38.892: INFO: Pod name my-hostname-basic-d1ad9760-ac2f-4b5d-92fe-6c811806a530: Found 0 pods out of 1
Jul  8 03:44:43.901: INFO: Pod name my-hostname-basic-d1ad9760-ac2f-4b5d-92fe-6c811806a530: Found 1 pods out of 1
Jul  8 03:44:43.901: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d1ad9760-ac2f-4b5d-92fe-6c811806a530" are running
Jul  8 03:44:43.905: INFO: Pod "my-hostname-basic-d1ad9760-ac2f-4b5d-92fe-6c811806a530-vwtrv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-08 03:44:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-08 03:44:40 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-08 03:44:40 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-08 03:44:38 +0000 UTC Reason: Message:}])
Jul  8 03:44:43.905: INFO: Trying to dial the pod
Jul  8 03:44:48.923: INFO: Controller my-hostname-basic-d1ad9760-ac2f-4b5d-92fe-6c811806a530: Got expected result from replica 1 [my-hostname-basic-d1ad9760-ac2f-4b5d-92fe-6c811806a530-vwtrv]: "my-hostname-basic-d1ad9760-ac2f-4b5d-92fe-6c811806a530-vwtrv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:44:48.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8504" for this suite.

• [SLOW TEST:10.246 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":320,"skipped":5020,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:44:48.932: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5399
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jul  8 03:44:49.102: INFO: The status of Pod labelsupdateda6d5285-b579-4f2d-a2a5-dbd1ad4267d8 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:44:51.109: INFO: The status of Pod labelsupdateda6d5285-b579-4f2d-a2a5-dbd1ad4267d8 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:44:53.115: INFO: The status of Pod labelsupdateda6d5285-b579-4f2d-a2a5-dbd1ad4267d8 is Running (Ready = true)
Jul  8 03:44:53.638: INFO: Successfully updated pod "labelsupdateda6d5285-b579-4f2d-a2a5-dbd1ad4267d8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:44:55.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5399" for this suite.

• [SLOW TEST:6.733 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":321,"skipped":5079,"failed":0}
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:44:55.665: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-300
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a9f3ec90-b0ae-46d5-8a8f-6b531fba272e
STEP: Creating the pod
Jul  8 03:44:55.829: INFO: The status of Pod pod-projected-configmaps-d9aa0645-843a-434b-9081-0bd4b2574729 is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:44:57.835: INFO: The status of Pod pod-projected-configmaps-d9aa0645-843a-434b-9081-0bd4b2574729 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-a9f3ec90-b0ae-46d5-8a8f-6b531fba272e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:44:59.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-300" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":322,"skipped":5079,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:44:59.869: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1598
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jul  8 03:45:00.030: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1598  040c4a52-26e3-4ab8-9781-44c1c9220cd3 54143 0 2021-07-08 03:45:00 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-07-08 03:45:00 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dd92v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dd92v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  8 03:45:00.033: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:45:02.039: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jul  8 03:45:04.040: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jul  8 03:45:04.040: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1598 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:45:04.040: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Verifying customized DNS server is configured on pod...
Jul  8 03:45:04.134: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1598 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  8 03:45:04.134: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
Jul  8 03:45:04.236: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:45:04.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1598" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":339,"completed":323,"skipped":5119,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:45:04.289: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5979
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-612d0859-c713-4f13-bbfd-dc4f6733e915 in namespace container-probe-5979
Jul  8 03:45:06.463: INFO: Started pod liveness-612d0859-c713-4f13-bbfd-dc4f6733e915 in namespace container-probe-5979
STEP: checking the pod's current state and verifying that restartCount is present
Jul  8 03:45:06.474: INFO: Initial restart count of pod liveness-612d0859-c713-4f13-bbfd-dc4f6733e915 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:49:07.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5979" for this suite.

• [SLOW TEST:243.079 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":339,"completed":324,"skipped":5123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:49:07.411: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-7016
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul  8 03:49:07.565: INFO: Waiting up to 1m0s for all nodes to be ready
Jul  8 03:50:07.642: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:50:07.647: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-4302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jul  8 03:50:09.852: INFO: found a healthy node: ip-172-31-6-217.us-east-2.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:50:19.951: INFO: pods created so far: [1 1 1]
Jul  8 03:50:19.951: INFO: length of pods created so far: 3
Jul  8 03:50:33.970: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:50:40.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4302" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:50:41.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7016" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:93.665 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":339,"completed":325,"skipped":5185,"failed":0}
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:50:41.076: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1411
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-89bcf1da-c73a-4c63-9007-fe4be23c5f1e
STEP: Creating secret with name secret-projected-all-test-volume-140f1abd-359c-4b13-bceb-5675396cd698
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul  8 03:50:41.234: INFO: Waiting up to 5m0s for pod "projected-volume-0e28b834-e1b7-4139-b344-d9774938e9e4" in namespace "projected-1411" to be "Succeeded or Failed"
Jul  8 03:50:41.241: INFO: Pod "projected-volume-0e28b834-e1b7-4139-b344-d9774938e9e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.847342ms
Jul  8 03:50:43.245: INFO: Pod "projected-volume-0e28b834-e1b7-4139-b344-d9774938e9e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011142488s
STEP: Saw pod success
Jul  8 03:50:43.245: INFO: Pod "projected-volume-0e28b834-e1b7-4139-b344-d9774938e9e4" satisfied condition "Succeeded or Failed"
Jul  8 03:50:43.248: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod projected-volume-0e28b834-e1b7-4139-b344-d9774938e9e4 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul  8 03:50:43.278: INFO: Waiting for pod projected-volume-0e28b834-e1b7-4139-b344-d9774938e9e4 to disappear
Jul  8 03:50:43.281: INFO: Pod projected-volume-0e28b834-e1b7-4139-b344-d9774938e9e4 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:50:43.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1411" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":339,"completed":326,"skipped":5188,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:50:43.291: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4185
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 03:50:43.451: INFO: Waiting up to 5m0s for pod "downwardapi-volume-150a8771-cc3b-49b1-82e4-9de416c7aed2" in namespace "projected-4185" to be "Succeeded or Failed"
Jul  8 03:50:43.455: INFO: Pod "downwardapi-volume-150a8771-cc3b-49b1-82e4-9de416c7aed2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.764646ms
Jul  8 03:50:45.466: INFO: Pod "downwardapi-volume-150a8771-cc3b-49b1-82e4-9de416c7aed2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015022225s
STEP: Saw pod success
Jul  8 03:50:45.467: INFO: Pod "downwardapi-volume-150a8771-cc3b-49b1-82e4-9de416c7aed2" satisfied condition "Succeeded or Failed"
Jul  8 03:50:45.469: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-150a8771-cc3b-49b1-82e4-9de416c7aed2 container client-container: <nil>
STEP: delete the pod
Jul  8 03:50:45.518: INFO: Waiting for pod downwardapi-volume-150a8771-cc3b-49b1-82e4-9de416c7aed2 to disappear
Jul  8 03:50:45.520: INFO: Pod downwardapi-volume-150a8771-cc3b-49b1-82e4-9de416c7aed2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:50:45.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4185" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":327,"skipped":5203,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:50:45.530: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul  8 03:50:45.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a63fa31f-ad9f-4357-b76f-f7952b15d4ad" in namespace "projected-5268" to be "Succeeded or Failed"
Jul  8 03:50:45.702: INFO: Pod "downwardapi-volume-a63fa31f-ad9f-4357-b76f-f7952b15d4ad": Phase="Pending", Reason="", readiness=false. Elapsed: 19.423259ms
Jul  8 03:50:47.709: INFO: Pod "downwardapi-volume-a63fa31f-ad9f-4357-b76f-f7952b15d4ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026574111s
Jul  8 03:50:49.715: INFO: Pod "downwardapi-volume-a63fa31f-ad9f-4357-b76f-f7952b15d4ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032077223s
STEP: Saw pod success
Jul  8 03:50:49.715: INFO: Pod "downwardapi-volume-a63fa31f-ad9f-4357-b76f-f7952b15d4ad" satisfied condition "Succeeded or Failed"
Jul  8 03:50:49.717: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod downwardapi-volume-a63fa31f-ad9f-4357-b76f-f7952b15d4ad container client-container: <nil>
STEP: delete the pod
Jul  8 03:50:49.742: INFO: Waiting for pod downwardapi-volume-a63fa31f-ad9f-4357-b76f-f7952b15d4ad to disappear
Jul  8 03:50:49.744: INFO: Pod downwardapi-volume-a63fa31f-ad9f-4357-b76f-f7952b15d4ad no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:50:49.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5268" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":328,"skipped":5218,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:50:49.754: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-854f39e5-67e1-47d0-bf70-d31d7c152cb2
STEP: Creating a pod to test consume secrets
Jul  8 03:50:49.913: INFO: Waiting up to 5m0s for pod "pod-secrets-ff25edc8-1014-4f67-9704-9f7ffe1a36cd" in namespace "secrets-8745" to be "Succeeded or Failed"
Jul  8 03:50:49.923: INFO: Pod "pod-secrets-ff25edc8-1014-4f67-9704-9f7ffe1a36cd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.55386ms
Jul  8 03:50:51.930: INFO: Pod "pod-secrets-ff25edc8-1014-4f67-9704-9f7ffe1a36cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016875229s
STEP: Saw pod success
Jul  8 03:50:51.930: INFO: Pod "pod-secrets-ff25edc8-1014-4f67-9704-9f7ffe1a36cd" satisfied condition "Succeeded or Failed"
Jul  8 03:50:51.933: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-secrets-ff25edc8-1014-4f67-9704-9f7ffe1a36cd container secret-volume-test: <nil>
STEP: delete the pod
Jul  8 03:50:51.961: INFO: Waiting for pod pod-secrets-ff25edc8-1014-4f67-9704-9f7ffe1a36cd to disappear
Jul  8 03:50:51.966: INFO: Pod pod-secrets-ff25edc8-1014-4f67-9704-9f7ffe1a36cd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:50:51.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8745" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":329,"skipped":5222,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:50:51.990: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0708 03:50:53.209975      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul  8 03:55:53.217: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:55:53.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-325" for this suite.

• [SLOW TEST:301.238 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":339,"completed":330,"skipped":5241,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:55:53.229: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  8 03:55:53.443: INFO: Number of nodes with available pods: 0
Jul  8 03:55:53.443: INFO: Node ip-172-31-3-228.us-east-2.compute.internal is running more than one daemon pod
Jul  8 03:55:54.463: INFO: Number of nodes with available pods: 1
Jul  8 03:55:54.463: INFO: Node ip-172-31-6-217.us-east-2.compute.internal is running more than one daemon pod
Jul  8 03:55:55.469: INFO: Number of nodes with available pods: 4
Jul  8 03:55:55.469: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul  8 03:55:55.503: INFO: Number of nodes with available pods: 4
Jul  8 03:55:55.503: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1643, will wait for the garbage collector to delete the pods
Jul  8 03:55:56.575: INFO: Deleting DaemonSet.extensions daemon-set took: 6.293946ms
Jul  8 03:55:56.777: INFO: Terminating DaemonSet.extensions daemon-set pods took: 202.048403ms
Jul  8 03:56:40.989: INFO: Number of nodes with available pods: 0
Jul  8 03:56:40.989: INFO: Number of running nodes: 0, number of available pods: 0
Jul  8 03:56:40.991: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"56318"},"items":null}

Jul  8 03:56:40.994: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"56318"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:56:41.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1643" for this suite.

• [SLOW TEST:47.786 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":339,"completed":331,"skipped":5245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:56:41.015: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7737
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Jul  8 03:56:41.165: INFO: Waiting up to 5m0s for pod "client-containers-62f30892-5d6d-4914-9104-771c360e0646" in namespace "containers-7737" to be "Succeeded or Failed"
Jul  8 03:56:41.170: INFO: Pod "client-containers-62f30892-5d6d-4914-9104-771c360e0646": Phase="Pending", Reason="", readiness=false. Elapsed: 4.169126ms
Jul  8 03:56:43.177: INFO: Pod "client-containers-62f30892-5d6d-4914-9104-771c360e0646": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011551292s
STEP: Saw pod success
Jul  8 03:56:43.177: INFO: Pod "client-containers-62f30892-5d6d-4914-9104-771c360e0646" satisfied condition "Succeeded or Failed"
Jul  8 03:56:43.180: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod client-containers-62f30892-5d6d-4914-9104-771c360e0646 container agnhost-container: <nil>
STEP: delete the pod
Jul  8 03:56:43.206: INFO: Waiting for pod client-containers-62f30892-5d6d-4914-9104-771c360e0646 to disappear
Jul  8 03:56:43.208: INFO: Pod client-containers-62f30892-5d6d-4914-9104-771c360e0646 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:56:43.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7737" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":339,"completed":332,"skipped":5272,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:56:43.218: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8419
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul  8 03:56:43.375: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul  8 03:56:46.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-8419 --namespace=crd-publish-openapi-8419 create -f -'
Jul  8 03:56:47.473: INFO: stderr: ""
Jul  8 03:56:47.473: INFO: stdout: "e2e-test-crd-publish-openapi-8869-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul  8 03:56:47.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-8419 --namespace=crd-publish-openapi-8419 delete e2e-test-crd-publish-openapi-8869-crds test-cr'
Jul  8 03:56:47.557: INFO: stderr: ""
Jul  8 03:56:47.558: INFO: stdout: "e2e-test-crd-publish-openapi-8869-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul  8 03:56:47.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-8419 --namespace=crd-publish-openapi-8419 apply -f -'
Jul  8 03:56:47.777: INFO: stderr: ""
Jul  8 03:56:47.777: INFO: stdout: "e2e-test-crd-publish-openapi-8869-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul  8 03:56:47.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-8419 --namespace=crd-publish-openapi-8419 delete e2e-test-crd-publish-openapi-8869-crds test-cr'
Jul  8 03:56:47.891: INFO: stderr: ""
Jul  8 03:56:47.891: INFO: stdout: "e2e-test-crd-publish-openapi-8869-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul  8 03:56:47.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=crd-publish-openapi-8419 explain e2e-test-crd-publish-openapi-8869-crds'
Jul  8 03:56:48.117: INFO: stderr: ""
Jul  8 03:56:48.117: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8869-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:56:51.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8419" for this suite.

• [SLOW TEST:8.429 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":339,"completed":333,"skipped":5274,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:56:51.647: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul  8 03:56:51.845: INFO: Waiting up to 5m0s for pod "pod-1165f8ba-4b9c-4e6a-8e3b-61dc21697c35" in namespace "emptydir-3108" to be "Succeeded or Failed"
Jul  8 03:56:51.883: INFO: Pod "pod-1165f8ba-4b9c-4e6a-8e3b-61dc21697c35": Phase="Pending", Reason="", readiness=false. Elapsed: 38.838097ms
Jul  8 03:56:53.890: INFO: Pod "pod-1165f8ba-4b9c-4e6a-8e3b-61dc21697c35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045683823s
STEP: Saw pod success
Jul  8 03:56:53.890: INFO: Pod "pod-1165f8ba-4b9c-4e6a-8e3b-61dc21697c35" satisfied condition "Succeeded or Failed"
Jul  8 03:56:53.893: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-1165f8ba-4b9c-4e6a-8e3b-61dc21697c35 container test-container: <nil>
STEP: delete the pod
Jul  8 03:56:53.914: INFO: Waiting for pod pod-1165f8ba-4b9c-4e6a-8e3b-61dc21697c35 to disappear
Jul  8 03:56:53.917: INFO: Pod pod-1165f8ba-4b9c-4e6a-8e3b-61dc21697c35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:56:53.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3108" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":334,"skipped":5292,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:56:53.926: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4446
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jul  8 03:56:54.076: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:56:56.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4446" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":339,"completed":335,"skipped":5309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:56:56.693: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1548
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jul  8 03:56:56.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6188 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Jul  8 03:56:57.060: INFO: stderr: ""
Jul  8 03:56:57.060: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jul  8 03:57:02.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6188 get pod e2e-test-httpd-pod -o json'
Jul  8 03:57:02.215: INFO: stderr: ""
Jul  8 03:57:02.215: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.42.3.69/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.42.3.69/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-07-08T03:56:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6188\",\n        \"resourceVersion\": \"56510\",\n        \"uid\": \"7c88795f-0003-46ef-881c-37f7eec68636\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-x4hkq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-6-217.us-east-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-x4hkq\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-08T03:56:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-08T03:56:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-08T03:56:58Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-08T03:56:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ab361c8e357c1fd82288d6f9679a1acb56a6a948f9cba6af31257f2e40d5a737\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-07-08T03:56:57Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.6.217\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.42.3.69\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.42.3.69\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-07-08T03:56:57Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul  8 03:57:02.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6188 replace -f -'
Jul  8 03:57:02.547: INFO: stderr: ""
Jul  8 03:57:02.547: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1552
Jul  8 03:57:02.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-057523812 --namespace=kubectl-6188 delete pods e2e-test-httpd-pod'
Jul  8 03:57:08.518: INFO: stderr: ""
Jul  8 03:57:08.518: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:57:08.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6188" for this suite.

• [SLOW TEST:11.838 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":339,"completed":336,"skipped":5396,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:57:08.531: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:57:08.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7224" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":337,"skipped":5407,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:57:08.711: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6205
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6205.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6205.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6205.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6205.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6205.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6205.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  8 03:57:10.903: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:10.905: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:10.908: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:10.910: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:10.917: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:10.919: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:10.922: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:10.924: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:10.932: INFO: Lookups using dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local]

Jul  8 03:57:15.935: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:15.938: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:15.940: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:15.943: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:15.949: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:15.951: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:15.954: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:15.962: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:15.967: INFO: Lookups using dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local]

Jul  8 03:57:20.935: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:20.938: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:20.940: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:20.943: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:20.950: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:20.954: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:20.956: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:20.962: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:20.966: INFO: Lookups using dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local]

Jul  8 03:57:25.939: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:25.942: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:25.944: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:25.946: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:25.960: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:25.963: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:25.965: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:25.967: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:25.972: INFO: Lookups using dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local]

Jul  8 03:57:30.935: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:30.938: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:30.940: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:30.943: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:30.949: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:30.951: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:30.954: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:30.956: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:30.960: INFO: Lookups using dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local]

Jul  8 03:57:35.941: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:35.951: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:35.954: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:35.957: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:35.971: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:35.978: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:35.982: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:35.985: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:35.991: INFO: Lookups using dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local jessie_udp@dns-test-service-2.dns-6205.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6205.svc.cluster.local]

Jul  8 03:57:40.936: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:40.938: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:40.941: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:40.943: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local from pod dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733: the server could not find the requested resource (get pods dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733)
Jul  8 03:57:40.961: INFO: Lookups using dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6205.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6205.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6205.svc.cluster.local]

Jul  8 03:57:45.965: INFO: DNS probes using dns-6205/dns-test-8ce7ae9f-970f-4721-b678-7b3a43eba733 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:57:46.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6205" for this suite.

• [SLOW TEST:37.419 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":339,"completed":338,"skipped":5412,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul  8 03:57:46.131: INFO: >>> kubeConfig: /tmp/kubeconfig-057523812
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-63fcce41-ea93-47d0-a0a7-10e15a9d8257
STEP: Creating a pod to test consume configMaps
Jul  8 03:57:46.324: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9520b4bc-ce45-433f-b9a6-f46cfed59f22" in namespace "projected-2961" to be "Succeeded or Failed"
Jul  8 03:57:46.331: INFO: Pod "pod-projected-configmaps-9520b4bc-ce45-433f-b9a6-f46cfed59f22": Phase="Pending", Reason="", readiness=false. Elapsed: 7.044714ms
Jul  8 03:57:48.338: INFO: Pod "pod-projected-configmaps-9520b4bc-ce45-433f-b9a6-f46cfed59f22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013576162s
STEP: Saw pod success
Jul  8 03:57:48.338: INFO: Pod "pod-projected-configmaps-9520b4bc-ce45-433f-b9a6-f46cfed59f22" satisfied condition "Succeeded or Failed"
Jul  8 03:57:48.340: INFO: Trying to get logs from node ip-172-31-6-217.us-east-2.compute.internal pod pod-projected-configmaps-9520b4bc-ce45-433f-b9a6-f46cfed59f22 container agnhost-container: <nil>
STEP: delete the pod
Jul  8 03:57:48.362: INFO: Waiting for pod pod-projected-configmaps-9520b4bc-ce45-433f-b9a6-f46cfed59f22 to disappear
Jul  8 03:57:48.365: INFO: Pod pod-projected-configmaps-9520b4bc-ce45-433f-b9a6-f46cfed59f22 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul  8 03:57:48.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2961" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":339,"skipped":5412,"failed":0}
SSSSSSSSSSSSSSSSSSSSJul  8 03:57:48.374: INFO: Running AfterSuite actions on all nodes
Jul  8 03:57:48.374: INFO: Running AfterSuite actions on node 1
Jul  8 03:57:48.374: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":339,"completed":339,"skipped":5432,"failed":0}

Ran 339 of 5771 Specs in 7970.019 seconds
SUCCESS! -- 339 Passed | 0 Failed | 0 Pending | 5432 Skipped
PASS

Ginkgo ran 1 suite in 2h12m51.646732814s
Test Suite Passed
