I0926 10:33:25.611718      22 e2e.go:129] Starting e2e run "b235255d-02aa-4be7-9984-ea35e3b446e5" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1632652405 - Will randomize all specs
Will run 346 of 6432 specs

Sep 26 10:33:27.657: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 10:33:27.661: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 26 10:33:27.691: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 26 10:33:27.720: INFO: 9 / 9 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 26 10:33:27.720: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep 26 10:33:27.720: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 26 10:33:27.730: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep 26 10:33:27.730: INFO: e2e test version: v1.22.2
Sep 26 10:33:27.731: INFO: kube-apiserver version: v1.22.2
Sep 26 10:33:27.732: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 10:33:27.737: INFO: Cluster IP family: ipv4
SSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:33:27.737: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
W0926 10:33:27.800201      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Sep 26 10:33:27.800: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:33:27.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6559" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":1,"skipped":5,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:33:27.857: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Sep 26 10:33:27.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-5410 create -f -'
Sep 26 10:33:28.187: INFO: stderr: ""
Sep 26 10:33:28.187: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 26 10:33:28.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-5410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 26 10:33:28.287: INFO: stderr: ""
Sep 26 10:33:28.287: INFO: stdout: "update-demo-nautilus-ljkp7 update-demo-nautilus-rrmbh "
Sep 26 10:33:28.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-5410 get pods update-demo-nautilus-ljkp7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 26 10:33:28.360: INFO: stderr: ""
Sep 26 10:33:28.360: INFO: stdout: ""
Sep 26 10:33:28.360: INFO: update-demo-nautilus-ljkp7 is created but not running
Sep 26 10:33:33.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-5410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 26 10:33:33.455: INFO: stderr: ""
Sep 26 10:33:33.455: INFO: stdout: "update-demo-nautilus-ljkp7 update-demo-nautilus-rrmbh "
Sep 26 10:33:33.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-5410 get pods update-demo-nautilus-ljkp7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 26 10:33:33.540: INFO: stderr: ""
Sep 26 10:33:33.540: INFO: stdout: "true"
Sep 26 10:33:33.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-5410 get pods update-demo-nautilus-ljkp7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 26 10:33:33.608: INFO: stderr: ""
Sep 26 10:33:33.608: INFO: stdout: "registry.dahuatech.com/cncf/nautilus:1.4"
Sep 26 10:33:33.608: INFO: validating pod update-demo-nautilus-ljkp7
Sep 26 10:33:33.614: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 26 10:33:33.614: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 26 10:33:33.614: INFO: update-demo-nautilus-ljkp7 is verified up and running
Sep 26 10:33:33.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-5410 get pods update-demo-nautilus-rrmbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 26 10:33:33.701: INFO: stderr: ""
Sep 26 10:33:33.701: INFO: stdout: "true"
Sep 26 10:33:33.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-5410 get pods update-demo-nautilus-rrmbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 26 10:33:33.794: INFO: stderr: ""
Sep 26 10:33:33.794: INFO: stdout: "registry.dahuatech.com/cncf/nautilus:1.4"
Sep 26 10:33:33.794: INFO: validating pod update-demo-nautilus-rrmbh
Sep 26 10:33:33.800: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 26 10:33:33.800: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 26 10:33:33.800: INFO: update-demo-nautilus-rrmbh is verified up and running
STEP: using delete to clean up resources
Sep 26 10:33:33.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-5410 delete --grace-period=0 --force -f -'
Sep 26 10:33:33.903: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 26 10:33:33.903: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 26 10:33:33.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-5410 get rc,svc -l name=update-demo --no-headers'
Sep 26 10:33:33.991: INFO: stderr: "No resources found in kubectl-5410 namespace.\n"
Sep 26 10:33:33.991: INFO: stdout: ""
Sep 26 10:33:33.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-5410 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 26 10:33:34.070: INFO: stderr: ""
Sep 26 10:33:34.070: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:33:34.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5410" for this suite.

• [SLOW TEST:6.226 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":2,"skipped":44,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:33:34.084: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 26 10:33:36.161: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:33:36.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9366" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":3,"skipped":94,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:33:36.188: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-bc98001d-d8a9-4439-a0b4-5a2baa33a992
STEP: Creating configMap with name cm-test-opt-upd-07f9daf9-2a8e-44b8-9d2f-a35219433ed6
STEP: Creating the pod
Sep 26 10:33:36.264: INFO: The status of Pod pod-configmaps-f371b4a7-0e8c-46ec-a711-c0db76744bb1 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:33:38.278: INFO: The status of Pod pod-configmaps-f371b4a7-0e8c-46ec-a711-c0db76744bb1 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-bc98001d-d8a9-4439-a0b4-5a2baa33a992
STEP: Updating configmap cm-test-opt-upd-07f9daf9-2a8e-44b8-9d2f-a35219433ed6
STEP: Creating configMap with name cm-test-opt-create-464a2414-945e-43b3-98a0-25f7dde011a5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:33:40.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7344" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":94,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:33:40.382: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9856
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9856
I0926 10:33:40.461871      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9856, replica count: 2
Sep 26 10:33:43.512: INFO: Creating new exec pod
I0926 10:33:43.512869      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 26 10:33:48.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-9856 exec execpodtggv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Sep 26 10:33:48.786: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 26 10:33:48.786: INFO: stdout: "externalname-service-pgfnp"
Sep 26 10:33:48.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-9856 exec execpodtggv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.215.143 80'
Sep 26 10:33:49.037: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.215.143 80\nConnection to 10.254.215.143 80 port [tcp/http] succeeded!\n"
Sep 26 10:33:49.037: INFO: stdout: ""
Sep 26 10:33:50.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-9856 exec execpodtggv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.215.143 80'
Sep 26 10:33:50.260: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.215.143 80\nConnection to 10.254.215.143 80 port [tcp/http] succeeded!\n"
Sep 26 10:33:50.260: INFO: stdout: "externalname-service-pgfnp"
Sep 26 10:33:50.260: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:33:50.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9856" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.929 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":5,"skipped":126,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:33:50.311: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-9d253897-7f6c-4a2a-a55a-1e467c9128a9 in namespace container-probe-8752
Sep 26 10:33:54.389: INFO: Started pod liveness-9d253897-7f6c-4a2a-a55a-1e467c9128a9 in namespace container-probe-8752
STEP: checking the pod's current state and verifying that restartCount is present
Sep 26 10:33:54.394: INFO: Initial restart count of pod liveness-9d253897-7f6c-4a2a-a55a-1e467c9128a9 is 0
Sep 26 10:34:12.512: INFO: Restart count of pod container-probe-8752/liveness-9d253897-7f6c-4a2a-a55a-1e467c9128a9 is now 1 (18.117678521s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:34:12.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8752" for this suite.

• [SLOW TEST:22.240 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":6,"skipped":144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:34:12.551: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 10:34:12.629: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84da21a6-da0b-4208-b3d6-fbf6717f09f6" in namespace "projected-8356" to be "Succeeded or Failed"
Sep 26 10:34:12.634: INFO: Pod "downwardapi-volume-84da21a6-da0b-4208-b3d6-fbf6717f09f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.819278ms
Sep 26 10:34:14.641: INFO: Pod "downwardapi-volume-84da21a6-da0b-4208-b3d6-fbf6717f09f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012033083s
Sep 26 10:34:16.654: INFO: Pod "downwardapi-volume-84da21a6-da0b-4208-b3d6-fbf6717f09f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02560839s
STEP: Saw pod success
Sep 26 10:34:16.654: INFO: Pod "downwardapi-volume-84da21a6-da0b-4208-b3d6-fbf6717f09f6" satisfied condition "Succeeded or Failed"
Sep 26 10:34:16.657: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-84da21a6-da0b-4208-b3d6-fbf6717f09f6 container client-container: <nil>
STEP: delete the pod
Sep 26 10:34:16.680: INFO: Waiting for pod downwardapi-volume-84da21a6-da0b-4208-b3d6-fbf6717f09f6 to disappear
Sep 26 10:34:16.684: INFO: Pod downwardapi-volume-84da21a6-da0b-4208-b3d6-fbf6717f09f6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:34:16.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8356" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":7,"skipped":172,"failed":0}

------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:34:16.696: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Sep 26 10:34:18.822: INFO: running pods: 0 < 3
Sep 26 10:34:20.832: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:34:22.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-428" for this suite.

• [SLOW TEST:6.150 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":8,"skipped":172,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:34:22.846: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Sep 26 10:34:22.898: INFO: Waiting up to 5m0s for pod "client-containers-6bb343c3-3b69-4755-9f7a-5ab744b5572a" in namespace "containers-7138" to be "Succeeded or Failed"
Sep 26 10:34:22.906: INFO: Pod "client-containers-6bb343c3-3b69-4755-9f7a-5ab744b5572a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.680505ms
Sep 26 10:34:24.911: INFO: Pod "client-containers-6bb343c3-3b69-4755-9f7a-5ab744b5572a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012914301s
STEP: Saw pod success
Sep 26 10:34:24.911: INFO: Pod "client-containers-6bb343c3-3b69-4755-9f7a-5ab744b5572a" satisfied condition "Succeeded or Failed"
Sep 26 10:34:24.914: INFO: Trying to get logs from node 10.37.21.193 pod client-containers-6bb343c3-3b69-4755-9f7a-5ab744b5572a container agnhost-container: <nil>
STEP: delete the pod
Sep 26 10:34:24.939: INFO: Waiting for pod client-containers-6bb343c3-3b69-4755-9f7a-5ab744b5572a to disappear
Sep 26 10:34:24.943: INFO: Pod client-containers-6bb343c3-3b69-4755-9f7a-5ab744b5572a no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:34:24.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7138" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":9,"skipped":191,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:34:24.952: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 26 10:34:25.030: INFO: Number of nodes with available pods: 0
Sep 26 10:34:25.030: INFO: Node 10.37.21.193 is running more than one daemon pod
Sep 26 10:34:26.044: INFO: Number of nodes with available pods: 0
Sep 26 10:34:26.044: INFO: Node 10.37.21.193 is running more than one daemon pod
Sep 26 10:34:27.043: INFO: Number of nodes with available pods: 3
Sep 26 10:34:27.043: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 26 10:34:27.066: INFO: Number of nodes with available pods: 3
Sep 26 10:34:27.066: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8334, will wait for the garbage collector to delete the pods
Sep 26 10:34:28.140: INFO: Deleting DaemonSet.extensions daemon-set took: 5.637472ms
Sep 26 10:34:28.256: INFO: Terminating DaemonSet.extensions daemon-set pods took: 115.431679ms
Sep 26 10:34:30.066: INFO: Number of nodes with available pods: 0
Sep 26 10:34:30.066: INFO: Number of running nodes: 0, number of available pods: 0
Sep 26 10:34:30.072: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"528302"},"items":null}

Sep 26 10:34:30.075: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"528303"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:34:30.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8334" for this suite.

• [SLOW TEST:5.148 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":10,"skipped":212,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:34:30.101: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:34:30.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-146" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":11,"skipped":229,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:34:30.153: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 10:34:30.202: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Creating first CR 
Sep 26 10:34:37.799: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-26T10:34:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-26T10:34:37Z]] name:name1 resourceVersion:528381 uid:1af61513-4a73-4fd0-84bc-e217581e5e82] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep 26 10:34:47.819: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-26T10:34:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-26T10:34:47Z]] name:name2 resourceVersion:528398 uid:f7f1cb7d-5e79-48c9-b227-2b4f38c3d3e3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep 26 10:34:57.840: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-26T10:34:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-26T10:34:57Z]] name:name1 resourceVersion:528414 uid:1af61513-4a73-4fd0-84bc-e217581e5e82] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep 26 10:35:07.862: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-26T10:34:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-26T10:35:07Z]] name:name2 resourceVersion:528431 uid:f7f1cb7d-5e79-48c9-b227-2b4f38c3d3e3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep 26 10:35:17.882: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-26T10:34:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-26T10:34:57Z]] name:name1 resourceVersion:528448 uid:1af61513-4a73-4fd0-84bc-e217581e5e82] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep 26 10:35:27.900: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-26T10:34:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-26T10:35:07Z]] name:name2 resourceVersion:528465 uid:f7f1cb7d-5e79-48c9-b227-2b4f38c3d3e3] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:35:38.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-3706" for this suite.

• [SLOW TEST:68.299 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":12,"skipped":229,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:35:38.452: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-6200a2ec-a2b4-4716-a0f6-6e15e89699c4
STEP: Creating a pod to test consume secrets
Sep 26 10:35:38.547: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b28fbd05-583e-46fb-9900-050ffee39ac8" in namespace "projected-1474" to be "Succeeded or Failed"
Sep 26 10:35:38.552: INFO: Pod "pod-projected-secrets-b28fbd05-583e-46fb-9900-050ffee39ac8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.152965ms
Sep 26 10:35:40.562: INFO: Pod "pod-projected-secrets-b28fbd05-583e-46fb-9900-050ffee39ac8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014662006s
STEP: Saw pod success
Sep 26 10:35:40.562: INFO: Pod "pod-projected-secrets-b28fbd05-583e-46fb-9900-050ffee39ac8" satisfied condition "Succeeded or Failed"
Sep 26 10:35:40.566: INFO: Trying to get logs from node 10.37.21.194 pod pod-projected-secrets-b28fbd05-583e-46fb-9900-050ffee39ac8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 26 10:35:40.594: INFO: Waiting for pod pod-projected-secrets-b28fbd05-583e-46fb-9900-050ffee39ac8 to disappear
Sep 26 10:35:40.598: INFO: Pod pod-projected-secrets-b28fbd05-583e-46fb-9900-050ffee39ac8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:35:40.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1474" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":13,"skipped":334,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:35:40.610: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 26 10:35:40.679: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:35:45.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-514" for this suite.

• [SLOW TEST:5.122 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":14,"skipped":348,"failed":0}
SS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:35:45.732: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:35:45.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6709" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":15,"skipped":350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:35:45.796: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 26 10:35:45.839: INFO: Waiting up to 5m0s for pod "pod-fa044ba5-d794-475b-b96d-a4fa7ad5f8aa" in namespace "emptydir-6487" to be "Succeeded or Failed"
Sep 26 10:35:45.848: INFO: Pod "pod-fa044ba5-d794-475b-b96d-a4fa7ad5f8aa": Phase="Pending", Reason="", readiness=false. Elapsed: 9.251443ms
Sep 26 10:35:47.860: INFO: Pod "pod-fa044ba5-d794-475b-b96d-a4fa7ad5f8aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0215139s
STEP: Saw pod success
Sep 26 10:35:47.860: INFO: Pod "pod-fa044ba5-d794-475b-b96d-a4fa7ad5f8aa" satisfied condition "Succeeded or Failed"
Sep 26 10:35:47.863: INFO: Trying to get logs from node 10.37.21.195 pod pod-fa044ba5-d794-475b-b96d-a4fa7ad5f8aa container test-container: <nil>
STEP: delete the pod
Sep 26 10:35:47.895: INFO: Waiting for pod pod-fa044ba5-d794-475b-b96d-a4fa7ad5f8aa to disappear
Sep 26 10:35:47.898: INFO: Pod pod-fa044ba5-d794-475b-b96d-a4fa7ad5f8aa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:35:47.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6487" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":16,"skipped":417,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:35:47.908: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-8918
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8918 to expose endpoints map[]
Sep 26 10:35:47.970: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Sep 26 10:35:48.983: INFO: successfully validated that service endpoint-test2 in namespace services-8918 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8918
Sep 26 10:35:48.995: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:35:51.008: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8918 to expose endpoints map[pod1:[80]]
Sep 26 10:35:51.024: INFO: successfully validated that service endpoint-test2 in namespace services-8918 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Sep 26 10:35:51.024: INFO: Creating new exec pod
Sep 26 10:35:54.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-8918 exec execpodd8mpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Sep 26 10:35:54.267: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep 26 10:35:54.267: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 10:35:54.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-8918 exec execpodd8mpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.146.248 80'
Sep 26 10:35:54.476: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.146.248 80\nConnection to 10.254.146.248 80 port [tcp/http] succeeded!\n"
Sep 26 10:35:54.476: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-8918
Sep 26 10:35:54.491: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:35:56.496: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8918 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 26 10:35:56.511: INFO: successfully validated that service endpoint-test2 in namespace services-8918 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Sep 26 10:35:57.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-8918 exec execpodd8mpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Sep 26 10:35:57.740: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep 26 10:35:57.740: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 10:35:57.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-8918 exec execpodd8mpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.146.248 80'
Sep 26 10:35:57.948: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.146.248 80\nConnection to 10.254.146.248 80 port [tcp/http] succeeded!\n"
Sep 26 10:35:57.948: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8918
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8918 to expose endpoints map[pod2:[80]]
Sep 26 10:35:58.996: INFO: successfully validated that service endpoint-test2 in namespace services-8918 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Sep 26 10:35:59.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-8918 exec execpodd8mpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Sep 26 10:36:00.196: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep 26 10:36:00.196: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 10:36:00.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-8918 exec execpodd8mpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.146.248 80'
Sep 26 10:36:00.406: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.146.248 80\nConnection to 10.254.146.248 80 port [tcp/http] succeeded!\n"
Sep 26 10:36:00.406: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-8918
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8918 to expose endpoints map[]
Sep 26 10:36:01.445: INFO: successfully validated that service endpoint-test2 in namespace services-8918 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:36:01.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8918" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.571 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":17,"skipped":441,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:36:01.479: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:36:08.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1491" for this suite.

• [SLOW TEST:7.090 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":18,"skipped":460,"failed":0}
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:36:08.570: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Sep 26 10:36:10.644: INFO: pods: 0 < 3
Sep 26 10:36:12.656: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Sep 26 10:36:18.760: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:36:20.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6218" for this suite.

• [SLOW TEST:12.263 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":19,"skipped":460,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:36:20.834: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:36:20.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-21" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":20,"skipped":493,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:36:20.888: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 10:36:20.948: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 26 10:36:20.960: INFO: Number of nodes with available pods: 0
Sep 26 10:36:20.960: INFO: Node 10.37.21.193 is running more than one daemon pod
Sep 26 10:36:21.980: INFO: Number of nodes with available pods: 0
Sep 26 10:36:21.980: INFO: Node 10.37.21.193 is running more than one daemon pod
Sep 26 10:36:22.973: INFO: Number of nodes with available pods: 3
Sep 26 10:36:22.973: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 26 10:36:23.004: INFO: Wrong image for pod: daemon-set-d7vqb. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:23.004: INFO: Wrong image for pod: daemon-set-v946l. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:23.004: INFO: Wrong image for pod: daemon-set-vtplv. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:24.014: INFO: Wrong image for pod: daemon-set-v946l. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:24.014: INFO: Wrong image for pod: daemon-set-vtplv. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:25.014: INFO: Wrong image for pod: daemon-set-v946l. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:25.014: INFO: Wrong image for pod: daemon-set-vtplv. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:26.023: INFO: Pod daemon-set-l8297 is not available
Sep 26 10:36:26.024: INFO: Wrong image for pod: daemon-set-v946l. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:26.024: INFO: Wrong image for pod: daemon-set-vtplv. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:27.013: INFO: Pod daemon-set-l8297 is not available
Sep 26 10:36:27.013: INFO: Wrong image for pod: daemon-set-v946l. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:27.013: INFO: Wrong image for pod: daemon-set-vtplv. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:28.018: INFO: Wrong image for pod: daemon-set-v946l. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:29.017: INFO: Wrong image for pod: daemon-set-v946l. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:30.015: INFO: Pod daemon-set-h8ctp is not available
Sep 26 10:36:30.015: INFO: Wrong image for pod: daemon-set-v946l. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:31.015: INFO: Pod daemon-set-h8ctp is not available
Sep 26 10:36:31.015: INFO: Wrong image for pod: daemon-set-v946l. Expected: registry.dahuatech.com/cncf/agnhost:2.32, got: registry.dahuatech.com/cncf/httpd:2.4.38-1.
Sep 26 10:36:33.019: INFO: Pod daemon-set-fdwbz is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 26 10:36:33.030: INFO: Number of nodes with available pods: 2
Sep 26 10:36:33.030: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 10:36:34.040: INFO: Number of nodes with available pods: 2
Sep 26 10:36:34.040: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 10:36:35.039: INFO: Number of nodes with available pods: 3
Sep 26 10:36:35.039: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3743, will wait for the garbage collector to delete the pods
Sep 26 10:36:35.118: INFO: Deleting DaemonSet.extensions daemon-set took: 5.912217ms
Sep 26 10:36:35.220: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.172352ms
Sep 26 10:36:37.429: INFO: Number of nodes with available pods: 0
Sep 26 10:36:37.429: INFO: Number of running nodes: 0, number of available pods: 0
Sep 26 10:36:37.432: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"529152"},"items":null}

Sep 26 10:36:37.435: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"529152"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:36:37.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3743" for this suite.

• [SLOW TEST:16.574 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":21,"skipped":494,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:36:37.463: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:36:53.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3681" for this suite.

• [SLOW TEST:16.249 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":22,"skipped":509,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:36:53.712: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-b35bb2e4-e2f4-4dbe-81eb-3b467300dc05
STEP: Creating a pod to test consume secrets
Sep 26 10:36:53.790: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b152bba9-b63a-4b7e-aafe-c471f4a71ae7" in namespace "projected-4547" to be "Succeeded or Failed"
Sep 26 10:36:53.795: INFO: Pod "pod-projected-secrets-b152bba9-b63a-4b7e-aafe-c471f4a71ae7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.136965ms
Sep 26 10:36:55.803: INFO: Pod "pod-projected-secrets-b152bba9-b63a-4b7e-aafe-c471f4a71ae7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012549134s
Sep 26 10:36:57.810: INFO: Pod "pod-projected-secrets-b152bba9-b63a-4b7e-aafe-c471f4a71ae7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019433313s
STEP: Saw pod success
Sep 26 10:36:57.810: INFO: Pod "pod-projected-secrets-b152bba9-b63a-4b7e-aafe-c471f4a71ae7" satisfied condition "Succeeded or Failed"
Sep 26 10:36:57.814: INFO: Trying to get logs from node 10.37.21.195 pod pod-projected-secrets-b152bba9-b63a-4b7e-aafe-c471f4a71ae7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 26 10:36:57.835: INFO: Waiting for pod pod-projected-secrets-b152bba9-b63a-4b7e-aafe-c471f4a71ae7 to disappear
Sep 26 10:36:57.838: INFO: Pod pod-projected-secrets-b152bba9-b63a-4b7e-aafe-c471f4a71ae7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:36:57.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4547" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":23,"skipped":512,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:36:57.847: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 10:36:57.920: INFO: The status of Pod busybox-host-aliases4ef839f9-41bd-47fe-9f74-139c0e6bb34e is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:36:59.932: INFO: The status of Pod busybox-host-aliases4ef839f9-41bd-47fe-9f74-139c0e6bb34e is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:37:01.933: INFO: The status of Pod busybox-host-aliases4ef839f9-41bd-47fe-9f74-139c0e6bb34e is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:37:01.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3182" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":24,"skipped":521,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:37:01.965: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 10:37:02.021: INFO: Waiting up to 5m0s for pod "downwardapi-volume-149057a9-9cb0-4171-8f15-71803ce042c5" in namespace "projected-2046" to be "Succeeded or Failed"
Sep 26 10:37:02.024: INFO: Pod "downwardapi-volume-149057a9-9cb0-4171-8f15-71803ce042c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.110267ms
Sep 26 10:37:04.031: INFO: Pod "downwardapi-volume-149057a9-9cb0-4171-8f15-71803ce042c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010329836s
STEP: Saw pod success
Sep 26 10:37:04.031: INFO: Pod "downwardapi-volume-149057a9-9cb0-4171-8f15-71803ce042c5" satisfied condition "Succeeded or Failed"
Sep 26 10:37:04.034: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-149057a9-9cb0-4171-8f15-71803ce042c5 container client-container: <nil>
STEP: delete the pod
Sep 26 10:37:04.053: INFO: Waiting for pod downwardapi-volume-149057a9-9cb0-4171-8f15-71803ce042c5 to disappear
Sep 26 10:37:04.055: INFO: Pod downwardapi-volume-149057a9-9cb0-4171-8f15-71803ce042c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:37:04.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2046" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":25,"skipped":535,"failed":0}
SSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:37:04.066: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Sep 26 10:37:04.136: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:37:04.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9025" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":26,"skipped":542,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:37:04.167: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-29a3b7ff-4edb-40b2-b5d2-dab6ae952c03
STEP: Creating secret with name s-test-opt-upd-147a0d87-aff3-40c0-987d-91b951b10a16
STEP: Creating the pod
Sep 26 10:37:04.230: INFO: The status of Pod pod-secrets-2b6b2962-c6d4-42ca-bd97-9e2cd6e53a78 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:37:06.240: INFO: The status of Pod pod-secrets-2b6b2962-c6d4-42ca-bd97-9e2cd6e53a78 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-29a3b7ff-4edb-40b2-b5d2-dab6ae952c03
STEP: Updating secret s-test-opt-upd-147a0d87-aff3-40c0-987d-91b951b10a16
STEP: Creating secret with name s-test-opt-create-f57c10e3-119d-4276-a9ed-58ce214d361e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:37:08.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7289" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":27,"skipped":544,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:37:08.336: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-2153
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-2153
Sep 26 10:37:08.395: INFO: Found 0 stateful pods, waiting for 1
Sep 26 10:37:18.407: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Sep 26 10:37:18.457: INFO: Deleting all statefulset in ns statefulset-2153
Sep 26 10:37:18.463: INFO: Scaling statefulset ss to 0
Sep 26 10:37:28.493: INFO: Waiting for statefulset status.replicas updated to 0
Sep 26 10:37:28.498: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:37:28.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2153" for this suite.

• [SLOW TEST:20.190 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":28,"skipped":598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:37:28.527: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 26 10:37:28.593: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 26 10:37:28.600: INFO: Waiting for terminating namespaces to be deleted...
Sep 26 10:37:28.603: INFO: 
Logging pods the apiserver thinks is on node 10.37.21.193 before test
Sep 26 10:37:28.610: INFO: apiserver-10.37.21.193 from default started at 2021-09-26 10:28:41 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.611: INFO: 	Container apiserver ready: true, restart count 1
Sep 26 10:37:28.611: INFO: etcd-10.37.21.193 from default started at 2021-09-26 05:42:03 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.611: INFO: 	Container etcd ready: true, restart count 0
Sep 26 10:37:28.611: INFO: keepalived-10.37.21.193 from default started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.611: INFO: 	Container keepalived ready: true, restart count 0
Sep 26 10:37:28.611: INFO: kube-controllermanager-10.37.21.193 from default started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.611: INFO: 	Container kube-controllermanager ready: true, restart count 2
Sep 26 10:37:28.611: INFO: kube-proxy-10.37.21.193 from default started at 2021-09-23 12:14:20 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.611: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 26 10:37:28.611: INFO: kube-scheduler-10.37.21.193 from default started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.611: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 26 10:37:28.611: INFO: calico-node-rv4lt from kube-system started at 2021-09-26 10:29:33 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.611: INFO: 	Container calico-node ready: true, restart count 0
Sep 26 10:37:28.611: INFO: coredns-7474c6f7bf-f6lkf from kube-system started at 2021-09-26 10:29:48 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.611: INFO: 	Container coredns ready: true, restart count 0
Sep 26 10:37:28.611: INFO: nginx-10.37.21.193 from kube-system started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.611: INFO: 	Container nginx ready: true, restart count 0
Sep 26 10:37:28.611: INFO: sonobuoy-e2e-job-ffce8e36c7dd478d from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 10:37:28.611: INFO: 	Container e2e ready: true, restart count 0
Sep 26 10:37:28.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 10:37:28.611: INFO: sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-lp2ds from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 10:37:28.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 10:37:28.611: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 26 10:37:28.611: INFO: 
Logging pods the apiserver thinks is on node 10.37.21.194 before test
Sep 26 10:37:28.617: INFO: apiserver-10.37.21.194 from default started at 2021-09-26 10:28:51 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.617: INFO: 	Container apiserver ready: true, restart count 0
Sep 26 10:37:28.617: INFO: etcd-10.37.21.194 from default started at 2021-09-23 12:14:21 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.617: INFO: 	Container etcd ready: true, restart count 0
Sep 26 10:37:28.617: INFO: keepalived-10.37.21.194 from default started at 2021-09-23 12:14:21 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.617: INFO: 	Container keepalived ready: true, restart count 0
Sep 26 10:37:28.617: INFO: kube-controllermanager-10.37.21.194 from default started at 2021-09-26 05:42:32 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.617: INFO: 	Container kube-controllermanager ready: true, restart count 2
Sep 26 10:37:28.617: INFO: kube-proxy-10.37.21.194 from default started at 2021-09-26 05:42:32 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.617: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 26 10:37:28.617: INFO: kube-scheduler-10.37.21.194 from default started at 2021-09-26 05:42:32 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.617: INFO: 	Container kube-scheduler ready: true, restart count 1
Sep 26 10:37:28.617: INFO: calico-node-dd6nc from kube-system started at 2021-09-26 10:29:33 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.617: INFO: 	Container calico-node ready: true, restart count 0
Sep 26 10:37:28.617: INFO: coredns-7474c6f7bf-kkpwf from kube-system started at 2021-09-26 10:29:53 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.617: INFO: 	Container coredns ready: true, restart count 0
Sep 26 10:37:28.617: INFO: nginx-10.37.21.194 from kube-system started at 2021-09-23 12:14:21 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.617: INFO: 	Container nginx ready: true, restart count 0
Sep 26 10:37:28.617: INFO: sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-46csd from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 10:37:28.617: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 10:37:28.617: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 26 10:37:28.617: INFO: 
Logging pods the apiserver thinks is on node 10.37.21.195 before test
Sep 26 10:37:28.625: INFO: apiserver-10.37.21.195 from default started at 2021-09-26 10:31:18 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.625: INFO: 	Container apiserver ready: true, restart count 0
Sep 26 10:37:28.625: INFO: etcd-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.625: INFO: 	Container etcd ready: true, restart count 1
Sep 26 10:37:28.625: INFO: keepalived-10.37.21.195 from default started at 2021-09-23 12:14:22 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.625: INFO: 	Container keepalived ready: true, restart count 1
Sep 26 10:37:28.625: INFO: kube-controllermanager-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.625: INFO: 	Container kube-controllermanager ready: true, restart count 4
Sep 26 10:37:28.625: INFO: kube-proxy-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.625: INFO: 	Container kube-proxy ready: true, restart count 2
Sep 26 10:37:28.625: INFO: kube-scheduler-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.625: INFO: 	Container kube-scheduler ready: true, restart count 1
Sep 26 10:37:28.625: INFO: calico-node-fhzl7 from kube-system started at 2021-09-26 10:29:33 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.625: INFO: 	Container calico-node ready: true, restart count 0
Sep 26 10:37:28.625: INFO: coredns-7474c6f7bf-plgdt from kube-system started at 2021-09-26 10:29:53 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.625: INFO: 	Container coredns ready: true, restart count 0
Sep 26 10:37:28.625: INFO: nginx-10.37.21.195 from kube-system started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.625: INFO: 	Container nginx ready: true, restart count 1
Sep 26 10:37:28.625: INFO: busybox-host-aliases4ef839f9-41bd-47fe-9f74-139c0e6bb34e from kubelet-test-3182 started at 2021-09-26 10:36:57 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.625: INFO: 	Container busybox-host-aliases4ef839f9-41bd-47fe-9f74-139c0e6bb34e ready: true, restart count 0
Sep 26 10:37:28.625: INFO: sonobuoy from sonobuoy started at 2021-09-26 10:33:22 +0000 UTC (1 container statuses recorded)
Sep 26 10:37:28.625: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 26 10:37:28.625: INFO: sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-76l7m from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 10:37:28.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 10:37:28.625: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16a858ef0c386824], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:37:29.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7570" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":29,"skipped":622,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:37:29.675: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Sep 26 10:37:29.732: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:37:31.744: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:37:33.745: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Sep 26 10:37:33.777: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:37:35.786: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Sep 26 10:37:35.798: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 26 10:37:35.801: INFO: Pod pod-with-prestop-http-hook still exists
Sep 26 10:37:37.803: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 26 10:37:37.809: INFO: Pod pod-with-prestop-http-hook still exists
Sep 26 10:37:39.802: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 26 10:37:39.813: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:37:39.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3241" for this suite.

• [SLOW TEST:10.159 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":30,"skipped":635,"failed":0}
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:37:39.835: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-4733
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Sep 26 10:37:39.910: INFO: Found 0 stateful pods, waiting for 3
Sep 26 10:37:49.933: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 26 10:37:49.933: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 26 10:37:49.933: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 26 10:37:49.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-4733 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 26 10:37:50.150: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 26 10:37:50.150: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 26 10:37:50.150: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.dahuatech.com/cncf/httpd:2.4.38-1 to registry.dahuatech.com/cncf/httpd:2.4.39-1
Sep 26 10:38:00.225: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 26 10:38:10.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-4733 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 26 10:38:10.478: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 26 10:38:10.478: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 26 10:38:10.478: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 26 10:38:20.513: INFO: Waiting for StatefulSet statefulset-4733/ss2 to complete update
STEP: Rolling back to a previous revision
Sep 26 10:38:30.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-4733 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 26 10:38:30.800: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 26 10:38:30.800: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 26 10:38:30.800: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 26 10:38:40.857: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 26 10:38:50.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-4733 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 26 10:38:51.114: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 26 10:38:51.114: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 26 10:38:51.114: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 26 10:39:01.147: INFO: Waiting for StatefulSet statefulset-4733/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Sep 26 10:39:11.169: INFO: Deleting all statefulset in ns statefulset-4733
Sep 26 10:39:11.172: INFO: Scaling statefulset ss2 to 0
Sep 26 10:39:21.214: INFO: Waiting for statefulset status.replicas updated to 0
Sep 26 10:39:21.217: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:39:21.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4733" for this suite.

• [SLOW TEST:101.417 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":31,"skipped":635,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:39:21.252: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9694.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9694.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9694.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9694.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9694.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9694.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9694.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9694.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9694.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 243.125.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.125.243_udp@PTR;check="$$(dig +tcp +noall +answer +search 243.125.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.125.243_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9694.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9694.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9694.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9694.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9694.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9694.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9694.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9694.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9694.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 243.125.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.125.243_udp@PTR;check="$$(dig +tcp +noall +answer +search 243.125.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.125.243_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 26 10:39:25.390: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9694.svc.cluster.local from pod dns-9694/dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a: the server could not find the requested resource (get pods dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a)
Sep 26 10:39:25.403: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9694/dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a: the server could not find the requested resource (get pods dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a)
Sep 26 10:39:25.408: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9694/dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a: the server could not find the requested resource (get pods dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a)
Sep 26 10:39:25.430: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9694.svc.cluster.local from pod dns-9694/dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a: the server could not find the requested resource (get pods dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a)
Sep 26 10:39:25.441: INFO: Unable to read jessie_udp@PodARecord from pod dns-9694/dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a: the server could not find the requested resource (get pods dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a)
Sep 26 10:39:25.445: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9694/dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a: the server could not find the requested resource (get pods dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a)
Sep 26 10:39:25.452: INFO: Lookups using dns-9694/dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a failed for: [wheezy_tcp@_http._tcp.dns-test-service.dns-9694.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_tcp@_http._tcp.dns-test-service.dns-9694.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 26 10:39:30.547: INFO: DNS probes using dns-9694/dns-test-d7e64ad0-67f5-4c14-8953-817caad2004a succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:39:30.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9694" for this suite.

• [SLOW TEST:9.377 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":32,"skipped":641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:39:30.629: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image registry.dahuatech.com/cncf/httpd:2.4.38-1
Sep 26 10:39:30.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-7343 run e2e-test-httpd-pod --image=registry.dahuatech.com/cncf/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep 26 10:39:30.775: INFO: stderr: ""
Sep 26 10:39:30.775: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep 26 10:39:35.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-7343 get pod e2e-test-httpd-pod -o json'
Sep 26 10:39:35.893: INFO: stderr: ""
Sep 26 10:39:35.893: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"a93152bc8150013978c2ef8cddb93860d94feaddb31c5f770304ea1bf427d299\",\n            \"cni.projectcalico.org/podIP\": \"192.168.181.107/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.181.107/32\"\n        },\n        \"creationTimestamp\": \"2021-09-26T10:39:30Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7343\",\n        \"resourceVersion\": \"530451\",\n        \"uid\": \"e9a1531a-5e48-4a9e-8a38-0561b705b617\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.dahuatech.com/cncf/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5lhww\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.37.21.195\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5lhww\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-26T10:39:30Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-26T10:39:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-26T10:39:33Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-26T10:39:30Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://dcc8854d10dce1bc45d46f279cf382cdb578946875aa02ec42c6ba5a67d86546\",\n                \"image\": \"registry.dahuatech.com/cncf/httpd:2.4.38-1\",\n                \"imageID\": \"docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-09-26T10:39:32Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.37.21.195\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.181.107\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.181.107\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-09-26T10:39:30Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 26 10:39:35.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-7343 replace -f -'
Sep 26 10:39:36.056: INFO: stderr: ""
Sep 26 10:39:36.056: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.dahuatech.com/cncf/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Sep 26 10:39:36.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-7343 delete pods e2e-test-httpd-pod'
Sep 26 10:39:38.227: INFO: stderr: ""
Sep 26 10:39:38.227: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:39:38.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7343" for this suite.

• [SLOW TEST:7.616 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1555
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":33,"skipped":672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:39:38.246: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 26 10:39:38.324: INFO: Number of nodes with available pods: 0
Sep 26 10:39:38.324: INFO: Node 10.37.21.193 is running more than one daemon pod
Sep 26 10:39:39.339: INFO: Number of nodes with available pods: 0
Sep 26 10:39:39.339: INFO: Node 10.37.21.193 is running more than one daemon pod
Sep 26 10:39:40.339: INFO: Number of nodes with available pods: 2
Sep 26 10:39:40.339: INFO: Node 10.37.21.194 is running more than one daemon pod
Sep 26 10:39:41.338: INFO: Number of nodes with available pods: 3
Sep 26 10:39:41.338: INFO: Number of running nodes: 3, number of available pods: 3
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Sep 26 10:39:41.365: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"530548"},"items":null}

Sep 26 10:39:41.370: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"530548"},"items":[{"metadata":{"name":"daemon-set-5vdsr","generateName":"daemon-set-","namespace":"daemonsets-6516","uid":"45e20867-dc46-4b41-8294-3b718f29744b","resourceVersion":"530546","creationTimestamp":"2021-09-26T10:39:38Z","labels":{"controller-revision-hash":"75f8d65b87","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"49cd0e3c0fd9a6cc6528f6f984b775fd59bd5ef1a3921df52ebc8eea1e38dbd3","cni.projectcalico.org/podIP":"192.168.50.151/32","cni.projectcalico.org/podIPs":"192.168.50.151/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6e8c4f16-85f9-4510-a0c8-33bc3e150582","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-09-26T10:39:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e8c4f16-85f9-4510-a0c8-33bc3e150582\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-09-26T10:39:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-09-26T10:39:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.50.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ll98c","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.dahuatech.com/cncf/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ll98c","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.37.21.194","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.37.21.194"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-09-26T10:39:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-09-26T10:39:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-09-26T10:39:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-09-26T10:39:38Z"}],"hostIP":"10.37.21.194","podIP":"192.168.50.151","podIPs":[{"ip":"192.168.50.151"}],"startTime":"2021-09-26T10:39:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-09-26T10:39:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.dahuatech.com/cncf/httpd:2.4.38-1","imageID":"docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4","containerID":"docker://c47a2e6ce527c043731cf29e2c763874e97e48883821508501f4727c2bcd7414","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jkfqs","generateName":"daemon-set-","namespace":"daemonsets-6516","uid":"005ec1b0-bad7-4c7d-a415-118e9d8eef60","resourceVersion":"530544","creationTimestamp":"2021-09-26T10:39:38Z","labels":{"controller-revision-hash":"75f8d65b87","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6a10278c9e409224258122fec25338ca1917c940c260d34c77818c3cde869afb","cni.projectcalico.org/podIP":"192.168.181.113/32","cni.projectcalico.org/podIPs":"192.168.181.113/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6e8c4f16-85f9-4510-a0c8-33bc3e150582","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-09-26T10:39:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e8c4f16-85f9-4510-a0c8-33bc3e150582\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-09-26T10:39:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-09-26T10:39:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.181.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kmh2b","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.dahuatech.com/cncf/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kmh2b","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.37.21.195","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.37.21.195"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-09-26T10:39:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-09-26T10:39:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-09-26T10:39:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-09-26T10:39:38Z"}],"hostIP":"10.37.21.195","podIP":"192.168.181.113","podIPs":[{"ip":"192.168.181.113"}],"startTime":"2021-09-26T10:39:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-09-26T10:39:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.dahuatech.com/cncf/httpd:2.4.38-1","imageID":"docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4","containerID":"docker://9df279e9e3c2d9d86b0ead0e6aee9c81e4f7a855e6ad37b786a44a55e10d3bf6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tp95v","generateName":"daemon-set-","namespace":"daemonsets-6516","uid":"e51549b3-b38c-4233-bf7b-09a4bef80ba9","resourceVersion":"530540","creationTimestamp":"2021-09-26T10:39:38Z","labels":{"controller-revision-hash":"75f8d65b87","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"719a878fbc38313b82176e93e143212add70047a44ab7382c8fedd7f369e8f81","cni.projectcalico.org/podIP":"192.168.84.81/32","cni.projectcalico.org/podIPs":"192.168.84.81/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6e8c4f16-85f9-4510-a0c8-33bc3e150582","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-09-26T10:39:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e8c4f16-85f9-4510-a0c8-33bc3e150582\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-09-26T10:39:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-09-26T10:39:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.84.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-n8f26","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.dahuatech.com/cncf/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-n8f26","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.37.21.193","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.37.21.193"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-09-26T10:39:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-09-26T10:39:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-09-26T10:39:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-09-26T10:39:38Z"}],"hostIP":"10.37.21.193","podIP":"192.168.84.81","podIPs":[{"ip":"192.168.84.81"}],"startTime":"2021-09-26T10:39:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-09-26T10:39:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.dahuatech.com/cncf/httpd:2.4.38-1","imageID":"docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4","containerID":"docker://a24a91404304120c3718f47a59f62232acedefaa84de888987c551bed9479c06","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:39:41.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6516" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":34,"skipped":720,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:39:41.399: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep 26 10:39:41.470: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9132  e4aee302-edcb-4176-84d0-947d04b19a2c 530559 0 2021-09-26 10:39:41 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-09-26 10:39:41 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bn6fl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.dahuatech.com/cncf/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bn6fl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 10:39:41.474: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:39:43.485: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Sep 26 10:39:43.485: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9132 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 10:39:43.485: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Verifying customized DNS server is configured on pod...
Sep 26 10:39:43.650: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9132 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 10:39:43.650: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 10:39:43.807: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:39:43.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9132" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":35,"skipped":742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:39:43.840: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5963
Sep 26 10:39:43.894: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:39:45.905: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Sep 26 10:39:45.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5963 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep 26 10:39:46.137: INFO: rc: 7
Sep 26 10:39:46.150: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 26 10:39:46.154: INFO: Pod kube-proxy-mode-detector no longer exists
Sep 26 10:39:46.154: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5963 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-5963
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5963
I0926 10:39:46.170485      22 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5963, replica count: 3
I0926 10:39:49.221946      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 26 10:39:49.232: INFO: Creating new exec pod
Sep 26 10:39:54.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5963 exec execpod-affinity6ggcb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Sep 26 10:39:54.477: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Sep 26 10:39:54.477: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 10:39:54.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5963 exec execpod-affinity6ggcb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.11.128 80'
Sep 26 10:39:54.731: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.11.128 80\nConnection to 10.254.11.128 80 port [tcp/http] succeeded!\n"
Sep 26 10:39:54.731: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 10:39:54.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5963 exec execpod-affinity6ggcb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.11.128:80/ ; done'
Sep 26 10:39:55.046: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n"
Sep 26 10:39:55.046: INFO: stdout: "\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr\naffinity-clusterip-timeout-zn2tr"
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Received response from host: affinity-clusterip-timeout-zn2tr
Sep 26 10:39:55.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5963 exec execpod-affinity6ggcb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.11.128:80/'
Sep 26 10:39:55.276: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n"
Sep 26 10:39:55.276: INFO: stdout: "affinity-clusterip-timeout-zn2tr"
Sep 26 10:40:15.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5963 exec execpod-affinity6ggcb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.11.128:80/'
Sep 26 10:40:15.515: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.11.128:80/\n"
Sep 26 10:40:15.516: INFO: stdout: "affinity-clusterip-timeout-jw69k"
Sep 26 10:40:15.516: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5963, will wait for the garbage collector to delete the pods
Sep 26 10:40:15.597: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 8.409713ms
Sep 26 10:40:15.698: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.64416ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:40:18.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5963" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:34.599 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":36,"skipped":771,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:40:18.439: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Sep 26 10:40:18.494: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-147 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:40:18.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-147" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":37,"skipped":784,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:40:18.584: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Sep 26 10:40:20.651: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:40:22.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5236" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":38,"skipped":813,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:40:22.732: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-6c9244d4-b0fb-467f-a5d1-fc9de24f8c04
STEP: Creating a pod to test consume secrets
Sep 26 10:40:22.840: INFO: Waiting up to 5m0s for pod "pod-secrets-e5f3b473-8bc9-4472-848e-807079570531" in namespace "secrets-2218" to be "Succeeded or Failed"
Sep 26 10:40:22.844: INFO: Pod "pod-secrets-e5f3b473-8bc9-4472-848e-807079570531": Phase="Pending", Reason="", readiness=false. Elapsed: 3.941834ms
Sep 26 10:40:24.849: INFO: Pod "pod-secrets-e5f3b473-8bc9-4472-848e-807079570531": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008760872s
STEP: Saw pod success
Sep 26 10:40:24.849: INFO: Pod "pod-secrets-e5f3b473-8bc9-4472-848e-807079570531" satisfied condition "Succeeded or Failed"
Sep 26 10:40:24.852: INFO: Trying to get logs from node 10.37.21.194 pod pod-secrets-e5f3b473-8bc9-4472-848e-807079570531 container secret-volume-test: <nil>
STEP: delete the pod
Sep 26 10:40:24.889: INFO: Waiting for pod pod-secrets-e5f3b473-8bc9-4472-848e-807079570531 to disappear
Sep 26 10:40:24.893: INFO: Pod pod-secrets-e5f3b473-8bc9-4472-848e-807079570531 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:40:24.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2218" for this suite.
STEP: Destroying namespace "secret-namespace-4538" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":39,"skipped":825,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:40:24.914: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-b972d85d-d8f3-4b6b-8f1c-f2583830e820 in namespace container-probe-5420
Sep 26 10:40:26.992: INFO: Started pod liveness-b972d85d-d8f3-4b6b-8f1c-f2583830e820 in namespace container-probe-5420
STEP: checking the pod's current state and verifying that restartCount is present
Sep 26 10:40:26.995: INFO: Initial restart count of pod liveness-b972d85d-d8f3-4b6b-8f1c-f2583830e820 is 0
Sep 26 10:40:47.102: INFO: Restart count of pod container-probe-5420/liveness-b972d85d-d8f3-4b6b-8f1c-f2583830e820 is now 1 (20.106912268s elapsed)
Sep 26 10:41:07.218: INFO: Restart count of pod container-probe-5420/liveness-b972d85d-d8f3-4b6b-8f1c-f2583830e820 is now 2 (40.223097712s elapsed)
Sep 26 10:41:27.350: INFO: Restart count of pod container-probe-5420/liveness-b972d85d-d8f3-4b6b-8f1c-f2583830e820 is now 3 (1m0.354809282s elapsed)
Sep 26 10:41:47.482: INFO: Restart count of pod container-probe-5420/liveness-b972d85d-d8f3-4b6b-8f1c-f2583830e820 is now 4 (1m20.487013242s elapsed)
Sep 26 10:42:59.970: INFO: Restart count of pod container-probe-5420/liveness-b972d85d-d8f3-4b6b-8f1c-f2583830e820 is now 5 (2m32.974604871s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:42:59.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5420" for this suite.

• [SLOW TEST:155.096 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":40,"skipped":842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:43:00.011: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-77bd6ac7-dce1-43e0-b98a-cf681e0c2447
STEP: Creating a pod to test consume secrets
Sep 26 10:43:00.099: INFO: Waiting up to 5m0s for pod "pod-secrets-e5621f2b-5cf9-4728-8a58-8ef3e73af23c" in namespace "secrets-7551" to be "Succeeded or Failed"
Sep 26 10:43:00.102: INFO: Pod "pod-secrets-e5621f2b-5cf9-4728-8a58-8ef3e73af23c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313857ms
Sep 26 10:43:02.109: INFO: Pod "pod-secrets-e5621f2b-5cf9-4728-8a58-8ef3e73af23c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010726046s
STEP: Saw pod success
Sep 26 10:43:02.109: INFO: Pod "pod-secrets-e5621f2b-5cf9-4728-8a58-8ef3e73af23c" satisfied condition "Succeeded or Failed"
Sep 26 10:43:02.113: INFO: Trying to get logs from node 10.37.21.194 pod pod-secrets-e5621f2b-5cf9-4728-8a58-8ef3e73af23c container secret-volume-test: <nil>
STEP: delete the pod
Sep 26 10:43:02.142: INFO: Waiting for pod pod-secrets-e5621f2b-5cf9-4728-8a58-8ef3e73af23c to disappear
Sep 26 10:43:02.145: INFO: Pod pod-secrets-e5621f2b-5cf9-4728-8a58-8ef3e73af23c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:43:02.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7551" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":41,"skipped":873,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:43:02.165: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:43:02.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5841" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":42,"skipped":901,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:43:02.279: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-33efc383-118a-4acd-856c-3b7f3af1e17b
STEP: Creating a pod to test consume configMaps
Sep 26 10:43:02.337: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9ee02fbf-0b01-4aae-bbac-0315787cea9d" in namespace "projected-4872" to be "Succeeded or Failed"
Sep 26 10:43:02.341: INFO: Pod "pod-projected-configmaps-9ee02fbf-0b01-4aae-bbac-0315787cea9d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.826123ms
Sep 26 10:43:04.351: INFO: Pod "pod-projected-configmaps-9ee02fbf-0b01-4aae-bbac-0315787cea9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013896433s
STEP: Saw pod success
Sep 26 10:43:04.351: INFO: Pod "pod-projected-configmaps-9ee02fbf-0b01-4aae-bbac-0315787cea9d" satisfied condition "Succeeded or Failed"
Sep 26 10:43:04.354: INFO: Trying to get logs from node 10.37.21.195 pod pod-projected-configmaps-9ee02fbf-0b01-4aae-bbac-0315787cea9d container agnhost-container: <nil>
STEP: delete the pod
Sep 26 10:43:04.382: INFO: Waiting for pod pod-projected-configmaps-9ee02fbf-0b01-4aae-bbac-0315787cea9d to disappear
Sep 26 10:43:04.385: INFO: Pod pod-projected-configmaps-9ee02fbf-0b01-4aae-bbac-0315787cea9d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:43:04.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4872" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":43,"skipped":921,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:43:04.395: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep 26 10:43:04.442: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 10:43:12.380: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:43:31.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9696" for this suite.

• [SLOW TEST:26.794 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":44,"skipped":965,"failed":0}
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:43:31.189: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 10:43:31.291: INFO: The status of Pod server-envvars-572ea451-5a8c-4828-ba31-a3ddb7f0a181 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:43:33.305: INFO: The status of Pod server-envvars-572ea451-5a8c-4828-ba31-a3ddb7f0a181 is Running (Ready = true)
Sep 26 10:43:33.363: INFO: Waiting up to 5m0s for pod "client-envvars-8f9e5c30-d0ec-4cce-b008-52c52224a903" in namespace "pods-6864" to be "Succeeded or Failed"
Sep 26 10:43:33.373: INFO: Pod "client-envvars-8f9e5c30-d0ec-4cce-b008-52c52224a903": Phase="Pending", Reason="", readiness=false. Elapsed: 10.003951ms
Sep 26 10:43:35.384: INFO: Pod "client-envvars-8f9e5c30-d0ec-4cce-b008-52c52224a903": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020925787s
STEP: Saw pod success
Sep 26 10:43:35.384: INFO: Pod "client-envvars-8f9e5c30-d0ec-4cce-b008-52c52224a903" satisfied condition "Succeeded or Failed"
Sep 26 10:43:35.387: INFO: Trying to get logs from node 10.37.21.194 pod client-envvars-8f9e5c30-d0ec-4cce-b008-52c52224a903 container env3cont: <nil>
STEP: delete the pod
Sep 26 10:43:35.416: INFO: Waiting for pod client-envvars-8f9e5c30-d0ec-4cce-b008-52c52224a903 to disappear
Sep 26 10:43:35.419: INFO: Pod client-envvars-8f9e5c30-d0ec-4cce-b008-52c52224a903 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:43:35.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6864" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":45,"skipped":965,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:43:35.428: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:45:01.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5370" for this suite.

• [SLOW TEST:86.092 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":46,"skipped":978,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:45:01.521: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 26 10:45:01.597: INFO: Waiting up to 5m0s for pod "pod-79b8aa1c-eb51-4212-b0f4-0dda21fcad17" in namespace "emptydir-9388" to be "Succeeded or Failed"
Sep 26 10:45:01.601: INFO: Pod "pod-79b8aa1c-eb51-4212-b0f4-0dda21fcad17": Phase="Pending", Reason="", readiness=false. Elapsed: 3.603443ms
Sep 26 10:45:03.613: INFO: Pod "pod-79b8aa1c-eb51-4212-b0f4-0dda21fcad17": Phase="Running", Reason="", readiness=true. Elapsed: 2.015216623s
Sep 26 10:45:05.620: INFO: Pod "pod-79b8aa1c-eb51-4212-b0f4-0dda21fcad17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022526935s
STEP: Saw pod success
Sep 26 10:45:05.620: INFO: Pod "pod-79b8aa1c-eb51-4212-b0f4-0dda21fcad17" satisfied condition "Succeeded or Failed"
Sep 26 10:45:05.624: INFO: Trying to get logs from node 10.37.21.194 pod pod-79b8aa1c-eb51-4212-b0f4-0dda21fcad17 container test-container: <nil>
STEP: delete the pod
Sep 26 10:45:05.652: INFO: Waiting for pod pod-79b8aa1c-eb51-4212-b0f4-0dda21fcad17 to disappear
Sep 26 10:45:05.655: INFO: Pod pod-79b8aa1c-eb51-4212-b0f4-0dda21fcad17 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:45:05.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9388" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":47,"skipped":990,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:45:05.662: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 10:45:05.729: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 26 10:45:05.737: INFO: Number of nodes with available pods: 0
Sep 26 10:45:05.737: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 26 10:45:05.755: INFO: Number of nodes with available pods: 0
Sep 26 10:45:05.756: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 10:45:06.763: INFO: Number of nodes with available pods: 0
Sep 26 10:45:06.763: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 10:45:07.762: INFO: Number of nodes with available pods: 1
Sep 26 10:45:07.762: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 26 10:45:07.782: INFO: Number of nodes with available pods: 1
Sep 26 10:45:07.782: INFO: Number of running nodes: 0, number of available pods: 1
Sep 26 10:45:08.792: INFO: Number of nodes with available pods: 0
Sep 26 10:45:08.792: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 26 10:45:08.804: INFO: Number of nodes with available pods: 0
Sep 26 10:45:08.804: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 10:45:09.811: INFO: Number of nodes with available pods: 0
Sep 26 10:45:09.811: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 10:45:10.811: INFO: Number of nodes with available pods: 0
Sep 26 10:45:10.811: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 10:45:11.811: INFO: Number of nodes with available pods: 0
Sep 26 10:45:11.811: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 10:45:12.811: INFO: Number of nodes with available pods: 1
Sep 26 10:45:12.811: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6480, will wait for the garbage collector to delete the pods
Sep 26 10:45:12.883: INFO: Deleting DaemonSet.extensions daemon-set took: 11.730516ms
Sep 26 10:45:12.984: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.694418ms
Sep 26 10:45:15.388: INFO: Number of nodes with available pods: 0
Sep 26 10:45:15.388: INFO: Number of running nodes: 0, number of available pods: 0
Sep 26 10:45:15.390: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"531876"},"items":null}

Sep 26 10:45:15.392: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"531876"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:45:15.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6480" for this suite.

• [SLOW TEST:9.761 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":48,"skipped":995,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:45:15.423: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Sep 26 10:45:15.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-7407 create -f -'
Sep 26 10:45:15.705: INFO: stderr: ""
Sep 26 10:45:15.705: INFO: stdout: "pod/pause created\n"
Sep 26 10:45:15.705: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 26 10:45:15.705: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7407" to be "running and ready"
Sep 26 10:45:15.711: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.980492ms
Sep 26 10:45:17.722: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.017478316s
Sep 26 10:45:17.722: INFO: Pod "pause" satisfied condition "running and ready"
Sep 26 10:45:17.722: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 26 10:45:17.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-7407 label pods pause testing-label=testing-label-value'
Sep 26 10:45:17.809: INFO: stderr: ""
Sep 26 10:45:17.809: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 26 10:45:17.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-7407 get pod pause -L testing-label'
Sep 26 10:45:17.893: INFO: stderr: ""
Sep 26 10:45:17.893: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 26 10:45:17.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-7407 label pods pause testing-label-'
Sep 26 10:45:17.976: INFO: stderr: ""
Sep 26 10:45:17.976: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 26 10:45:17.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-7407 get pod pause -L testing-label'
Sep 26 10:45:18.050: INFO: stderr: ""
Sep 26 10:45:18.050: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Sep 26 10:45:18.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-7407 delete --grace-period=0 --force -f -'
Sep 26 10:45:18.137: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 26 10:45:18.137: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 26 10:45:18.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-7407 get rc,svc -l name=pause --no-headers'
Sep 26 10:45:18.210: INFO: stderr: "No resources found in kubectl-7407 namespace.\n"
Sep 26 10:45:18.210: INFO: stdout: ""
Sep 26 10:45:18.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-7407 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 26 10:45:18.280: INFO: stderr: ""
Sep 26 10:45:18.280: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:45:18.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7407" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":49,"skipped":1007,"failed":0}
SSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:45:18.291: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Sep 26 10:45:18.338: INFO: Waiting up to 5m0s for pod "client-containers-a4e123b0-fe45-4d15-97ac-6ab473e15822" in namespace "containers-1672" to be "Succeeded or Failed"
Sep 26 10:45:18.341: INFO: Pod "client-containers-a4e123b0-fe45-4d15-97ac-6ab473e15822": Phase="Pending", Reason="", readiness=false. Elapsed: 2.670499ms
Sep 26 10:45:20.348: INFO: Pod "client-containers-a4e123b0-fe45-4d15-97ac-6ab473e15822": Phase="Running", Reason="", readiness=true. Elapsed: 2.010367223s
Sep 26 10:45:22.363: INFO: Pod "client-containers-a4e123b0-fe45-4d15-97ac-6ab473e15822": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024648447s
STEP: Saw pod success
Sep 26 10:45:22.363: INFO: Pod "client-containers-a4e123b0-fe45-4d15-97ac-6ab473e15822" satisfied condition "Succeeded or Failed"
Sep 26 10:45:22.366: INFO: Trying to get logs from node 10.37.21.194 pod client-containers-a4e123b0-fe45-4d15-97ac-6ab473e15822 container agnhost-container: <nil>
STEP: delete the pod
Sep 26 10:45:22.394: INFO: Waiting for pod client-containers-a4e123b0-fe45-4d15-97ac-6ab473e15822 to disappear
Sep 26 10:45:22.396: INFO: Pod client-containers-a4e123b0-fe45-4d15-97ac-6ab473e15822 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:45:22.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1672" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":50,"skipped":1010,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:45:22.406: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep 26 10:45:22.453: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 10:45:30.460: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:45:49.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7286" for this suite.

• [SLOW TEST:26.799 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":51,"skipped":1016,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:45:49.205: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 10:45:49.268: INFO: Creating ReplicaSet my-hostname-basic-cc9a845f-3e19-415a-a691-faf12300e0d2
Sep 26 10:45:49.277: INFO: Pod name my-hostname-basic-cc9a845f-3e19-415a-a691-faf12300e0d2: Found 0 pods out of 1
Sep 26 10:45:54.289: INFO: Pod name my-hostname-basic-cc9a845f-3e19-415a-a691-faf12300e0d2: Found 1 pods out of 1
Sep 26 10:45:54.289: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-cc9a845f-3e19-415a-a691-faf12300e0d2" is running
Sep 26 10:45:54.292: INFO: Pod "my-hostname-basic-cc9a845f-3e19-415a-a691-faf12300e0d2-sqtrb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-26 10:45:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-26 10:45:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-26 10:45:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-26 10:45:49 +0000 UTC Reason: Message:}])
Sep 26 10:45:54.292: INFO: Trying to dial the pod
Sep 26 10:45:59.322: INFO: Controller my-hostname-basic-cc9a845f-3e19-415a-a691-faf12300e0d2: Got expected result from replica 1 [my-hostname-basic-cc9a845f-3e19-415a-a691-faf12300e0d2-sqtrb]: "my-hostname-basic-cc9a845f-3e19-415a-a691-faf12300e0d2-sqtrb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:45:59.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6158" for this suite.

• [SLOW TEST:10.138 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":52,"skipped":1022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:45:59.344: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 10:46:01.430: INFO: Deleting pod "var-expansion-fc793aac-55f4-4205-ba83-4bd2185ce058" in namespace "var-expansion-2909"
Sep 26 10:46:01.444: INFO: Wait up to 5m0s for pod "var-expansion-fc793aac-55f4-4205-ba83-4bd2185ce058" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:46:05.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2909" for this suite.

• [SLOW TEST:6.125 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":53,"skipped":1058,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:46:05.469: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 10:46:05.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-1487 create -f -'
Sep 26 10:46:05.711: INFO: stderr: ""
Sep 26 10:46:05.711: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep 26 10:46:05.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-1487 create -f -'
Sep 26 10:46:05.899: INFO: stderr: ""
Sep 26 10:46:05.900: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 26 10:46:06.911: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 26 10:46:06.911: INFO: Found 0 / 1
Sep 26 10:46:07.911: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 26 10:46:07.912: INFO: Found 1 / 1
Sep 26 10:46:07.912: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 26 10:46:07.916: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 26 10:46:07.916: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 26 10:46:07.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-1487 describe pod agnhost-primary-c7ljl'
Sep 26 10:46:08.001: INFO: stderr: ""
Sep 26 10:46:08.001: INFO: stdout: "Name:         agnhost-primary-c7ljl\nNamespace:    kubectl-1487\nPriority:     0\nNode:         10.37.21.194/10.37.21.194\nStart Time:   Sun, 26 Sep 2021 10:46:05 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 2ae305ecace464c2ccf1cf587f5b250a8d1261e9546d412f911c8b5b20d69476\n              cni.projectcalico.org/podIP: 192.168.50.153/32\n              cni.projectcalico.org/podIPs: 192.168.50.153/32\nStatus:       Running\nIP:           192.168.50.153\nIPs:\n  IP:           192.168.50.153\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://ab6d956a2089c1c3040c43b3f94566c35cc6247253a91f369f67f7d8ceec8ced\n    Image:          registry.dahuatech.com/cncf/agnhost:2.32\n    Image ID:       docker-pullable://registry.dahuatech.com/cncf/agnhost@sha256:e76d7fb8b95056e3909026f6024f19c496a908f7471ad3426302c4fb684885f9\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 26 Sep 2021 10:46:06 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5npwm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5npwm:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1487/agnhost-primary-c7ljl to 10.37.21.194\n  Normal  Pulled     1s    kubelet            Container image \"registry.dahuatech.com/cncf/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Sep 26 10:46:08.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-1487 describe rc agnhost-primary'
Sep 26 10:46:08.094: INFO: stderr: ""
Sep 26 10:46:08.094: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1487\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.dahuatech.com/cncf/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-c7ljl\n"
Sep 26 10:46:08.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-1487 describe service agnhost-primary'
Sep 26 10:46:08.177: INFO: stderr: ""
Sep 26 10:46:08.177: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1487\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.254.102.27\nIPs:               10.254.102.27\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.50.153:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 26 10:46:08.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-1487 describe node 10.37.21.193'
Sep 26 10:46:08.314: INFO: stderr: ""
Sep 26 10:46:08.314: INFO: stdout: "Name:               10.37.21.193\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    dhc.dahuatech.com/business=\n                    dhc.dahuatech.com/database=\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.37.21.193\n                    kubernetes.io/os=linux\n                    masternum=master1\n                    node-role.kubernetes.io/master=\n                    route-reflector=true\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.37.21.193/22\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.84.64\n                    projectcalico.org/RouteReflectorClusterID: 244.0.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 23 Sep 2021 12:14:28 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.37.21.193\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 26 Sep 2021 10:46:01 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sun, 26 Sep 2021 10:29:38 +0000   Sun, 26 Sep 2021 10:29:38 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sun, 26 Sep 2021 10:43:14 +0000   Thu, 23 Sep 2021 12:14:26 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sun, 26 Sep 2021 10:43:14 +0000   Thu, 23 Sep 2021 12:14:26 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sun, 26 Sep 2021 10:43:14 +0000   Thu, 23 Sep 2021 12:14:26 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sun, 26 Sep 2021 10:43:14 +0000   Sun, 26 Sep 2021 05:42:02 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.37.21.193\n  Hostname:    10.37.21.193\nCapacity:\n  cpu:                32\n  ephemeral-storage:  157207556Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             65965220Ki\n  pods:               110\nAllocatable:\n  cpu:                32\n  ephemeral-storage:  144882483370\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             65862820Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 89d5174f3e64481bb2501eaa208c0731\n  System UUID:                3A074D56-6FDA-FBD9-7E14-214BB34A3038\n  Boot ID:                    b3c58ec4-af01-4748-b2df-c0157e5b6215\n  Kernel Version:             4.14.224\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.5\n  Kubelet Version:            v1.22.2\n  Kube-Proxy Version:         v1.22.2\nPodCIDR:                      192.168.2.0/24\nPodCIDRs:                     192.168.2.0/24\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                     apiserver-10.37.21.193                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n  default                     etcd-10.37.21.193                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n  default                     keepalived-10.37.21.193                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n  default                     kube-controllermanager-10.37.21.193                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n  default                     kube-proxy-10.37.21.193                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n  default                     kube-scheduler-10.37.21.193                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n  kube-system                 calico-node-rv4lt                                          250m (0%)     0 (0%)      0 (0%)           0 (0%)         16m\n  kube-system                 coredns-7474c6f7bf-f6lkf                                   1 (3%)        1 (3%)      128Mi (0%)       2Gi (3%)       16m\n  kube-system                 nginx-10.37.21.193                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n  sonobuoy                    sonobuoy-e2e-job-ffce8e36c7dd478d                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-lp2ds    0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1250m (3%)  1 (3%)\n  memory             128Mi (0%)  2Gi (3%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                    Age   From     Message\n  ----     ------                    ----  ----     -------\n  Normal   Starting                  5h4m  kubelet  Starting kubelet.\n  Warning  CheckLimitsForResolvConf  5h4m  kubelet  Resolv.conf file '/etc/resolv.conf' contains search line consisting of more than 3 domains!\n  Normal   NodeHasSufficientMemory   5h4m  kubelet  Node 10.37.21.193 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure     5h4m  kubelet  Node 10.37.21.193 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID      5h4m  kubelet  Node 10.37.21.193 status is now: NodeHasSufficientPID\n  Normal   NodeNotReady              5h4m  kubelet  Node 10.37.21.193 status is now: NodeNotReady\n  Normal   NodeAllocatableEnforced   5h4m  kubelet  Updated Node Allocatable limit across pods\n  Normal   NodeReady                 5h4m  kubelet  Node 10.37.21.193 status is now: NodeReady\n"
Sep 26 10:46:08.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-1487 describe namespace kubectl-1487'
Sep 26 10:46:08.392: INFO: stderr: ""
Sep 26 10:46:08.392: INFO: stdout: "Name:         kubectl-1487\nLabels:       e2e-framework=kubectl\n              e2e-run=b235255d-02aa-4be7-9984-ea35e3b446e5\n              kubernetes.io/metadata.name=kubectl-1487\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:46:08.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1487" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":54,"skipped":1065,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:46:08.408: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:48:00.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-325" for this suite.

• [SLOW TEST:112.102 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":55,"skipped":1117,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:48:00.510: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 10:48:00.578: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0f12cf8-78da-4718-8321-fb06d71506ea" in namespace "projected-8896" to be "Succeeded or Failed"
Sep 26 10:48:00.581: INFO: Pod "downwardapi-volume-d0f12cf8-78da-4718-8321-fb06d71506ea": Phase="Pending", Reason="", readiness=false. Elapsed: 3.567243ms
Sep 26 10:48:02.594: INFO: Pod "downwardapi-volume-d0f12cf8-78da-4718-8321-fb06d71506ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016425818s
STEP: Saw pod success
Sep 26 10:48:02.594: INFO: Pod "downwardapi-volume-d0f12cf8-78da-4718-8321-fb06d71506ea" satisfied condition "Succeeded or Failed"
Sep 26 10:48:02.599: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-d0f12cf8-78da-4718-8321-fb06d71506ea container client-container: <nil>
STEP: delete the pod
Sep 26 10:48:02.633: INFO: Waiting for pod downwardapi-volume-d0f12cf8-78da-4718-8321-fb06d71506ea to disappear
Sep 26 10:48:02.638: INFO: Pod downwardapi-volume-d0f12cf8-78da-4718-8321-fb06d71506ea no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:48:02.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8896" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":56,"skipped":1121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:48:02.650: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 26 10:48:02.721: INFO: Waiting up to 5m0s for pod "pod-27a5a0fa-8543-4728-aad8-07d4d7da9444" in namespace "emptydir-1423" to be "Succeeded or Failed"
Sep 26 10:48:02.724: INFO: Pod "pod-27a5a0fa-8543-4728-aad8-07d4d7da9444": Phase="Pending", Reason="", readiness=false. Elapsed: 3.641507ms
Sep 26 10:48:04.733: INFO: Pod "pod-27a5a0fa-8543-4728-aad8-07d4d7da9444": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012186804s
STEP: Saw pod success
Sep 26 10:48:04.733: INFO: Pod "pod-27a5a0fa-8543-4728-aad8-07d4d7da9444" satisfied condition "Succeeded or Failed"
Sep 26 10:48:04.738: INFO: Trying to get logs from node 10.37.21.194 pod pod-27a5a0fa-8543-4728-aad8-07d4d7da9444 container test-container: <nil>
STEP: delete the pod
Sep 26 10:48:04.757: INFO: Waiting for pod pod-27a5a0fa-8543-4728-aad8-07d4d7da9444 to disappear
Sep 26 10:48:04.760: INFO: Pod pod-27a5a0fa-8543-4728-aad8-07d4d7da9444 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:48:04.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1423" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":57,"skipped":1150,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:48:04.771: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 10:48:04.825: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:48:12.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4544" for this suite.

• [SLOW TEST:8.224 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":58,"skipped":1172,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:48:12.995: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 10:48:13.055: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88342169-27ec-49a1-95df-1f4f8d5b46f0" in namespace "projected-7560" to be "Succeeded or Failed"
Sep 26 10:48:13.059: INFO: Pod "downwardapi-volume-88342169-27ec-49a1-95df-1f4f8d5b46f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.184655ms
Sep 26 10:48:15.068: INFO: Pod "downwardapi-volume-88342169-27ec-49a1-95df-1f4f8d5b46f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012858772s
STEP: Saw pod success
Sep 26 10:48:15.068: INFO: Pod "downwardapi-volume-88342169-27ec-49a1-95df-1f4f8d5b46f0" satisfied condition "Succeeded or Failed"
Sep 26 10:48:15.070: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-88342169-27ec-49a1-95df-1f4f8d5b46f0 container client-container: <nil>
STEP: delete the pod
Sep 26 10:48:15.087: INFO: Waiting for pod downwardapi-volume-88342169-27ec-49a1-95df-1f4f8d5b46f0 to disappear
Sep 26 10:48:15.090: INFO: Pod downwardapi-volume-88342169-27ec-49a1-95df-1f4f8d5b46f0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:48:15.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7560" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":59,"skipped":1178,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:48:15.100: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 26 10:48:17.178: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:48:17.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5336" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":60,"skipped":1193,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:48:17.202: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 10:48:17.251: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2f04be1-db2f-465b-9029-11e55cbac94a" in namespace "projected-2544" to be "Succeeded or Failed"
Sep 26 10:48:17.255: INFO: Pod "downwardapi-volume-e2f04be1-db2f-465b-9029-11e55cbac94a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.440714ms
Sep 26 10:48:19.266: INFO: Pod "downwardapi-volume-e2f04be1-db2f-465b-9029-11e55cbac94a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014315145s
STEP: Saw pod success
Sep 26 10:48:19.266: INFO: Pod "downwardapi-volume-e2f04be1-db2f-465b-9029-11e55cbac94a" satisfied condition "Succeeded or Failed"
Sep 26 10:48:19.269: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-e2f04be1-db2f-465b-9029-11e55cbac94a container client-container: <nil>
STEP: delete the pod
Sep 26 10:48:19.290: INFO: Waiting for pod downwardapi-volume-e2f04be1-db2f-465b-9029-11e55cbac94a to disappear
Sep 26 10:48:19.293: INFO: Pod downwardapi-volume-e2f04be1-db2f-465b-9029-11e55cbac94a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:48:19.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2544" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":61,"skipped":1221,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:48:19.303: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 26 10:48:19.359: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 26 10:49:19.407: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Sep 26 10:49:19.452: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep 26 10:49:19.458: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep 26 10:49:19.477: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep 26 10:49:19.483: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Sep 26 10:49:19.500: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Sep 26 10:49:19.507: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:49:35.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5924" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:76.317 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":62,"skipped":1239,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:49:35.620: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Sep 26 10:49:35.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-6085 create -f -'
Sep 26 10:49:35.875: INFO: stderr: ""
Sep 26 10:49:35.875: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Sep 26 10:49:35.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-6085 diff -f -'
Sep 26 10:49:36.059: INFO: rc: 1
Sep 26 10:49:36.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-6085 delete -f -'
Sep 26 10:49:36.137: INFO: stderr: ""
Sep 26 10:49:36.137: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:49:36.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6085" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":63,"skipped":1244,"failed":0}
SSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:49:36.155: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 26 10:49:38.228: INFO: &Pod{ObjectMeta:{send-events-0af4698f-ffd8-4131-893c-8170516cc0f9  events-9750  861ebbe3-f80f-4465-8ed6-c844d363573a 533003 0 2021-09-26 10:49:36 +0000 UTC <nil> <nil> map[name:foo time:196761681] map[cni.projectcalico.org/containerID:dc7a0172d62854f082402e12999077ef5168e1b4314bfa61b64d9b68bf4c2ee2 cni.projectcalico.org/podIP:192.168.181.122/32 cni.projectcalico.org/podIPs:192.168.181.122/32] [] []  [{e2e.test Update v1 2021-09-26 10:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 10:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 10:49:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.181.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dkdv9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:registry.dahuatech.com/cncf/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dkdv9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 10:49:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 10:49:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 10:49:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 10:49:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.195,PodIP:192.168.181.122,StartTime:2021-09-26 10:49:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 10:49:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/agnhost:2.32,ImageID:docker-pullable://registry.dahuatech.com/cncf/agnhost@sha256:e76d7fb8b95056e3909026f6024f19c496a908f7471ad3426302c4fb684885f9,ContainerID:docker://a50feacae2eb8a342897f97e240595ba5f55e9768c762161bbd30f374b991824,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.181.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep 26 10:49:40.238: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 26 10:49:42.248: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:49:42.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9750" for this suite.

• [SLOW TEST:6.112 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":64,"skipped":1248,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:49:42.267: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Sep 26 10:49:46.335: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6030 PodName:var-expansion-19f0849a-cf90-41ef-807d-d04cb755df36 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 10:49:46.335: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: test for file in mounted path
Sep 26 10:49:46.470: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6030 PodName:var-expansion-19f0849a-cf90-41ef-807d-d04cb755df36 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 10:49:46.470: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: updating the annotation value
Sep 26 10:49:47.151: INFO: Successfully updated pod "var-expansion-19f0849a-cf90-41ef-807d-d04cb755df36"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Sep 26 10:49:47.155: INFO: Deleting pod "var-expansion-19f0849a-cf90-41ef-807d-d04cb755df36" in namespace "var-expansion-6030"
Sep 26 10:49:47.162: INFO: Wait up to 5m0s for pod "var-expansion-19f0849a-cf90-41ef-807d-d04cb755df36" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:50:19.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6030" for this suite.

• [SLOW TEST:36.930 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":65,"skipped":1254,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:50:19.197: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-369/secret-test-8e4a3880-3457-4378-bae8-a14d3d379497
STEP: Creating a pod to test consume secrets
Sep 26 10:50:19.270: INFO: Waiting up to 5m0s for pod "pod-configmaps-cdce787c-7dcd-4a6c-94ce-09bb1dd26819" in namespace "secrets-369" to be "Succeeded or Failed"
Sep 26 10:50:19.273: INFO: Pod "pod-configmaps-cdce787c-7dcd-4a6c-94ce-09bb1dd26819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.96871ms
Sep 26 10:50:21.284: INFO: Pod "pod-configmaps-cdce787c-7dcd-4a6c-94ce-09bb1dd26819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014223411s
Sep 26 10:50:23.293: INFO: Pod "pod-configmaps-cdce787c-7dcd-4a6c-94ce-09bb1dd26819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022419764s
STEP: Saw pod success
Sep 26 10:50:23.293: INFO: Pod "pod-configmaps-cdce787c-7dcd-4a6c-94ce-09bb1dd26819" satisfied condition "Succeeded or Failed"
Sep 26 10:50:23.297: INFO: Trying to get logs from node 10.37.21.195 pod pod-configmaps-cdce787c-7dcd-4a6c-94ce-09bb1dd26819 container env-test: <nil>
STEP: delete the pod
Sep 26 10:50:23.335: INFO: Waiting for pod pod-configmaps-cdce787c-7dcd-4a6c-94ce-09bb1dd26819 to disappear
Sep 26 10:50:23.339: INFO: Pod pod-configmaps-cdce787c-7dcd-4a6c-94ce-09bb1dd26819 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:50:23.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-369" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":66,"skipped":1275,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:50:23.358: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 10:50:23.444: INFO: The status of Pod busybox-scheduling-4c8c4ae1-296a-4f27-bc81-70c7262d50b5 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:50:25.453: INFO: The status of Pod busybox-scheduling-4c8c4ae1-296a-4f27-bc81-70c7262d50b5 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:50:25.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8603" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":67,"skipped":1284,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:50:25.574: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Sep 26 10:50:25.628: INFO: Waiting up to 5m0s for pod "var-expansion-7d7ee895-8fb5-4335-900e-7ee1edf37541" in namespace "var-expansion-6933" to be "Succeeded or Failed"
Sep 26 10:50:25.632: INFO: Pod "var-expansion-7d7ee895-8fb5-4335-900e-7ee1edf37541": Phase="Pending", Reason="", readiness=false. Elapsed: 3.41208ms
Sep 26 10:50:27.642: INFO: Pod "var-expansion-7d7ee895-8fb5-4335-900e-7ee1edf37541": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013534058s
STEP: Saw pod success
Sep 26 10:50:27.642: INFO: Pod "var-expansion-7d7ee895-8fb5-4335-900e-7ee1edf37541" satisfied condition "Succeeded or Failed"
Sep 26 10:50:27.645: INFO: Trying to get logs from node 10.37.21.195 pod var-expansion-7d7ee895-8fb5-4335-900e-7ee1edf37541 container dapi-container: <nil>
STEP: delete the pod
Sep 26 10:50:27.668: INFO: Waiting for pod var-expansion-7d7ee895-8fb5-4335-900e-7ee1edf37541 to disappear
Sep 26 10:50:27.671: INFO: Pod var-expansion-7d7ee895-8fb5-4335-900e-7ee1edf37541 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:50:27.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6933" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":68,"skipped":1293,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:50:27.679: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 26 10:50:27.739: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 26 10:51:27.781: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:51:27.786: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Sep 26 10:51:31.911: INFO: found a healthy node: 10.37.21.195
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 10:51:46.009: INFO: pods created so far: [1 1 1]
Sep 26 10:51:46.009: INFO: length of pods created so far: 3
Sep 26 10:51:48.026: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:51:55.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-993" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:51:55.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-865" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:87.474 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":69,"skipped":1302,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:51:55.154: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Sep 26 10:51:55.239: INFO: Waiting up to 5m0s for pod "var-expansion-6aac77c5-d444-4064-8445-32fe03ba29b1" in namespace "var-expansion-6467" to be "Succeeded or Failed"
Sep 26 10:51:55.243: INFO: Pod "var-expansion-6aac77c5-d444-4064-8445-32fe03ba29b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.197819ms
Sep 26 10:51:57.250: INFO: Pod "var-expansion-6aac77c5-d444-4064-8445-32fe03ba29b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011420516s
STEP: Saw pod success
Sep 26 10:51:57.250: INFO: Pod "var-expansion-6aac77c5-d444-4064-8445-32fe03ba29b1" satisfied condition "Succeeded or Failed"
Sep 26 10:51:57.253: INFO: Trying to get logs from node 10.37.21.194 pod var-expansion-6aac77c5-d444-4064-8445-32fe03ba29b1 container dapi-container: <nil>
STEP: delete the pod
Sep 26 10:51:57.287: INFO: Waiting for pod var-expansion-6aac77c5-d444-4064-8445-32fe03ba29b1 to disappear
Sep 26 10:51:57.290: INFO: Pod var-expansion-6aac77c5-d444-4064-8445-32fe03ba29b1 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:51:57.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6467" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":70,"skipped":1323,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:51:57.298: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Sep 26 10:51:57.352: INFO: Waiting up to 5m0s for pod "test-pod-00c47fe8-79ff-44ea-a69b-15e79fe61dd9" in namespace "svcaccounts-1720" to be "Succeeded or Failed"
Sep 26 10:51:57.356: INFO: Pod "test-pod-00c47fe8-79ff-44ea-a69b-15e79fe61dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.809451ms
Sep 26 10:51:59.366: INFO: Pod "test-pod-00c47fe8-79ff-44ea-a69b-15e79fe61dd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013946147s
STEP: Saw pod success
Sep 26 10:51:59.366: INFO: Pod "test-pod-00c47fe8-79ff-44ea-a69b-15e79fe61dd9" satisfied condition "Succeeded or Failed"
Sep 26 10:51:59.369: INFO: Trying to get logs from node 10.37.21.194 pod test-pod-00c47fe8-79ff-44ea-a69b-15e79fe61dd9 container agnhost-container: <nil>
STEP: delete the pod
Sep 26 10:51:59.394: INFO: Waiting for pod test-pod-00c47fe8-79ff-44ea-a69b-15e79fe61dd9 to disappear
Sep 26 10:51:59.397: INFO: Pod test-pod-00c47fe8-79ff-44ea-a69b-15e79fe61dd9 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:51:59.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1720" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":71,"skipped":1325,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:51:59.408: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 26 10:51:59.464: INFO: Waiting up to 5m0s for pod "pod-df71a507-fe0d-48c8-8715-7f753ad034ac" in namespace "emptydir-1640" to be "Succeeded or Failed"
Sep 26 10:51:59.467: INFO: Pod "pod-df71a507-fe0d-48c8-8715-7f753ad034ac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.204474ms
Sep 26 10:52:01.476: INFO: Pod "pod-df71a507-fe0d-48c8-8715-7f753ad034ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011484566s
STEP: Saw pod success
Sep 26 10:52:01.476: INFO: Pod "pod-df71a507-fe0d-48c8-8715-7f753ad034ac" satisfied condition "Succeeded or Failed"
Sep 26 10:52:01.479: INFO: Trying to get logs from node 10.37.21.194 pod pod-df71a507-fe0d-48c8-8715-7f753ad034ac container test-container: <nil>
STEP: delete the pod
Sep 26 10:52:01.516: INFO: Waiting for pod pod-df71a507-fe0d-48c8-8715-7f753ad034ac to disappear
Sep 26 10:52:01.520: INFO: Pod pod-df71a507-fe0d-48c8-8715-7f753ad034ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:52:01.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1640" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1330,"failed":0}
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:52:01.538: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Sep 26 10:52:01.584: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:52:05.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6050" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":73,"skipped":1333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:52:05.643: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:52:16.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7660" for this suite.

• [SLOW TEST:11.125 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":74,"skipped":1357,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:52:16.769: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-4752
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 26 10:52:16.834: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 26 10:52:16.870: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 10:52:18.886: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 10:52:20.881: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 10:52:22.882: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 10:52:24.881: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 10:52:26.877: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 10:52:28.880: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 10:52:30.883: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 10:52:32.885: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 10:52:34.886: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 10:52:36.918: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 26 10:52:36.926: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 26 10:52:36.932: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 26 10:52:40.980: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep 26 10:52:40.980: INFO: Going to poll 192.168.84.85 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Sep 26 10:52:40.984: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.84.85:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4752 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 10:52:40.984: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 10:52:41.133: INFO: Found all 1 expected endpoints: [netserver-0]
Sep 26 10:52:41.133: INFO: Going to poll 192.168.50.171 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Sep 26 10:52:41.139: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.50.171:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4752 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 10:52:41.139: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 10:52:41.313: INFO: Found all 1 expected endpoints: [netserver-1]
Sep 26 10:52:41.313: INFO: Going to poll 192.168.181.126 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Sep 26 10:52:41.319: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.181.126:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4752 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 10:52:41.319: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 10:52:41.516: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:52:41.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4752" for this suite.

• [SLOW TEST:24.773 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1358,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:52:41.542: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-812fd767-3652-471b-87bb-732eb22bd43e
STEP: Creating a pod to test consume configMaps
Sep 26 10:52:41.623: INFO: Waiting up to 5m0s for pod "pod-configmaps-863da628-373e-477a-9a1c-c5741529ab4d" in namespace "configmap-2945" to be "Succeeded or Failed"
Sep 26 10:52:41.628: INFO: Pod "pod-configmaps-863da628-373e-477a-9a1c-c5741529ab4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.456311ms
Sep 26 10:52:43.639: INFO: Pod "pod-configmaps-863da628-373e-477a-9a1c-c5741529ab4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015503704s
STEP: Saw pod success
Sep 26 10:52:43.639: INFO: Pod "pod-configmaps-863da628-373e-477a-9a1c-c5741529ab4d" satisfied condition "Succeeded or Failed"
Sep 26 10:52:43.643: INFO: Trying to get logs from node 10.37.21.195 pod pod-configmaps-863da628-373e-477a-9a1c-c5741529ab4d container configmap-volume-test: <nil>
STEP: delete the pod
Sep 26 10:52:43.684: INFO: Waiting for pod pod-configmaps-863da628-373e-477a-9a1c-c5741529ab4d to disappear
Sep 26 10:52:43.687: INFO: Pod pod-configmaps-863da628-373e-477a-9a1c-c5741529ab4d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:52:43.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2945" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":76,"skipped":1373,"failed":0}

------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:52:43.702: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:52:43.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-464" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":77,"skipped":1373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:52:43.808: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 10:52:43.852: INFO: Creating deployment "test-recreate-deployment"
Sep 26 10:52:43.856: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 26 10:52:43.863: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Sep 26 10:52:45.877: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 26 10:52:45.881: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 26 10:52:45.891: INFO: Updating deployment test-recreate-deployment
Sep 26 10:52:45.891: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 26 10:52:45.974: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2891  86800975-d83a-45b6-add7-a8cba611b68c 534115 2 2021-09-26 10:52:43 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-26 10:52:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 10:52:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd registry.dahuatech.com/cncf/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a692e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-09-26 10:52:45 +0000 UTC,LastTransitionTime:2021-09-26 10:52:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-7c94d799f9" is progressing.,LastUpdateTime:2021-09-26 10:52:45 +0000 UTC,LastTransitionTime:2021-09-26 10:52:43 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 26 10:52:45.978: INFO: New ReplicaSet "test-recreate-deployment-7c94d799f9" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-7c94d799f9  deployment-2891  d7f02e09-1d26-45b4-b6d5-94d39dbda9fc 534112 1 2021-09-26 10:52:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7c94d799f9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 86800975-d83a-45b6-add7-a8cba611b68c 0xc000a69b27 0xc000a69b28}] []  [{kube-controller-manager Update apps/v1 2021-09-26 10:52:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86800975-d83a-45b6-add7-a8cba611b68c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 10:52:45 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7c94d799f9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7c94d799f9] map[] [] []  []} {[] [] [{httpd registry.dahuatech.com/cncf/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a69bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 26 10:52:45.978: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 26 10:52:45.978: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-645b77674f  deployment-2891  deb58d41-89e6-4bd8-8273-001f2fdf8678 534104 2 2021-09-26 10:52:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:645b77674f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 86800975-d83a-45b6-add7-a8cba611b68c 0xc000a69997 0xc000a69998}] []  [{kube-controller-manager Update apps/v1 2021-09-26 10:52:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86800975-d83a-45b6-add7-a8cba611b68c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 10:52:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 645b77674f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:645b77674f] map[] [] []  []} {[] [] [{agnhost registry.dahuatech.com/cncf/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a69a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 26 10:52:45.981: INFO: Pod "test-recreate-deployment-7c94d799f9-gznh5" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-7c94d799f9-gznh5 test-recreate-deployment-7c94d799f9- deployment-2891  a9fc635c-7677-4161-9a3e-29a1982b8476 534116 0 2021-09-26 10:52:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7c94d799f9] map[] [{apps/v1 ReplicaSet test-recreate-deployment-7c94d799f9 d7f02e09-1d26-45b4-b6d5-94d39dbda9fc 0xc0008ce0f7 0xc0008ce0f8}] []  [{kube-controller-manager Update v1 2021-09-26 10:52:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7f02e09-1d26-45b4-b6d5-94d39dbda9fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-09-26 10:52:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k7plv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7plv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 10:52:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 10:52:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 10:52:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 10:52:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.195,PodIP:,StartTime:2021-09-26 10:52:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:52:45.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2891" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":78,"skipped":1406,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:52:45.991: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 26 10:52:46.046: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 26 10:52:46.056: INFO: Waiting for terminating namespaces to be deleted...
Sep 26 10:52:46.059: INFO: 
Logging pods the apiserver thinks is on node 10.37.21.193 before test
Sep 26 10:52:46.069: INFO: apiserver-10.37.21.193 from default started at 2021-09-26 10:28:41 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container apiserver ready: true, restart count 1
Sep 26 10:52:46.069: INFO: etcd-10.37.21.193 from default started at 2021-09-26 05:42:03 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container etcd ready: true, restart count 0
Sep 26 10:52:46.069: INFO: keepalived-10.37.21.193 from default started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container keepalived ready: true, restart count 0
Sep 26 10:52:46.069: INFO: kube-controllermanager-10.37.21.193 from default started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container kube-controllermanager ready: true, restart count 2
Sep 26 10:52:46.069: INFO: kube-proxy-10.37.21.193 from default started at 2021-09-23 12:14:20 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 26 10:52:46.069: INFO: kube-scheduler-10.37.21.193 from default started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 26 10:52:46.069: INFO: calico-node-rv4lt from kube-system started at 2021-09-26 10:29:33 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container calico-node ready: true, restart count 0
Sep 26 10:52:46.069: INFO: coredns-7474c6f7bf-f6lkf from kube-system started at 2021-09-26 10:29:48 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container coredns ready: true, restart count 0
Sep 26 10:52:46.069: INFO: nginx-10.37.21.193 from kube-system started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container nginx ready: true, restart count 0
Sep 26 10:52:46.069: INFO: host-test-container-pod from pod-network-test-4752 started at 2021-09-26 10:52:36 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container agnhost-container ready: true, restart count 0
Sep 26 10:52:46.069: INFO: netserver-0 from pod-network-test-4752 started at 2021-09-26 10:52:16 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container webserver ready: true, restart count 0
Sep 26 10:52:46.069: INFO: sonobuoy-e2e-job-ffce8e36c7dd478d from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container e2e ready: true, restart count 0
Sep 26 10:52:46.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 10:52:46.069: INFO: sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-lp2ds from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 10:52:46.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 10:52:46.069: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 26 10:52:46.069: INFO: 
Logging pods the apiserver thinks is on node 10.37.21.194 before test
Sep 26 10:52:46.079: INFO: apiserver-10.37.21.194 from default started at 2021-09-26 10:28:51 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.079: INFO: 	Container apiserver ready: true, restart count 0
Sep 26 10:52:46.079: INFO: etcd-10.37.21.194 from default started at 2021-09-23 12:14:21 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.079: INFO: 	Container etcd ready: true, restart count 0
Sep 26 10:52:46.079: INFO: keepalived-10.37.21.194 from default started at 2021-09-23 12:14:21 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.079: INFO: 	Container keepalived ready: true, restart count 0
Sep 26 10:52:46.079: INFO: kube-controllermanager-10.37.21.194 from default started at 2021-09-26 05:42:32 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.079: INFO: 	Container kube-controllermanager ready: true, restart count 2
Sep 26 10:52:46.079: INFO: kube-proxy-10.37.21.194 from default started at 2021-09-26 05:42:32 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.079: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 26 10:52:46.079: INFO: kube-scheduler-10.37.21.194 from default started at 2021-09-26 05:42:32 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.079: INFO: 	Container kube-scheduler ready: true, restart count 1
Sep 26 10:52:46.079: INFO: calico-node-dd6nc from kube-system started at 2021-09-26 10:29:33 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.079: INFO: 	Container calico-node ready: true, restart count 0
Sep 26 10:52:46.079: INFO: coredns-7474c6f7bf-kkpwf from kube-system started at 2021-09-26 10:29:53 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.079: INFO: 	Container coredns ready: true, restart count 0
Sep 26 10:52:46.079: INFO: nginx-10.37.21.194 from kube-system started at 2021-09-23 12:14:21 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.079: INFO: 	Container nginx ready: true, restart count 0
Sep 26 10:52:46.079: INFO: netserver-1 from pod-network-test-4752 started at 2021-09-26 10:52:16 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.079: INFO: 	Container webserver ready: true, restart count 0
Sep 26 10:52:46.079: INFO: test-container-pod from pod-network-test-4752 started at 2021-09-26 10:52:36 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.079: INFO: 	Container webserver ready: true, restart count 0
Sep 26 10:52:46.079: INFO: sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-46csd from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 10:52:46.079: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 10:52:46.079: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 26 10:52:46.079: INFO: 
Logging pods the apiserver thinks is on node 10.37.21.195 before test
Sep 26 10:52:46.088: INFO: apiserver-10.37.21.195 from default started at 2021-09-26 10:31:18 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container apiserver ready: true, restart count 0
Sep 26 10:52:46.089: INFO: etcd-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container etcd ready: true, restart count 1
Sep 26 10:52:46.089: INFO: keepalived-10.37.21.195 from default started at 2021-09-23 12:14:22 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container keepalived ready: true, restart count 1
Sep 26 10:52:46.089: INFO: kube-controllermanager-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container kube-controllermanager ready: true, restart count 4
Sep 26 10:52:46.089: INFO: kube-proxy-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container kube-proxy ready: true, restart count 2
Sep 26 10:52:46.089: INFO: kube-scheduler-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container kube-scheduler ready: true, restart count 1
Sep 26 10:52:46.089: INFO: test-recreate-deployment-7c94d799f9-gznh5 from deployment-2891 started at 2021-09-26 10:52:45 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container httpd ready: false, restart count 0
Sep 26 10:52:46.089: INFO: calico-node-fhzl7 from kube-system started at 2021-09-26 10:29:33 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container calico-node ready: true, restart count 0
Sep 26 10:52:46.089: INFO: coredns-7474c6f7bf-plgdt from kube-system started at 2021-09-26 10:29:53 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container coredns ready: true, restart count 0
Sep 26 10:52:46.089: INFO: nginx-10.37.21.195 from kube-system started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container nginx ready: true, restart count 1
Sep 26 10:52:46.089: INFO: netserver-2 from pod-network-test-4752 started at 2021-09-26 10:52:16 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container webserver ready: true, restart count 0
Sep 26 10:52:46.089: INFO: sonobuoy from sonobuoy started at 2021-09-26 10:33:22 +0000 UTC (1 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 26 10:52:46.089: INFO: sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-76l7m from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 10:52:46.089: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 10:52:46.089: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4d472ef5-b7c6-4f29-9d05-ca936e6cc85b 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.37.21.194 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-4d472ef5-b7c6-4f29-9d05-ca936e6cc85b off the node 10.37.21.194
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4d472ef5-b7c6-4f29-9d05-ca936e6cc85b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:57:50.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7472" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.337 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":79,"skipped":1418,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:57:50.328: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:58:18.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-400" for this suite.

• [SLOW TEST:28.195 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":80,"skipped":1425,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:58:18.524: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 26 10:58:18.601: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6656  df95f74f-d533-49c4-acf6-7b857e235e03 534846 0 2021-09-26 10:58:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-26 10:58:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 10:58:18.601: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6656  df95f74f-d533-49c4-acf6-7b857e235e03 534847 0 2021-09-26 10:58:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-26 10:58:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 26 10:58:18.615: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6656  df95f74f-d533-49c4-acf6-7b857e235e03 534848 0 2021-09-26 10:58:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-26 10:58:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 10:58:18.615: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6656  df95f74f-d533-49c4-acf6-7b857e235e03 534849 0 2021-09-26 10:58:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-26 10:58:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 10:58:18.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6656" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":81,"skipped":1437,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 10:58:18.627: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-3cf6c2aa-0e86-425d-96ee-7ce0dce02d84 in namespace container-probe-1243
Sep 26 10:58:22.700: INFO: Started pod test-webserver-3cf6c2aa-0e86-425d-96ee-7ce0dce02d84 in namespace container-probe-1243
STEP: checking the pod's current state and verifying that restartCount is present
Sep 26 10:58:22.712: INFO: Initial restart count of pod test-webserver-3cf6c2aa-0e86-425d-96ee-7ce0dce02d84 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:02:23.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1243" for this suite.

• [SLOW TEST:244.602 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":82,"skipped":1542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:02:23.229: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:02:50.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6477" for this suite.

• [SLOW TEST:27.532 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1568,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:02:50.762: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:02:54.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7649" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1576,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:02:54.843: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 11:02:54.920: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85b26bb0-b893-4c7f-8fe0-4c3a0060cbaa" in namespace "downward-api-2630" to be "Succeeded or Failed"
Sep 26 11:02:54.924: INFO: Pod "downwardapi-volume-85b26bb0-b893-4c7f-8fe0-4c3a0060cbaa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.791503ms
Sep 26 11:02:56.932: INFO: Pod "downwardapi-volume-85b26bb0-b893-4c7f-8fe0-4c3a0060cbaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011769713s
Sep 26 11:02:58.944: INFO: Pod "downwardapi-volume-85b26bb0-b893-4c7f-8fe0-4c3a0060cbaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023658382s
STEP: Saw pod success
Sep 26 11:02:58.944: INFO: Pod "downwardapi-volume-85b26bb0-b893-4c7f-8fe0-4c3a0060cbaa" satisfied condition "Succeeded or Failed"
Sep 26 11:02:58.948: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-85b26bb0-b893-4c7f-8fe0-4c3a0060cbaa container client-container: <nil>
STEP: delete the pod
Sep 26 11:02:58.988: INFO: Waiting for pod downwardapi-volume-85b26bb0-b893-4c7f-8fe0-4c3a0060cbaa to disappear
Sep 26 11:02:58.991: INFO: Pod downwardapi-volume-85b26bb0-b893-4c7f-8fe0-4c3a0060cbaa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:02:58.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2630" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":85,"skipped":1576,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:02:59.004: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:02:59.772: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:03:02.807: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:03:02.815: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:03:11.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1468" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:12.083 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":86,"skipped":1605,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:03:11.087: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:03:11.389: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:03:14.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:03:14.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7997" for this suite.
STEP: Destroying namespace "webhook-7997-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":87,"skipped":1609,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:03:14.561: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-5951
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-5951
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5951
Sep 26 11:03:14.616: INFO: Found 0 stateful pods, waiting for 1
Sep 26 11:03:24.625: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 26 11:03:24.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-5951 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 26 11:03:24.929: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 26 11:03:24.929: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 26 11:03:24.929: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 26 11:03:24.935: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 26 11:03:34.946: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 26 11:03:34.946: INFO: Waiting for statefulset status.replicas updated to 0
Sep 26 11:03:34.962: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 26 11:03:34.962: INFO: ss-0  10.37.21.195  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:14 +0000 UTC  }]
Sep 26 11:03:34.962: INFO: 
Sep 26 11:03:34.962: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 26 11:03:35.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99518676s
Sep 26 11:03:36.982: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985322454s
Sep 26 11:03:37.992: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975990018s
Sep 26 11:03:39.005: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.966005005s
Sep 26 11:03:40.016: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.952649738s
Sep 26 11:03:41.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.942222692s
Sep 26 11:03:42.037: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.933862232s
Sep 26 11:03:43.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.921300519s
Sep 26 11:03:44.050: INFO: Verifying statefulset ss doesn't scale past 3 for another 913.834367ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5951
Sep 26 11:03:45.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-5951 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 26 11:03:45.303: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 26 11:03:45.303: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 26 11:03:45.303: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 26 11:03:45.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-5951 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 26 11:03:45.515: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 26 11:03:45.515: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 26 11:03:45.515: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 26 11:03:45.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-5951 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 26 11:03:45.748: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 26 11:03:45.748: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 26 11:03:45.748: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 26 11:03:45.759: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 26 11:03:45.759: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 26 11:03:45.759: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 26 11:03:45.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-5951 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 26 11:03:45.998: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 26 11:03:45.998: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 26 11:03:45.998: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 26 11:03:45.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-5951 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 26 11:03:46.214: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 26 11:03:46.215: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 26 11:03:46.215: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 26 11:03:46.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-5951 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 26 11:03:46.438: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 26 11:03:46.438: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 26 11:03:46.438: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 26 11:03:46.438: INFO: Waiting for statefulset status.replicas updated to 0
Sep 26 11:03:46.443: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep 26 11:03:56.459: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 26 11:03:56.459: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 26 11:03:56.459: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 26 11:03:56.479: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 26 11:03:56.479: INFO: ss-0  10.37.21.195  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:14 +0000 UTC  }]
Sep 26 11:03:56.479: INFO: ss-1  10.37.21.194  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:35 +0000 UTC  }]
Sep 26 11:03:56.479: INFO: ss-2  10.37.21.194  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:35 +0000 UTC  }]
Sep 26 11:03:56.479: INFO: 
Sep 26 11:03:56.479: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 26 11:03:57.493: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 26 11:03:57.493: INFO: ss-1  10.37.21.194  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:35 +0000 UTC  }]
Sep 26 11:03:57.493: INFO: ss-2  10.37.21.194  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:03:35 +0000 UTC  }]
Sep 26 11:03:57.493: INFO: 
Sep 26 11:03:57.493: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 26 11:03:58.505: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.982053371s
Sep 26 11:03:59.517: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.970161041s
Sep 26 11:04:00.525: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.95780083s
Sep 26 11:04:01.533: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.949142759s
Sep 26 11:04:02.545: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.942052975s
Sep 26 11:04:03.552: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.930293678s
Sep 26 11:04:04.559: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.923378944s
Sep 26 11:04:05.570: INFO: Verifying statefulset ss doesn't scale past 0 for another 916.080103ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5951
Sep 26 11:04:06.582: INFO: Scaling statefulset ss to 0
Sep 26 11:04:06.600: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Sep 26 11:04:06.603: INFO: Deleting all statefulset in ns statefulset-5951
Sep 26 11:04:06.606: INFO: Scaling statefulset ss to 0
Sep 26 11:04:06.616: INFO: Waiting for statefulset status.replicas updated to 0
Sep 26 11:04:06.619: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:04:06.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5951" for this suite.

• [SLOW TEST:52.081 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":88,"skipped":1612,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:04:06.643: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:04:07.282: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:04:10.307: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:04:10.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-727" for this suite.
STEP: Destroying namespace "webhook-727-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":89,"skipped":1640,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:04:10.436: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:04:10.486: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 26 11:04:18.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-4521 --namespace=crd-publish-openapi-4521 create -f -'
Sep 26 11:04:19.237: INFO: stderr: ""
Sep 26 11:04:19.237: INFO: stdout: "e2e-test-crd-publish-openapi-1564-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 26 11:04:19.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-4521 --namespace=crd-publish-openapi-4521 delete e2e-test-crd-publish-openapi-1564-crds test-cr'
Sep 26 11:04:19.357: INFO: stderr: ""
Sep 26 11:04:19.357: INFO: stdout: "e2e-test-crd-publish-openapi-1564-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 26 11:04:19.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-4521 --namespace=crd-publish-openapi-4521 apply -f -'
Sep 26 11:04:19.537: INFO: stderr: ""
Sep 26 11:04:19.537: INFO: stdout: "e2e-test-crd-publish-openapi-1564-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 26 11:04:19.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-4521 --namespace=crd-publish-openapi-4521 delete e2e-test-crd-publish-openapi-1564-crds test-cr'
Sep 26 11:04:19.620: INFO: stderr: ""
Sep 26 11:04:19.620: INFO: stdout: "e2e-test-crd-publish-openapi-1564-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep 26 11:04:19.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-4521 explain e2e-test-crd-publish-openapi-1564-crds'
Sep 26 11:04:19.790: INFO: stderr: ""
Sep 26 11:04:19.790: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1564-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:04:23.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4521" for this suite.

• [SLOW TEST:12.891 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":90,"skipped":1647,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:04:23.327: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:04:23.405: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:05:24.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1575" for this suite.

• [SLOW TEST:61.471 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":91,"skipped":1647,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:05:24.799: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Sep 26 11:05:24.893: INFO: Waiting up to 5m0s for pod "downward-api-b505d6d9-b3c8-49d0-8194-30a8f8944e84" in namespace "downward-api-1614" to be "Succeeded or Failed"
Sep 26 11:05:24.896: INFO: Pod "downward-api-b505d6d9-b3c8-49d0-8194-30a8f8944e84": Phase="Pending", Reason="", readiness=false. Elapsed: 3.740969ms
Sep 26 11:05:26.904: INFO: Pod "downward-api-b505d6d9-b3c8-49d0-8194-30a8f8944e84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011625992s
STEP: Saw pod success
Sep 26 11:05:26.904: INFO: Pod "downward-api-b505d6d9-b3c8-49d0-8194-30a8f8944e84" satisfied condition "Succeeded or Failed"
Sep 26 11:05:26.908: INFO: Trying to get logs from node 10.37.21.194 pod downward-api-b505d6d9-b3c8-49d0-8194-30a8f8944e84 container dapi-container: <nil>
STEP: delete the pod
Sep 26 11:05:26.936: INFO: Waiting for pod downward-api-b505d6d9-b3c8-49d0-8194-30a8f8944e84 to disappear
Sep 26 11:05:26.940: INFO: Pod downward-api-b505d6d9-b3c8-49d0-8194-30a8f8944e84 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:05:26.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1614" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":92,"skipped":1663,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:05:26.950: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:05:27.623: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 26 11:05:29.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768251127, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768251127, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768251127, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768251127, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-657f8c86c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:05:32.676: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:05:32.686: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5553-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:05:40.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8136" for this suite.
STEP: Destroying namespace "webhook-8136-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.921 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":93,"skipped":1688,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:05:40.871: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:05:40.928: INFO: Creating deployment "webserver-deployment"
Sep 26 11:05:40.934: INFO: Waiting for observed generation 1
Sep 26 11:05:42.949: INFO: Waiting for all required pods to come up
Sep 26 11:05:42.953: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 26 11:05:44.968: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 26 11:05:44.973: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 26 11:05:44.980: INFO: Updating deployment webserver-deployment
Sep 26 11:05:44.980: INFO: Waiting for observed generation 2
Sep 26 11:05:46.988: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 26 11:05:46.991: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 26 11:05:46.993: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 26 11:05:47.001: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 26 11:05:47.001: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 26 11:05:47.005: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 26 11:05:47.011: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 26 11:05:47.011: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 26 11:05:47.022: INFO: Updating deployment webserver-deployment
Sep 26 11:05:47.022: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 26 11:05:47.031: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 26 11:05:47.035: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 26 11:05:47.045: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5017  bce85582-4ce4-4318-8794-3ed2948a6534 536710 3 2021-09-26 11:05:40 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-26 11:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 11:05:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047b4bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-26 11:05:43 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-09-26 11:05:45 +0000 UTC,LastTransitionTime:2021-09-26 11:05:40 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep 26 11:05:47.054: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-5017  618eecf5-e429-4d75-a08c-f8edb7c36395 536714 3 2021-09-26 11:05:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment bce85582-4ce4-4318-8794-3ed2948a6534 0xc004729c07 0xc004729c08}] []  [{kube-controller-manager Update apps/v1 2021-09-26 11:05:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bce85582-4ce4-4318-8794-3ed2948a6534\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 11:05:45 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004729cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 26 11:05:47.054: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 26 11:05:47.054: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7485cd56c  deployment-5017  8bba8a6d-21b5-4a59-bea4-87ab99231a32 536712 3 2021-09-26 11:05:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment bce85582-4ce4-4318-8794-3ed2948a6534 0xc004729af7 0xc004729af8}] []  [{kube-controller-manager Update apps/v1 2021-09-26 11:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bce85582-4ce4-4318-8794-3ed2948a6534\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 11:05:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7485cd56c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[] [] []  []} {[] [] [{httpd registry.dahuatech.com/cncf/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004729b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep 26 11:05:47.071: INFO: Pod "webserver-deployment-7485cd56c-98s55" is available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-98s55 webserver-deployment-7485cd56c- deployment-5017  1a3c1ab1-8450-4d9c-8c64-861769176a4b 536574 0 2021-09-26 11:05:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[cni.projectcalico.org/containerID:7528fab053bd176aa3e595fa4296b17ea0e59c6bf5f8f9d940672894117ea1fe cni.projectcalico.org/podIP:192.168.181.81/32 cni.projectcalico.org/podIPs:192.168.181.81/32] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0047d0467 0xc0047d0468}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:05:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:05:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.181.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-727pq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-727pq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.195,PodIP:192.168.181.81,StartTime:2021-09-26 11:05:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 11:05:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://adffb9d4a72e5f0012ac83cd3223573d195560336710abc3cba80e074d714c88,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.181.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.072: INFO: Pod "webserver-deployment-7485cd56c-bslgw" is available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-bslgw webserver-deployment-7485cd56c- deployment-5017  2164eca0-f8ae-45e8-8473-09a77555fb21 536589 0 2021-09-26 11:05:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[cni.projectcalico.org/containerID:d455e8851b5bff786066c8eafa5ca600afb3179e1197c1b172feeaa2a50bcf35 cni.projectcalico.org/podIP:192.168.50.187/32 cni.projectcalico.org/podIPs:192.168.50.187/32] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0047d0817 0xc0047d0818}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:05:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:05:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.50.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vlxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vlxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.194,PodIP:192.168.50.187,StartTime:2021-09-26 11:05:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 11:05:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://5cf93d0bca055afd117dbb9103ada057222209d1b604cb80ac80d8120a083809,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.50.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.072: INFO: Pod "webserver-deployment-7485cd56c-cgvvf" is not available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-cgvvf webserver-deployment-7485cd56c- deployment-5017  0b16cfa3-942f-4e5a-a60c-c641349d535f 536729 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0047d0b17 0xc0047d0b18}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78n6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78n6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.072: INFO: Pod "webserver-deployment-7485cd56c-csss2" is not available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-csss2 webserver-deployment-7485cd56c- deployment-5017  bfd2961b-345e-4204-9680-e83721c9d802 536727 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0047d0d70 0xc0047d0d71}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r4r76,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4r76,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.073: INFO: Pod "webserver-deployment-7485cd56c-f9x62" is not available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-f9x62 webserver-deployment-7485cd56c- deployment-5017  941c3f64-fef2-47ff-8baf-7daf00e2f4f1 536720 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0047d0f20 0xc0047d0f21}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mk8bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mk8bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.073: INFO: Pod "webserver-deployment-7485cd56c-fl2bk" is available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-fl2bk webserver-deployment-7485cd56c- deployment-5017  365a4e85-9628-4ad1-88c5-85dd66085643 536584 0 2021-09-26 11:05:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[cni.projectcalico.org/containerID:31a64e8c6bcff737ee95d5e1af3dd2b058895b015f9db2a218b83b2a982ae734 cni.projectcalico.org/podIP:192.168.50.181/32 cni.projectcalico.org/podIPs:192.168.50.181/32] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0047d1140 0xc0047d1141}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:05:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:05:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.50.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pl78q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pl78q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.194,PodIP:192.168.50.181,StartTime:2021-09-26 11:05:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 11:05:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://580a4b9baf650c66dd2f5f1515e1071fb46c70390e39bb8daaf9e50244acf67b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.50.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.074: INFO: Pod "webserver-deployment-7485cd56c-fvr5k" is not available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-fvr5k webserver-deployment-7485cd56c- deployment-5017  5411ce04-7b99-49db-b0ea-8c4be9048b36 536726 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0047d1417 0xc0047d1418}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t42c6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t42c6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.074: INFO: Pod "webserver-deployment-7485cd56c-h5kct" is not available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-h5kct webserver-deployment-7485cd56c- deployment-5017  26db98f1-b480-40c1-a802-7a19ae264fc4 536730 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0047d1580 0xc0047d1581}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fb7m8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fb7m8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.075: INFO: Pod "webserver-deployment-7485cd56c-lb5d4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-lb5d4 webserver-deployment-7485cd56c- deployment-5017  b545e8bd-275d-48ea-809c-8c4864c05f56 536731 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0047d16e0 0xc0047d16e1}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nnxzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nnxzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.075: INFO: Pod "webserver-deployment-7485cd56c-md5jk" is available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-md5jk webserver-deployment-7485cd56c- deployment-5017  22e475bf-cfe1-4d29-95bc-ad4972fac26a 536566 0 2021-09-26 11:05:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[cni.projectcalico.org/containerID:f8727eb2bce732ae8b3fd9f3e5a6be7f4d3d21d42df09c6290358504e760f881 cni.projectcalico.org/podIP:192.168.84.86/32 cni.projectcalico.org/podIPs:192.168.84.86/32] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0047d1890 0xc0047d1891}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:05:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:05:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.84.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4b8bj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4b8bj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.193,PodIP:192.168.84.86,StartTime:2021-09-26 11:05:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 11:05:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://b8fe042ddcb548fe5983dbe5a8ad2d350394ccb6c23c50e559a4f812bea94ded,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.84.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.075: INFO: Pod "webserver-deployment-7485cd56c-q78d5" is available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-q78d5 webserver-deployment-7485cd56c- deployment-5017  a7d87037-b5f2-4e22-8baf-d2a720cf6f77 536572 0 2021-09-26 11:05:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[cni.projectcalico.org/containerID:33c7d4a5b48fd31273524644e8b8056c567224ca4221347b07661a155cf7022e cni.projectcalico.org/podIP:192.168.181.76/32 cni.projectcalico.org/podIPs:192.168.181.76/32] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0047d1b67 0xc0047d1b68}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:05:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:05:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.181.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6b94,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6b94,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.195,PodIP:192.168.181.76,StartTime:2021-09-26 11:05:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 11:05:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://8ece910a8eae70a6dbb99af1bfc5360321355f16bc8f5ecbb79873dbac73c43a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.181.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.076: INFO: Pod "webserver-deployment-7485cd56c-rj5p9" is not available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-rj5p9 webserver-deployment-7485cd56c- deployment-5017  a2c8c5ed-c53a-4685-ab3e-13e960f62263 536728 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0047d1e67 0xc0047d1e68}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8qxrt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8qxrt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.076: INFO: Pod "webserver-deployment-7485cd56c-rq258" is available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-rq258 webserver-deployment-7485cd56c- deployment-5017  fc9a639c-2271-4f08-9764-8c06eb333e3a 536563 0 2021-09-26 11:05:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[cni.projectcalico.org/containerID:fd319b13a70abef8f9897db62e866813160ca389d1e482784c186e1ad3f37f68 cni.projectcalico.org/podIP:192.168.84.91/32 cni.projectcalico.org/podIPs:192.168.84.91/32] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0048080a0 0xc0048080a1}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:05:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:05:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.84.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2q5df,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2q5df,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.193,PodIP:192.168.84.91,StartTime:2021-09-26 11:05:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 11:05:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://b8fe6cf687d0a316f2ab55568048bb50f14e41141bb3483f58086b17972c7c19,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.84.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.076: INFO: Pod "webserver-deployment-7485cd56c-tgl6p" is available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-tgl6p webserver-deployment-7485cd56c- deployment-5017  a330c6aa-dbe1-40b3-b6e6-fdda774917d6 536586 0 2021-09-26 11:05:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[cni.projectcalico.org/containerID:be61c81b25750feb083a69d67d939692fe6765dfe8795f9ecaf1f302811b1df9 cni.projectcalico.org/podIP:192.168.50.161/32 cni.projectcalico.org/podIPs:192.168.50.161/32] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0048083a7 0xc0048083a8}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:05:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:05:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.50.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhzp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhzp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.194,PodIP:192.168.50.161,StartTime:2021-09-26 11:05:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 11:05:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://43ca8e0a1b2ff9a621875865713263740a20b852c5c18de8afef84f58d124233,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.50.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.077: INFO: Pod "webserver-deployment-7485cd56c-xzxp9" is available:
&Pod{ObjectMeta:{webserver-deployment-7485cd56c-xzxp9 webserver-deployment-7485cd56c- deployment-5017  ab4f66bd-77c7-4b0e-b6c1-36509f76aece 536568 0 2021-09-26 11:05:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[cni.projectcalico.org/containerID:1753b99628236adfa78aa7ffd6ed88eb45543920dc5bef4d93aa8bb6fd4d36a7 cni.projectcalico.org/podIP:192.168.84.67/32 cni.projectcalico.org/podIPs:192.168.84.67/32] [{apps/v1 ReplicaSet webserver-deployment-7485cd56c 8bba8a6d-21b5-4a59-bea4-87ab99231a32 0xc0048086f7 0xc0048086f8}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bba8a6d-21b5-4a59-bea4-87ab99231a32\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:05:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:05:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.84.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8vw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8vw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.193,PodIP:192.168.84.67,StartTime:2021-09-26 11:05:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 11:05:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://4d692ec51ad37cf7614d91b18f6e3e1b44d5c9fc1c7a4f6ccc6f6ff7b652458e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.84.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.077: INFO: Pod "webserver-deployment-795d758f88-6vrpj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6vrpj webserver-deployment-795d758f88- deployment-5017  114cfda3-a90f-4655-8ca6-f520a2f8ce69 536738 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 618eecf5-e429-4d75-a08c-f8edb7c36395 0xc004808967 0xc004808968}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"618eecf5-e429-4d75-a08c-f8edb7c36395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gxhh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gxhh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.078: INFO: Pod "webserver-deployment-795d758f88-7bj7k" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7bj7k webserver-deployment-795d758f88- deployment-5017  03142b51-f555-4b71-a186-d6c3b93dc9f1 536698 0 2021-09-26 11:05:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:1a4453cd8ef97ba785dbf3937149c5fe37d9cf895969207e7291cdcc2a46177a cni.projectcalico.org/podIP:192.168.50.191/32 cni.projectcalico.org/podIPs:192.168.50.191/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 618eecf5-e429-4d75-a08c-f8edb7c36395 0xc004808b50 0xc004808b51}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"618eecf5-e429-4d75-a08c-f8edb7c36395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-09-26 11:05:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-09-26 11:05:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdnxc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdnxc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.194,PodIP:,StartTime:2021-09-26 11:05:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.078: INFO: Pod "webserver-deployment-795d758f88-dbsgw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-dbsgw webserver-deployment-795d758f88- deployment-5017  a7df8a31-5c5d-45c8-acac-a4e51f551efd 536725 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 618eecf5-e429-4d75-a08c-f8edb7c36395 0xc004808de7 0xc004808de8}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"618eecf5-e429-4d75-a08c-f8edb7c36395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n2skv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n2skv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.078: INFO: Pod "webserver-deployment-795d758f88-dm2nt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-dm2nt webserver-deployment-795d758f88- deployment-5017  24aecf8f-fb58-4dea-b0f9-bffe18627dbf 536736 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 618eecf5-e429-4d75-a08c-f8edb7c36395 0xc004809060 0xc004809061}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"618eecf5-e429-4d75-a08c-f8edb7c36395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x57mc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x57mc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.078: INFO: Pod "webserver-deployment-795d758f88-hk8lw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hk8lw webserver-deployment-795d758f88- deployment-5017  dcbb1eb5-0cec-4370-8a49-39e6cacee2e1 536737 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 618eecf5-e429-4d75-a08c-f8edb7c36395 0xc0048091c0 0xc0048091c1}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"618eecf5-e429-4d75-a08c-f8edb7c36395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vxf9s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vxf9s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.079: INFO: Pod "webserver-deployment-795d758f88-j45gt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-j45gt webserver-deployment-795d758f88- deployment-5017  b4081144-9995-4c1f-9b44-f25c442b6453 536708 0 2021-09-26 11:05:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:24384983c385bc6aa852d5d5e0a03a84262ef9cda91ded41b85acb2e659fabbe cni.projectcalico.org/podIP:192.168.181.77/32 cni.projectcalico.org/podIPs:192.168.181.77/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 618eecf5-e429-4d75-a08c-f8edb7c36395 0xc004809420 0xc004809421}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"618eecf5-e429-4d75-a08c-f8edb7c36395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:05:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.181.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g7nsf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g7nsf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.195,PodIP:192.168.181.77,StartTime:2021-09-26 11:05:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 10.254.0.2:53: server misbehaving,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.181.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.079: INFO: Pod "webserver-deployment-795d758f88-l5rfc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-l5rfc webserver-deployment-795d758f88- deployment-5017  87d06486-65e5-4204-87f2-fff3811a30e9 536723 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 618eecf5-e429-4d75-a08c-f8edb7c36395 0xc004809827 0xc004809828}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"618eecf5-e429-4d75-a08c-f8edb7c36395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-25hqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-25hqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.079: INFO: Pod "webserver-deployment-795d758f88-pkjbl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pkjbl webserver-deployment-795d758f88- deployment-5017  378e6ac7-bd66-4dde-a0db-6bd69cbdd995 536735 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 618eecf5-e429-4d75-a08c-f8edb7c36395 0xc004809ae0 0xc004809ae1}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"618eecf5-e429-4d75-a08c-f8edb7c36395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6r8mj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6r8mj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.079: INFO: Pod "webserver-deployment-795d758f88-tgkc2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tgkc2 webserver-deployment-795d758f88- deployment-5017  53f2bcbf-e1c2-4b36-8f46-0c4b39c482fa 536724 0 2021-09-26 11:05:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 618eecf5-e429-4d75-a08c-f8edb7c36395 0xc004809d30 0xc004809d31}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"618eecf5-e429-4d75-a08c-f8edb7c36395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwpg7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwpg7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.080: INFO: Pod "webserver-deployment-795d758f88-wb6xw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wb6xw webserver-deployment-795d758f88- deployment-5017  59cca005-c308-48b5-9c04-5699f2d70641 536705 0 2021-09-26 11:05:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:4778a29dcb8edf71a22783f950069113876df69ad11fa4277cd36bc4c321d3e5 cni.projectcalico.org/podIP:192.168.181.75/32 cni.projectcalico.org/podIPs:192.168.181.75/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 618eecf5-e429-4d75-a08c-f8edb7c36395 0xc004809ea0 0xc004809ea1}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"618eecf5-e429-4d75-a08c-f8edb7c36395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:05:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:05:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.181.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bl5ph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bl5ph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.195,PodIP:192.168.181.75,StartTime:2021-09-26 11:05:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 10.254.0.2:53: server misbehaving,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.181.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.080: INFO: Pod "webserver-deployment-795d758f88-wpxtx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wpxtx webserver-deployment-795d758f88- deployment-5017  204ac84f-57ab-4e97-9e22-c8f8984b4e07 536674 0 2021-09-26 11:05:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:2a6d06229cd49f11dfb84a8d4f20349e27b03a6be215e34573358ff4bb0c77c9 cni.projectcalico.org/podIP:192.168.50.131/32 cni.projectcalico.org/podIPs:192.168.50.131/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 618eecf5-e429-4d75-a08c-f8edb7c36395 0xc00483a1d7 0xc00483a1d8}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"618eecf5-e429-4d75-a08c-f8edb7c36395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:05:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:05:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.50.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-npm59,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-npm59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.194,PodIP:192.168.50.131,StartTime:2021-09-26 11:05:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 10.254.0.2:53: server misbehaving,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.50.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 11:05:47.080: INFO: Pod "webserver-deployment-795d758f88-xr9pj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xr9pj webserver-deployment-795d758f88- deployment-5017  eadef7c7-4e33-4fce-8e91-e9eba94dc25b 536681 0 2021-09-26 11:05:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:20bf8fee47953605136fd0dab544233b2ac50116aad631aded4f80ba165b81e6 cni.projectcalico.org/podIP:192.168.50.132/32 cni.projectcalico.org/podIPs:192.168.50.132/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 618eecf5-e429-4d75-a08c-f8edb7c36395 0xc00483a607 0xc00483a608}] []  [{kube-controller-manager Update v1 2021-09-26 11:05:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"618eecf5-e429-4d75-a08c-f8edb7c36395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-09-26 11:05:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-09-26 11:05:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttrkl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttrkl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:05:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.194,PodIP:,StartTime:2021-09-26 11:05:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:05:47.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5017" for this suite.

• [SLOW TEST:6.239 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":94,"skipped":1689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:05:47.110: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3310
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3310
STEP: creating replication controller externalsvc in namespace services-3310
I0926 11:05:47.210616      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3310, replica count: 2
I0926 11:05:50.261324      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0926 11:05:53.263535      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0926 11:05:56.264444      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0926 11:05:59.265993      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep 26 11:05:59.306: INFO: Creating new exec pod
Sep 26 11:06:01.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-3310 exec execpodg9gnc -- /bin/sh -x -c nslookup clusterip-service.services-3310.svc.cluster.local'
Sep 26 11:06:01.588: INFO: stderr: "+ nslookup clusterip-service.services-3310.svc.cluster.local\n"
Sep 26 11:06:01.588: INFO: stdout: "Server:\t\t10.254.0.2\nAddress:\t10.254.0.2#53\n\nclusterip-service.services-3310.svc.cluster.local\tcanonical name = externalsvc.services-3310.svc.cluster.local.\nName:\texternalsvc.services-3310.svc.cluster.local\nAddress: 10.254.79.198\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3310, will wait for the garbage collector to delete the pods
Sep 26 11:06:01.660: INFO: Deleting ReplicationController externalsvc took: 12.846077ms
Sep 26 11:06:01.762: INFO: Terminating ReplicationController externalsvc pods took: 101.200398ms
Sep 26 11:06:04.087: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:06:04.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3310" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:17.004 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":95,"skipped":1723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:06:04.115: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:06:04.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1950" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":96,"skipped":1755,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:06:04.186: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:06:04.222: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 26 11:06:12.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-4985 --namespace=crd-publish-openapi-4985 create -f -'
Sep 26 11:06:12.475: INFO: stderr: ""
Sep 26 11:06:12.475: INFO: stdout: "e2e-test-crd-publish-openapi-3046-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 26 11:06:12.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-4985 --namespace=crd-publish-openapi-4985 delete e2e-test-crd-publish-openapi-3046-crds test-cr'
Sep 26 11:06:12.553: INFO: stderr: ""
Sep 26 11:06:12.553: INFO: stdout: "e2e-test-crd-publish-openapi-3046-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 26 11:06:12.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-4985 --namespace=crd-publish-openapi-4985 apply -f -'
Sep 26 11:06:12.782: INFO: stderr: ""
Sep 26 11:06:12.782: INFO: stdout: "e2e-test-crd-publish-openapi-3046-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 26 11:06:12.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-4985 --namespace=crd-publish-openapi-4985 delete e2e-test-crd-publish-openapi-3046-crds test-cr'
Sep 26 11:06:12.858: INFO: stderr: ""
Sep 26 11:06:12.858: INFO: stdout: "e2e-test-crd-publish-openapi-3046-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 26 11:06:12.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-4985 explain e2e-test-crd-publish-openapi-3046-crds'
Sep 26 11:06:13.024: INFO: stderr: ""
Sep 26 11:06:13.024: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3046-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:06:16.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4985" for this suite.

• [SLOW TEST:12.335 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":97,"skipped":1755,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:06:16.522: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7262.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7262.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7262.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7262.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7262.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7262.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 26 11:06:18.620: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7262/dns-test-2987b9ea-8b23-4555-9ba6-95d1fe91d72a: the server could not find the requested resource (get pods dns-test-2987b9ea-8b23-4555-9ba6-95d1fe91d72a)
Sep 26 11:06:18.623: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7262/dns-test-2987b9ea-8b23-4555-9ba6-95d1fe91d72a: the server could not find the requested resource (get pods dns-test-2987b9ea-8b23-4555-9ba6-95d1fe91d72a)
Sep 26 11:06:18.632: INFO: Unable to read jessie_udp@PodARecord from pod dns-7262/dns-test-2987b9ea-8b23-4555-9ba6-95d1fe91d72a: the server could not find the requested resource (get pods dns-test-2987b9ea-8b23-4555-9ba6-95d1fe91d72a)
Sep 26 11:06:18.634: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7262/dns-test-2987b9ea-8b23-4555-9ba6-95d1fe91d72a: the server could not find the requested resource (get pods dns-test-2987b9ea-8b23-4555-9ba6-95d1fe91d72a)
Sep 26 11:06:18.634: INFO: Lookups using dns-7262/dns-test-2987b9ea-8b23-4555-9ba6-95d1fe91d72a failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 26 11:06:23.662: INFO: DNS probes using dns-7262/dns-test-2987b9ea-8b23-4555-9ba6-95d1fe91d72a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:06:23.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7262" for this suite.

• [SLOW TEST:7.205 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":98,"skipped":1829,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:06:23.727: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-9lnfw in namespace proxy-5743
I0926 11:06:23.790552      22 runners.go:190] Created replication controller with name: proxy-service-9lnfw, namespace: proxy-5743, replica count: 1
I0926 11:06:24.841903      22 runners.go:190] proxy-service-9lnfw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0926 11:06:25.843103      22 runners.go:190] proxy-service-9lnfw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0926 11:06:26.843408      22 runners.go:190] proxy-service-9lnfw Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 26 11:06:26.851: INFO: setup took 3.079150381s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 26 11:06:26.855: INFO: (0) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 4.083502ms)
Sep 26 11:06:26.855: INFO: (0) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.637605ms)
Sep 26 11:06:26.856: INFO: (0) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.675503ms)
Sep 26 11:06:26.856: INFO: (0) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.983388ms)
Sep 26 11:06:26.856: INFO: (0) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 4.812282ms)
Sep 26 11:06:26.856: INFO: (0) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 5.634909ms)
Sep 26 11:06:26.857: INFO: (0) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 5.829048ms)
Sep 26 11:06:26.857: INFO: (0) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 6.026897ms)
Sep 26 11:06:26.857: INFO: (0) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 6.633637ms)
Sep 26 11:06:26.858: INFO: (0) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 7.174526ms)
Sep 26 11:06:26.859: INFO: (0) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 7.85849ms)
Sep 26 11:06:26.865: INFO: (0) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 14.286143ms)
Sep 26 11:06:26.865: INFO: (0) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 14.414458ms)
Sep 26 11:06:26.865: INFO: (0) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 14.600702ms)
Sep 26 11:06:26.865: INFO: (0) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 14.446913ms)
Sep 26 11:06:26.865: INFO: (0) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 14.584419ms)
Sep 26 11:06:26.871: INFO: (1) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 5.495973ms)
Sep 26 11:06:26.871: INFO: (1) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 5.750314ms)
Sep 26 11:06:26.871: INFO: (1) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 5.595825ms)
Sep 26 11:06:26.872: INFO: (1) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 5.553565ms)
Sep 26 11:06:26.872: INFO: (1) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 6.072971ms)
Sep 26 11:06:26.873: INFO: (1) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 7.260867ms)
Sep 26 11:06:26.873: INFO: (1) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 7.535087ms)
Sep 26 11:06:26.874: INFO: (1) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 7.934437ms)
Sep 26 11:06:26.874: INFO: (1) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 7.233207ms)
Sep 26 11:06:26.874: INFO: (1) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 7.800882ms)
Sep 26 11:06:26.874: INFO: (1) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 7.782922ms)
Sep 26 11:06:26.874: INFO: (1) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 7.670039ms)
Sep 26 11:06:26.875: INFO: (1) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 7.961652ms)
Sep 26 11:06:26.875: INFO: (1) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 8.40348ms)
Sep 26 11:06:26.875: INFO: (1) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 8.9007ms)
Sep 26 11:06:26.875: INFO: (1) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 9.563497ms)
Sep 26 11:06:26.879: INFO: (2) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.791865ms)
Sep 26 11:06:26.879: INFO: (2) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 4.019365ms)
Sep 26 11:06:26.880: INFO: (2) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.557626ms)
Sep 26 11:06:26.880: INFO: (2) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 4.528394ms)
Sep 26 11:06:26.880: INFO: (2) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 4.686914ms)
Sep 26 11:06:26.881: INFO: (2) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 5.717102ms)
Sep 26 11:06:26.881: INFO: (2) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 5.873713ms)
Sep 26 11:06:26.881: INFO: (2) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 5.852835ms)
Sep 26 11:06:26.882: INFO: (2) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 6.758546ms)
Sep 26 11:06:26.882: INFO: (2) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 6.715893ms)
Sep 26 11:06:26.882: INFO: (2) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 6.739205ms)
Sep 26 11:06:26.883: INFO: (2) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 7.105485ms)
Sep 26 11:06:26.883: INFO: (2) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 7.4352ms)
Sep 26 11:06:26.883: INFO: (2) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 7.474173ms)
Sep 26 11:06:26.883: INFO: (2) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 7.445625ms)
Sep 26 11:06:26.884: INFO: (2) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 8.206807ms)
Sep 26 11:06:26.887: INFO: (3) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.341874ms)
Sep 26 11:06:26.887: INFO: (3) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 3.40075ms)
Sep 26 11:06:26.887: INFO: (3) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 3.488327ms)
Sep 26 11:06:26.888: INFO: (3) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.61073ms)
Sep 26 11:06:26.888: INFO: (3) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.111545ms)
Sep 26 11:06:26.888: INFO: (3) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 4.198745ms)
Sep 26 11:06:26.888: INFO: (3) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 4.209481ms)
Sep 26 11:06:26.888: INFO: (3) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 4.292876ms)
Sep 26 11:06:26.889: INFO: (3) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 4.715352ms)
Sep 26 11:06:26.889: INFO: (3) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.959793ms)
Sep 26 11:06:26.889: INFO: (3) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 5.017825ms)
Sep 26 11:06:26.889: INFO: (3) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 5.159838ms)
Sep 26 11:06:26.889: INFO: (3) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 5.103357ms)
Sep 26 11:06:26.890: INFO: (3) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 6.030305ms)
Sep 26 11:06:26.890: INFO: (3) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 5.924783ms)
Sep 26 11:06:26.890: INFO: (3) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 6.002944ms)
Sep 26 11:06:26.893: INFO: (4) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 2.845217ms)
Sep 26 11:06:26.894: INFO: (4) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.009821ms)
Sep 26 11:06:26.894: INFO: (4) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 4.071787ms)
Sep 26 11:06:26.895: INFO: (4) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.498328ms)
Sep 26 11:06:26.895: INFO: (4) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 4.47208ms)
Sep 26 11:06:26.895: INFO: (4) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.919162ms)
Sep 26 11:06:26.895: INFO: (4) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 4.919582ms)
Sep 26 11:06:26.895: INFO: (4) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 5.148881ms)
Sep 26 11:06:26.895: INFO: (4) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 5.210823ms)
Sep 26 11:06:26.895: INFO: (4) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 5.488159ms)
Sep 26 11:06:26.896: INFO: (4) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 5.426641ms)
Sep 26 11:06:26.896: INFO: (4) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 5.685704ms)
Sep 26 11:06:26.897: INFO: (4) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 6.501457ms)
Sep 26 11:06:26.897: INFO: (4) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 6.50676ms)
Sep 26 11:06:26.897: INFO: (4) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 6.740054ms)
Sep 26 11:06:26.897: INFO: (4) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 7.201802ms)
Sep 26 11:06:26.900: INFO: (5) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 3.131609ms)
Sep 26 11:06:26.901: INFO: (5) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 3.268522ms)
Sep 26 11:06:26.901: INFO: (5) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.975741ms)
Sep 26 11:06:26.901: INFO: (5) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 3.959487ms)
Sep 26 11:06:26.901: INFO: (5) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 3.996717ms)
Sep 26 11:06:26.902: INFO: (5) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 4.399568ms)
Sep 26 11:06:26.902: INFO: (5) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.335084ms)
Sep 26 11:06:26.902: INFO: (5) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.400381ms)
Sep 26 11:06:26.902: INFO: (5) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.55294ms)
Sep 26 11:06:26.902: INFO: (5) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 4.919641ms)
Sep 26 11:06:26.902: INFO: (5) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 4.925575ms)
Sep 26 11:06:26.903: INFO: (5) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 5.386289ms)
Sep 26 11:06:26.903: INFO: (5) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 5.543752ms)
Sep 26 11:06:26.903: INFO: (5) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 5.550537ms)
Sep 26 11:06:26.904: INFO: (5) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 6.155431ms)
Sep 26 11:06:26.904: INFO: (5) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 6.548026ms)
Sep 26 11:06:26.907: INFO: (6) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 2.745177ms)
Sep 26 11:06:26.908: INFO: (6) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 3.38422ms)
Sep 26 11:06:26.908: INFO: (6) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 3.915576ms)
Sep 26 11:06:26.908: INFO: (6) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.968268ms)
Sep 26 11:06:26.908: INFO: (6) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 3.913775ms)
Sep 26 11:06:26.908: INFO: (6) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.252317ms)
Sep 26 11:06:26.909: INFO: (6) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 4.216972ms)
Sep 26 11:06:26.909: INFO: (6) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 4.496071ms)
Sep 26 11:06:26.909: INFO: (6) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 4.557272ms)
Sep 26 11:06:26.909: INFO: (6) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.626312ms)
Sep 26 11:06:26.910: INFO: (6) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 5.502081ms)
Sep 26 11:06:26.911: INFO: (6) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 6.266136ms)
Sep 26 11:06:26.911: INFO: (6) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 6.460161ms)
Sep 26 11:06:26.911: INFO: (6) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 6.730726ms)
Sep 26 11:06:26.911: INFO: (6) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 7.16583ms)
Sep 26 11:06:26.911: INFO: (6) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 7.24947ms)
Sep 26 11:06:26.915: INFO: (7) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.134201ms)
Sep 26 11:06:26.915: INFO: (7) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.345357ms)
Sep 26 11:06:26.915: INFO: (7) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 3.812952ms)
Sep 26 11:06:26.915: INFO: (7) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 3.935246ms)
Sep 26 11:06:26.916: INFO: (7) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.538244ms)
Sep 26 11:06:26.916: INFO: (7) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 4.827514ms)
Sep 26 11:06:26.916: INFO: (7) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 4.823037ms)
Sep 26 11:06:26.917: INFO: (7) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 5.434806ms)
Sep 26 11:06:26.917: INFO: (7) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 5.385976ms)
Sep 26 11:06:26.917: INFO: (7) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 5.612048ms)
Sep 26 11:06:26.917: INFO: (7) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 5.798667ms)
Sep 26 11:06:26.917: INFO: (7) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 5.705508ms)
Sep 26 11:06:26.918: INFO: (7) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 6.030638ms)
Sep 26 11:06:26.918: INFO: (7) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 6.06047ms)
Sep 26 11:06:26.918: INFO: (7) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 6.346301ms)
Sep 26 11:06:26.918: INFO: (7) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 6.80621ms)
Sep 26 11:06:26.921: INFO: (8) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 3.064847ms)
Sep 26 11:06:26.922: INFO: (8) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 3.713097ms)
Sep 26 11:06:26.922: INFO: (8) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 3.74933ms)
Sep 26 11:06:26.922: INFO: (8) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.811737ms)
Sep 26 11:06:26.923: INFO: (8) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.133468ms)
Sep 26 11:06:26.923: INFO: (8) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 4.304275ms)
Sep 26 11:06:26.923: INFO: (8) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.304012ms)
Sep 26 11:06:26.923: INFO: (8) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.489048ms)
Sep 26 11:06:26.923: INFO: (8) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 4.797135ms)
Sep 26 11:06:26.923: INFO: (8) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 4.86409ms)
Sep 26 11:06:26.923: INFO: (8) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 4.856674ms)
Sep 26 11:06:26.924: INFO: (8) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 5.311532ms)
Sep 26 11:06:26.924: INFO: (8) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 5.480929ms)
Sep 26 11:06:26.924: INFO: (8) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 5.443667ms)
Sep 26 11:06:26.924: INFO: (8) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 5.941859ms)
Sep 26 11:06:26.925: INFO: (8) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 6.172326ms)
Sep 26 11:06:26.927: INFO: (9) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 2.742438ms)
Sep 26 11:06:26.928: INFO: (9) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.595211ms)
Sep 26 11:06:26.928: INFO: (9) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.51239ms)
Sep 26 11:06:26.929: INFO: (9) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 3.685892ms)
Sep 26 11:06:26.929: INFO: (9) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.765132ms)
Sep 26 11:06:26.929: INFO: (9) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 4.003997ms)
Sep 26 11:06:26.929: INFO: (9) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.909147ms)
Sep 26 11:06:26.929: INFO: (9) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 3.907259ms)
Sep 26 11:06:26.930: INFO: (9) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 5.281222ms)
Sep 26 11:06:26.930: INFO: (9) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 5.349604ms)
Sep 26 11:06:26.930: INFO: (9) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 5.404459ms)
Sep 26 11:06:26.930: INFO: (9) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 5.375798ms)
Sep 26 11:06:26.931: INFO: (9) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 6.208901ms)
Sep 26 11:06:26.931: INFO: (9) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 6.159717ms)
Sep 26 11:06:26.931: INFO: (9) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 6.240469ms)
Sep 26 11:06:26.931: INFO: (9) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 6.507493ms)
Sep 26 11:06:26.935: INFO: (10) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 3.171989ms)
Sep 26 11:06:26.935: INFO: (10) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 3.472426ms)
Sep 26 11:06:26.935: INFO: (10) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 3.620712ms)
Sep 26 11:06:26.935: INFO: (10) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 3.561358ms)
Sep 26 11:06:26.935: INFO: (10) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.683901ms)
Sep 26 11:06:26.936: INFO: (10) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 4.339377ms)
Sep 26 11:06:26.936: INFO: (10) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.444076ms)
Sep 26 11:06:26.936: INFO: (10) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.474034ms)
Sep 26 11:06:26.936: INFO: (10) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.437682ms)
Sep 26 11:06:26.936: INFO: (10) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 4.43552ms)
Sep 26 11:06:26.937: INFO: (10) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 5.040222ms)
Sep 26 11:06:26.937: INFO: (10) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 5.198649ms)
Sep 26 11:06:26.937: INFO: (10) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 5.377649ms)
Sep 26 11:06:26.937: INFO: (10) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 5.418661ms)
Sep 26 11:06:26.937: INFO: (10) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 5.954904ms)
Sep 26 11:06:26.938: INFO: (10) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 6.139685ms)
Sep 26 11:06:26.940: INFO: (11) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 2.618084ms)
Sep 26 11:06:26.941: INFO: (11) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 3.035539ms)
Sep 26 11:06:26.941: INFO: (11) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.281855ms)
Sep 26 11:06:26.941: INFO: (11) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 2.834147ms)
Sep 26 11:06:26.942: INFO: (11) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.431787ms)
Sep 26 11:06:26.942: INFO: (11) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.557047ms)
Sep 26 11:06:26.942: INFO: (11) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 4.298694ms)
Sep 26 11:06:26.943: INFO: (11) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 4.719867ms)
Sep 26 11:06:26.944: INFO: (11) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 6.25238ms)
Sep 26 11:06:26.944: INFO: (11) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 6.375648ms)
Sep 26 11:06:26.944: INFO: (11) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 6.459527ms)
Sep 26 11:06:26.944: INFO: (11) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 5.540449ms)
Sep 26 11:06:26.944: INFO: (11) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 6.238579ms)
Sep 26 11:06:26.944: INFO: (11) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 5.938426ms)
Sep 26 11:06:26.945: INFO: (11) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 5.950157ms)
Sep 26 11:06:26.946: INFO: (11) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 7.45496ms)
Sep 26 11:06:26.949: INFO: (12) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.08304ms)
Sep 26 11:06:26.950: INFO: (12) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 3.474541ms)
Sep 26 11:06:26.950: INFO: (12) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 3.386846ms)
Sep 26 11:06:26.950: INFO: (12) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 3.451404ms)
Sep 26 11:06:26.950: INFO: (12) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 3.575078ms)
Sep 26 11:06:26.950: INFO: (12) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.796187ms)
Sep 26 11:06:26.950: INFO: (12) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.954893ms)
Sep 26 11:06:26.950: INFO: (12) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 3.994569ms)
Sep 26 11:06:26.950: INFO: (12) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 4.044865ms)
Sep 26 11:06:26.950: INFO: (12) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.997755ms)
Sep 26 11:06:26.951: INFO: (12) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 5.031512ms)
Sep 26 11:06:26.951: INFO: (12) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 4.987032ms)
Sep 26 11:06:26.952: INFO: (12) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 5.712308ms)
Sep 26 11:06:26.952: INFO: (12) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 5.649416ms)
Sep 26 11:06:26.952: INFO: (12) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 5.988004ms)
Sep 26 11:06:26.953: INFO: (12) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 6.628178ms)
Sep 26 11:06:26.956: INFO: (13) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 3.446326ms)
Sep 26 11:06:26.956: INFO: (13) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.26429ms)
Sep 26 11:06:26.956: INFO: (13) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.370098ms)
Sep 26 11:06:26.956: INFO: (13) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 3.443927ms)
Sep 26 11:06:26.957: INFO: (13) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.264535ms)
Sep 26 11:06:26.957: INFO: (13) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 4.344189ms)
Sep 26 11:06:26.957: INFO: (13) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 4.33601ms)
Sep 26 11:06:26.957: INFO: (13) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.413126ms)
Sep 26 11:06:26.958: INFO: (13) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 4.507034ms)
Sep 26 11:06:26.958: INFO: (13) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 4.72543ms)
Sep 26 11:06:26.959: INFO: (13) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 5.458073ms)
Sep 26 11:06:26.959: INFO: (13) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 5.633018ms)
Sep 26 11:06:26.959: INFO: (13) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 5.934792ms)
Sep 26 11:06:26.959: INFO: (13) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 6.050898ms)
Sep 26 11:06:26.959: INFO: (13) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 5.974087ms)
Sep 26 11:06:26.960: INFO: (13) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 6.380216ms)
Sep 26 11:06:26.963: INFO: (14) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 2.996341ms)
Sep 26 11:06:26.963: INFO: (14) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 3.246669ms)
Sep 26 11:06:26.963: INFO: (14) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.466779ms)
Sep 26 11:06:26.963: INFO: (14) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.611797ms)
Sep 26 11:06:26.964: INFO: (14) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.22027ms)
Sep 26 11:06:26.964: INFO: (14) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 4.329227ms)
Sep 26 11:06:26.964: INFO: (14) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.222466ms)
Sep 26 11:06:26.964: INFO: (14) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 4.407907ms)
Sep 26 11:06:26.964: INFO: (14) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 4.368291ms)
Sep 26 11:06:26.964: INFO: (14) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 4.658137ms)
Sep 26 11:06:26.965: INFO: (14) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 5.229902ms)
Sep 26 11:06:26.965: INFO: (14) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 5.634548ms)
Sep 26 11:06:26.965: INFO: (14) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 5.757127ms)
Sep 26 11:06:26.965: INFO: (14) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 5.767491ms)
Sep 26 11:06:26.966: INFO: (14) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 6.23609ms)
Sep 26 11:06:26.966: INFO: (14) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 6.412069ms)
Sep 26 11:06:26.969: INFO: (15) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 2.707469ms)
Sep 26 11:06:26.969: INFO: (15) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 3.210377ms)
Sep 26 11:06:26.970: INFO: (15) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 3.288286ms)
Sep 26 11:06:26.970: INFO: (15) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.343716ms)
Sep 26 11:06:26.970: INFO: (15) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 3.453204ms)
Sep 26 11:06:26.970: INFO: (15) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 3.538848ms)
Sep 26 11:06:26.971: INFO: (15) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.543636ms)
Sep 26 11:06:26.971: INFO: (15) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.632472ms)
Sep 26 11:06:26.971: INFO: (15) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.586754ms)
Sep 26 11:06:26.971: INFO: (15) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 4.757609ms)
Sep 26 11:06:26.971: INFO: (15) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 5.178097ms)
Sep 26 11:06:26.972: INFO: (15) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 5.365695ms)
Sep 26 11:06:26.972: INFO: (15) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 5.432362ms)
Sep 26 11:06:26.972: INFO: (15) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 5.72461ms)
Sep 26 11:06:26.973: INFO: (15) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 6.165594ms)
Sep 26 11:06:26.973: INFO: (15) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 6.342034ms)
Sep 26 11:06:26.976: INFO: (16) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 3.091879ms)
Sep 26 11:06:26.976: INFO: (16) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 3.141198ms)
Sep 26 11:06:26.977: INFO: (16) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.902612ms)
Sep 26 11:06:26.977: INFO: (16) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 3.727799ms)
Sep 26 11:06:26.977: INFO: (16) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 2.960216ms)
Sep 26 11:06:26.977: INFO: (16) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 3.503826ms)
Sep 26 11:06:26.978: INFO: (16) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.277171ms)
Sep 26 11:06:26.978: INFO: (16) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 4.873989ms)
Sep 26 11:06:26.978: INFO: (16) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 4.398151ms)
Sep 26 11:06:26.978: INFO: (16) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 4.629488ms)
Sep 26 11:06:26.978: INFO: (16) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 5.33153ms)
Sep 26 11:06:26.978: INFO: (16) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 5.026171ms)
Sep 26 11:06:26.979: INFO: (16) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 4.600165ms)
Sep 26 11:06:26.979: INFO: (16) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 5.202694ms)
Sep 26 11:06:26.979: INFO: (16) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 5.936809ms)
Sep 26 11:06:26.982: INFO: (16) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 8.089511ms)
Sep 26 11:06:26.985: INFO: (17) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 2.647641ms)
Sep 26 11:06:26.985: INFO: (17) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 3.232283ms)
Sep 26 11:06:26.985: INFO: (17) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.622399ms)
Sep 26 11:06:26.986: INFO: (17) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.549465ms)
Sep 26 11:06:26.986: INFO: (17) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.50524ms)
Sep 26 11:06:26.986: INFO: (17) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.112483ms)
Sep 26 11:06:26.986: INFO: (17) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 4.127031ms)
Sep 26 11:06:26.986: INFO: (17) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 4.38459ms)
Sep 26 11:06:26.986: INFO: (17) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 4.415737ms)
Sep 26 11:06:26.986: INFO: (17) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 4.506108ms)
Sep 26 11:06:26.986: INFO: (17) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 4.472839ms)
Sep 26 11:06:26.988: INFO: (17) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 6.190071ms)
Sep 26 11:06:26.988: INFO: (17) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 6.034889ms)
Sep 26 11:06:26.988: INFO: (17) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 6.255548ms)
Sep 26 11:06:26.989: INFO: (17) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 6.792924ms)
Sep 26 11:06:26.989: INFO: (17) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 6.821769ms)
Sep 26 11:06:26.992: INFO: (18) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 2.982185ms)
Sep 26 11:06:26.992: INFO: (18) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.042632ms)
Sep 26 11:06:26.992: INFO: (18) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 3.45502ms)
Sep 26 11:06:26.992: INFO: (18) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 3.29424ms)
Sep 26 11:06:26.993: INFO: (18) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 3.726196ms)
Sep 26 11:06:26.993: INFO: (18) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.25986ms)
Sep 26 11:06:26.993: INFO: (18) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 4.320067ms)
Sep 26 11:06:26.994: INFO: (18) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 4.979298ms)
Sep 26 11:06:26.994: INFO: (18) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 5.018297ms)
Sep 26 11:06:26.994: INFO: (18) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 5.008218ms)
Sep 26 11:06:26.995: INFO: (18) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 5.874581ms)
Sep 26 11:06:26.995: INFO: (18) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 5.985112ms)
Sep 26 11:06:26.995: INFO: (18) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 6.124253ms)
Sep 26 11:06:26.996: INFO: (18) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 6.69805ms)
Sep 26 11:06:26.996: INFO: (18) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 6.671089ms)
Sep 26 11:06:26.996: INFO: (18) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 7.01793ms)
Sep 26 11:06:27.000: INFO: (19) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 3.29806ms)
Sep 26 11:06:27.000: INFO: (19) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:460/proxy/: tls baz (200; 3.368244ms)
Sep 26 11:06:27.000: INFO: (19) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">test<... (200; 3.480215ms)
Sep 26 11:06:27.001: INFO: (19) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:443/proxy/tlsrewritem... (200; 4.176492ms)
Sep 26 11:06:27.001: INFO: (19) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 5.025927ms)
Sep 26 11:06:27.001: INFO: (19) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname1/proxy/: foo (200; 5.185281ms)
Sep 26 11:06:27.001: INFO: (19) /api/v1/namespaces/proxy-5743/pods/https:proxy-service-9lnfw-s2jnp:462/proxy/: tls qux (200; 5.02638ms)
Sep 26 11:06:27.001: INFO: (19) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname1/proxy/: tls baz (200; 5.193701ms)
Sep 26 11:06:27.001: INFO: (19) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:162/proxy/: bar (200; 5.065565ms)
Sep 26 11:06:27.002: INFO: (19) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp:160/proxy/: foo (200; 5.256185ms)
Sep 26 11:06:27.002: INFO: (19) /api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/http:proxy-service-9lnfw-s2jnp:1080/proxy/rewriteme">... (200; 5.316463ms)
Sep 26 11:06:27.002: INFO: (19) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname2/proxy/: bar (200; 5.859622ms)
Sep 26 11:06:27.002: INFO: (19) /api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/: <a href="/api/v1/namespaces/proxy-5743/pods/proxy-service-9lnfw-s2jnp/proxy/rewriteme">test</a> (200; 5.827745ms)
Sep 26 11:06:27.002: INFO: (19) /api/v1/namespaces/proxy-5743/services/proxy-service-9lnfw:portname1/proxy/: foo (200; 6.156239ms)
Sep 26 11:06:27.003: INFO: (19) /api/v1/namespaces/proxy-5743/services/http:proxy-service-9lnfw:portname2/proxy/: bar (200; 6.816202ms)
Sep 26 11:06:27.003: INFO: (19) /api/v1/namespaces/proxy-5743/services/https:proxy-service-9lnfw:tlsportname2/proxy/: tls qux (200; 6.799206ms)
STEP: deleting ReplicationController proxy-service-9lnfw in namespace proxy-5743, will wait for the garbage collector to delete the pods
Sep 26 11:06:27.069: INFO: Deleting ReplicationController proxy-service-9lnfw took: 12.179216ms
Sep 26 11:06:27.170: INFO: Terminating ReplicationController proxy-service-9lnfw pods took: 101.1631ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:06:29.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5743" for this suite.

• [SLOW TEST:6.058 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":99,"skipped":1843,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:06:29.785: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:06:29.829: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 26 11:06:37.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-7698 --namespace=crd-publish-openapi-7698 create -f -'
Sep 26 11:06:38.221: INFO: stderr: ""
Sep 26 11:06:38.221: INFO: stdout: "e2e-test-crd-publish-openapi-9704-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 26 11:06:38.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-7698 --namespace=crd-publish-openapi-7698 delete e2e-test-crd-publish-openapi-9704-crds test-cr'
Sep 26 11:06:38.330: INFO: stderr: ""
Sep 26 11:06:38.330: INFO: stdout: "e2e-test-crd-publish-openapi-9704-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 26 11:06:38.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-7698 --namespace=crd-publish-openapi-7698 apply -f -'
Sep 26 11:06:38.520: INFO: stderr: ""
Sep 26 11:06:38.520: INFO: stdout: "e2e-test-crd-publish-openapi-9704-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 26 11:06:38.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-7698 --namespace=crd-publish-openapi-7698 delete e2e-test-crd-publish-openapi-9704-crds test-cr'
Sep 26 11:06:38.595: INFO: stderr: ""
Sep 26 11:06:38.595: INFO: stdout: "e2e-test-crd-publish-openapi-9704-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 26 11:06:38.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-7698 explain e2e-test-crd-publish-openapi-9704-crds'
Sep 26 11:06:38.779: INFO: stderr: ""
Sep 26 11:06:38.779: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9704-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:06:42.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7698" for this suite.

• [SLOW TEST:12.493 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":100,"skipped":1852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:06:42.279: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:06:42.337: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep 26 11:06:50.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 --namespace=crd-publish-openapi-9413 create -f -'
Sep 26 11:06:51.123: INFO: stderr: ""
Sep 26 11:06:51.123: INFO: stdout: "e2e-test-crd-publish-openapi-7213-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 26 11:06:51.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 --namespace=crd-publish-openapi-9413 delete e2e-test-crd-publish-openapi-7213-crds test-foo'
Sep 26 11:06:51.206: INFO: stderr: ""
Sep 26 11:06:51.206: INFO: stdout: "e2e-test-crd-publish-openapi-7213-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 26 11:06:51.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 --namespace=crd-publish-openapi-9413 apply -f -'
Sep 26 11:06:51.417: INFO: stderr: ""
Sep 26 11:06:51.417: INFO: stdout: "e2e-test-crd-publish-openapi-7213-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 26 11:06:51.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 --namespace=crd-publish-openapi-9413 delete e2e-test-crd-publish-openapi-7213-crds test-foo'
Sep 26 11:06:51.498: INFO: stderr: ""
Sep 26 11:06:51.498: INFO: stdout: "e2e-test-crd-publish-openapi-7213-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep 26 11:06:51.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 --namespace=crd-publish-openapi-9413 create -f -'
Sep 26 11:06:51.674: INFO: rc: 1
Sep 26 11:06:51.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 --namespace=crd-publish-openapi-9413 apply -f -'
Sep 26 11:06:51.844: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep 26 11:06:51.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 --namespace=crd-publish-openapi-9413 create -f -'
Sep 26 11:06:52.010: INFO: rc: 1
Sep 26 11:06:52.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 --namespace=crd-publish-openapi-9413 apply -f -'
Sep 26 11:06:52.191: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep 26 11:06:52.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 explain e2e-test-crd-publish-openapi-7213-crds'
Sep 26 11:06:52.368: INFO: stderr: ""
Sep 26 11:06:52.368: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7213-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep 26 11:06:52.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 explain e2e-test-crd-publish-openapi-7213-crds.metadata'
Sep 26 11:06:52.540: INFO: stderr: ""
Sep 26 11:06:52.540: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7213-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 26 11:06:52.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 explain e2e-test-crd-publish-openapi-7213-crds.spec'
Sep 26 11:06:52.724: INFO: stderr: ""
Sep 26 11:06:52.724: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7213-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 26 11:06:52.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 explain e2e-test-crd-publish-openapi-7213-crds.spec.bars'
Sep 26 11:06:52.902: INFO: stderr: ""
Sep 26 11:06:52.902: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7213-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep 26 11:06:52.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=crd-publish-openapi-9413 explain e2e-test-crd-publish-openapi-7213-crds.spec.bars2'
Sep 26 11:06:53.090: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:06:56.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9413" for this suite.

• [SLOW TEST:14.358 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":101,"skipped":1881,"failed":0}
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:06:56.637: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Sep 26 11:06:56.703: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 26 11:07:56.746: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:07:56.751: INFO: Starting informer...
STEP: Starting pod...
Sep 26 11:07:56.970: INFO: Pod is running on 10.37.21.194. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep 26 11:07:56.991: INFO: Pod wasn't evicted. Proceeding
Sep 26 11:07:56.991: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep 26 11:09:12.009: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:09:12.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-209" for this suite.

• [SLOW TEST:135.413 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":102,"skipped":1885,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:09:12.050: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-3238
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 26 11:09:12.222: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 26 11:09:12.260: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:09:14.268: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:09:16.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:09:18.267: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:09:20.275: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:09:22.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:09:24.276: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:09:26.270: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:09:28.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:09:30.277: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:09:32.274: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 26 11:09:32.286: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 26 11:09:32.292: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 26 11:09:34.346: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep 26 11:09:34.346: INFO: Going to poll 192.168.84.96 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Sep 26 11:09:34.349: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.84.96 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3238 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:09:34.349: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:09:35.490: INFO: Found all 1 expected endpoints: [netserver-0]
Sep 26 11:09:35.490: INFO: Going to poll 192.168.50.136 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Sep 26 11:09:35.497: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.50.136 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3238 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:09:35.497: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:09:36.645: INFO: Found all 1 expected endpoints: [netserver-1]
Sep 26 11:09:36.645: INFO: Going to poll 192.168.181.87 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Sep 26 11:09:36.657: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.181.87 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3238 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:09:36.657: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:09:37.809: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:09:37.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3238" for this suite.

• [SLOW TEST:25.780 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":103,"skipped":1889,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:09:37.830: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-4179
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4179 to expose endpoints map[]
Sep 26 11:09:37.914: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Sep 26 11:09:38.926: INFO: successfully validated that service multi-endpoint-test in namespace services-4179 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4179
Sep 26 11:09:38.939: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:09:40.953: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4179 to expose endpoints map[pod1:[100]]
Sep 26 11:09:40.968: INFO: successfully validated that service multi-endpoint-test in namespace services-4179 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4179
Sep 26 11:09:40.978: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:09:42.997: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4179 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 26 11:09:43.038: INFO: successfully validated that service multi-endpoint-test in namespace services-4179 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Sep 26 11:09:43.038: INFO: Creating new exec pod
Sep 26 11:09:46.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-4179 exec execpodgp4bv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Sep 26 11:09:46.303: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Sep 26 11:09:46.303: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:09:46.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-4179 exec execpodgp4bv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.148.59 80'
Sep 26 11:09:46.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.148.59 80\nConnection to 10.254.148.59 80 port [tcp/http] succeeded!\n"
Sep 26 11:09:46.496: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:09:46.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-4179 exec execpodgp4bv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Sep 26 11:09:46.705: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Sep 26 11:09:46.705: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:09:46.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-4179 exec execpodgp4bv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.148.59 81'
Sep 26 11:09:46.929: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.148.59 81\nConnection to 10.254.148.59 81 port [tcp/*] succeeded!\n"
Sep 26 11:09:46.929: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4179
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4179 to expose endpoints map[pod2:[101]]
Sep 26 11:09:47.984: INFO: successfully validated that service multi-endpoint-test in namespace services-4179 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4179
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4179 to expose endpoints map[]
Sep 26 11:09:49.013: INFO: successfully validated that service multi-endpoint-test in namespace services-4179 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:09:49.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4179" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.213 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":104,"skipped":1920,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:09:49.043: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:09:49.102: INFO: created pod
Sep 26 11:09:49.102: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6728" to be "Succeeded or Failed"
Sep 26 11:09:49.106: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.343564ms
Sep 26 11:09:51.114: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011842661s
STEP: Saw pod success
Sep 26 11:09:51.114: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Sep 26 11:10:21.115: INFO: polling logs
Sep 26 11:10:21.133: INFO: Pod logs: 
2021/09/26 11:09:50 OK: Got token
2021/09/26 11:09:50 validating with in-cluster discovery
2021/09/26 11:09:50 OK: got issuer https://kubernetes.default.svc.cluster.local.
2021/09/26 11:09:50 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local.", Subject:"system:serviceaccount:svcaccounts-6728:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1632655189, NotBefore:1632654589, IssuedAt:1632654589, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6728", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"49d5d8fe-686d-41dd-a243-035a3b6a99de"}}}
2021/09/26 11:09:50 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local.
2021/09/26 11:09:50 OK: Validated signature on JWT
2021/09/26 11:09:50 OK: Got valid claims from token!
2021/09/26 11:09:50 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local.", Subject:"system:serviceaccount:svcaccounts-6728:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1632655189, NotBefore:1632654589, IssuedAt:1632654589, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6728", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"49d5d8fe-686d-41dd-a243-035a3b6a99de"}}}

Sep 26 11:10:21.133: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:21.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6728" for this suite.

• [SLOW TEST:32.130 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":105,"skipped":1934,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:21.174: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-b6c6487c-9789-4514-a695-e1d96d95ca91
STEP: Creating a pod to test consume configMaps
Sep 26 11:10:21.265: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-921d203f-35bd-418b-ae19-7dd52b4881f8" in namespace "projected-8655" to be "Succeeded or Failed"
Sep 26 11:10:21.270: INFO: Pod "pod-projected-configmaps-921d203f-35bd-418b-ae19-7dd52b4881f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.71908ms
Sep 26 11:10:23.281: INFO: Pod "pod-projected-configmaps-921d203f-35bd-418b-ae19-7dd52b4881f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015646555s
Sep 26 11:10:25.290: INFO: Pod "pod-projected-configmaps-921d203f-35bd-418b-ae19-7dd52b4881f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024540099s
STEP: Saw pod success
Sep 26 11:10:25.290: INFO: Pod "pod-projected-configmaps-921d203f-35bd-418b-ae19-7dd52b4881f8" satisfied condition "Succeeded or Failed"
Sep 26 11:10:25.295: INFO: Trying to get logs from node 10.37.21.195 pod pod-projected-configmaps-921d203f-35bd-418b-ae19-7dd52b4881f8 container agnhost-container: <nil>
STEP: delete the pod
Sep 26 11:10:25.325: INFO: Waiting for pod pod-projected-configmaps-921d203f-35bd-418b-ae19-7dd52b4881f8 to disappear
Sep 26 11:10:25.328: INFO: Pod pod-projected-configmaps-921d203f-35bd-418b-ae19-7dd52b4881f8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:25.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8655" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":106,"skipped":1936,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:25.337: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-30e5937a-e3d0-471c-8224-9bb4e5d3dbfc
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:25.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6921" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":107,"skipped":1937,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:25.406: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 26 11:10:25.467: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep 26 11:10:25.471: INFO: starting watch
STEP: patching
STEP: updating
Sep 26 11:10:25.486: INFO: waiting for watch events with expected annotations
Sep 26 11:10:25.486: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:25.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6998" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":108,"skipped":1953,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:25.519: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:10:25.561: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8299
I0926 11:10:25.574063      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8299, replica count: 1
I0926 11:10:26.625894      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0926 11:10:27.626856      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0926 11:10:28.627138      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 26 11:10:28.749: INFO: Created: latency-svc-5sxx4
Sep 26 11:10:28.765: INFO: Got endpoints: latency-svc-5sxx4 [37.752648ms]
Sep 26 11:10:28.776: INFO: Created: latency-svc-2fr4p
Sep 26 11:10:28.783: INFO: Created: latency-svc-6hqjh
Sep 26 11:10:28.783: INFO: Got endpoints: latency-svc-2fr4p [17.543013ms]
Sep 26 11:10:28.792: INFO: Created: latency-svc-pbcwv
Sep 26 11:10:28.799: INFO: Got endpoints: latency-svc-6hqjh [33.598305ms]
Sep 26 11:10:28.801: INFO: Got endpoints: latency-svc-pbcwv [35.778815ms]
Sep 26 11:10:28.804: INFO: Created: latency-svc-b4dvq
Sep 26 11:10:28.810: INFO: Got endpoints: latency-svc-b4dvq [44.565856ms]
Sep 26 11:10:28.810: INFO: Created: latency-svc-4ttw9
Sep 26 11:10:28.816: INFO: Created: latency-svc-gf5v2
Sep 26 11:10:28.818: INFO: Got endpoints: latency-svc-4ttw9 [52.223973ms]
Sep 26 11:10:28.826: INFO: Got endpoints: latency-svc-gf5v2 [60.580539ms]
Sep 26 11:10:28.826: INFO: Created: latency-svc-7jjd2
Sep 26 11:10:28.832: INFO: Got endpoints: latency-svc-7jjd2 [66.75465ms]
Sep 26 11:10:28.833: INFO: Created: latency-svc-b5qz6
Sep 26 11:10:28.842: INFO: Got endpoints: latency-svc-b5qz6 [76.202202ms]
Sep 26 11:10:28.842: INFO: Created: latency-svc-9j7nm
Sep 26 11:10:28.849: INFO: Created: latency-svc-d2g4k
Sep 26 11:10:28.852: INFO: Got endpoints: latency-svc-9j7nm [87.049965ms]
Sep 26 11:10:28.861: INFO: Created: latency-svc-pl4kn
Sep 26 11:10:28.861: INFO: Got endpoints: latency-svc-d2g4k [95.157656ms]
Sep 26 11:10:28.865: INFO: Created: latency-svc-drmtl
Sep 26 11:10:28.869: INFO: Got endpoints: latency-svc-pl4kn [103.170844ms]
Sep 26 11:10:28.873: INFO: Got endpoints: latency-svc-drmtl [107.661378ms]
Sep 26 11:10:28.875: INFO: Created: latency-svc-9flj5
Sep 26 11:10:28.878: INFO: Got endpoints: latency-svc-9flj5 [112.693069ms]
Sep 26 11:10:28.882: INFO: Created: latency-svc-rqc2k
Sep 26 11:10:28.885: INFO: Got endpoints: latency-svc-rqc2k [119.892454ms]
Sep 26 11:10:28.887: INFO: Created: latency-svc-n6bql
Sep 26 11:10:28.893: INFO: Got endpoints: latency-svc-n6bql [127.653389ms]
Sep 26 11:10:28.897: INFO: Created: latency-svc-c5mgp
Sep 26 11:10:28.901: INFO: Got endpoints: latency-svc-c5mgp [118.608596ms]
Sep 26 11:10:28.903: INFO: Created: latency-svc-lrcw5
Sep 26 11:10:28.908: INFO: Got endpoints: latency-svc-lrcw5 [109.451127ms]
Sep 26 11:10:28.911: INFO: Created: latency-svc-v7x9f
Sep 26 11:10:28.917: INFO: Created: latency-svc-jqt76
Sep 26 11:10:28.917: INFO: Got endpoints: latency-svc-v7x9f [115.660622ms]
Sep 26 11:10:28.926: INFO: Got endpoints: latency-svc-jqt76 [116.229377ms]
Sep 26 11:10:28.926: INFO: Created: latency-svc-qdpvb
Sep 26 11:10:28.928: INFO: Created: latency-svc-gtkq4
Sep 26 11:10:28.932: INFO: Got endpoints: latency-svc-qdpvb [114.070235ms]
Sep 26 11:10:28.935: INFO: Got endpoints: latency-svc-gtkq4 [108.889801ms]
Sep 26 11:10:28.936: INFO: Created: latency-svc-tthw6
Sep 26 11:10:28.940: INFO: Got endpoints: latency-svc-tthw6 [108.151443ms]
Sep 26 11:10:28.943: INFO: Created: latency-svc-d5kgt
Sep 26 11:10:28.947: INFO: Got endpoints: latency-svc-d5kgt [105.469014ms]
Sep 26 11:10:28.949: INFO: Created: latency-svc-488dw
Sep 26 11:10:28.955: INFO: Got endpoints: latency-svc-488dw [102.20255ms]
Sep 26 11:10:28.956: INFO: Created: latency-svc-8tb5j
Sep 26 11:10:28.960: INFO: Got endpoints: latency-svc-8tb5j [99.323061ms]
Sep 26 11:10:28.969: INFO: Created: latency-svc-nw42m
Sep 26 11:10:28.972: INFO: Created: latency-svc-f75lk
Sep 26 11:10:28.975: INFO: Got endpoints: latency-svc-nw42m [105.908165ms]
Sep 26 11:10:28.977: INFO: Got endpoints: latency-svc-f75lk [104.144943ms]
Sep 26 11:10:28.980: INFO: Created: latency-svc-xrgw6
Sep 26 11:10:28.985: INFO: Got endpoints: latency-svc-xrgw6 [107.008192ms]
Sep 26 11:10:28.986: INFO: Created: latency-svc-8h9lc
Sep 26 11:10:28.991: INFO: Got endpoints: latency-svc-8h9lc [106.101944ms]
Sep 26 11:10:28.997: INFO: Created: latency-svc-lmxzf
Sep 26 11:10:29.001: INFO: Got endpoints: latency-svc-lmxzf [107.601808ms]
Sep 26 11:10:29.004: INFO: Created: latency-svc-pfdxx
Sep 26 11:10:29.008: INFO: Created: latency-svc-mc2df
Sep 26 11:10:29.011: INFO: Got endpoints: latency-svc-pfdxx [109.638746ms]
Sep 26 11:10:29.013: INFO: Got endpoints: latency-svc-mc2df [104.649484ms]
Sep 26 11:10:29.016: INFO: Created: latency-svc-dc4bw
Sep 26 11:10:29.024: INFO: Created: latency-svc-882z4
Sep 26 11:10:29.024: INFO: Got endpoints: latency-svc-dc4bw [106.869355ms]
Sep 26 11:10:29.028: INFO: Got endpoints: latency-svc-882z4 [101.233856ms]
Sep 26 11:10:29.030: INFO: Created: latency-svc-tg2mj
Sep 26 11:10:29.035: INFO: Got endpoints: latency-svc-tg2mj [103.245611ms]
Sep 26 11:10:29.037: INFO: Created: latency-svc-rgwh8
Sep 26 11:10:29.043: INFO: Got endpoints: latency-svc-rgwh8 [107.714679ms]
Sep 26 11:10:29.045: INFO: Created: latency-svc-j5xwf
Sep 26 11:10:29.049: INFO: Got endpoints: latency-svc-j5xwf [108.446936ms]
Sep 26 11:10:29.052: INFO: Created: latency-svc-cnhjb
Sep 26 11:10:29.056: INFO: Got endpoints: latency-svc-cnhjb [108.969896ms]
Sep 26 11:10:29.060: INFO: Created: latency-svc-9qw8w
Sep 26 11:10:29.065: INFO: Got endpoints: latency-svc-9qw8w [110.467724ms]
Sep 26 11:10:29.067: INFO: Created: latency-svc-j9b66
Sep 26 11:10:29.075: INFO: Got endpoints: latency-svc-j9b66 [114.815631ms]
Sep 26 11:10:29.077: INFO: Created: latency-svc-nz2nt
Sep 26 11:10:29.082: INFO: Created: latency-svc-25jh2
Sep 26 11:10:29.082: INFO: Got endpoints: latency-svc-nz2nt [107.662725ms]
Sep 26 11:10:29.088: INFO: Got endpoints: latency-svc-25jh2 [110.478857ms]
Sep 26 11:10:29.089: INFO: Created: latency-svc-wfpgh
Sep 26 11:10:29.094: INFO: Got endpoints: latency-svc-wfpgh [109.355908ms]
Sep 26 11:10:29.096: INFO: Created: latency-svc-trzvn
Sep 26 11:10:29.102: INFO: Got endpoints: latency-svc-trzvn [110.422337ms]
Sep 26 11:10:29.103: INFO: Created: latency-svc-vlwss
Sep 26 11:10:29.109: INFO: Created: latency-svc-rh9q8
Sep 26 11:10:29.113: INFO: Got endpoints: latency-svc-vlwss [112.039488ms]
Sep 26 11:10:29.114: INFO: Got endpoints: latency-svc-rh9q8 [103.03152ms]
Sep 26 11:10:29.117: INFO: Created: latency-svc-crztg
Sep 26 11:10:29.121: INFO: Created: latency-svc-xndb9
Sep 26 11:10:29.122: INFO: Got endpoints: latency-svc-crztg [109.328698ms]
Sep 26 11:10:29.128: INFO: Created: latency-svc-4ngv9
Sep 26 11:10:29.130: INFO: Got endpoints: latency-svc-xndb9 [105.865051ms]
Sep 26 11:10:29.133: INFO: Got endpoints: latency-svc-4ngv9 [105.890615ms]
Sep 26 11:10:29.136: INFO: Created: latency-svc-j4t7j
Sep 26 11:10:29.142: INFO: Created: latency-svc-wvclx
Sep 26 11:10:29.144: INFO: Got endpoints: latency-svc-j4t7j [108.635374ms]
Sep 26 11:10:29.148: INFO: Got endpoints: latency-svc-wvclx [104.774802ms]
Sep 26 11:10:29.149: INFO: Created: latency-svc-vfj5w
Sep 26 11:10:29.154: INFO: Got endpoints: latency-svc-vfj5w [104.894766ms]
Sep 26 11:10:29.155: INFO: Created: latency-svc-mnlnt
Sep 26 11:10:29.159: INFO: Got endpoints: latency-svc-mnlnt [102.540767ms]
Sep 26 11:10:29.161: INFO: Created: latency-svc-c6xbf
Sep 26 11:10:29.166: INFO: Created: latency-svc-nkqh4
Sep 26 11:10:29.168: INFO: Got endpoints: latency-svc-c6xbf [102.947176ms]
Sep 26 11:10:29.173: INFO: Got endpoints: latency-svc-nkqh4 [97.918285ms]
Sep 26 11:10:29.178: INFO: Created: latency-svc-g929j
Sep 26 11:10:29.181: INFO: Created: latency-svc-ds9gw
Sep 26 11:10:29.184: INFO: Got endpoints: latency-svc-g929j [101.933171ms]
Sep 26 11:10:29.186: INFO: Got endpoints: latency-svc-ds9gw [98.072282ms]
Sep 26 11:10:29.188: INFO: Created: latency-svc-ws745
Sep 26 11:10:29.193: INFO: Got endpoints: latency-svc-ws745 [98.608608ms]
Sep 26 11:10:29.196: INFO: Created: latency-svc-hhh86
Sep 26 11:10:29.201: INFO: Created: latency-svc-vlw52
Sep 26 11:10:29.202: INFO: Got endpoints: latency-svc-hhh86 [100.207172ms]
Sep 26 11:10:29.207: INFO: Created: latency-svc-9zmjq
Sep 26 11:10:29.211: INFO: Got endpoints: latency-svc-vlw52 [97.890056ms]
Sep 26 11:10:29.212: INFO: Got endpoints: latency-svc-9zmjq [97.5898ms]
Sep 26 11:10:29.216: INFO: Created: latency-svc-sdfbr
Sep 26 11:10:29.221: INFO: Created: latency-svc-z5j95
Sep 26 11:10:29.222: INFO: Got endpoints: latency-svc-sdfbr [99.193749ms]
Sep 26 11:10:29.228: INFO: Created: latency-svc-5ffcv
Sep 26 11:10:29.228: INFO: Got endpoints: latency-svc-z5j95 [98.271056ms]
Sep 26 11:10:29.233: INFO: Got endpoints: latency-svc-5ffcv [99.713646ms]
Sep 26 11:10:29.235: INFO: Created: latency-svc-k952x
Sep 26 11:10:29.239: INFO: Created: latency-svc-bj28r
Sep 26 11:10:29.240: INFO: Got endpoints: latency-svc-k952x [96.691652ms]
Sep 26 11:10:29.247: INFO: Got endpoints: latency-svc-bj28r [99.275366ms]
Sep 26 11:10:29.248: INFO: Created: latency-svc-74s87
Sep 26 11:10:29.251: INFO: Got endpoints: latency-svc-74s87 [97.488079ms]
Sep 26 11:10:29.256: INFO: Created: latency-svc-vbwj7
Sep 26 11:10:29.258: INFO: Created: latency-svc-gg4b7
Sep 26 11:10:29.262: INFO: Got endpoints: latency-svc-vbwj7 [102.975593ms]
Sep 26 11:10:29.264: INFO: Got endpoints: latency-svc-gg4b7 [96.206289ms]
Sep 26 11:10:29.265: INFO: Created: latency-svc-w6g4c
Sep 26 11:10:29.270: INFO: Got endpoints: latency-svc-w6g4c [97.191377ms]
Sep 26 11:10:29.272: INFO: Created: latency-svc-r4dth
Sep 26 11:10:29.276: INFO: Got endpoints: latency-svc-r4dth [91.352761ms]
Sep 26 11:10:29.278: INFO: Created: latency-svc-rwd9p
Sep 26 11:10:29.284: INFO: Created: latency-svc-p5rmw
Sep 26 11:10:29.287: INFO: Got endpoints: latency-svc-p5rmw [94.050906ms]
Sep 26 11:10:29.289: INFO: Got endpoints: latency-svc-rwd9p [103.22515ms]
Sep 26 11:10:29.289: INFO: Created: latency-svc-2xwq6
Sep 26 11:10:29.295: INFO: Got endpoints: latency-svc-2xwq6 [93.163603ms]
Sep 26 11:10:29.297: INFO: Created: latency-svc-sx2sf
Sep 26 11:10:29.303: INFO: Got endpoints: latency-svc-sx2sf [91.97648ms]
Sep 26 11:10:29.303: INFO: Created: latency-svc-bnbm5
Sep 26 11:10:29.308: INFO: Created: latency-svc-r5vqq
Sep 26 11:10:29.309: INFO: Got endpoints: latency-svc-bnbm5 [97.52411ms]
Sep 26 11:10:29.313: INFO: Got endpoints: latency-svc-r5vqq [91.429718ms]
Sep 26 11:10:29.315: INFO: Created: latency-svc-wwtw8
Sep 26 11:10:29.319: INFO: Got endpoints: latency-svc-wwtw8 [16.072279ms]
Sep 26 11:10:29.327: INFO: Created: latency-svc-gfm52
Sep 26 11:10:29.327: INFO: Got endpoints: latency-svc-gfm52 [99.41374ms]
Sep 26 11:10:29.331: INFO: Created: latency-svc-4v755
Sep 26 11:10:29.336: INFO: Got endpoints: latency-svc-4v755 [102.50189ms]
Sep 26 11:10:29.338: INFO: Created: latency-svc-g5vcw
Sep 26 11:10:29.343: INFO: Got endpoints: latency-svc-g5vcw [102.361079ms]
Sep 26 11:10:29.347: INFO: Created: latency-svc-8m5nd
Sep 26 11:10:29.352: INFO: Created: latency-svc-sz6fx
Sep 26 11:10:29.352: INFO: Got endpoints: latency-svc-8m5nd [104.940809ms]
Sep 26 11:10:29.357: INFO: Created: latency-svc-t7p2f
Sep 26 11:10:29.358: INFO: Got endpoints: latency-svc-sz6fx [106.779753ms]
Sep 26 11:10:29.362: INFO: Got endpoints: latency-svc-t7p2f [100.309949ms]
Sep 26 11:10:29.365: INFO: Created: latency-svc-pq7hr
Sep 26 11:10:29.368: INFO: Got endpoints: latency-svc-pq7hr [103.582595ms]
Sep 26 11:10:29.369: INFO: Created: latency-svc-js9nq
Sep 26 11:10:29.374: INFO: Got endpoints: latency-svc-js9nq [104.056228ms]
Sep 26 11:10:29.376: INFO: Created: latency-svc-mgmqv
Sep 26 11:10:29.382: INFO: Got endpoints: latency-svc-mgmqv [105.85442ms]
Sep 26 11:10:29.383: INFO: Created: latency-svc-6qb88
Sep 26 11:10:29.390: INFO: Created: latency-svc-5p66g
Sep 26 11:10:29.392: INFO: Got endpoints: latency-svc-6qb88 [105.171051ms]
Sep 26 11:10:29.393: INFO: Created: latency-svc-5wzp6
Sep 26 11:10:29.396: INFO: Got endpoints: latency-svc-5p66g [107.298865ms]
Sep 26 11:10:29.400: INFO: Created: latency-svc-wv92h
Sep 26 11:10:29.400: INFO: Got endpoints: latency-svc-5wzp6 [104.919111ms]
Sep 26 11:10:29.406: INFO: Got endpoints: latency-svc-wv92h [96.776041ms]
Sep 26 11:10:29.408: INFO: Created: latency-svc-99k8p
Sep 26 11:10:29.412: INFO: Got endpoints: latency-svc-99k8p [98.933596ms]
Sep 26 11:10:29.414: INFO: Created: latency-svc-h4ckv
Sep 26 11:10:29.419: INFO: Created: latency-svc-z5qsr
Sep 26 11:10:29.421: INFO: Got endpoints: latency-svc-h4ckv [101.99402ms]
Sep 26 11:10:29.424: INFO: Got endpoints: latency-svc-z5qsr [96.834687ms]
Sep 26 11:10:29.429: INFO: Created: latency-svc-2cbd5
Sep 26 11:10:29.433: INFO: Created: latency-svc-l958q
Sep 26 11:10:29.433: INFO: Got endpoints: latency-svc-2cbd5 [97.542777ms]
Sep 26 11:10:29.438: INFO: Got endpoints: latency-svc-l958q [94.612326ms]
Sep 26 11:10:29.439: INFO: Created: latency-svc-knnqh
Sep 26 11:10:29.444: INFO: Got endpoints: latency-svc-knnqh [91.811727ms]
Sep 26 11:10:29.444: INFO: Created: latency-svc-cv489
Sep 26 11:10:29.448: INFO: Got endpoints: latency-svc-cv489 [89.905686ms]
Sep 26 11:10:29.452: INFO: Created: latency-svc-cgbsb
Sep 26 11:10:29.457: INFO: Created: latency-svc-6xlgg
Sep 26 11:10:29.459: INFO: Got endpoints: latency-svc-cgbsb [97.127732ms]
Sep 26 11:10:29.463: INFO: Got endpoints: latency-svc-6xlgg [94.555578ms]
Sep 26 11:10:29.463: INFO: Created: latency-svc-kt5lv
Sep 26 11:10:29.468: INFO: Got endpoints: latency-svc-kt5lv [93.834076ms]
Sep 26 11:10:29.470: INFO: Created: latency-svc-fwlhp
Sep 26 11:10:29.476: INFO: Got endpoints: latency-svc-fwlhp [94.198736ms]
Sep 26 11:10:29.478: INFO: Created: latency-svc-582n7
Sep 26 11:10:29.483: INFO: Got endpoints: latency-svc-582n7 [90.900649ms]
Sep 26 11:10:29.483: INFO: Created: latency-svc-96v64
Sep 26 11:10:29.487: INFO: Got endpoints: latency-svc-96v64 [91.055684ms]
Sep 26 11:10:29.489: INFO: Created: latency-svc-jwvgj
Sep 26 11:10:29.494: INFO: Created: latency-svc-v5hbq
Sep 26 11:10:29.498: INFO: Got endpoints: latency-svc-v5hbq [91.71099ms]
Sep 26 11:10:29.499: INFO: Got endpoints: latency-svc-jwvgj [98.591119ms]
Sep 26 11:10:29.501: INFO: Created: latency-svc-hfh2d
Sep 26 11:10:29.505: INFO: Got endpoints: latency-svc-hfh2d [93.210588ms]
Sep 26 11:10:29.507: INFO: Created: latency-svc-8t8zz
Sep 26 11:10:29.512: INFO: Created: latency-svc-v5672
Sep 26 11:10:29.512: INFO: Got endpoints: latency-svc-8t8zz [91.405341ms]
Sep 26 11:10:29.516: INFO: Got endpoints: latency-svc-v5672 [92.064068ms]
Sep 26 11:10:29.517: INFO: Created: latency-svc-4mlhk
Sep 26 11:10:29.522: INFO: Got endpoints: latency-svc-4mlhk [88.8276ms]
Sep 26 11:10:29.522: INFO: Created: latency-svc-h4cpm
Sep 26 11:10:29.528: INFO: Got endpoints: latency-svc-h4cpm [90.66409ms]
Sep 26 11:10:29.528: INFO: Created: latency-svc-vqmzf
Sep 26 11:10:29.534: INFO: Got endpoints: latency-svc-vqmzf [90.056979ms]
Sep 26 11:10:29.534: INFO: Created: latency-svc-d49xm
Sep 26 11:10:29.539: INFO: Got endpoints: latency-svc-d49xm [90.443709ms]
Sep 26 11:10:29.541: INFO: Created: latency-svc-8wg77
Sep 26 11:10:29.546: INFO: Got endpoints: latency-svc-8wg77 [87.037349ms]
Sep 26 11:10:29.548: INFO: Created: latency-svc-d8ccp
Sep 26 11:10:29.551: INFO: Created: latency-svc-lns8w
Sep 26 11:10:29.553: INFO: Got endpoints: latency-svc-d8ccp [90.238305ms]
Sep 26 11:10:29.557: INFO: Got endpoints: latency-svc-lns8w [88.470714ms]
Sep 26 11:10:29.557: INFO: Created: latency-svc-k64sz
Sep 26 11:10:29.563: INFO: Created: latency-svc-v8rv5
Sep 26 11:10:29.563: INFO: Got endpoints: latency-svc-k64sz [87.377356ms]
Sep 26 11:10:29.567: INFO: Got endpoints: latency-svc-v8rv5 [83.822162ms]
Sep 26 11:10:29.570: INFO: Created: latency-svc-g7smn
Sep 26 11:10:29.573: INFO: Got endpoints: latency-svc-g7smn [85.883196ms]
Sep 26 11:10:29.575: INFO: Created: latency-svc-8jnq7
Sep 26 11:10:29.580: INFO: Got endpoints: latency-svc-8jnq7 [82.550412ms]
Sep 26 11:10:29.582: INFO: Created: latency-svc-vv5fs
Sep 26 11:10:29.586: INFO: Created: latency-svc-rg9tq
Sep 26 11:10:29.588: INFO: Got endpoints: latency-svc-vv5fs [89.188992ms]
Sep 26 11:10:29.592: INFO: Got endpoints: latency-svc-rg9tq [86.258006ms]
Sep 26 11:10:29.592: INFO: Created: latency-svc-cbc7b
Sep 26 11:10:29.596: INFO: Created: latency-svc-qsh6j
Sep 26 11:10:29.603: INFO: Got endpoints: latency-svc-qsh6j [86.622954ms]
Sep 26 11:10:29.605: INFO: Got endpoints: latency-svc-cbc7b [92.926236ms]
Sep 26 11:10:29.610: INFO: Created: latency-svc-tj4lk
Sep 26 11:10:29.613: INFO: Created: latency-svc-smrj2
Sep 26 11:10:29.617: INFO: Got endpoints: latency-svc-tj4lk [94.549927ms]
Sep 26 11:10:29.619: INFO: Got endpoints: latency-svc-smrj2 [91.051409ms]
Sep 26 11:10:29.623: INFO: Created: latency-svc-9skz7
Sep 26 11:10:29.627: INFO: Created: latency-svc-qv25s
Sep 26 11:10:29.629: INFO: Got endpoints: latency-svc-9skz7 [95.01198ms]
Sep 26 11:10:29.631: INFO: Got endpoints: latency-svc-qv25s [91.799324ms]
Sep 26 11:10:29.634: INFO: Created: latency-svc-gld57
Sep 26 11:10:29.638: INFO: Got endpoints: latency-svc-gld57 [91.215712ms]
Sep 26 11:10:29.640: INFO: Created: latency-svc-hsqv6
Sep 26 11:10:29.645: INFO: Got endpoints: latency-svc-hsqv6 [92.334443ms]
Sep 26 11:10:29.646: INFO: Created: latency-svc-gdt5g
Sep 26 11:10:29.653: INFO: Got endpoints: latency-svc-gdt5g [96.847918ms]
Sep 26 11:10:29.655: INFO: Created: latency-svc-cfcsb
Sep 26 11:10:29.659: INFO: Created: latency-svc-h6tgv
Sep 26 11:10:29.660: INFO: Got endpoints: latency-svc-cfcsb [97.04229ms]
Sep 26 11:10:29.664: INFO: Created: latency-svc-sptw2
Sep 26 11:10:29.664: INFO: Got endpoints: latency-svc-h6tgv [97.311381ms]
Sep 26 11:10:29.669: INFO: Got endpoints: latency-svc-sptw2 [95.314664ms]
Sep 26 11:10:29.671: INFO: Created: latency-svc-lg44g
Sep 26 11:10:29.676: INFO: Got endpoints: latency-svc-lg44g [95.532514ms]
Sep 26 11:10:29.678: INFO: Created: latency-svc-cphsg
Sep 26 11:10:29.682: INFO: Got endpoints: latency-svc-cphsg [93.526007ms]
Sep 26 11:10:29.683: INFO: Created: latency-svc-dl5vj
Sep 26 11:10:29.688: INFO: Got endpoints: latency-svc-dl5vj [95.94164ms]
Sep 26 11:10:29.690: INFO: Created: latency-svc-f2vrl
Sep 26 11:10:29.696: INFO: Got endpoints: latency-svc-f2vrl [92.473135ms]
Sep 26 11:10:29.696: INFO: Created: latency-svc-jjl4z
Sep 26 11:10:29.701: INFO: Got endpoints: latency-svc-jjl4z [95.354644ms]
Sep 26 11:10:29.704: INFO: Created: latency-svc-vfsck
Sep 26 11:10:29.708: INFO: Got endpoints: latency-svc-vfsck [91.4822ms]
Sep 26 11:10:29.709: INFO: Created: latency-svc-hdb4z
Sep 26 11:10:29.715: INFO: Got endpoints: latency-svc-hdb4z [95.947964ms]
Sep 26 11:10:29.716: INFO: Created: latency-svc-nvhtv
Sep 26 11:10:29.724: INFO: Created: latency-svc-2m6kb
Sep 26 11:10:29.724: INFO: Got endpoints: latency-svc-nvhtv [94.565341ms]
Sep 26 11:10:29.728: INFO: Got endpoints: latency-svc-2m6kb [96.939017ms]
Sep 26 11:10:29.728: INFO: Created: latency-svc-fjfcv
Sep 26 11:10:29.734: INFO: Created: latency-svc-msvmz
Sep 26 11:10:29.744: INFO: Got endpoints: latency-svc-fjfcv [106.647344ms]
Sep 26 11:10:29.748: INFO: Got endpoints: latency-svc-msvmz [102.419769ms]
Sep 26 11:10:29.751: INFO: Created: latency-svc-xf7st
Sep 26 11:10:29.755: INFO: Got endpoints: latency-svc-xf7st [101.901686ms]
Sep 26 11:10:29.756: INFO: Created: latency-svc-fg772
Sep 26 11:10:29.763: INFO: Got endpoints: latency-svc-fg772 [102.729747ms]
Sep 26 11:10:29.765: INFO: Created: latency-svc-fz8xz
Sep 26 11:10:29.771: INFO: Got endpoints: latency-svc-fz8xz [106.118225ms]
Sep 26 11:10:29.773: INFO: Created: latency-svc-6ml7c
Sep 26 11:10:29.778: INFO: Created: latency-svc-2mrxf
Sep 26 11:10:29.780: INFO: Got endpoints: latency-svc-6ml7c [111.660473ms]
Sep 26 11:10:29.792: INFO: Got endpoints: latency-svc-2mrxf [115.690348ms]
Sep 26 11:10:29.797: INFO: Created: latency-svc-jsz5c
Sep 26 11:10:29.801: INFO: Created: latency-svc-48f7r
Sep 26 11:10:29.804: INFO: Got endpoints: latency-svc-jsz5c [122.301051ms]
Sep 26 11:10:29.810: INFO: Got endpoints: latency-svc-48f7r [122.212503ms]
Sep 26 11:10:29.813: INFO: Created: latency-svc-vxbvt
Sep 26 11:10:29.819: INFO: Got endpoints: latency-svc-vxbvt [123.496604ms]
Sep 26 11:10:29.824: INFO: Created: latency-svc-qtxr6
Sep 26 11:10:29.828: INFO: Created: latency-svc-hv84g
Sep 26 11:10:29.834: INFO: Got endpoints: latency-svc-qtxr6 [133.055629ms]
Sep 26 11:10:29.834: INFO: Created: latency-svc-qd9ks
Sep 26 11:10:29.836: INFO: Got endpoints: latency-svc-hv84g [127.622753ms]
Sep 26 11:10:29.841: INFO: Created: latency-svc-5p7fn
Sep 26 11:10:29.845: INFO: Got endpoints: latency-svc-qd9ks [129.95178ms]
Sep 26 11:10:29.852: INFO: Created: latency-svc-tvtpb
Sep 26 11:10:29.853: INFO: Got endpoints: latency-svc-5p7fn [129.774932ms]
Sep 26 11:10:29.857: INFO: Got endpoints: latency-svc-tvtpb [129.797314ms]
Sep 26 11:10:29.860: INFO: Created: latency-svc-p2724
Sep 26 11:10:29.865: INFO: Got endpoints: latency-svc-p2724 [121.045592ms]
Sep 26 11:10:29.868: INFO: Created: latency-svc-nndpr
Sep 26 11:10:29.872: INFO: Created: latency-svc-qg7tj
Sep 26 11:10:29.882: INFO: Created: latency-svc-jwfch
Sep 26 11:10:29.887: INFO: Created: latency-svc-95428
Sep 26 11:10:29.891: INFO: Got endpoints: latency-svc-nndpr [143.495791ms]
Sep 26 11:10:29.893: INFO: Got endpoints: latency-svc-95428 [121.818719ms]
Sep 26 11:10:29.893: INFO: Got endpoints: latency-svc-qg7tj [137.238539ms]
Sep 26 11:10:29.895: INFO: Got endpoints: latency-svc-jwfch [131.563516ms]
Sep 26 11:10:29.896: INFO: Created: latency-svc-gcnzx
Sep 26 11:10:29.900: INFO: Created: latency-svc-4z5x8
Sep 26 11:10:29.903: INFO: Got endpoints: latency-svc-gcnzx [122.727476ms]
Sep 26 11:10:29.905: INFO: Got endpoints: latency-svc-4z5x8 [112.962623ms]
Sep 26 11:10:29.908: INFO: Created: latency-svc-7vm9j
Sep 26 11:10:29.911: INFO: Got endpoints: latency-svc-7vm9j [107.181603ms]
Sep 26 11:10:29.915: INFO: Created: latency-svc-xjbz2
Sep 26 11:10:29.918: INFO: Created: latency-svc-94qtn
Sep 26 11:10:29.921: INFO: Got endpoints: latency-svc-xjbz2 [110.794301ms]
Sep 26 11:10:29.922: INFO: Got endpoints: latency-svc-94qtn [102.796781ms]
Sep 26 11:10:29.925: INFO: Created: latency-svc-w5swr
Sep 26 11:10:29.929: INFO: Got endpoints: latency-svc-w5swr [95.471192ms]
Sep 26 11:10:29.932: INFO: Created: latency-svc-82848
Sep 26 11:10:29.936: INFO: Created: latency-svc-v558c
Sep 26 11:10:29.938: INFO: Got endpoints: latency-svc-82848 [101.798735ms]
Sep 26 11:10:29.941: INFO: Got endpoints: latency-svc-v558c [95.34462ms]
Sep 26 11:10:29.943: INFO: Created: latency-svc-7ltrh
Sep 26 11:10:29.947: INFO: Got endpoints: latency-svc-7ltrh [93.892322ms]
Sep 26 11:10:29.949: INFO: Created: latency-svc-vpxpf
Sep 26 11:10:29.954: INFO: Created: latency-svc-4dlml
Sep 26 11:10:29.954: INFO: Got endpoints: latency-svc-vpxpf [97.004812ms]
Sep 26 11:10:29.960: INFO: Created: latency-svc-l6fbr
Sep 26 11:10:29.961: INFO: Got endpoints: latency-svc-4dlml [95.087212ms]
Sep 26 11:10:29.965: INFO: Created: latency-svc-4nwxs
Sep 26 11:10:29.969: INFO: Got endpoints: latency-svc-l6fbr [77.322934ms]
Sep 26 11:10:29.970: INFO: Got endpoints: latency-svc-4nwxs [77.75967ms]
Sep 26 11:10:29.973: INFO: Created: latency-svc-tcwxw
Sep 26 11:10:29.978: INFO: Got endpoints: latency-svc-tcwxw [85.382001ms]
Sep 26 11:10:29.982: INFO: Created: latency-svc-r7dvb
Sep 26 11:10:29.987: INFO: Got endpoints: latency-svc-r7dvb [92.040002ms]
Sep 26 11:10:29.988: INFO: Created: latency-svc-sh457
Sep 26 11:10:29.994: INFO: Created: latency-svc-dcxgq
Sep 26 11:10:29.994: INFO: Got endpoints: latency-svc-sh457 [91.069698ms]
Sep 26 11:10:30.000: INFO: Got endpoints: latency-svc-dcxgq [95.143542ms]
Sep 26 11:10:30.001: INFO: Created: latency-svc-9pm8r
Sep 26 11:10:30.006: INFO: Got endpoints: latency-svc-9pm8r [94.094035ms]
Sep 26 11:10:30.008: INFO: Created: latency-svc-44x22
Sep 26 11:10:30.014: INFO: Created: latency-svc-95vk8
Sep 26 11:10:30.015: INFO: Got endpoints: latency-svc-44x22 [93.991777ms]
Sep 26 11:10:30.024: INFO: Created: latency-svc-97fwr
Sep 26 11:10:30.027: INFO: Created: latency-svc-r2rlt
Sep 26 11:10:30.030: INFO: Got endpoints: latency-svc-97fwr [100.228461ms]
Sep 26 11:10:30.034: INFO: Got endpoints: latency-svc-95vk8 [111.559951ms]
Sep 26 11:10:30.034: INFO: Created: latency-svc-4cds8
Sep 26 11:10:30.039: INFO: Got endpoints: latency-svc-4cds8 [98.483462ms]
Sep 26 11:10:30.040: INFO: Got endpoints: latency-svc-r2rlt [102.265358ms]
Sep 26 11:10:30.041: INFO: Created: latency-svc-t5gxp
Sep 26 11:10:30.047: INFO: Got endpoints: latency-svc-t5gxp [99.454672ms]
Sep 26 11:10:30.047: INFO: Created: latency-svc-slmcv
Sep 26 11:10:30.053: INFO: Got endpoints: latency-svc-slmcv [98.758112ms]
Sep 26 11:10:30.053: INFO: Created: latency-svc-v9pwt
Sep 26 11:10:30.062: INFO: Created: latency-svc-frb99
Sep 26 11:10:30.062: INFO: Got endpoints: latency-svc-v9pwt [101.83275ms]
Sep 26 11:10:30.064: INFO: Created: latency-svc-ksg8g
Sep 26 11:10:30.067: INFO: Got endpoints: latency-svc-frb99 [98.375166ms]
Sep 26 11:10:30.071: INFO: Got endpoints: latency-svc-ksg8g [100.612603ms]
Sep 26 11:10:30.072: INFO: Created: latency-svc-b2vzc
Sep 26 11:10:30.077: INFO: Got endpoints: latency-svc-b2vzc [98.885792ms]
Sep 26 11:10:30.080: INFO: Created: latency-svc-xvmr2
Sep 26 11:10:30.085: INFO: Got endpoints: latency-svc-xvmr2 [97.790236ms]
Sep 26 11:10:30.085: INFO: Created: latency-svc-xqqq7
Sep 26 11:10:30.093: INFO: Got endpoints: latency-svc-xqqq7 [98.710726ms]
Sep 26 11:10:30.095: INFO: Created: latency-svc-gm4fs
Sep 26 11:10:30.101: INFO: Got endpoints: latency-svc-gm4fs [100.82898ms]
Sep 26 11:10:30.102: INFO: Created: latency-svc-65x4w
Sep 26 11:10:30.107: INFO: Got endpoints: latency-svc-65x4w [101.027783ms]
Sep 26 11:10:30.110: INFO: Created: latency-svc-bb777
Sep 26 11:10:30.115: INFO: Created: latency-svc-l2v4b
Sep 26 11:10:30.115: INFO: Got endpoints: latency-svc-bb777 [100.061751ms]
Sep 26 11:10:30.127: INFO: Got endpoints: latency-svc-l2v4b [97.553779ms]
Sep 26 11:10:30.128: INFO: Created: latency-svc-b2jp4
Sep 26 11:10:30.136: INFO: Got endpoints: latency-svc-b2jp4 [102.570104ms]
Sep 26 11:10:30.139: INFO: Created: latency-svc-b688h
Sep 26 11:10:30.143: INFO: Got endpoints: latency-svc-b688h [104.206919ms]
Sep 26 11:10:30.144: INFO: Latencies: [16.072279ms 17.543013ms 33.598305ms 35.778815ms 44.565856ms 52.223973ms 60.580539ms 66.75465ms 76.202202ms 77.322934ms 77.75967ms 82.550412ms 83.822162ms 85.382001ms 85.883196ms 86.258006ms 86.622954ms 87.037349ms 87.049965ms 87.377356ms 88.470714ms 88.8276ms 89.188992ms 89.905686ms 90.056979ms 90.238305ms 90.443709ms 90.66409ms 90.900649ms 91.051409ms 91.055684ms 91.069698ms 91.215712ms 91.352761ms 91.405341ms 91.429718ms 91.4822ms 91.71099ms 91.799324ms 91.811727ms 91.97648ms 92.040002ms 92.064068ms 92.334443ms 92.473135ms 92.926236ms 93.163603ms 93.210588ms 93.526007ms 93.834076ms 93.892322ms 93.991777ms 94.050906ms 94.094035ms 94.198736ms 94.549927ms 94.555578ms 94.565341ms 94.612326ms 95.01198ms 95.087212ms 95.143542ms 95.157656ms 95.314664ms 95.34462ms 95.354644ms 95.471192ms 95.532514ms 95.94164ms 95.947964ms 96.206289ms 96.691652ms 96.776041ms 96.834687ms 96.847918ms 96.939017ms 97.004812ms 97.04229ms 97.127732ms 97.191377ms 97.311381ms 97.488079ms 97.52411ms 97.542777ms 97.553779ms 97.5898ms 97.790236ms 97.890056ms 97.918285ms 98.072282ms 98.271056ms 98.375166ms 98.483462ms 98.591119ms 98.608608ms 98.710726ms 98.758112ms 98.885792ms 98.933596ms 99.193749ms 99.275366ms 99.323061ms 99.41374ms 99.454672ms 99.713646ms 100.061751ms 100.207172ms 100.228461ms 100.309949ms 100.612603ms 100.82898ms 101.027783ms 101.233856ms 101.798735ms 101.83275ms 101.901686ms 101.933171ms 101.99402ms 102.20255ms 102.265358ms 102.361079ms 102.419769ms 102.50189ms 102.540767ms 102.570104ms 102.729747ms 102.796781ms 102.947176ms 102.975593ms 103.03152ms 103.170844ms 103.22515ms 103.245611ms 103.582595ms 104.056228ms 104.144943ms 104.206919ms 104.649484ms 104.774802ms 104.894766ms 104.919111ms 104.940809ms 105.171051ms 105.469014ms 105.85442ms 105.865051ms 105.890615ms 105.908165ms 106.101944ms 106.118225ms 106.647344ms 106.779753ms 106.869355ms 107.008192ms 107.181603ms 107.298865ms 107.601808ms 107.661378ms 107.662725ms 107.714679ms 108.151443ms 108.446936ms 108.635374ms 108.889801ms 108.969896ms 109.328698ms 109.355908ms 109.451127ms 109.638746ms 110.422337ms 110.467724ms 110.478857ms 110.794301ms 111.559951ms 111.660473ms 112.039488ms 112.693069ms 112.962623ms 114.070235ms 114.815631ms 115.660622ms 115.690348ms 116.229377ms 118.608596ms 119.892454ms 121.045592ms 121.818719ms 122.212503ms 122.301051ms 122.727476ms 123.496604ms 127.622753ms 127.653389ms 129.774932ms 129.797314ms 129.95178ms 131.563516ms 133.055629ms 137.238539ms 143.495791ms]
Sep 26 11:10:30.144: INFO: 50 %ile: 99.275366ms
Sep 26 11:10:30.144: INFO: 90 %ile: 115.660622ms
Sep 26 11:10:30.144: INFO: 99 %ile: 137.238539ms
Sep 26 11:10:30.144: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:30.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8299" for this suite.
•{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":109,"skipped":1974,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:30.160: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:34.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7503" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":110,"skipped":1983,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:34.109: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Sep 26 11:10:34.169: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Sep 26 11:10:34.189: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:34.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9869" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":111,"skipped":2007,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:34.222: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Sep 26 11:10:34.267: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:10:36.278: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Sep 26 11:10:36.298: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:10:38.303: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:10:40.307: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Sep 26 11:10:40.317: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 26 11:10:40.321: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 26 11:10:42.322: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 26 11:10:42.331: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:42.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3897" for this suite.

• [SLOW TEST:8.127 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":112,"skipped":2022,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:42.349: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-9920d8de-5221-4cee-a11b-63dc18836e67
STEP: Creating a pod to test consume configMaps
Sep 26 11:10:42.408: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5edaa52f-7efd-464c-a8ed-464c5dbdc013" in namespace "projected-231" to be "Succeeded or Failed"
Sep 26 11:10:42.412: INFO: Pod "pod-projected-configmaps-5edaa52f-7efd-464c-a8ed-464c5dbdc013": Phase="Pending", Reason="", readiness=false. Elapsed: 3.729943ms
Sep 26 11:10:44.422: INFO: Pod "pod-projected-configmaps-5edaa52f-7efd-464c-a8ed-464c5dbdc013": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013625792s
STEP: Saw pod success
Sep 26 11:10:44.422: INFO: Pod "pod-projected-configmaps-5edaa52f-7efd-464c-a8ed-464c5dbdc013" satisfied condition "Succeeded or Failed"
Sep 26 11:10:44.425: INFO: Trying to get logs from node 10.37.21.194 pod pod-projected-configmaps-5edaa52f-7efd-464c-a8ed-464c5dbdc013 container agnhost-container: <nil>
STEP: delete the pod
Sep 26 11:10:44.449: INFO: Waiting for pod pod-projected-configmaps-5edaa52f-7efd-464c-a8ed-464c5dbdc013 to disappear
Sep 26 11:10:44.452: INFO: Pod pod-projected-configmaps-5edaa52f-7efd-464c-a8ed-464c5dbdc013 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:44.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-231" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":2039,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:44.465: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-d317b48e-2bf5-4ae7-ad5f-ece3207f07c3
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:44.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5491" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":114,"skipped":2056,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:44.540: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 11:10:44.612: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87b2cc69-6e6b-47c8-bd3c-cfb29a09eddc" in namespace "projected-1394" to be "Succeeded or Failed"
Sep 26 11:10:44.614: INFO: Pod "downwardapi-volume-87b2cc69-6e6b-47c8-bd3c-cfb29a09eddc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.665725ms
Sep 26 11:10:46.624: INFO: Pod "downwardapi-volume-87b2cc69-6e6b-47c8-bd3c-cfb29a09eddc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011762789s
STEP: Saw pod success
Sep 26 11:10:46.624: INFO: Pod "downwardapi-volume-87b2cc69-6e6b-47c8-bd3c-cfb29a09eddc" satisfied condition "Succeeded or Failed"
Sep 26 11:10:46.627: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-87b2cc69-6e6b-47c8-bd3c-cfb29a09eddc container client-container: <nil>
STEP: delete the pod
Sep 26 11:10:46.647: INFO: Waiting for pod downwardapi-volume-87b2cc69-6e6b-47c8-bd3c-cfb29a09eddc to disappear
Sep 26 11:10:46.650: INFO: Pod downwardapi-volume-87b2cc69-6e6b-47c8-bd3c-cfb29a09eddc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:46.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1394" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":115,"skipped":2097,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:46.660: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Sep 26 11:10:46.704: INFO: namespace kubectl-9019
Sep 26 11:10:46.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-9019 create -f -'
Sep 26 11:10:46.924: INFO: stderr: ""
Sep 26 11:10:46.924: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 26 11:10:47.934: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 26 11:10:47.934: INFO: Found 0 / 1
Sep 26 11:10:48.929: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 26 11:10:48.929: INFO: Found 1 / 1
Sep 26 11:10:48.929: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 26 11:10:48.933: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 26 11:10:48.933: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 26 11:10:48.933: INFO: wait on agnhost-primary startup in kubectl-9019 
Sep 26 11:10:48.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-9019 logs agnhost-primary-lzvvg agnhost-primary'
Sep 26 11:10:49.030: INFO: stderr: ""
Sep 26 11:10:49.030: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep 26 11:10:49.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-9019 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep 26 11:10:49.130: INFO: stderr: ""
Sep 26 11:10:49.130: INFO: stdout: "service/rm2 exposed\n"
Sep 26 11:10:49.137: INFO: Service rm2 in namespace kubectl-9019 found.
STEP: exposing service
Sep 26 11:10:51.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-9019 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep 26 11:10:51.237: INFO: stderr: ""
Sep 26 11:10:51.237: INFO: stdout: "service/rm3 exposed\n"
Sep 26 11:10:51.242: INFO: Service rm3 in namespace kubectl-9019 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:53.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9019" for this suite.

• [SLOW TEST:6.631 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1233
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":116,"skipped":2098,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:53.291: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 26 11:10:53.346: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 26 11:10:53.356: INFO: Waiting for terminating namespaces to be deleted...
Sep 26 11:10:53.363: INFO: 
Logging pods the apiserver thinks is on node 10.37.21.193 before test
Sep 26 11:10:53.383: INFO: apiserver-10.37.21.193 from default started at 2021-09-26 10:28:41 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.383: INFO: 	Container apiserver ready: true, restart count 1
Sep 26 11:10:53.383: INFO: etcd-10.37.21.193 from default started at 2021-09-26 05:42:03 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.383: INFO: 	Container etcd ready: true, restart count 0
Sep 26 11:10:53.383: INFO: keepalived-10.37.21.193 from default started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.383: INFO: 	Container keepalived ready: true, restart count 0
Sep 26 11:10:53.383: INFO: kube-controllermanager-10.37.21.193 from default started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.383: INFO: 	Container kube-controllermanager ready: true, restart count 2
Sep 26 11:10:53.383: INFO: kube-proxy-10.37.21.193 from default started at 2021-09-23 12:14:20 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.383: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 26 11:10:53.383: INFO: kube-scheduler-10.37.21.193 from default started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.383: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 26 11:10:53.383: INFO: calico-node-rv4lt from kube-system started at 2021-09-26 10:29:33 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.383: INFO: 	Container calico-node ready: true, restart count 0
Sep 26 11:10:53.383: INFO: coredns-7474c6f7bf-f6lkf from kube-system started at 2021-09-26 10:29:48 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.383: INFO: 	Container coredns ready: true, restart count 0
Sep 26 11:10:53.383: INFO: nginx-10.37.21.193 from kube-system started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.383: INFO: 	Container nginx ready: true, restart count 0
Sep 26 11:10:53.383: INFO: sonobuoy-e2e-job-ffce8e36c7dd478d from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 11:10:53.383: INFO: 	Container e2e ready: true, restart count 0
Sep 26 11:10:53.383: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 11:10:53.383: INFO: sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-lp2ds from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 11:10:53.383: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 11:10:53.383: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 26 11:10:53.383: INFO: 
Logging pods the apiserver thinks is on node 10.37.21.194 before test
Sep 26 11:10:53.395: INFO: apiserver-10.37.21.194 from default started at 2021-09-26 10:28:51 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.395: INFO: 	Container apiserver ready: true, restart count 0
Sep 26 11:10:53.395: INFO: etcd-10.37.21.194 from default started at 2021-09-23 12:14:21 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.395: INFO: 	Container etcd ready: true, restart count 0
Sep 26 11:10:53.395: INFO: keepalived-10.37.21.194 from default started at 2021-09-23 12:14:21 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.395: INFO: 	Container keepalived ready: true, restart count 0
Sep 26 11:10:53.395: INFO: kube-controllermanager-10.37.21.194 from default started at 2021-09-26 05:42:32 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.395: INFO: 	Container kube-controllermanager ready: true, restart count 2
Sep 26 11:10:53.395: INFO: kube-proxy-10.37.21.194 from default started at 2021-09-26 05:42:32 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.395: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 26 11:10:53.395: INFO: kube-scheduler-10.37.21.194 from default started at 2021-09-26 05:42:32 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.395: INFO: 	Container kube-scheduler ready: true, restart count 1
Sep 26 11:10:53.395: INFO: calico-node-dd6nc from kube-system started at 2021-09-26 10:29:33 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.395: INFO: 	Container calico-node ready: true, restart count 0
Sep 26 11:10:53.395: INFO: coredns-7474c6f7bf-p9kv9 from kube-system started at 2021-09-26 11:07:58 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.395: INFO: 	Container coredns ready: true, restart count 0
Sep 26 11:10:53.395: INFO: nginx-10.37.21.194 from kube-system started at 2021-09-23 12:14:21 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.395: INFO: 	Container nginx ready: true, restart count 0
Sep 26 11:10:53.395: INFO: agnhost-primary-lzvvg from kubectl-9019 started at 2021-09-26 11:10:46 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.395: INFO: 	Container agnhost-primary ready: true, restart count 0
Sep 26 11:10:53.395: INFO: sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-46csd from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 11:10:53.395: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 11:10:53.395: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 26 11:10:53.395: INFO: 
Logging pods the apiserver thinks is on node 10.37.21.195 before test
Sep 26 11:10:53.403: INFO: apiserver-10.37.21.195 from default started at 2021-09-26 10:31:18 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.403: INFO: 	Container apiserver ready: true, restart count 0
Sep 26 11:10:53.403: INFO: etcd-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.403: INFO: 	Container etcd ready: true, restart count 1
Sep 26 11:10:53.403: INFO: keepalived-10.37.21.195 from default started at 2021-09-23 12:14:22 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.403: INFO: 	Container keepalived ready: true, restart count 1
Sep 26 11:10:53.403: INFO: kube-controllermanager-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.403: INFO: 	Container kube-controllermanager ready: true, restart count 4
Sep 26 11:10:53.403: INFO: kube-proxy-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.403: INFO: 	Container kube-proxy ready: true, restart count 2
Sep 26 11:10:53.403: INFO: kube-scheduler-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.403: INFO: 	Container kube-scheduler ready: true, restart count 1
Sep 26 11:10:53.403: INFO: calico-node-fhzl7 from kube-system started at 2021-09-26 10:29:33 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.403: INFO: 	Container calico-node ready: true, restart count 0
Sep 26 11:10:53.403: INFO: coredns-7474c6f7bf-plgdt from kube-system started at 2021-09-26 10:29:53 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.403: INFO: 	Container coredns ready: true, restart count 0
Sep 26 11:10:53.403: INFO: nginx-10.37.21.195 from kube-system started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.403: INFO: 	Container nginx ready: true, restart count 1
Sep 26 11:10:53.403: INFO: sonobuoy from sonobuoy started at 2021-09-26 10:33:22 +0000 UTC (1 container statuses recorded)
Sep 26 11:10:53.403: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 26 11:10:53.403: INFO: sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-76l7m from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 11:10:53.403: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 11:10:53.403: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node 10.37.21.193
STEP: verifying the node has the label node 10.37.21.194
STEP: verifying the node has the label node 10.37.21.195
Sep 26 11:10:53.468: INFO: Pod apiserver-10.37.21.193 requesting resource cpu=0m on Node 10.37.21.193
Sep 26 11:10:53.468: INFO: Pod apiserver-10.37.21.194 requesting resource cpu=0m on Node 10.37.21.194
Sep 26 11:10:53.468: INFO: Pod apiserver-10.37.21.195 requesting resource cpu=0m on Node 10.37.21.195
Sep 26 11:10:53.468: INFO: Pod etcd-10.37.21.193 requesting resource cpu=0m on Node 10.37.21.193
Sep 26 11:10:53.468: INFO: Pod etcd-10.37.21.194 requesting resource cpu=0m on Node 10.37.21.194
Sep 26 11:10:53.468: INFO: Pod etcd-10.37.21.195 requesting resource cpu=0m on Node 10.37.21.195
Sep 26 11:10:53.468: INFO: Pod keepalived-10.37.21.193 requesting resource cpu=0m on Node 10.37.21.193
Sep 26 11:10:53.468: INFO: Pod keepalived-10.37.21.194 requesting resource cpu=0m on Node 10.37.21.194
Sep 26 11:10:53.468: INFO: Pod keepalived-10.37.21.195 requesting resource cpu=0m on Node 10.37.21.195
Sep 26 11:10:53.468: INFO: Pod kube-controllermanager-10.37.21.193 requesting resource cpu=0m on Node 10.37.21.193
Sep 26 11:10:53.468: INFO: Pod kube-controllermanager-10.37.21.194 requesting resource cpu=0m on Node 10.37.21.194
Sep 26 11:10:53.468: INFO: Pod kube-controllermanager-10.37.21.195 requesting resource cpu=0m on Node 10.37.21.195
Sep 26 11:10:53.468: INFO: Pod kube-proxy-10.37.21.193 requesting resource cpu=0m on Node 10.37.21.193
Sep 26 11:10:53.468: INFO: Pod kube-proxy-10.37.21.194 requesting resource cpu=0m on Node 10.37.21.194
Sep 26 11:10:53.468: INFO: Pod kube-proxy-10.37.21.195 requesting resource cpu=0m on Node 10.37.21.195
Sep 26 11:10:53.468: INFO: Pod kube-scheduler-10.37.21.193 requesting resource cpu=0m on Node 10.37.21.193
Sep 26 11:10:53.468: INFO: Pod kube-scheduler-10.37.21.194 requesting resource cpu=0m on Node 10.37.21.194
Sep 26 11:10:53.468: INFO: Pod kube-scheduler-10.37.21.195 requesting resource cpu=0m on Node 10.37.21.195
Sep 26 11:10:53.468: INFO: Pod calico-node-dd6nc requesting resource cpu=250m on Node 10.37.21.194
Sep 26 11:10:53.468: INFO: Pod calico-node-fhzl7 requesting resource cpu=250m on Node 10.37.21.195
Sep 26 11:10:53.468: INFO: Pod calico-node-rv4lt requesting resource cpu=250m on Node 10.37.21.193
Sep 26 11:10:53.468: INFO: Pod coredns-7474c6f7bf-f6lkf requesting resource cpu=1000m on Node 10.37.21.193
Sep 26 11:10:53.468: INFO: Pod coredns-7474c6f7bf-p9kv9 requesting resource cpu=1000m on Node 10.37.21.194
Sep 26 11:10:53.468: INFO: Pod coredns-7474c6f7bf-plgdt requesting resource cpu=1000m on Node 10.37.21.195
Sep 26 11:10:53.468: INFO: Pod nginx-10.37.21.193 requesting resource cpu=0m on Node 10.37.21.193
Sep 26 11:10:53.468: INFO: Pod nginx-10.37.21.194 requesting resource cpu=0m on Node 10.37.21.194
Sep 26 11:10:53.468: INFO: Pod nginx-10.37.21.195 requesting resource cpu=0m on Node 10.37.21.195
Sep 26 11:10:53.468: INFO: Pod agnhost-primary-lzvvg requesting resource cpu=0m on Node 10.37.21.194
Sep 26 11:10:53.468: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.37.21.195
Sep 26 11:10:53.468: INFO: Pod sonobuoy-e2e-job-ffce8e36c7dd478d requesting resource cpu=0m on Node 10.37.21.193
Sep 26 11:10:53.468: INFO: Pod sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-46csd requesting resource cpu=0m on Node 10.37.21.194
Sep 26 11:10:53.468: INFO: Pod sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-76l7m requesting resource cpu=0m on Node 10.37.21.195
Sep 26 11:10:53.468: INFO: Pod sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-lp2ds requesting resource cpu=0m on Node 10.37.21.193
STEP: Starting Pods to consume most of the cluster CPU.
Sep 26 11:10:53.468: INFO: Creating a pod which consumes cpu=21525m on Node 10.37.21.193
Sep 26 11:10:53.479: INFO: Creating a pod which consumes cpu=21525m on Node 10.37.21.194
Sep 26 11:10:53.487: INFO: Creating a pod which consumes cpu=21525m on Node 10.37.21.195
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1cccc9c0-72f7-4d4d-b2b0-2d7ea134b1f5.16a85ac1d69a5f58], Reason = [Scheduled], Message = [Successfully assigned sched-pred-812/filler-pod-1cccc9c0-72f7-4d4d-b2b0-2d7ea134b1f5 to 10.37.21.194]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1cccc9c0-72f7-4d4d-b2b0-2d7ea134b1f5.16a85ac2131beebd], Reason = [Pulled], Message = [Container image "registry.dahuatech.com/cncf/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1cccc9c0-72f7-4d4d-b2b0-2d7ea134b1f5.16a85ac21638feb4], Reason = [Created], Message = [Created container filler-pod-1cccc9c0-72f7-4d4d-b2b0-2d7ea134b1f5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1cccc9c0-72f7-4d4d-b2b0-2d7ea134b1f5.16a85ac21e2355ba], Reason = [Started], Message = [Started container filler-pod-1cccc9c0-72f7-4d4d-b2b0-2d7ea134b1f5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b00ae86b-a453-4459-b1d5-142feebad836.16a85ac1d758db77], Reason = [Scheduled], Message = [Successfully assigned sched-pred-812/filler-pod-b00ae86b-a453-4459-b1d5-142feebad836 to 10.37.21.195]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b00ae86b-a453-4459-b1d5-142feebad836.16a85ac213a4f8b0], Reason = [Pulled], Message = [Container image "registry.dahuatech.com/cncf/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b00ae86b-a453-4459-b1d5-142feebad836.16a85ac217a934f6], Reason = [Created], Message = [Created container filler-pod-b00ae86b-a453-4459-b1d5-142feebad836]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b00ae86b-a453-4459-b1d5-142feebad836.16a85ac21faf5e23], Reason = [Started], Message = [Started container filler-pod-b00ae86b-a453-4459-b1d5-142feebad836]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c4a37dd1-3c69-43fa-b277-ed0cf79ffa9a.16a85ac1d68e190c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-812/filler-pod-c4a37dd1-3c69-43fa-b277-ed0cf79ffa9a to 10.37.21.193]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c4a37dd1-3c69-43fa-b277-ed0cf79ffa9a.16a85ac212e4ec04], Reason = [Pulled], Message = [Container image "registry.dahuatech.com/cncf/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c4a37dd1-3c69-43fa-b277-ed0cf79ffa9a.16a85ac2154abe13], Reason = [Created], Message = [Created container filler-pod-c4a37dd1-3c69-43fa-b277-ed0cf79ffa9a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c4a37dd1-3c69-43fa-b277-ed0cf79ffa9a.16a85ac21dad86e2], Reason = [Started], Message = [Started container filler-pod-c4a37dd1-3c69-43fa-b277-ed0cf79ffa9a]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16a85ac2c82aa541], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.37.21.193
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.37.21.194
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.37.21.195
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:10:58.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-812" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.328 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":117,"skipped":2110,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:10:58.620: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-e9595aed-9133-4972-99f2-375e5b211463
STEP: Creating a pod to test consume configMaps
Sep 26 11:10:58.689: INFO: Waiting up to 5m0s for pod "pod-configmaps-56284d99-d649-41ca-a4fa-bfacedc19136" in namespace "configmap-3542" to be "Succeeded or Failed"
Sep 26 11:10:58.694: INFO: Pod "pod-configmaps-56284d99-d649-41ca-a4fa-bfacedc19136": Phase="Pending", Reason="", readiness=false. Elapsed: 4.592969ms
Sep 26 11:11:00.706: INFO: Pod "pod-configmaps-56284d99-d649-41ca-a4fa-bfacedc19136": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016964442s
Sep 26 11:11:02.718: INFO: Pod "pod-configmaps-56284d99-d649-41ca-a4fa-bfacedc19136": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029074543s
STEP: Saw pod success
Sep 26 11:11:02.718: INFO: Pod "pod-configmaps-56284d99-d649-41ca-a4fa-bfacedc19136" satisfied condition "Succeeded or Failed"
Sep 26 11:11:02.721: INFO: Trying to get logs from node 10.37.21.195 pod pod-configmaps-56284d99-d649-41ca-a4fa-bfacedc19136 container agnhost-container: <nil>
STEP: delete the pod
Sep 26 11:11:02.750: INFO: Waiting for pod pod-configmaps-56284d99-d649-41ca-a4fa-bfacedc19136 to disappear
Sep 26 11:11:02.753: INFO: Pod pod-configmaps-56284d99-d649-41ca-a4fa-bfacedc19136 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:11:02.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3542" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":118,"skipped":2118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:11:02.765: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:11:15.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4790" for this suite.

• [SLOW TEST:13.178 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":119,"skipped":2163,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:11:15.943: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-d298320e-d1ae-4de9-b2b9-eb5a9b3f41aa
STEP: Creating the pod
Sep 26 11:11:16.026: INFO: The status of Pod pod-projected-configmaps-e2aaad16-c832-4054-9eb5-a3729968cc84 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:11:18.036: INFO: The status of Pod pod-projected-configmaps-e2aaad16-c832-4054-9eb5-a3729968cc84 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-d298320e-d1ae-4de9-b2b9-eb5a9b3f41aa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:11:20.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1836" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":120,"skipped":2183,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:11:20.097: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:11:20.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-7603" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":121,"skipped":2200,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:11:20.226: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2861
STEP: creating service affinity-clusterip-transition in namespace services-2861
STEP: creating replication controller affinity-clusterip-transition in namespace services-2861
I0926 11:11:20.290790      22 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-2861, replica count: 3
I0926 11:11:23.343509      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 26 11:11:23.359: INFO: Creating new exec pod
Sep 26 11:11:28.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-2861 exec execpod-affinity6g9bh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Sep 26 11:11:28.618: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep 26 11:11:28.618: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:11:28.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-2861 exec execpod-affinity6g9bh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.78.88 80'
Sep 26 11:11:28.833: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.78.88 80\nConnection to 10.254.78.88 80 port [tcp/http] succeeded!\n"
Sep 26 11:11:28.833: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:11:28.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-2861 exec execpod-affinity6g9bh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.78.88:80/ ; done'
Sep 26 11:11:29.159: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n"
Sep 26 11:11:29.159: INFO: stdout: "\naffinity-clusterip-transition-pbzlr\naffinity-clusterip-transition-pmfzx\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-pmfzx\naffinity-clusterip-transition-pmfzx\naffinity-clusterip-transition-pmfzx\naffinity-clusterip-transition-pmfzx\naffinity-clusterip-transition-pmfzx\naffinity-clusterip-transition-pbzlr\naffinity-clusterip-transition-pbzlr\naffinity-clusterip-transition-pmfzx\naffinity-clusterip-transition-pmfzx\naffinity-clusterip-transition-pmfzx\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-pbzlr\naffinity-clusterip-transition-pmfzx"
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pbzlr
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pmfzx
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pmfzx
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pmfzx
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pmfzx
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pmfzx
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pmfzx
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pbzlr
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pbzlr
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pmfzx
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pmfzx
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pmfzx
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pbzlr
Sep 26 11:11:29.159: INFO: Received response from host: affinity-clusterip-transition-pmfzx
Sep 26 11:11:29.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-2861 exec execpod-affinity6g9bh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.78.88:80/ ; done'
Sep 26 11:11:29.460: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.78.88:80/\n"
Sep 26 11:11:29.460: INFO: stdout: "\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4\naffinity-clusterip-transition-r67w4"
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Received response from host: affinity-clusterip-transition-r67w4
Sep 26 11:11:29.460: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2861, will wait for the garbage collector to delete the pods
Sep 26 11:11:29.536: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.360993ms
Sep 26 11:11:29.637: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.036932ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:11:31.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2861" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.443 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":122,"skipped":2218,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:11:31.670: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:11:31.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4740" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":123,"skipped":2262,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:11:31.742: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:11:33.807: INFO: Deleting pod "var-expansion-f64a208f-8a99-4009-a496-76c8b1f82000" in namespace "var-expansion-1696"
Sep 26 11:11:33.818: INFO: Wait up to 5m0s for pod "var-expansion-f64a208f-8a99-4009-a496-76c8b1f82000" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:11:37.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1696" for this suite.

• [SLOW TEST:6.107 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":124,"skipped":2269,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:11:37.849: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:11:38.365: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:11:41.396: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:11:41.403: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1957-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:11:49.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3943" for this suite.
STEP: Destroying namespace "webhook-3943-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.807 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":125,"skipped":2306,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:11:49.657: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:11:49.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9853" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":126,"skipped":2322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:11:49.823: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2710
STEP: creating service affinity-nodeport in namespace services-2710
STEP: creating replication controller affinity-nodeport in namespace services-2710
I0926 11:11:49.881007      22 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2710, replica count: 3
I0926 11:11:52.932520      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 26 11:11:52.947: INFO: Creating new exec pod
Sep 26 11:11:55.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-2710 exec execpod-affinityt4tmc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Sep 26 11:11:56.212: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep 26 11:11:56.212: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:11:56.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-2710 exec execpod-affinityt4tmc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.52.229 80'
Sep 26 11:11:56.431: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.52.229 80\nConnection to 10.254.52.229 80 port [tcp/http] succeeded!\n"
Sep 26 11:11:56.431: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:11:56.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-2710 exec execpod-affinityt4tmc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.193 31186'
Sep 26 11:11:56.628: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.193 31186\nConnection to 10.37.21.193 31186 port [tcp/*] succeeded!\n"
Sep 26 11:11:56.628: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:11:56.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-2710 exec execpod-affinityt4tmc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.195 31186'
Sep 26 11:11:56.830: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 10.37.21.195 31186\nConnection to 10.37.21.195 31186 port [tcp/*] succeeded!\n"
Sep 26 11:11:56.830: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:11:56.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-2710 exec execpod-affinityt4tmc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.37.21.193:31186/ ; done'
Sep 26 11:11:57.120: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:31186/\n"
Sep 26 11:11:57.120: INFO: stdout: "\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5\naffinity-nodeport-52wn5"
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Received response from host: affinity-nodeport-52wn5
Sep 26 11:11:57.120: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2710, will wait for the garbage collector to delete the pods
Sep 26 11:11:57.209: INFO: Deleting ReplicationController affinity-nodeport took: 8.159317ms
Sep 26 11:11:57.309: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.486335ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:11:59.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2710" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.516 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":127,"skipped":2348,"failed":0}
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:11:59.339: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Sep 26 11:12:39.603: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep 26 11:12:39.603: INFO: Deleting pod "simpletest.rc-4j42m" in namespace "gc-2478"
W0926 11:12:39.603760      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 26 11:12:39.627: INFO: Deleting pod "simpletest.rc-7v6f2" in namespace "gc-2478"
Sep 26 11:12:39.642: INFO: Deleting pod "simpletest.rc-9k46d" in namespace "gc-2478"
Sep 26 11:12:39.656: INFO: Deleting pod "simpletest.rc-cmnxq" in namespace "gc-2478"
Sep 26 11:12:39.668: INFO: Deleting pod "simpletest.rc-fhhwx" in namespace "gc-2478"
Sep 26 11:12:39.702: INFO: Deleting pod "simpletest.rc-l78nk" in namespace "gc-2478"
Sep 26 11:12:39.721: INFO: Deleting pod "simpletest.rc-plvtc" in namespace "gc-2478"
Sep 26 11:12:39.741: INFO: Deleting pod "simpletest.rc-thxq8" in namespace "gc-2478"
Sep 26 11:12:39.760: INFO: Deleting pod "simpletest.rc-tq99s" in namespace "gc-2478"
Sep 26 11:12:39.808: INFO: Deleting pod "simpletest.rc-vhbzg" in namespace "gc-2478"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:12:39.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2478" for this suite.

• [SLOW TEST:40.534 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":128,"skipped":2348,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:12:39.874: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-4839ba7b-e48d-450f-b340-4a9fbc8d4383
STEP: Creating a pod to test consume configMaps
Sep 26 11:12:40.024: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3edfc3c8-c5e7-446c-a82a-5ccc24c5ad7a" in namespace "projected-9014" to be "Succeeded or Failed"
Sep 26 11:12:40.031: INFO: Pod "pod-projected-configmaps-3edfc3c8-c5e7-446c-a82a-5ccc24c5ad7a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.696296ms
Sep 26 11:12:42.039: INFO: Pod "pod-projected-configmaps-3edfc3c8-c5e7-446c-a82a-5ccc24c5ad7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014504597s
STEP: Saw pod success
Sep 26 11:12:42.039: INFO: Pod "pod-projected-configmaps-3edfc3c8-c5e7-446c-a82a-5ccc24c5ad7a" satisfied condition "Succeeded or Failed"
Sep 26 11:12:42.042: INFO: Trying to get logs from node 10.37.21.195 pod pod-projected-configmaps-3edfc3c8-c5e7-446c-a82a-5ccc24c5ad7a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 26 11:12:42.065: INFO: Waiting for pod pod-projected-configmaps-3edfc3c8-c5e7-446c-a82a-5ccc24c5ad7a to disappear
Sep 26 11:12:42.067: INFO: Pod pod-projected-configmaps-3edfc3c8-c5e7-446c-a82a-5ccc24c5ad7a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:12:42.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9014" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":129,"skipped":2354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:12:42.076: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Sep 26 11:12:42.123: INFO: Waiting up to 5m0s for pod "client-containers-8f73c8cc-7d08-4058-b2c7-542f51584b7b" in namespace "containers-3825" to be "Succeeded or Failed"
Sep 26 11:12:42.126: INFO: Pod "client-containers-8f73c8cc-7d08-4058-b2c7-542f51584b7b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.155779ms
Sep 26 11:12:44.136: INFO: Pod "client-containers-8f73c8cc-7d08-4058-b2c7-542f51584b7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013240414s
STEP: Saw pod success
Sep 26 11:12:44.136: INFO: Pod "client-containers-8f73c8cc-7d08-4058-b2c7-542f51584b7b" satisfied condition "Succeeded or Failed"
Sep 26 11:12:44.141: INFO: Trying to get logs from node 10.37.21.194 pod client-containers-8f73c8cc-7d08-4058-b2c7-542f51584b7b container agnhost-container: <nil>
STEP: delete the pod
Sep 26 11:12:44.163: INFO: Waiting for pod client-containers-8f73c8cc-7d08-4058-b2c7-542f51584b7b to disappear
Sep 26 11:12:44.169: INFO: Pod client-containers-8f73c8cc-7d08-4058-b2c7-542f51584b7b no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:12:44.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3825" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":130,"skipped":2383,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:12:44.178: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:12:49.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1508" for this suite.

• [SLOW TEST:5.058 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":131,"skipped":2398,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:12:49.237: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:12:49.703: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:12:52.731: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:12:52.739: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3092-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:13:01.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7687" for this suite.
STEP: Destroying namespace "webhook-7687-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.860 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":132,"skipped":2414,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:13:01.098: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 26 11:13:01.250: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep 26 11:13:01.262: INFO: starting watch
STEP: patching
STEP: updating
Sep 26 11:13:01.287: INFO: waiting for watch events with expected annotations
Sep 26 11:13:01.287: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:13:01.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6595" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":133,"skipped":2425,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:13:01.358: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 11:13:01.412: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54291f3a-472d-48e4-a437-b158b03b347b" in namespace "downward-api-5932" to be "Succeeded or Failed"
Sep 26 11:13:01.416: INFO: Pod "downwardapi-volume-54291f3a-472d-48e4-a437-b158b03b347b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.538808ms
Sep 26 11:13:03.430: INFO: Pod "downwardapi-volume-54291f3a-472d-48e4-a437-b158b03b347b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018373294s
STEP: Saw pod success
Sep 26 11:13:03.430: INFO: Pod "downwardapi-volume-54291f3a-472d-48e4-a437-b158b03b347b" satisfied condition "Succeeded or Failed"
Sep 26 11:13:03.435: INFO: Trying to get logs from node 10.37.21.195 pod downwardapi-volume-54291f3a-472d-48e4-a437-b158b03b347b container client-container: <nil>
STEP: delete the pod
Sep 26 11:13:03.462: INFO: Waiting for pod downwardapi-volume-54291f3a-472d-48e4-a437-b158b03b347b to disappear
Sep 26 11:13:03.466: INFO: Pod downwardapi-volume-54291f3a-472d-48e4-a437-b158b03b347b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:13:03.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5932" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":134,"skipped":2429,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:13:03.478: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-9c122096-de4c-411b-9df0-676b2e725583
STEP: Creating a pod to test consume secrets
Sep 26 11:13:03.546: INFO: Waiting up to 5m0s for pod "pod-secrets-47b161f1-a403-41d5-8b7f-1956887ce3ab" in namespace "secrets-3251" to be "Succeeded or Failed"
Sep 26 11:13:03.550: INFO: Pod "pod-secrets-47b161f1-a403-41d5-8b7f-1956887ce3ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.760221ms
Sep 26 11:13:05.559: INFO: Pod "pod-secrets-47b161f1-a403-41d5-8b7f-1956887ce3ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012554889s
STEP: Saw pod success
Sep 26 11:13:05.559: INFO: Pod "pod-secrets-47b161f1-a403-41d5-8b7f-1956887ce3ab" satisfied condition "Succeeded or Failed"
Sep 26 11:13:05.562: INFO: Trying to get logs from node 10.37.21.195 pod pod-secrets-47b161f1-a403-41d5-8b7f-1956887ce3ab container secret-volume-test: <nil>
STEP: delete the pod
Sep 26 11:13:05.582: INFO: Waiting for pod pod-secrets-47b161f1-a403-41d5-8b7f-1956887ce3ab to disappear
Sep 26 11:13:05.588: INFO: Pod pod-secrets-47b161f1-a403-41d5-8b7f-1956887ce3ab no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:13:05.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3251" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":135,"skipped":2463,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:13:05.605: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 26 11:13:05.662: INFO: Waiting up to 5m0s for pod "pod-7caab32f-370f-4f92-a736-d712e846f233" in namespace "emptydir-4348" to be "Succeeded or Failed"
Sep 26 11:13:05.666: INFO: Pod "pod-7caab32f-370f-4f92-a736-d712e846f233": Phase="Pending", Reason="", readiness=false. Elapsed: 3.80683ms
Sep 26 11:13:07.675: INFO: Pod "pod-7caab32f-370f-4f92-a736-d712e846f233": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012772757s
STEP: Saw pod success
Sep 26 11:13:07.675: INFO: Pod "pod-7caab32f-370f-4f92-a736-d712e846f233" satisfied condition "Succeeded or Failed"
Sep 26 11:13:07.679: INFO: Trying to get logs from node 10.37.21.194 pod pod-7caab32f-370f-4f92-a736-d712e846f233 container test-container: <nil>
STEP: delete the pod
Sep 26 11:13:07.698: INFO: Waiting for pod pod-7caab32f-370f-4f92-a736-d712e846f233 to disappear
Sep 26 11:13:07.704: INFO: Pod pod-7caab32f-370f-4f92-a736-d712e846f233 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:13:07.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4348" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":136,"skipped":2503,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:13:07.718: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 26 11:13:07.775: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 26 11:14:07.830: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:07.835: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:14:07.924: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Sep 26 11:14:07.928: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:07.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7509" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:07.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3616" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.306 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":137,"skipped":2524,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:08.024: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 26 11:14:08.078: INFO: Waiting up to 5m0s for pod "pod-a8fb073c-644d-4f0e-93f4-d40f902df1db" in namespace "emptydir-9204" to be "Succeeded or Failed"
Sep 26 11:14:08.081: INFO: Pod "pod-a8fb073c-644d-4f0e-93f4-d40f902df1db": Phase="Pending", Reason="", readiness=false. Elapsed: 3.700541ms
Sep 26 11:14:10.093: INFO: Pod "pod-a8fb073c-644d-4f0e-93f4-d40f902df1db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015530497s
STEP: Saw pod success
Sep 26 11:14:10.093: INFO: Pod "pod-a8fb073c-644d-4f0e-93f4-d40f902df1db" satisfied condition "Succeeded or Failed"
Sep 26 11:14:10.097: INFO: Trying to get logs from node 10.37.21.195 pod pod-a8fb073c-644d-4f0e-93f4-d40f902df1db container test-container: <nil>
STEP: delete the pod
Sep 26 11:14:10.126: INFO: Waiting for pod pod-a8fb073c-644d-4f0e-93f4-d40f902df1db to disappear
Sep 26 11:14:10.129: INFO: Pod pod-a8fb073c-644d-4f0e-93f4-d40f902df1db no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:10.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9204" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":138,"skipped":2528,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:10.141: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:14:10.220: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-45d804a5-9ee7-4032-8ed7-6ca984ed9793" in namespace "security-context-test-1132" to be "Succeeded or Failed"
Sep 26 11:14:10.225: INFO: Pod "busybox-readonly-false-45d804a5-9ee7-4032-8ed7-6ca984ed9793": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359783ms
Sep 26 11:14:12.234: INFO: Pod "busybox-readonly-false-45d804a5-9ee7-4032-8ed7-6ca984ed9793": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013727598s
Sep 26 11:14:12.234: INFO: Pod "busybox-readonly-false-45d804a5-9ee7-4032-8ed7-6ca984ed9793" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:12.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1132" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":139,"skipped":2575,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:12.248: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Sep 26 11:14:12.314: INFO: The status of Pod annotationupdate828aa9a3-bb9b-499f-8007-39b06b003b7f is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:14:14.325: INFO: The status of Pod annotationupdate828aa9a3-bb9b-499f-8007-39b06b003b7f is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:14:16.331: INFO: The status of Pod annotationupdate828aa9a3-bb9b-499f-8007-39b06b003b7f is Running (Ready = true)
Sep 26 11:14:16.860: INFO: Successfully updated pod "annotationupdate828aa9a3-bb9b-499f-8007-39b06b003b7f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:18.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7122" for this suite.

• [SLOW TEST:6.652 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":140,"skipped":2590,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:18.901: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:14:18.949: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: creating the pod
STEP: submitting the pod to kubernetes
Sep 26 11:14:18.963: INFO: The status of Pod pod-logs-websocket-914091ab-ca0d-4746-8f88-f5c49a78337c is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:14:20.970: INFO: The status of Pod pod-logs-websocket-914091ab-ca0d-4746-8f88-f5c49a78337c is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:20.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8865" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":141,"skipped":2601,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:21.000: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:21.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4886" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":142,"skipped":2605,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:21.054: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:38.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1118" for this suite.

• [SLOW TEST:17.150 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":143,"skipped":2615,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:38.205: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Sep 26 11:14:38.261: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-1530 proxy --unix-socket=/tmp/kubectl-proxy-unix842962161/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:38.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1530" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":144,"skipped":2624,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:38.323: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Sep 26 11:14:38.377: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 26 11:14:43.396: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Sep 26 11:14:43.402: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Sep 26 11:14:43.424: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Sep 26 11:14:43.427: INFO: Observed &ReplicaSet event: ADDED
Sep 26 11:14:43.427: INFO: Observed &ReplicaSet event: MODIFIED
Sep 26 11:14:43.427: INFO: Observed &ReplicaSet event: MODIFIED
Sep 26 11:14:43.427: INFO: Observed &ReplicaSet event: MODIFIED
Sep 26 11:14:43.427: INFO: Found replicaset test-rs in namespace replicaset-1743 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 26 11:14:43.427: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Sep 26 11:14:43.427: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep 26 11:14:43.441: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Sep 26 11:14:43.443: INFO: Observed &ReplicaSet event: ADDED
Sep 26 11:14:43.444: INFO: Observed &ReplicaSet event: MODIFIED
Sep 26 11:14:43.444: INFO: Observed &ReplicaSet event: MODIFIED
Sep 26 11:14:43.444: INFO: Observed &ReplicaSet event: MODIFIED
Sep 26 11:14:43.444: INFO: Observed replicaset test-rs in namespace replicaset-1743 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep 26 11:14:43.444: INFO: Observed &ReplicaSet event: MODIFIED
Sep 26 11:14:43.444: INFO: Found replicaset test-rs in namespace replicaset-1743 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Sep 26 11:14:43.444: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:43.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1743" for this suite.

• [SLOW TEST:5.136 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":145,"skipped":2632,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:43.459: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 11:14:43.524: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c38add06-aca9-45f7-ad92-88e77bab9848" in namespace "downward-api-9104" to be "Succeeded or Failed"
Sep 26 11:14:43.528: INFO: Pod "downwardapi-volume-c38add06-aca9-45f7-ad92-88e77bab9848": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019963ms
Sep 26 11:14:45.541: INFO: Pod "downwardapi-volume-c38add06-aca9-45f7-ad92-88e77bab9848": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01686568s
STEP: Saw pod success
Sep 26 11:14:45.541: INFO: Pod "downwardapi-volume-c38add06-aca9-45f7-ad92-88e77bab9848" satisfied condition "Succeeded or Failed"
Sep 26 11:14:45.545: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-c38add06-aca9-45f7-ad92-88e77bab9848 container client-container: <nil>
STEP: delete the pod
Sep 26 11:14:45.567: INFO: Waiting for pod downwardapi-volume-c38add06-aca9-45f7-ad92-88e77bab9848 to disappear
Sep 26 11:14:45.571: INFO: Pod downwardapi-volume-c38add06-aca9-45f7-ad92-88e77bab9848 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:45.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9104" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2644,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:45.582: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:45.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5621" for this suite.
STEP: Destroying namespace "nspatchtest-1f84972e-a722-4c88-8c7c-7d8177f21561-7384" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":147,"skipped":2645,"failed":0}

------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:45.692: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:51.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-182" for this suite.

• [SLOW TEST:6.145 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":148,"skipped":2645,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:51.837: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image registry.dahuatech.com/cncf/httpd:2.4.38-1
Sep 26 11:14:51.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-2434 run e2e-test-httpd-pod --image=registry.dahuatech.com/cncf/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep 26 11:14:51.973: INFO: stderr: ""
Sep 26 11:14:51.974: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Sep 26 11:14:51.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-2434 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.dahuatech.com/cncf/busybox:1.29-1"}]}} --dry-run=server'
Sep 26 11:14:52.141: INFO: stderr: ""
Sep 26 11:14:52.142: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.dahuatech.com/cncf/httpd:2.4.38-1
Sep 26 11:14:52.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-2434 delete pods e2e-test-httpd-pod'
Sep 26 11:14:54.797: INFO: stderr: ""
Sep 26 11:14:54.797: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:54.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2434" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":149,"skipped":2664,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:54.809: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-90a845a6-0b39-4e25-969d-e5f73479a297
STEP: Creating a pod to test consume secrets
Sep 26 11:14:54.869: INFO: Waiting up to 5m0s for pod "pod-secrets-51c1f9ec-abce-4c9d-a9d2-3ff6a267487c" in namespace "secrets-7556" to be "Succeeded or Failed"
Sep 26 11:14:54.872: INFO: Pod "pod-secrets-51c1f9ec-abce-4c9d-a9d2-3ff6a267487c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.137157ms
Sep 26 11:14:56.881: INFO: Pod "pod-secrets-51c1f9ec-abce-4c9d-a9d2-3ff6a267487c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01237503s
STEP: Saw pod success
Sep 26 11:14:56.881: INFO: Pod "pod-secrets-51c1f9ec-abce-4c9d-a9d2-3ff6a267487c" satisfied condition "Succeeded or Failed"
Sep 26 11:14:56.885: INFO: Trying to get logs from node 10.37.21.195 pod pod-secrets-51c1f9ec-abce-4c9d-a9d2-3ff6a267487c container secret-volume-test: <nil>
STEP: delete the pod
Sep 26 11:14:56.907: INFO: Waiting for pod pod-secrets-51c1f9ec-abce-4c9d-a9d2-3ff6a267487c to disappear
Sep 26 11:14:56.912: INFO: Pod pod-secrets-51c1f9ec-abce-4c9d-a9d2-3ff6a267487c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:56.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7556" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":150,"skipped":2669,"failed":0}
SSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:56.924: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:14:57.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1240" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":151,"skipped":2676,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:14:57.049: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep 26 11:15:03.136: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W0926 11:15:03.136003      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 26 11:15:03.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-920" for this suite.

• [SLOW TEST:6.101 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":152,"skipped":2682,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:15:03.150: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 11:15:03.260: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5326c1cf-a02f-45a4-976f-58025d5313d1" in namespace "downward-api-7298" to be "Succeeded or Failed"
Sep 26 11:15:03.269: INFO: Pod "downwardapi-volume-5326c1cf-a02f-45a4-976f-58025d5313d1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.811313ms
Sep 26 11:15:05.276: INFO: Pod "downwardapi-volume-5326c1cf-a02f-45a4-976f-58025d5313d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015914235s
STEP: Saw pod success
Sep 26 11:15:05.276: INFO: Pod "downwardapi-volume-5326c1cf-a02f-45a4-976f-58025d5313d1" satisfied condition "Succeeded or Failed"
Sep 26 11:15:05.278: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-5326c1cf-a02f-45a4-976f-58025d5313d1 container client-container: <nil>
STEP: delete the pod
Sep 26 11:15:05.298: INFO: Waiting for pod downwardapi-volume-5326c1cf-a02f-45a4-976f-58025d5313d1 to disappear
Sep 26 11:15:05.301: INFO: Pod downwardapi-volume-5326c1cf-a02f-45a4-976f-58025d5313d1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:15:05.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7298" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":153,"skipped":2689,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:15:05.311: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:15:05.381: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"6d6c0374-f3ae-483d-9a5f-586ffded31e9", Controller:(*bool)(0xc0082ddd32), BlockOwnerDeletion:(*bool)(0xc0082ddd33)}}
Sep 26 11:15:05.391: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"929f8461-1eb3-40f0-b865-4ac37411d80e", Controller:(*bool)(0xc007d45392), BlockOwnerDeletion:(*bool)(0xc007d45393)}}
Sep 26 11:15:05.399: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"81314182-9775-4457-a9ee-1f4f9ed74894", Controller:(*bool)(0xc0040c4b42), BlockOwnerDeletion:(*bool)(0xc0040c4b43)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:15:10.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-412" for this suite.

• [SLOW TEST:5.133 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":154,"skipped":2696,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:15:10.444: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Sep 26 11:15:10.501: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:15:12.511: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:15:14.513: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 26 11:15:15.545: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:15:16.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5528" for this suite.

• [SLOW TEST:6.138 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":155,"skipped":2698,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:15:16.582: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-3816
Sep 26 11:15:16.641: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:15:18.651: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Sep 26 11:15:18.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-3816 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep 26 11:15:18.900: INFO: rc: 7
Sep 26 11:15:18.917: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 26 11:15:18.921: INFO: Pod kube-proxy-mode-detector no longer exists
Sep 26 11:15:18.921: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-3816 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-3816
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3816
I0926 11:15:18.946324      22 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3816, replica count: 3
I0926 11:15:21.998239      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 26 11:15:22.011: INFO: Creating new exec pod
Sep 26 11:15:27.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-3816 exec execpod-affinity47b5c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Sep 26 11:15:27.269: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Sep 26 11:15:27.269: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:15:27.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-3816 exec execpod-affinity47b5c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.136.126 80'
Sep 26 11:15:27.517: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.136.126 80\nConnection to 10.254.136.126 80 port [tcp/http] succeeded!\n"
Sep 26 11:15:27.517: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:15:27.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-3816 exec execpod-affinity47b5c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.194 32469'
Sep 26 11:15:27.746: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.194 32469\nConnection to 10.37.21.194 32469 port [tcp/*] succeeded!\n"
Sep 26 11:15:27.746: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:15:27.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-3816 exec execpod-affinity47b5c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.195 32469'
Sep 26 11:15:27.968: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.195 32469\nConnection to 10.37.21.195 32469 port [tcp/*] succeeded!\n"
Sep 26 11:15:27.968: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:15:27.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-3816 exec execpod-affinity47b5c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.37.21.193:32469/ ; done'
Sep 26 11:15:28.288: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n"
Sep 26 11:15:28.288: INFO: stdout: "\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj\naffinity-nodeport-timeout-wj2nj"
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Received response from host: affinity-nodeport-timeout-wj2nj
Sep 26 11:15:28.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-3816 exec execpod-affinity47b5c -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.37.21.193:32469/'
Sep 26 11:15:28.525: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n"
Sep 26 11:15:28.525: INFO: stdout: "affinity-nodeport-timeout-wj2nj"
Sep 26 11:15:48.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-3816 exec execpod-affinity47b5c -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.37.21.193:32469/'
Sep 26 11:15:48.775: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.37.21.193:32469/\n"
Sep 26 11:15:48.775: INFO: stdout: "affinity-nodeport-timeout-d8t29"
Sep 26 11:15:48.775: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3816, will wait for the garbage collector to delete the pods
Sep 26 11:15:48.858: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 8.415737ms
Sep 26 11:15:48.959: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.563692ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:15:51.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3816" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:34.515 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":156,"skipped":2699,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:15:51.097: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:15:53.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8530" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":157,"skipped":2712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:15:53.200: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:15:53.287: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-1339
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:15:59.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-382" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:15:59.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1339" for this suite.

• [SLOW TEST:6.263 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":158,"skipped":2744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:15:59.464: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:15:59.506: INFO: Got root ca configmap in namespace "svcaccounts-9371"
Sep 26 11:15:59.511: INFO: Deleted root ca configmap in namespace "svcaccounts-9371"
STEP: waiting for a new root ca configmap created
Sep 26 11:16:00.017: INFO: Recreated root ca configmap in namespace "svcaccounts-9371"
Sep 26 11:16:00.024: INFO: Updated root ca configmap in namespace "svcaccounts-9371"
STEP: waiting for the root ca configmap reconciled
Sep 26 11:16:00.533: INFO: Reconciled root ca configmap in namespace "svcaccounts-9371"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:00.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9371" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":159,"skipped":2776,"failed":0}
S
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:00.545: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:16:00.590: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: creating the pod
STEP: submitting the pod to kubernetes
Sep 26 11:16:00.600: INFO: The status of Pod pod-exec-websocket-dd182e64-f427-48d9-81e0-3c45e1b26780 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:16:02.612: INFO: The status of Pod pod-exec-websocket-dd182e64-f427-48d9-81e0-3c45e1b26780 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:02.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4277" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":160,"skipped":2777,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:02.766: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 11:16:02.828: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7896bdbd-c058-4e70-a2bf-ad794ea4b266" in namespace "downward-api-9635" to be "Succeeded or Failed"
Sep 26 11:16:02.831: INFO: Pod "downwardapi-volume-7896bdbd-c058-4e70-a2bf-ad794ea4b266": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468808ms
Sep 26 11:16:04.838: INFO: Pod "downwardapi-volume-7896bdbd-c058-4e70-a2bf-ad794ea4b266": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010680609s
STEP: Saw pod success
Sep 26 11:16:04.838: INFO: Pod "downwardapi-volume-7896bdbd-c058-4e70-a2bf-ad794ea4b266" satisfied condition "Succeeded or Failed"
Sep 26 11:16:04.842: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-7896bdbd-c058-4e70-a2bf-ad794ea4b266 container client-container: <nil>
STEP: delete the pod
Sep 26 11:16:04.859: INFO: Waiting for pod downwardapi-volume-7896bdbd-c058-4e70-a2bf-ad794ea4b266 to disappear
Sep 26 11:16:04.863: INFO: Pod downwardapi-volume-7896bdbd-c058-4e70-a2bf-ad794ea4b266 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:04.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9635" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":161,"skipped":2781,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:04.873: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 26 11:16:04.920: INFO: Waiting up to 5m0s for pod "pod-3ce325ff-2ca2-44ae-aea8-7d9f02a01108" in namespace "emptydir-9899" to be "Succeeded or Failed"
Sep 26 11:16:04.923: INFO: Pod "pod-3ce325ff-2ca2-44ae-aea8-7d9f02a01108": Phase="Pending", Reason="", readiness=false. Elapsed: 3.100822ms
Sep 26 11:16:06.938: INFO: Pod "pod-3ce325ff-2ca2-44ae-aea8-7d9f02a01108": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017905305s
STEP: Saw pod success
Sep 26 11:16:06.938: INFO: Pod "pod-3ce325ff-2ca2-44ae-aea8-7d9f02a01108" satisfied condition "Succeeded or Failed"
Sep 26 11:16:06.941: INFO: Trying to get logs from node 10.37.21.194 pod pod-3ce325ff-2ca2-44ae-aea8-7d9f02a01108 container test-container: <nil>
STEP: delete the pod
Sep 26 11:16:06.963: INFO: Waiting for pod pod-3ce325ff-2ca2-44ae-aea8-7d9f02a01108 to disappear
Sep 26 11:16:06.968: INFO: Pod pod-3ce325ff-2ca2-44ae-aea8-7d9f02a01108 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:06.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9899" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":162,"skipped":2802,"failed":0}
SS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:06.981: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:16:07.614: INFO: Checking APIGroup: apiregistration.k8s.io
Sep 26 11:16:07.616: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep 26 11:16:07.616: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Sep 26 11:16:07.616: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep 26 11:16:07.616: INFO: Checking APIGroup: apps
Sep 26 11:16:07.617: INFO: PreferredVersion.GroupVersion: apps/v1
Sep 26 11:16:07.617: INFO: Versions found [{apps/v1 v1}]
Sep 26 11:16:07.617: INFO: apps/v1 matches apps/v1
Sep 26 11:16:07.617: INFO: Checking APIGroup: events.k8s.io
Sep 26 11:16:07.618: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep 26 11:16:07.618: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Sep 26 11:16:07.618: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep 26 11:16:07.618: INFO: Checking APIGroup: authentication.k8s.io
Sep 26 11:16:07.619: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep 26 11:16:07.619: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Sep 26 11:16:07.619: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep 26 11:16:07.619: INFO: Checking APIGroup: authorization.k8s.io
Sep 26 11:16:07.620: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep 26 11:16:07.620: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Sep 26 11:16:07.620: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep 26 11:16:07.620: INFO: Checking APIGroup: autoscaling
Sep 26 11:16:07.621: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Sep 26 11:16:07.621: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Sep 26 11:16:07.621: INFO: autoscaling/v1 matches autoscaling/v1
Sep 26 11:16:07.621: INFO: Checking APIGroup: batch
Sep 26 11:16:07.622: INFO: PreferredVersion.GroupVersion: batch/v1
Sep 26 11:16:07.622: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Sep 26 11:16:07.622: INFO: batch/v1 matches batch/v1
Sep 26 11:16:07.622: INFO: Checking APIGroup: certificates.k8s.io
Sep 26 11:16:07.623: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep 26 11:16:07.623: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Sep 26 11:16:07.623: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep 26 11:16:07.623: INFO: Checking APIGroup: networking.k8s.io
Sep 26 11:16:07.623: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep 26 11:16:07.623: INFO: Versions found [{networking.k8s.io/v1 v1}]
Sep 26 11:16:07.623: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep 26 11:16:07.623: INFO: Checking APIGroup: policy
Sep 26 11:16:07.624: INFO: PreferredVersion.GroupVersion: policy/v1
Sep 26 11:16:07.624: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Sep 26 11:16:07.624: INFO: policy/v1 matches policy/v1
Sep 26 11:16:07.624: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep 26 11:16:07.625: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep 26 11:16:07.625: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Sep 26 11:16:07.625: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep 26 11:16:07.625: INFO: Checking APIGroup: storage.k8s.io
Sep 26 11:16:07.626: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep 26 11:16:07.626: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep 26 11:16:07.626: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep 26 11:16:07.626: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep 26 11:16:07.627: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep 26 11:16:07.627: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Sep 26 11:16:07.627: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep 26 11:16:07.627: INFO: Checking APIGroup: apiextensions.k8s.io
Sep 26 11:16:07.628: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep 26 11:16:07.628: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Sep 26 11:16:07.628: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep 26 11:16:07.628: INFO: Checking APIGroup: scheduling.k8s.io
Sep 26 11:16:07.629: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep 26 11:16:07.629: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Sep 26 11:16:07.629: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep 26 11:16:07.629: INFO: Checking APIGroup: coordination.k8s.io
Sep 26 11:16:07.629: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep 26 11:16:07.630: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Sep 26 11:16:07.630: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep 26 11:16:07.630: INFO: Checking APIGroup: node.k8s.io
Sep 26 11:16:07.630: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep 26 11:16:07.630: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Sep 26 11:16:07.630: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep 26 11:16:07.630: INFO: Checking APIGroup: discovery.k8s.io
Sep 26 11:16:07.631: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Sep 26 11:16:07.631: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Sep 26 11:16:07.631: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Sep 26 11:16:07.631: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep 26 11:16:07.632: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Sep 26 11:16:07.632: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Sep 26 11:16:07.632: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Sep 26 11:16:07.632: INFO: Checking APIGroup: crd.projectcalico.org
Sep 26 11:16:07.632: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Sep 26 11:16:07.633: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Sep 26 11:16:07.633: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Sep 26 11:16:07.633: INFO: Checking APIGroup: ipsection.ipsections
Sep 26 11:16:07.633: INFO: PreferredVersion.GroupVersion: ipsection.ipsections/v1
Sep 26 11:16:07.633: INFO: Versions found [{ipsection.ipsections/v1 v1}]
Sep 26 11:16:07.633: INFO: ipsection.ipsections/v1 matches ipsection.ipsections/v1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:07.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7104" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":163,"skipped":2804,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:07.643: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Sep 26 11:16:07.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-5328 api-versions'
Sep 26 11:16:07.754: INFO: stderr: ""
Sep 26 11:16:07.754: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nipsection.ipsections/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:07.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5328" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":164,"skipped":2822,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:07.768: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-0f0e7075-5cd9-4fb0-8328-2e51722a8107
STEP: Creating a pod to test consume secrets
Sep 26 11:16:07.829: INFO: Waiting up to 5m0s for pod "pod-secrets-283715f6-3b2f-4e9c-8ee9-428e4806ef18" in namespace "secrets-8472" to be "Succeeded or Failed"
Sep 26 11:16:07.833: INFO: Pod "pod-secrets-283715f6-3b2f-4e9c-8ee9-428e4806ef18": Phase="Pending", Reason="", readiness=false. Elapsed: 3.214872ms
Sep 26 11:16:09.846: INFO: Pod "pod-secrets-283715f6-3b2f-4e9c-8ee9-428e4806ef18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016833691s
Sep 26 11:16:11.851: INFO: Pod "pod-secrets-283715f6-3b2f-4e9c-8ee9-428e4806ef18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021041104s
STEP: Saw pod success
Sep 26 11:16:11.851: INFO: Pod "pod-secrets-283715f6-3b2f-4e9c-8ee9-428e4806ef18" satisfied condition "Succeeded or Failed"
Sep 26 11:16:11.854: INFO: Trying to get logs from node 10.37.21.194 pod pod-secrets-283715f6-3b2f-4e9c-8ee9-428e4806ef18 container secret-volume-test: <nil>
STEP: delete the pod
Sep 26 11:16:11.877: INFO: Waiting for pod pod-secrets-283715f6-3b2f-4e9c-8ee9-428e4806ef18 to disappear
Sep 26 11:16:11.881: INFO: Pod pod-secrets-283715f6-3b2f-4e9c-8ee9-428e4806ef18 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:11.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8472" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":165,"skipped":2876,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:11.890: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-24b695d5-bfa1-423a-b8df-2c747c9a0e2a
STEP: Creating secret with name secret-projected-all-test-volume-cc4d14a9-a673-49c9-8a36-86fd4969c195
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 26 11:16:11.951: INFO: Waiting up to 5m0s for pod "projected-volume-1be7d270-31fa-476f-938b-5932ba7e2861" in namespace "projected-9498" to be "Succeeded or Failed"
Sep 26 11:16:11.954: INFO: Pod "projected-volume-1be7d270-31fa-476f-938b-5932ba7e2861": Phase="Pending", Reason="", readiness=false. Elapsed: 2.940771ms
Sep 26 11:16:13.966: INFO: Pod "projected-volume-1be7d270-31fa-476f-938b-5932ba7e2861": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015840193s
Sep 26 11:16:15.977: INFO: Pod "projected-volume-1be7d270-31fa-476f-938b-5932ba7e2861": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026002898s
STEP: Saw pod success
Sep 26 11:16:15.977: INFO: Pod "projected-volume-1be7d270-31fa-476f-938b-5932ba7e2861" satisfied condition "Succeeded or Failed"
Sep 26 11:16:15.980: INFO: Trying to get logs from node 10.37.21.194 pod projected-volume-1be7d270-31fa-476f-938b-5932ba7e2861 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 26 11:16:16.000: INFO: Waiting for pod projected-volume-1be7d270-31fa-476f-938b-5932ba7e2861 to disappear
Sep 26 11:16:16.005: INFO: Pod projected-volume-1be7d270-31fa-476f-938b-5932ba7e2861 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:16.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9498" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":166,"skipped":2898,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:16.016: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6847.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6847.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 26 11:16:20.111: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6847/dns-test-687a01de-4141-4ede-ae7f-26e830b2c583: the server could not find the requested resource (get pods dns-test-687a01de-4141-4ede-ae7f-26e830b2c583)
Sep 26 11:16:20.123: INFO: Unable to read jessie_udp@PodARecord from pod dns-6847/dns-test-687a01de-4141-4ede-ae7f-26e830b2c583: the server could not find the requested resource (get pods dns-test-687a01de-4141-4ede-ae7f-26e830b2c583)
Sep 26 11:16:20.126: INFO: Lookups using dns-6847/dns-test-687a01de-4141-4ede-ae7f-26e830b2c583 failed for: [wheezy_udp@PodARecord jessie_udp@PodARecord]

Sep 26 11:16:25.161: INFO: DNS probes using dns-6847/dns-test-687a01de-4141-4ede-ae7f-26e830b2c583 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:25.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6847" for this suite.

• [SLOW TEST:9.175 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":167,"skipped":2912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:25.192: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Sep 26 11:16:25.262: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep 26 11:16:25.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4594 create -f -'
Sep 26 11:16:25.454: INFO: stderr: ""
Sep 26 11:16:25.454: INFO: stdout: "service/agnhost-replica created\n"
Sep 26 11:16:25.455: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep 26 11:16:25.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4594 create -f -'
Sep 26 11:16:25.636: INFO: stderr: ""
Sep 26 11:16:25.636: INFO: stdout: "service/agnhost-primary created\n"
Sep 26 11:16:25.636: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 26 11:16:25.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4594 create -f -'
Sep 26 11:16:25.841: INFO: stderr: ""
Sep 26 11:16:25.841: INFO: stdout: "service/frontend created\n"
Sep 26 11:16:25.841: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.dahuatech.com/cncf/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep 26 11:16:25.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4594 create -f -'
Sep 26 11:16:26.044: INFO: stderr: ""
Sep 26 11:16:26.044: INFO: stdout: "deployment.apps/frontend created\n"
Sep 26 11:16:26.044: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.dahuatech.com/cncf/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 26 11:16:26.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4594 create -f -'
Sep 26 11:16:26.258: INFO: stderr: ""
Sep 26 11:16:26.258: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep 26 11:16:26.259: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.dahuatech.com/cncf/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 26 11:16:26.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4594 create -f -'
Sep 26 11:16:26.461: INFO: stderr: ""
Sep 26 11:16:26.461: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Sep 26 11:16:26.461: INFO: Waiting for all frontend pods to be Running.
Sep 26 11:16:31.512: INFO: Waiting for frontend to serve content.
Sep 26 11:16:31.521: INFO: Trying to add a new entry to the guestbook.
Sep 26 11:16:31.529: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep 26 11:16:31.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4594 delete --grace-period=0 --force -f -'
Sep 26 11:16:31.632: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 26 11:16:31.632: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Sep 26 11:16:31.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4594 delete --grace-period=0 --force -f -'
Sep 26 11:16:31.724: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 26 11:16:31.724: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep 26 11:16:31.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4594 delete --grace-period=0 --force -f -'
Sep 26 11:16:31.826: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 26 11:16:31.826: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 26 11:16:31.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4594 delete --grace-period=0 --force -f -'
Sep 26 11:16:31.894: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 26 11:16:31.894: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 26 11:16:31.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4594 delete --grace-period=0 --force -f -'
Sep 26 11:16:31.989: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 26 11:16:31.990: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep 26 11:16:31.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4594 delete --grace-period=0 --force -f -'
Sep 26 11:16:32.088: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 26 11:16:32.088: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:32.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4594" for this suite.

• [SLOW TEST:6.913 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":168,"skipped":2944,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:32.104: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:43.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3543" for this suite.

• [SLOW TEST:11.161 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":169,"skipped":2960,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:43.266: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-4900/configmap-test-418a722e-5f82-4c10-86b5-4050ae043885
STEP: Creating a pod to test consume configMaps
Sep 26 11:16:43.379: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d1e9246-74fd-4e7c-807d-581a69d83499" in namespace "configmap-4900" to be "Succeeded or Failed"
Sep 26 11:16:43.390: INFO: Pod "pod-configmaps-2d1e9246-74fd-4e7c-807d-581a69d83499": Phase="Pending", Reason="", readiness=false. Elapsed: 11.06341ms
Sep 26 11:16:45.400: INFO: Pod "pod-configmaps-2d1e9246-74fd-4e7c-807d-581a69d83499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021081729s
STEP: Saw pod success
Sep 26 11:16:45.400: INFO: Pod "pod-configmaps-2d1e9246-74fd-4e7c-807d-581a69d83499" satisfied condition "Succeeded or Failed"
Sep 26 11:16:45.404: INFO: Trying to get logs from node 10.37.21.194 pod pod-configmaps-2d1e9246-74fd-4e7c-807d-581a69d83499 container env-test: <nil>
STEP: delete the pod
Sep 26 11:16:45.425: INFO: Waiting for pod pod-configmaps-2d1e9246-74fd-4e7c-807d-581a69d83499 to disappear
Sep 26 11:16:45.430: INFO: Pod pod-configmaps-2d1e9246-74fd-4e7c-807d-581a69d83499 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:45.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4900" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":170,"skipped":2985,"failed":0}

------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:45.440: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:16:51.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6210" for this suite.
STEP: Destroying namespace "nsdeletetest-7085" for this suite.
Sep 26 11:16:51.625: INFO: Namespace nsdeletetest-7085 was already deleted
STEP: Destroying namespace "nsdeletetest-2863" for this suite.

• [SLOW TEST:6.190 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":171,"skipped":2985,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:16:51.630: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep 26 11:17:01.776: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep 26 11:17:01.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-745qh" in namespace "gc-3392"
W0926 11:17:01.776146      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 26 11:17:01.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-7cm8f" in namespace "gc-3392"
Sep 26 11:17:01.811: INFO: Deleting pod "simpletest-rc-to-be-deleted-8znrr" in namespace "gc-3392"
Sep 26 11:17:01.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xv7h" in namespace "gc-3392"
Sep 26 11:17:01.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-dw8bg" in namespace "gc-3392"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:17:01.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3392" for this suite.

• [SLOW TEST:10.254 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":172,"skipped":3001,"failed":0}
SSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:17:01.884: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 26 11:17:03.203: INFO: starting watch
STEP: patching
STEP: updating
Sep 26 11:17:03.217: INFO: waiting for watch events with expected annotations
Sep 26 11:17:03.217: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:17:03.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5973" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":173,"skipped":3005,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:17:03.314: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:22:03.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-755" for this suite.

• [SLOW TEST:300.122 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":174,"skipped":3023,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:22:03.436: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-ea0d6650-1c18-4062-9896-f2f630d5bb3a
STEP: Creating the pod
Sep 26 11:22:03.552: INFO: The status of Pod pod-configmaps-a9bf2651-8116-4269-b589-45a27300991b is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:22:05.558: INFO: The status of Pod pod-configmaps-a9bf2651-8116-4269-b589-45a27300991b is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-ea0d6650-1c18-4062-9896-f2f630d5bb3a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:22:07.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6740" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3038,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:22:07.638: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 26 11:22:07.710: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8861  c3a512e4-1b4d-4d63-a861-116ac308d6bc 545230 0 2021-09-26 11:22:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-26 11:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 11:22:07.710: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8861  c3a512e4-1b4d-4d63-a861-116ac308d6bc 545230 0 2021-09-26 11:22:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-26 11:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 26 11:22:17.737: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8861  c3a512e4-1b4d-4d63-a861-116ac308d6bc 545272 0 2021-09-26 11:22:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-26 11:22:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 11:22:17.737: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8861  c3a512e4-1b4d-4d63-a861-116ac308d6bc 545272 0 2021-09-26 11:22:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-26 11:22:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 26 11:22:27.752: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8861  c3a512e4-1b4d-4d63-a861-116ac308d6bc 545289 0 2021-09-26 11:22:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-26 11:22:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 11:22:27.752: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8861  c3a512e4-1b4d-4d63-a861-116ac308d6bc 545289 0 2021-09-26 11:22:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-26 11:22:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 26 11:22:37.774: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8861  c3a512e4-1b4d-4d63-a861-116ac308d6bc 545306 0 2021-09-26 11:22:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-26 11:22:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 11:22:37.774: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8861  c3a512e4-1b4d-4d63-a861-116ac308d6bc 545306 0 2021-09-26 11:22:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-26 11:22:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 26 11:22:47.796: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8861  631a9826-cbea-4587-bc64-341132c866cf 545323 0 2021-09-26 11:22:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-26 11:22:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 11:22:47.797: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8861  631a9826-cbea-4587-bc64-341132c866cf 545323 0 2021-09-26 11:22:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-26 11:22:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 26 11:22:57.813: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8861  631a9826-cbea-4587-bc64-341132c866cf 545340 0 2021-09-26 11:22:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-26 11:22:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 11:22:57.813: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8861  631a9826-cbea-4587-bc64-341132c866cf 545340 0 2021-09-26 11:22:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-26 11:22:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:23:07.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8861" for this suite.

• [SLOW TEST:60.202 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":176,"skipped":3049,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:23:07.841: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3041
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3041
STEP: creating replication controller externalsvc in namespace services-3041
I0926 11:23:07.950770      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3041, replica count: 2
I0926 11:23:11.004895      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep 26 11:23:11.044: INFO: Creating new exec pod
Sep 26 11:23:13.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-3041 exec execpodn2nn4 -- /bin/sh -x -c nslookup nodeport-service.services-3041.svc.cluster.local'
Sep 26 11:23:13.396: INFO: stderr: "+ nslookup nodeport-service.services-3041.svc.cluster.local\n"
Sep 26 11:23:13.396: INFO: stdout: "Server:\t\t10.254.0.2\nAddress:\t10.254.0.2#53\n\nnodeport-service.services-3041.svc.cluster.local\tcanonical name = externalsvc.services-3041.svc.cluster.local.\nName:\texternalsvc.services-3041.svc.cluster.local\nAddress: 10.254.188.241\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3041, will wait for the garbage collector to delete the pods
Sep 26 11:23:13.463: INFO: Deleting ReplicationController externalsvc took: 9.63188ms
Sep 26 11:23:13.564: INFO: Terminating ReplicationController externalsvc pods took: 100.841261ms
Sep 26 11:23:16.090: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:23:16.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3041" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.276 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":177,"skipped":3067,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:23:16.116: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-10db11c3-8c19-4338-9a07-7d8222b62fac
STEP: Creating a pod to test consume configMaps
Sep 26 11:23:16.173: INFO: Waiting up to 5m0s for pod "pod-configmaps-649b8eb5-9cd7-4923-81c5-444ee5628f34" in namespace "configmap-8218" to be "Succeeded or Failed"
Sep 26 11:23:16.176: INFO: Pod "pod-configmaps-649b8eb5-9cd7-4923-81c5-444ee5628f34": Phase="Pending", Reason="", readiness=false. Elapsed: 3.236021ms
Sep 26 11:23:18.185: INFO: Pod "pod-configmaps-649b8eb5-9cd7-4923-81c5-444ee5628f34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011975797s
STEP: Saw pod success
Sep 26 11:23:18.185: INFO: Pod "pod-configmaps-649b8eb5-9cd7-4923-81c5-444ee5628f34" satisfied condition "Succeeded or Failed"
Sep 26 11:23:18.188: INFO: Trying to get logs from node 10.37.21.194 pod pod-configmaps-649b8eb5-9cd7-4923-81c5-444ee5628f34 container agnhost-container: <nil>
STEP: delete the pod
Sep 26 11:23:18.214: INFO: Waiting for pod pod-configmaps-649b8eb5-9cd7-4923-81c5-444ee5628f34 to disappear
Sep 26 11:23:18.217: INFO: Pod pod-configmaps-649b8eb5-9cd7-4923-81c5-444ee5628f34 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:23:18.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8218" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":178,"skipped":3068,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:23:18.228: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 26 11:23:18.287: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 26 11:23:18.294: INFO: Waiting for terminating namespaces to be deleted...
Sep 26 11:23:18.297: INFO: 
Logging pods the apiserver thinks is on node 10.37.21.193 before test
Sep 26 11:23:18.307: INFO: apiserver-10.37.21.193 from default started at 2021-09-26 10:28:41 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.307: INFO: 	Container apiserver ready: true, restart count 1
Sep 26 11:23:18.307: INFO: etcd-10.37.21.193 from default started at 2021-09-26 05:42:03 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.307: INFO: 	Container etcd ready: true, restart count 0
Sep 26 11:23:18.307: INFO: keepalived-10.37.21.193 from default started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.307: INFO: 	Container keepalived ready: true, restart count 0
Sep 26 11:23:18.307: INFO: kube-controllermanager-10.37.21.193 from default started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.307: INFO: 	Container kube-controllermanager ready: true, restart count 2
Sep 26 11:23:18.307: INFO: kube-proxy-10.37.21.193 from default started at 2021-09-23 12:14:20 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.307: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 26 11:23:18.307: INFO: kube-scheduler-10.37.21.193 from default started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.307: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 26 11:23:18.307: INFO: calico-node-rv4lt from kube-system started at 2021-09-26 10:29:33 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.307: INFO: 	Container calico-node ready: true, restart count 0
Sep 26 11:23:18.307: INFO: coredns-7474c6f7bf-f6lkf from kube-system started at 2021-09-26 10:29:48 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.307: INFO: 	Container coredns ready: true, restart count 0
Sep 26 11:23:18.308: INFO: nginx-10.37.21.193 from kube-system started at 2021-09-26 05:42:02 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.308: INFO: 	Container nginx ready: true, restart count 0
Sep 26 11:23:18.308: INFO: sonobuoy-e2e-job-ffce8e36c7dd478d from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 11:23:18.308: INFO: 	Container e2e ready: true, restart count 0
Sep 26 11:23:18.308: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 11:23:18.308: INFO: sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-lp2ds from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 11:23:18.308: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 11:23:18.308: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 26 11:23:18.308: INFO: 
Logging pods the apiserver thinks is on node 10.37.21.194 before test
Sep 26 11:23:18.315: INFO: apiserver-10.37.21.194 from default started at 2021-09-26 10:28:51 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.315: INFO: 	Container apiserver ready: true, restart count 0
Sep 26 11:23:18.315: INFO: etcd-10.37.21.194 from default started at 2021-09-23 12:14:21 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.315: INFO: 	Container etcd ready: true, restart count 0
Sep 26 11:23:18.315: INFO: keepalived-10.37.21.194 from default started at 2021-09-23 12:14:21 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.315: INFO: 	Container keepalived ready: true, restart count 0
Sep 26 11:23:18.315: INFO: kube-controllermanager-10.37.21.194 from default started at 2021-09-26 05:42:32 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.315: INFO: 	Container kube-controllermanager ready: true, restart count 2
Sep 26 11:23:18.315: INFO: kube-proxy-10.37.21.194 from default started at 2021-09-26 05:42:32 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.315: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 26 11:23:18.315: INFO: kube-scheduler-10.37.21.194 from default started at 2021-09-26 05:42:32 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.315: INFO: 	Container kube-scheduler ready: true, restart count 1
Sep 26 11:23:18.315: INFO: calico-node-dd6nc from kube-system started at 2021-09-26 10:29:33 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.315: INFO: 	Container calico-node ready: true, restart count 0
Sep 26 11:23:18.315: INFO: coredns-7474c6f7bf-p9kv9 from kube-system started at 2021-09-26 11:07:58 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.315: INFO: 	Container coredns ready: true, restart count 0
Sep 26 11:23:18.315: INFO: nginx-10.37.21.194 from kube-system started at 2021-09-23 12:14:21 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.315: INFO: 	Container nginx ready: true, restart count 0
Sep 26 11:23:18.315: INFO: execpodn2nn4 from services-3041 started at 2021-09-26 11:23:11 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.315: INFO: 	Container agnhost-container ready: true, restart count 0
Sep 26 11:23:18.315: INFO: sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-46csd from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 11:23:18.315: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 11:23:18.315: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 26 11:23:18.315: INFO: 
Logging pods the apiserver thinks is on node 10.37.21.195 before test
Sep 26 11:23:18.323: INFO: apiserver-10.37.21.195 from default started at 2021-09-26 10:31:18 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.323: INFO: 	Container apiserver ready: true, restart count 0
Sep 26 11:23:18.323: INFO: etcd-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.323: INFO: 	Container etcd ready: true, restart count 1
Sep 26 11:23:18.323: INFO: keepalived-10.37.21.195 from default started at 2021-09-23 12:14:22 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.323: INFO: 	Container keepalived ready: true, restart count 1
Sep 26 11:23:18.323: INFO: kube-controllermanager-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.323: INFO: 	Container kube-controllermanager ready: true, restart count 4
Sep 26 11:23:18.323: INFO: kube-proxy-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.323: INFO: 	Container kube-proxy ready: true, restart count 2
Sep 26 11:23:18.323: INFO: kube-scheduler-10.37.21.195 from default started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.323: INFO: 	Container kube-scheduler ready: true, restart count 1
Sep 26 11:23:18.323: INFO: calico-node-fhzl7 from kube-system started at 2021-09-26 10:29:33 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.323: INFO: 	Container calico-node ready: true, restart count 0
Sep 26 11:23:18.323: INFO: coredns-7474c6f7bf-plgdt from kube-system started at 2021-09-26 10:29:53 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.323: INFO: 	Container coredns ready: true, restart count 0
Sep 26 11:23:18.323: INFO: nginx-10.37.21.195 from kube-system started at 2021-09-26 05:42:55 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.323: INFO: 	Container nginx ready: true, restart count 1
Sep 26 11:23:18.323: INFO: sonobuoy from sonobuoy started at 2021-09-26 10:33:22 +0000 UTC (1 container statuses recorded)
Sep 26 11:23:18.323: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 26 11:23:18.323: INFO: sonobuoy-systemd-logs-daemon-set-01264a6c0e4740c4-76l7m from sonobuoy started at 2021-09-26 10:33:24 +0000 UTC (2 container statuses recorded)
Sep 26 11:23:18.323: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 26 11:23:18.323: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1e8ade93-09fe-4595-b9a0-137c1f3f20f6 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-1e8ade93-09fe-4595-b9a0-137c1f3f20f6 off the node 10.37.21.195
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1e8ade93-09fe-4595-b9a0-137c1f3f20f6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:23:24.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1338" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.206 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":179,"skipped":3080,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:23:24.434: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-0617daac-1957-4a0d-bb62-4a5c3a8ae9e5
STEP: Creating a pod to test consume configMaps
Sep 26 11:23:24.488: INFO: Waiting up to 5m0s for pod "pod-configmaps-e839cd4e-7c15-4fe4-bcbc-7121a9a97eb2" in namespace "configmap-2164" to be "Succeeded or Failed"
Sep 26 11:23:24.491: INFO: Pod "pod-configmaps-e839cd4e-7c15-4fe4-bcbc-7121a9a97eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.388168ms
Sep 26 11:23:26.496: INFO: Pod "pod-configmaps-e839cd4e-7c15-4fe4-bcbc-7121a9a97eb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008141446s
STEP: Saw pod success
Sep 26 11:23:26.496: INFO: Pod "pod-configmaps-e839cd4e-7c15-4fe4-bcbc-7121a9a97eb2" satisfied condition "Succeeded or Failed"
Sep 26 11:23:26.500: INFO: Trying to get logs from node 10.37.21.194 pod pod-configmaps-e839cd4e-7c15-4fe4-bcbc-7121a9a97eb2 container agnhost-container: <nil>
STEP: delete the pod
Sep 26 11:23:26.520: INFO: Waiting for pod pod-configmaps-e839cd4e-7c15-4fe4-bcbc-7121a9a97eb2 to disappear
Sep 26 11:23:26.524: INFO: Pod pod-configmaps-e839cd4e-7c15-4fe4-bcbc-7121a9a97eb2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:23:26.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2164" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":180,"skipped":3093,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:23:26.535: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-0194bfed-e5b3-4e48-b98c-24c92aedafbf
Sep 26 11:23:26.591: INFO: Pod name my-hostname-basic-0194bfed-e5b3-4e48-b98c-24c92aedafbf: Found 0 pods out of 1
Sep 26 11:23:31.611: INFO: Pod name my-hostname-basic-0194bfed-e5b3-4e48-b98c-24c92aedafbf: Found 1 pods out of 1
Sep 26 11:23:31.611: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0194bfed-e5b3-4e48-b98c-24c92aedafbf" are running
Sep 26 11:23:31.615: INFO: Pod "my-hostname-basic-0194bfed-e5b3-4e48-b98c-24c92aedafbf-ccr6p" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-26 11:23:26 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-26 11:23:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-26 11:23:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-26 11:23:26 +0000 UTC Reason: Message:}])
Sep 26 11:23:31.615: INFO: Trying to dial the pod
Sep 26 11:23:36.629: INFO: Controller my-hostname-basic-0194bfed-e5b3-4e48-b98c-24c92aedafbf: Got expected result from replica 1 [my-hostname-basic-0194bfed-e5b3-4e48-b98c-24c92aedafbf-ccr6p]: "my-hostname-basic-0194bfed-e5b3-4e48-b98c-24c92aedafbf-ccr6p", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:23:36.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1911" for this suite.

• [SLOW TEST:10.106 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":181,"skipped":3095,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:23:36.642: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 11:23:36.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1143126-1158-4292-973d-939522731eb5" in namespace "projected-1488" to be "Succeeded or Failed"
Sep 26 11:23:36.720: INFO: Pod "downwardapi-volume-d1143126-1158-4292-973d-939522731eb5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.047624ms
Sep 26 11:23:38.734: INFO: Pod "downwardapi-volume-d1143126-1158-4292-973d-939522731eb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01714022s
STEP: Saw pod success
Sep 26 11:23:38.734: INFO: Pod "downwardapi-volume-d1143126-1158-4292-973d-939522731eb5" satisfied condition "Succeeded or Failed"
Sep 26 11:23:38.738: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-d1143126-1158-4292-973d-939522731eb5 container client-container: <nil>
STEP: delete the pod
Sep 26 11:23:38.762: INFO: Waiting for pod downwardapi-volume-d1143126-1158-4292-973d-939522731eb5 to disappear
Sep 26 11:23:38.765: INFO: Pod downwardapi-volume-d1143126-1158-4292-973d-939522731eb5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:23:38.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1488" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3109,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:23:38.780: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Sep 26 11:23:38.826: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:23:42.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4457" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":183,"skipped":3118,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:23:43.002: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:23:43.058: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 26 11:23:45.108: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:23:46.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2300" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":184,"skipped":3135,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:23:46.133: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:23:46.184: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 26 11:23:51.197: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 26 11:23:51.197: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 26 11:23:53.214: INFO: Creating deployment "test-rollover-deployment"
Sep 26 11:23:53.233: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 26 11:23:55.249: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 26 11:23:55.255: INFO: Ensure that both replica sets have 1 created replica
Sep 26 11:23:55.261: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 26 11:23:55.274: INFO: Updating deployment test-rollover-deployment
Sep 26 11:23:55.274: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 26 11:23:57.285: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 26 11:23:57.291: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 26 11:23:57.298: INFO: all replica sets need to contain the pod-template-hash label
Sep 26 11:23:57.298: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252235, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-546dbc64f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 26 11:23:59.307: INFO: all replica sets need to contain the pod-template-hash label
Sep 26 11:23:59.307: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252237, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-546dbc64f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 26 11:24:01.313: INFO: all replica sets need to contain the pod-template-hash label
Sep 26 11:24:01.313: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252237, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-546dbc64f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 26 11:24:03.321: INFO: all replica sets need to contain the pod-template-hash label
Sep 26 11:24:03.321: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252237, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-546dbc64f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 26 11:24:05.317: INFO: all replica sets need to contain the pod-template-hash label
Sep 26 11:24:05.317: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252237, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-546dbc64f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 26 11:24:07.308: INFO: all replica sets need to contain the pod-template-hash label
Sep 26 11:24:07.308: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252237, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252233, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-546dbc64f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 26 11:24:09.314: INFO: 
Sep 26 11:24:09.314: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 26 11:24:09.325: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4302  2d1e9617-c0b5-4ff0-8027-6e108e80fed8 546024 2 2021-09-26 11:23:53 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-26 11:23:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 11:24:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost registry.dahuatech.com/cncf/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b99458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-26 11:23:53 +0000 UTC,LastTransitionTime:2021-09-26 11:23:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-546dbc64f4" has successfully progressed.,LastUpdateTime:2021-09-26 11:24:07 +0000 UTC,LastTransitionTime:2021-09-26 11:23:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 26 11:24:09.329: INFO: New ReplicaSet "test-rollover-deployment-546dbc64f4" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-546dbc64f4  deployment-4302  f7c43d96-d25c-433c-8905-1a8295c4d0c6 546014 2 2021-09-26 11:23:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:546dbc64f4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 2d1e9617-c0b5-4ff0-8027-6e108e80fed8 0xc002b99947 0xc002b99948}] []  [{kube-controller-manager Update apps/v1 2021-09-26 11:23:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d1e9617-c0b5-4ff0-8027-6e108e80fed8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 11:24:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 546dbc64f4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:546dbc64f4] map[] [] []  []} {[] [] [{agnhost registry.dahuatech.com/cncf/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b99a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 26 11:24:09.329: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 26 11:24:09.329: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4302  b0f2d47f-f58f-414a-baa5-665a1f9f5ff1 546023 2 2021-09-26 11:23:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 2d1e9617-c0b5-4ff0-8027-6e108e80fed8 0xc002b99817 0xc002b99818}] []  [{e2e.test Update apps/v1 2021-09-26 11:23:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 11:24:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d1e9617-c0b5-4ff0-8027-6e108e80fed8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2021-09-26 11:24:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd registry.dahuatech.com/cncf/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002b998d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 26 11:24:09.329: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-4302  ac09a5cd-b3bc-435c-b38c-cbacef200c0f 545979 2 2021-09-26 11:23:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 2d1e9617-c0b5-4ff0-8027-6e108e80fed8 0xc002b99a77 0xc002b99a78}] []  [{kube-controller-manager Update apps/v1 2021-09-26 11:23:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d1e9617-c0b5-4ff0-8027-6e108e80fed8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 11:23:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b99b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 26 11:24:09.333: INFO: Pod "test-rollover-deployment-546dbc64f4-2s7qs" is available:
&Pod{ObjectMeta:{test-rollover-deployment-546dbc64f4-2s7qs test-rollover-deployment-546dbc64f4- deployment-4302  861726a1-364d-4275-a179-d5c2fe47a4bd 545995 0 2021-09-26 11:23:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:546dbc64f4] map[cni.projectcalico.org/containerID:9801edf96f8c9af43be01003b608d82e49cc0f21f4527cc74d68a5161778c1aa cni.projectcalico.org/podIP:192.168.181.126/32 cni.projectcalico.org/podIPs:192.168.181.126/32] [{apps/v1 ReplicaSet test-rollover-deployment-546dbc64f4 f7c43d96-d25c-433c-8905-1a8295c4d0c6 0xc00302c087 0xc00302c088}] []  [{kube-controller-manager Update v1 2021-09-26 11:23:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c43d96-d25c-433c-8905-1a8295c4d0c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:23:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.181.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vzvwz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.dahuatech.com/cncf/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vzvwz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:23:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:23:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.195,PodIP:192.168.181.126,StartTime:2021-09-26 11:23:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 11:23:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/agnhost:2.32,ImageID:docker-pullable://registry.dahuatech.com/cncf/agnhost@sha256:e76d7fb8b95056e3909026f6024f19c496a908f7471ad3426302c4fb684885f9,ContainerID:docker://96680db868a2aa6458cb6bb58cb09e5b72298df00724f8b51b69dd25ab405c1e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.181.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:24:09.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4302" for this suite.

• [SLOW TEST:23.216 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":185,"skipped":3153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:24:09.350: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Sep 26 11:24:09.442: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:24:11.457: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:24:13.457: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:24:14.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1973" for this suite.

• [SLOW TEST:5.152 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":186,"skipped":3201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:24:14.503: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-52pw
STEP: Creating a pod to test atomic-volume-subpath
Sep 26 11:24:14.567: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-52pw" in namespace "subpath-6945" to be "Succeeded or Failed"
Sep 26 11:24:14.575: INFO: Pod "pod-subpath-test-secret-52pw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.11165ms
Sep 26 11:24:16.591: INFO: Pod "pod-subpath-test-secret-52pw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023846841s
Sep 26 11:24:18.606: INFO: Pod "pod-subpath-test-secret-52pw": Phase="Running", Reason="", readiness=true. Elapsed: 4.039102732s
Sep 26 11:24:20.612: INFO: Pod "pod-subpath-test-secret-52pw": Phase="Running", Reason="", readiness=true. Elapsed: 6.045282762s
Sep 26 11:24:22.625: INFO: Pod "pod-subpath-test-secret-52pw": Phase="Running", Reason="", readiness=true. Elapsed: 8.058217036s
Sep 26 11:24:24.635: INFO: Pod "pod-subpath-test-secret-52pw": Phase="Running", Reason="", readiness=true. Elapsed: 10.06819577s
Sep 26 11:24:26.647: INFO: Pod "pod-subpath-test-secret-52pw": Phase="Running", Reason="", readiness=true. Elapsed: 12.079796507s
Sep 26 11:24:28.663: INFO: Pod "pod-subpath-test-secret-52pw": Phase="Running", Reason="", readiness=true. Elapsed: 14.095940387s
Sep 26 11:24:30.680: INFO: Pod "pod-subpath-test-secret-52pw": Phase="Running", Reason="", readiness=true. Elapsed: 16.112717173s
Sep 26 11:24:32.692: INFO: Pod "pod-subpath-test-secret-52pw": Phase="Running", Reason="", readiness=true. Elapsed: 18.124822588s
Sep 26 11:24:34.711: INFO: Pod "pod-subpath-test-secret-52pw": Phase="Running", Reason="", readiness=true. Elapsed: 20.144106257s
Sep 26 11:24:36.725: INFO: Pod "pod-subpath-test-secret-52pw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.15815664s
STEP: Saw pod success
Sep 26 11:24:36.725: INFO: Pod "pod-subpath-test-secret-52pw" satisfied condition "Succeeded or Failed"
Sep 26 11:24:36.728: INFO: Trying to get logs from node 10.37.21.195 pod pod-subpath-test-secret-52pw container test-container-subpath-secret-52pw: <nil>
STEP: delete the pod
Sep 26 11:24:36.763: INFO: Waiting for pod pod-subpath-test-secret-52pw to disappear
Sep 26 11:24:36.766: INFO: Pod pod-subpath-test-secret-52pw no longer exists
STEP: Deleting pod pod-subpath-test-secret-52pw
Sep 26 11:24:36.766: INFO: Deleting pod "pod-subpath-test-secret-52pw" in namespace "subpath-6945"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:24:36.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6945" for this suite.

• [SLOW TEST:22.278 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":187,"skipped":3248,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:24:36.781: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Sep 26 11:24:36.846: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Sep 26 11:24:36.852: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 26 11:24:36.852: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Sep 26 11:24:36.861: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 26 11:24:36.861: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Sep 26 11:24:36.872: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep 26 11:24:36.872: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Sep 26 11:24:43.929: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:24:43.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9578" for this suite.

• [SLOW TEST:7.179 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":188,"skipped":3302,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:24:43.961: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-5736
STEP: creating replication controller nodeport-test in namespace services-5736
I0926 11:24:44.036870      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-5736, replica count: 2
Sep 26 11:24:47.088: INFO: Creating new exec pod
I0926 11:24:47.088445      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 26 11:24:50.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5736 exec execpodt2tcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Sep 26 11:24:50.394: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 26 11:24:50.394: INFO: stdout: "nodeport-test-dghdd"
Sep 26 11:24:50.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5736 exec execpodt2tcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.20.213 80'
Sep 26 11:24:50.615: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.20.213 80\nConnection to 10.254.20.213 80 port [tcp/http] succeeded!\n"
Sep 26 11:24:50.615: INFO: stdout: "nodeport-test-wsdb9"
Sep 26 11:24:50.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5736 exec execpodt2tcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.194 32352'
Sep 26 11:24:50.839: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.194 32352\nConnection to 10.37.21.194 32352 port [tcp/*] succeeded!\n"
Sep 26 11:24:50.839: INFO: stdout: ""
Sep 26 11:24:51.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5736 exec execpodt2tcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.194 32352'
Sep 26 11:24:52.058: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.194 32352\nConnection to 10.37.21.194 32352 port [tcp/*] succeeded!\n"
Sep 26 11:24:52.058: INFO: stdout: ""
Sep 26 11:24:52.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5736 exec execpodt2tcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.194 32352'
Sep 26 11:24:53.087: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.194 32352\nConnection to 10.37.21.194 32352 port [tcp/*] succeeded!\n"
Sep 26 11:24:53.087: INFO: stdout: "nodeport-test-dghdd"
Sep 26 11:24:53.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5736 exec execpodt2tcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.195 32352'
Sep 26 11:24:53.330: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.195 32352\nConnection to 10.37.21.195 32352 port [tcp/*] succeeded!\n"
Sep 26 11:24:53.330: INFO: stdout: ""
Sep 26 11:24:54.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5736 exec execpodt2tcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.195 32352'
Sep 26 11:24:54.552: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.195 32352\nConnection to 10.37.21.195 32352 port [tcp/*] succeeded!\n"
Sep 26 11:24:54.552: INFO: stdout: "nodeport-test-wsdb9"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:24:54.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5736" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.616 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":189,"skipped":3310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:24:54.578: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:24:54.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-891" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":190,"skipped":3375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:24:54.707: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-7a257ac3-a959-4375-abb3-8da15b04265e in namespace container-probe-1612
Sep 26 11:24:56.766: INFO: Started pod busybox-7a257ac3-a959-4375-abb3-8da15b04265e in namespace container-probe-1612
STEP: checking the pod's current state and verifying that restartCount is present
Sep 26 11:24:56.770: INFO: Initial restart count of pod busybox-7a257ac3-a959-4375-abb3-8da15b04265e is 0
Sep 26 11:25:47.093: INFO: Restart count of pod container-probe-1612/busybox-7a257ac3-a959-4375-abb3-8da15b04265e is now 1 (50.323051646s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:25:47.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1612" for this suite.

• [SLOW TEST:52.417 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3398,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:25:47.124: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:25:47.620: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:25:50.656: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:25:50.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4084" for this suite.
STEP: Destroying namespace "webhook-4084-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":192,"skipped":3406,"failed":0}
SS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:25:50.887: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:25:50.946: INFO: The status of Pod busybox-readonly-fs5347e285-8d0b-41db-bb52-257b6eeb0f81 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:25:52.956: INFO: The status of Pod busybox-readonly-fs5347e285-8d0b-41db-bb52-257b6eeb0f81 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:25:54.955: INFO: The status of Pod busybox-readonly-fs5347e285-8d0b-41db-bb52-257b6eeb0f81 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:25:54.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8135" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":193,"skipped":3408,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:25:54.994: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:25:55.699: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:25:58.730: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep 26 11:25:58.756: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:25:58.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9078" for this suite.
STEP: Destroying namespace "webhook-9078-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":194,"skipped":3421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:25:58.824: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-9555d2be-f079-4127-b0c8-9ec7af8316cd
STEP: Creating a pod to test consume secrets
Sep 26 11:25:58.880: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7c233934-12ec-411e-995d-0941e66b291f" in namespace "projected-6185" to be "Succeeded or Failed"
Sep 26 11:25:58.884: INFO: Pod "pod-projected-secrets-7c233934-12ec-411e-995d-0941e66b291f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.286914ms
Sep 26 11:26:00.894: INFO: Pod "pod-projected-secrets-7c233934-12ec-411e-995d-0941e66b291f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013961187s
STEP: Saw pod success
Sep 26 11:26:00.894: INFO: Pod "pod-projected-secrets-7c233934-12ec-411e-995d-0941e66b291f" satisfied condition "Succeeded or Failed"
Sep 26 11:26:00.898: INFO: Trying to get logs from node 10.37.21.194 pod pod-projected-secrets-7c233934-12ec-411e-995d-0941e66b291f container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 26 11:26:00.922: INFO: Waiting for pod pod-projected-secrets-7c233934-12ec-411e-995d-0941e66b291f to disappear
Sep 26 11:26:00.926: INFO: Pod pod-projected-secrets-7c233934-12ec-411e-995d-0941e66b291f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:26:00.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6185" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":195,"skipped":3470,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:26:00.937: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Sep 26 11:26:01.012: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:26:03.026: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.37.21.195 on the node which pod1 resides and expect scheduled
Sep 26 11:26:03.039: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:26:05.048: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.37.21.195 but use UDP protocol on the node which pod2 resides
Sep 26 11:26:05.058: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:26:07.068: INFO: The status of Pod pod3 is Running (Ready = true)
Sep 26 11:26:07.079: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:26:09.087: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Sep 26 11:26:09.090: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.37.21.195 http://127.0.0.1:54323/hostname] Namespace:hostport-5868 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:26:09.090: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.37.21.195, port: 54323
Sep 26 11:26:09.205: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.37.21.195:54323/hostname] Namespace:hostport-5868 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:26:09.205: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.37.21.195, port: 54323 UDP
Sep 26 11:26:09.344: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.37.21.195 54323] Namespace:hostport-5868 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:26:09.344: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:26:14.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-5868" for this suite.

• [SLOW TEST:13.628 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":196,"skipped":3491,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:26:14.565: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-4670
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Sep 26 11:26:14.641: INFO: Found 0 stateful pods, waiting for 3
Sep 26 11:26:24.659: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 26 11:26:24.659: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 26 11:26:24.659: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.dahuatech.com/cncf/httpd:2.4.38-1 to registry.dahuatech.com/cncf/httpd:2.4.39-1
Sep 26 11:26:24.697: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 26 11:26:34.760: INFO: Updating stateful set ss2
Sep 26 11:26:34.766: INFO: Waiting for Pod statefulset-4670/ss2-2 to have revision ss2-5bc95f5dff update revision ss2-54fb9b4d58
STEP: Restoring Pods to the correct revision when they are deleted
Sep 26 11:26:44.824: INFO: Found 1 stateful pods, waiting for 3
Sep 26 11:26:54.841: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 26 11:26:54.841: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 26 11:26:54.841: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 26 11:26:54.873: INFO: Updating stateful set ss2
Sep 26 11:26:54.881: INFO: Waiting for Pod statefulset-4670/ss2-1 to have revision ss2-5bc95f5dff update revision ss2-54fb9b4d58
Sep 26 11:27:04.935: INFO: Updating stateful set ss2
Sep 26 11:27:04.946: INFO: Waiting for StatefulSet statefulset-4670/ss2 to complete update
Sep 26 11:27:04.946: INFO: Waiting for Pod statefulset-4670/ss2-0 to have revision ss2-5bc95f5dff update revision ss2-54fb9b4d58
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Sep 26 11:27:14.965: INFO: Deleting all statefulset in ns statefulset-4670
Sep 26 11:27:14.968: INFO: Scaling statefulset ss2 to 0
Sep 26 11:27:24.994: INFO: Waiting for statefulset status.replicas updated to 0
Sep 26 11:27:24.998: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:27:25.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4670" for this suite.

• [SLOW TEST:70.464 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":197,"skipped":3504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:27:25.029: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 26 11:27:27.105: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:27:27.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1634" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":198,"skipped":3528,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:27:27.132: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:27:43.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6414" for this suite.

• [SLOW TEST:16.243 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":199,"skipped":3530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:27:43.376: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Sep 26 11:27:43.480: INFO: Waiting up to 5m0s for pod "security-context-37b9d40b-e352-40fd-8d91-1acd01dd1420" in namespace "security-context-6062" to be "Succeeded or Failed"
Sep 26 11:27:43.486: INFO: Pod "security-context-37b9d40b-e352-40fd-8d91-1acd01dd1420": Phase="Pending", Reason="", readiness=false. Elapsed: 5.699412ms
Sep 26 11:27:45.500: INFO: Pod "security-context-37b9d40b-e352-40fd-8d91-1acd01dd1420": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019894034s
Sep 26 11:27:47.505: INFO: Pod "security-context-37b9d40b-e352-40fd-8d91-1acd01dd1420": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024868291s
STEP: Saw pod success
Sep 26 11:27:47.505: INFO: Pod "security-context-37b9d40b-e352-40fd-8d91-1acd01dd1420" satisfied condition "Succeeded or Failed"
Sep 26 11:27:47.509: INFO: Trying to get logs from node 10.37.21.194 pod security-context-37b9d40b-e352-40fd-8d91-1acd01dd1420 container test-container: <nil>
STEP: delete the pod
Sep 26 11:27:47.548: INFO: Waiting for pod security-context-37b9d40b-e352-40fd-8d91-1acd01dd1420 to disappear
Sep 26 11:27:47.551: INFO: Pod security-context-37b9d40b-e352-40fd-8d91-1acd01dd1420 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:27:47.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6062" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":200,"skipped":3574,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:27:47.563: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Sep 26 11:27:47.641: INFO: The status of Pod annotationupdate7aa3799c-8a99-4354-a686-20ac3e0e77ec is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:27:49.650: INFO: The status of Pod annotationupdate7aa3799c-8a99-4354-a686-20ac3e0e77ec is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:27:51.659: INFO: The status of Pod annotationupdate7aa3799c-8a99-4354-a686-20ac3e0e77ec is Running (Ready = true)
Sep 26 11:27:52.206: INFO: Successfully updated pod "annotationupdate7aa3799c-8a99-4354-a686-20ac3e0e77ec"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:27:54.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8119" for this suite.

• [SLOW TEST:6.685 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":201,"skipped":3601,"failed":0}
S
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:27:54.249: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:28:02.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6090" for this suite.

• [SLOW TEST:8.072 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":202,"skipped":3602,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:28:02.321: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Sep 26 11:28:02.390: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:28:04.397: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:28:06.408: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Sep 26 11:28:06.426: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:28:08.436: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 26 11:28:08.439: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8860 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:08.439: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:08.551: INFO: Exec stderr: ""
Sep 26 11:28:08.551: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8860 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:08.551: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:08.688: INFO: Exec stderr: ""
Sep 26 11:28:08.688: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8860 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:08.688: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:08.804: INFO: Exec stderr: ""
Sep 26 11:28:08.804: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8860 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:08.804: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:08.941: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 26 11:28:08.941: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8860 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:08.941: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:09.089: INFO: Exec stderr: ""
Sep 26 11:28:09.089: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8860 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:09.089: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:09.225: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 26 11:28:09.225: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8860 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:09.226: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:09.348: INFO: Exec stderr: ""
Sep 26 11:28:09.349: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8860 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:09.349: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:09.505: INFO: Exec stderr: ""
Sep 26 11:28:09.505: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8860 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:09.505: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:09.634: INFO: Exec stderr: ""
Sep 26 11:28:09.634: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8860 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:09.634: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:09.781: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:28:09.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8860" for this suite.

• [SLOW TEST:7.480 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":203,"skipped":3613,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:28:09.801: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-945b6643-52bd-4fcc-ada0-ba55c13f831d
STEP: Creating a pod to test consume configMaps
Sep 26 11:28:09.862: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-248ce8c0-cbd2-4f35-8a83-4d8f85fe36ae" in namespace "projected-6445" to be "Succeeded or Failed"
Sep 26 11:28:09.866: INFO: Pod "pod-projected-configmaps-248ce8c0-cbd2-4f35-8a83-4d8f85fe36ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394727ms
Sep 26 11:28:11.874: INFO: Pod "pod-projected-configmaps-248ce8c0-cbd2-4f35-8a83-4d8f85fe36ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011933215s
STEP: Saw pod success
Sep 26 11:28:11.874: INFO: Pod "pod-projected-configmaps-248ce8c0-cbd2-4f35-8a83-4d8f85fe36ae" satisfied condition "Succeeded or Failed"
Sep 26 11:28:11.876: INFO: Trying to get logs from node 10.37.21.194 pod pod-projected-configmaps-248ce8c0-cbd2-4f35-8a83-4d8f85fe36ae container agnhost-container: <nil>
STEP: delete the pod
Sep 26 11:28:11.896: INFO: Waiting for pod pod-projected-configmaps-248ce8c0-cbd2-4f35-8a83-4d8f85fe36ae to disappear
Sep 26 11:28:11.900: INFO: Pod pod-projected-configmaps-248ce8c0-cbd2-4f35-8a83-4d8f85fe36ae no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:28:11.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6445" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":204,"skipped":3614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:28:11.908: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:28:23.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8085" for this suite.

• [SLOW TEST:11.222 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":205,"skipped":3646,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:28:23.131: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-4923
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 26 11:28:23.209: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 26 11:28:23.266: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:28:25.274: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:28:27.282: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:28:29.346: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:28:31.282: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:28:33.283: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:28:35.281: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:28:37.282: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:28:39.274: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:28:41.278: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 11:28:43.290: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 26 11:28:43.301: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 26 11:28:43.307: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 26 11:28:47.348: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep 26 11:28:47.348: INFO: Breadth first check of 192.168.84.100 on host 10.37.21.193...
Sep 26 11:28:47.351: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.50.141:9080/dial?request=hostname&protocol=http&host=192.168.84.100&port=8083&tries=1'] Namespace:pod-network-test-4923 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:47.351: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:47.486: INFO: Waiting for responses: map[]
Sep 26 11:28:47.486: INFO: reached 192.168.84.100 after 0/1 tries
Sep 26 11:28:47.486: INFO: Breadth first check of 192.168.50.147 on host 10.37.21.194...
Sep 26 11:28:47.490: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.50.141:9080/dial?request=hostname&protocol=http&host=192.168.50.147&port=8083&tries=1'] Namespace:pod-network-test-4923 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:47.490: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:47.638: INFO: Waiting for responses: map[]
Sep 26 11:28:47.638: INFO: reached 192.168.50.147 after 0/1 tries
Sep 26 11:28:47.638: INFO: Breadth first check of 192.168.181.85 on host 10.37.21.195...
Sep 26 11:28:47.644: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.50.141:9080/dial?request=hostname&protocol=http&host=192.168.181.85&port=8083&tries=1'] Namespace:pod-network-test-4923 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:28:47.644: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:28:47.799: INFO: Waiting for responses: map[]
Sep 26 11:28:47.799: INFO: reached 192.168.181.85 after 0/1 tries
Sep 26 11:28:47.799: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:28:47.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4923" for this suite.

• [SLOW TEST:24.684 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":206,"skipped":3670,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:28:47.816: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 26 11:28:47.883: INFO: Waiting up to 5m0s for pod "pod-0952f533-e8a5-4cc4-bdf7-80c9495d95ef" in namespace "emptydir-1446" to be "Succeeded or Failed"
Sep 26 11:28:47.886: INFO: Pod "pod-0952f533-e8a5-4cc4-bdf7-80c9495d95ef": Phase="Pending", Reason="", readiness=false. Elapsed: 3.513134ms
Sep 26 11:28:49.893: INFO: Pod "pod-0952f533-e8a5-4cc4-bdf7-80c9495d95ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010485309s
STEP: Saw pod success
Sep 26 11:28:49.893: INFO: Pod "pod-0952f533-e8a5-4cc4-bdf7-80c9495d95ef" satisfied condition "Succeeded or Failed"
Sep 26 11:28:49.897: INFO: Trying to get logs from node 10.37.21.195 pod pod-0952f533-e8a5-4cc4-bdf7-80c9495d95ef container test-container: <nil>
STEP: delete the pod
Sep 26 11:28:49.926: INFO: Waiting for pod pod-0952f533-e8a5-4cc4-bdf7-80c9495d95ef to disappear
Sep 26 11:28:49.929: INFO: Pod pod-0952f533-e8a5-4cc4-bdf7-80c9495d95ef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:28:49.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1446" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":207,"skipped":3687,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:28:49.939: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image registry.dahuatech.com/cncf/httpd:2.4.38-1
Sep 26 11:28:49.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-8991 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.dahuatech.com/cncf/httpd:2.4.38-1'
Sep 26 11:28:50.076: INFO: stderr: ""
Sep 26 11:28:50.076: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Sep 26 11:28:50.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-8991 delete pods e2e-test-httpd-pod'
Sep 26 11:28:52.955: INFO: stderr: ""
Sep 26 11:28:52.955: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:28:52.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8991" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":208,"skipped":3730,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:28:52.972: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Sep 26 11:28:53.022: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:28:55.032: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Sep 26 11:28:55.047: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:28:57.056: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 26 11:28:57.075: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 26 11:28:57.079: INFO: Pod pod-with-poststart-http-hook still exists
Sep 26 11:28:59.080: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 26 11:28:59.090: INFO: Pod pod-with-poststart-http-hook still exists
Sep 26 11:29:01.080: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 26 11:29:01.089: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:29:01.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9110" for this suite.

• [SLOW TEST:8.132 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":209,"skipped":3747,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:29:01.104: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Sep 26 11:29:01.168: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 26 11:29:06.183: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:29:06.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2466" for this suite.

• [SLOW TEST:5.113 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":210,"skipped":3754,"failed":0}
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:29:06.217: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Sep 26 11:29:06.266: INFO: Waiting up to 5m0s for pod "var-expansion-138e5bc4-f957-4df3-a1f6-5c81c2453db4" in namespace "var-expansion-227" to be "Succeeded or Failed"
Sep 26 11:29:06.270: INFO: Pod "var-expansion-138e5bc4-f957-4df3-a1f6-5c81c2453db4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.707062ms
Sep 26 11:29:08.278: INFO: Pod "var-expansion-138e5bc4-f957-4df3-a1f6-5c81c2453db4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011739593s
Sep 26 11:29:10.285: INFO: Pod "var-expansion-138e5bc4-f957-4df3-a1f6-5c81c2453db4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01878739s
STEP: Saw pod success
Sep 26 11:29:10.285: INFO: Pod "var-expansion-138e5bc4-f957-4df3-a1f6-5c81c2453db4" satisfied condition "Succeeded or Failed"
Sep 26 11:29:10.289: INFO: Trying to get logs from node 10.37.21.193 pod var-expansion-138e5bc4-f957-4df3-a1f6-5c81c2453db4 container dapi-container: <nil>
STEP: delete the pod
Sep 26 11:29:10.325: INFO: Waiting for pod var-expansion-138e5bc4-f957-4df3-a1f6-5c81c2453db4 to disappear
Sep 26 11:29:10.327: INFO: Pod var-expansion-138e5bc4-f957-4df3-a1f6-5c81c2453db4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:29:10.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-227" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":211,"skipped":3754,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:29:10.335: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:29:10.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2960" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":212,"skipped":3755,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:29:10.418: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Sep 26 11:29:10.472: INFO: observed Pod pod-test in namespace pods-7307 in phase Pending with labels: map[test-pod-static:true] & conditions []
Sep 26 11:29:10.475: INFO: observed Pod pod-test in namespace pods-7307 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:10 +0000 UTC  }]
Sep 26 11:29:10.487: INFO: observed Pod pod-test in namespace pods-7307 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:10 +0000 UTC  }]
Sep 26 11:29:11.525: INFO: observed Pod pod-test in namespace pods-7307 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:10 +0000 UTC  }]
Sep 26 11:29:12.468: INFO: Found Pod pod-test in namespace pods-7307 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-26 11:29:10 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Sep 26 11:29:12.484: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Sep 26 11:29:12.507: INFO: observed event type ADDED
Sep 26 11:29:12.507: INFO: observed event type MODIFIED
Sep 26 11:29:12.508: INFO: observed event type MODIFIED
Sep 26 11:29:12.508: INFO: observed event type MODIFIED
Sep 26 11:29:12.508: INFO: observed event type MODIFIED
Sep 26 11:29:12.508: INFO: observed event type MODIFIED
Sep 26 11:29:12.508: INFO: observed event type MODIFIED
Sep 26 11:29:12.508: INFO: observed event type MODIFIED
Sep 26 11:29:14.518: INFO: observed event type MODIFIED
Sep 26 11:29:14.759: INFO: observed event type MODIFIED
Sep 26 11:29:15.549: INFO: observed event type MODIFIED
Sep 26 11:29:15.559: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:29:15.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7307" for this suite.

• [SLOW TEST:5.160 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":213,"skipped":3757,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:29:15.579: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:29:15.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5736" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":214,"skipped":3762,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:29:15.668: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:29:15.714: INFO: The status of Pod test-webserver-e97f752e-d302-465c-9d12-1835c8dc824b is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:29:17.726: INFO: The status of Pod test-webserver-e97f752e-d302-465c-9d12-1835c8dc824b is Running (Ready = false)
Sep 26 11:29:19.726: INFO: The status of Pod test-webserver-e97f752e-d302-465c-9d12-1835c8dc824b is Running (Ready = false)
Sep 26 11:29:21.724: INFO: The status of Pod test-webserver-e97f752e-d302-465c-9d12-1835c8dc824b is Running (Ready = false)
Sep 26 11:29:23.724: INFO: The status of Pod test-webserver-e97f752e-d302-465c-9d12-1835c8dc824b is Running (Ready = false)
Sep 26 11:29:25.726: INFO: The status of Pod test-webserver-e97f752e-d302-465c-9d12-1835c8dc824b is Running (Ready = false)
Sep 26 11:29:27.730: INFO: The status of Pod test-webserver-e97f752e-d302-465c-9d12-1835c8dc824b is Running (Ready = false)
Sep 26 11:29:29.730: INFO: The status of Pod test-webserver-e97f752e-d302-465c-9d12-1835c8dc824b is Running (Ready = false)
Sep 26 11:29:31.723: INFO: The status of Pod test-webserver-e97f752e-d302-465c-9d12-1835c8dc824b is Running (Ready = false)
Sep 26 11:29:33.730: INFO: The status of Pod test-webserver-e97f752e-d302-465c-9d12-1835c8dc824b is Running (Ready = false)
Sep 26 11:29:35.723: INFO: The status of Pod test-webserver-e97f752e-d302-465c-9d12-1835c8dc824b is Running (Ready = false)
Sep 26 11:29:37.730: INFO: The status of Pod test-webserver-e97f752e-d302-465c-9d12-1835c8dc824b is Running (Ready = true)
Sep 26 11:29:37.735: INFO: Container started at 2021-09-26 11:29:16 +0000 UTC, pod became ready at 2021-09-26 11:29:35 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:29:37.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3895" for this suite.

• [SLOW TEST:22.085 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":215,"skipped":3770,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:29:37.753: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 26 11:29:37.829: INFO: Waiting up to 5m0s for pod "pod-17677934-a6c8-4960-a4d3-85b138011cb8" in namespace "emptydir-7669" to be "Succeeded or Failed"
Sep 26 11:29:37.832: INFO: Pod "pod-17677934-a6c8-4960-a4d3-85b138011cb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.717848ms
Sep 26 11:29:39.846: INFO: Pod "pod-17677934-a6c8-4960-a4d3-85b138011cb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016583421s
STEP: Saw pod success
Sep 26 11:29:39.846: INFO: Pod "pod-17677934-a6c8-4960-a4d3-85b138011cb8" satisfied condition "Succeeded or Failed"
Sep 26 11:29:39.850: INFO: Trying to get logs from node 10.37.21.194 pod pod-17677934-a6c8-4960-a4d3-85b138011cb8 container test-container: <nil>
STEP: delete the pod
Sep 26 11:29:39.878: INFO: Waiting for pod pod-17677934-a6c8-4960-a4d3-85b138011cb8 to disappear
Sep 26 11:29:39.881: INFO: Pod pod-17677934-a6c8-4960-a4d3-85b138011cb8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:29:39.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7669" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":216,"skipped":3777,"failed":0}

------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:29:39.892: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-2842
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-2842
Sep 26 11:29:40.054: INFO: Found 0 stateful pods, waiting for 1
Sep 26 11:29:50.131: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Sep 26 11:29:50.158: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Sep 26 11:29:50.166: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Sep 26 11:29:50.167: INFO: Observed &StatefulSet event: ADDED
Sep 26 11:29:50.167: INFO: Found Statefulset ss in namespace statefulset-2842 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep 26 11:29:50.167: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Sep 26 11:29:50.167: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep 26 11:29:50.179: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Sep 26 11:29:50.181: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Sep 26 11:29:50.181: INFO: Deleting all statefulset in ns statefulset-2842
Sep 26 11:29:50.184: INFO: Scaling statefulset ss to 0
Sep 26 11:30:00.238: INFO: Waiting for statefulset status.replicas updated to 0
Sep 26 11:30:00.243: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:30:00.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2842" for this suite.

• [SLOW TEST:20.389 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":217,"skipped":3777,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:30:00.282: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 26 11:30:02.878: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2663 pod-service-account-4af296b8-7b3c-4e1b-a5eb-c4ca0db71700 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 26 11:30:03.084: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2663 pod-service-account-4af296b8-7b3c-4e1b-a5eb-c4ca0db71700 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 26 11:30:03.336: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2663 pod-service-account-4af296b8-7b3c-4e1b-a5eb-c4ca0db71700 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:30:03.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2663" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":218,"skipped":3791,"failed":0}
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:30:03.608: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Sep 26 11:30:03.663: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 26 11:31:03.701: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:31:03.705: INFO: Starting informer...
STEP: Starting pods...
Sep 26 11:31:03.937: INFO: Pod1 is running on 10.37.21.195. Tainting Node
Sep 26 11:31:06.168: INFO: Pod2 is running on 10.37.21.195. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep 26 11:31:12.014: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep 26 11:31:32.695: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:31:32.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6815" for this suite.

• [SLOW TEST:89.115 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":219,"skipped":3797,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:31:32.723: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:31:33.068: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:31:36.106: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:31:36.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4059" for this suite.
STEP: Destroying namespace "webhook-4059-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":220,"skipped":3803,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:31:36.225: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Sep 26 11:31:38.373: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1592 PodName:pod-sharedvolume-87522e00-cf44-4e31-836f-59e08efdbfef ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 11:31:38.373: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:31:38.510: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:31:38.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1592" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":221,"skipped":3811,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:31:38.527: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:31:39.261: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:31:42.349: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:31:52.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2230" for this suite.
STEP: Destroying namespace "webhook-2230-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.057 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":222,"skipped":3828,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:31:52.585: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:31:52.653: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 26 11:31:57.672: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 26 11:31:57.673: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 26 11:31:57.695: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2161  d7351c44-6b71-4670-907c-05eb3f3b927e 549186 1 2021-09-26 11:31:57 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-09-26 11:31:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost registry.dahuatech.com/cncf/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044a55b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep 26 11:31:57.699: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Sep 26 11:31:57.699: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep 26 11:31:57.699: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2161  4daaeaa3-b913-4b94-a809-bb2f28420c37 549187 1 2021-09-26 11:31:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d7351c44-6b71-4670-907c-05eb3f3b927e 0xc0044a5bc7 0xc0044a5bc8}] []  [{e2e.test Update apps/v1 2021-09-26 11:31:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 11:31:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2021-09-26 11:31:57 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"d7351c44-6b71-4670-907c-05eb3f3b927e\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd registry.dahuatech.com/cncf/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0044a5c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 26 11:31:57.704: INFO: Pod "test-cleanup-controller-dbzfv" is available:
&Pod{ObjectMeta:{test-cleanup-controller-dbzfv test-cleanup-controller- deployment-2161  4cfb660b-fb07-454f-b78b-f85d38855566 549168 0 2021-09-26 11:31:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:908b4a11b603792acbd720ecc1ab91ccb71bbfdaa8db4c97c6481b9793c49511 cni.projectcalico.org/podIP:192.168.181.86/32 cni.projectcalico.org/podIPs:192.168.181.86/32] [{apps/v1 ReplicaSet test-cleanup-controller 4daaeaa3-b913-4b94-a809-bb2f28420c37 0xc00142a937 0xc00142a938}] []  [{kube-controller-manager Update v1 2021-09-26 11:31:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4daaeaa3-b913-4b94-a809-bb2f28420c37\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 11:31:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 11:31:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.181.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5bfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5bfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:31:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:31:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:31:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 11:31:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.195,PodIP:192.168.181.86,StartTime:2021-09-26 11:31:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 11:31:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://d7b619226cd07263c5f6901f300a1d359e30f9d05c109e54781761bb511981ca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.181.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:31:57.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2161" for this suite.

• [SLOW TEST:5.138 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":223,"skipped":3865,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:31:57.723: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 26 11:31:57.779: INFO: Waiting up to 5m0s for pod "pod-aaa19243-f437-4edc-af84-3a47ee620743" in namespace "emptydir-5987" to be "Succeeded or Failed"
Sep 26 11:31:57.783: INFO: Pod "pod-aaa19243-f437-4edc-af84-3a47ee620743": Phase="Pending", Reason="", readiness=false. Elapsed: 3.64205ms
Sep 26 11:31:59.795: INFO: Pod "pod-aaa19243-f437-4edc-af84-3a47ee620743": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01620163s
Sep 26 11:32:01.803: INFO: Pod "pod-aaa19243-f437-4edc-af84-3a47ee620743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023789916s
STEP: Saw pod success
Sep 26 11:32:01.803: INFO: Pod "pod-aaa19243-f437-4edc-af84-3a47ee620743" satisfied condition "Succeeded or Failed"
Sep 26 11:32:01.807: INFO: Trying to get logs from node 10.37.21.194 pod pod-aaa19243-f437-4edc-af84-3a47ee620743 container test-container: <nil>
STEP: delete the pod
Sep 26 11:32:01.839: INFO: Waiting for pod pod-aaa19243-f437-4edc-af84-3a47ee620743 to disappear
Sep 26 11:32:01.842: INFO: Pod pod-aaa19243-f437-4edc-af84-3a47ee620743 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:32:01.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5987" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":224,"skipped":3870,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:32:01.852: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:32:17.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4816" for this suite.
STEP: Destroying namespace "nsdeletetest-4014" for this suite.
Sep 26 11:32:17.055: INFO: Namespace nsdeletetest-4014 was already deleted
STEP: Destroying namespace "nsdeletetest-982" for this suite.

• [SLOW TEST:15.208 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":225,"skipped":3887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:32:17.061: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Sep 26 11:32:17.105: INFO: created test-pod-1
Sep 26 11:32:17.110: INFO: created test-pod-2
Sep 26 11:32:17.116: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Sep 26 11:32:17.151: INFO: Pod quantity 3 is different from expected quantity 0
Sep 26 11:32:18.158: INFO: Pod quantity 3 is different from expected quantity 0
Sep 26 11:32:19.164: INFO: Pod quantity 3 is different from expected quantity 0
Sep 26 11:32:20.159: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:32:21.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2229" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":226,"skipped":3917,"failed":0}

------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:32:21.176: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-9c790885-7882-4ebf-b6d9-c5cf22c0880c in namespace container-probe-299
Sep 26 11:32:23.256: INFO: Started pod liveness-9c790885-7882-4ebf-b6d9-c5cf22c0880c in namespace container-probe-299
STEP: checking the pod's current state and verifying that restartCount is present
Sep 26 11:32:23.261: INFO: Initial restart count of pod liveness-9c790885-7882-4ebf-b6d9-c5cf22c0880c is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:36:24.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-299" for this suite.

• [SLOW TEST:243.595 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":3917,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:36:24.771: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Sep 26 11:36:24.858: INFO: created test-event-1
Sep 26 11:36:24.862: INFO: created test-event-2
Sep 26 11:36:24.866: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Sep 26 11:36:24.872: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Sep 26 11:36:24.904: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:36:24.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7664" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":228,"skipped":3926,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:36:24.918: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Sep 26 11:36:24.956: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Sep 26 11:36:26.005: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Sep 26 11:36:28.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252986, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252986, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252986, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768252986, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-9558f8cbb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 26 11:36:31.005: INFO: Waited 925.597045ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Sep 26 11:36:31.066: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:36:31.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9854" for this suite.

• [SLOW TEST:6.631 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":229,"skipped":3936,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:36:31.549: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 26 11:36:31.636: INFO: starting watch
STEP: patching
STEP: updating
Sep 26 11:36:31.647: INFO: waiting for watch events with expected annotations
Sep 26 11:36:31.647: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:36:31.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-4587" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":230,"skipped":3946,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:36:31.856: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Sep 26 11:36:31.900: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:36:56.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2317" for this suite.

• [SLOW TEST:24.768 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":231,"skipped":3951,"failed":0}
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:36:56.625: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Sep 26 11:36:56.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-2285 run logs-generator --image=registry.dahuatech.com/cncf/agnhost:2.32 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 26 11:36:56.864: INFO: stderr: ""
Sep 26 11:36:56.864: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Sep 26 11:36:56.864: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 26 11:36:56.864: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2285" to be "running and ready, or succeeded"
Sep 26 11:36:56.868: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.939932ms
Sep 26 11:36:58.885: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.020779813s
Sep 26 11:36:58.885: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 26 11:36:58.885: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep 26 11:36:58.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-2285 logs logs-generator logs-generator'
Sep 26 11:36:59.017: INFO: stderr: ""
Sep 26 11:36:59.017: INFO: stdout: "I0926 11:36:58.111592       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/264 360\nI0926 11:36:58.311810       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/r4q 436\nI0926 11:36:58.512333       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/g8d 257\nI0926 11:36:58.711627       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/ldcz 410\nI0926 11:36:58.912079       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/ldx 324\n"
STEP: limiting log lines
Sep 26 11:36:59.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-2285 logs logs-generator logs-generator --tail=1'
Sep 26 11:36:59.102: INFO: stderr: ""
Sep 26 11:36:59.102: INFO: stdout: "I0926 11:36:58.912079       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/ldx 324\n"
Sep 26 11:36:59.102: INFO: got output "I0926 11:36:58.912079       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/ldx 324\n"
STEP: limiting log bytes
Sep 26 11:36:59.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-2285 logs logs-generator logs-generator --limit-bytes=1'
Sep 26 11:36:59.204: INFO: stderr: ""
Sep 26 11:36:59.204: INFO: stdout: "I"
Sep 26 11:36:59.204: INFO: got output "I"
STEP: exposing timestamps
Sep 26 11:36:59.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-2285 logs logs-generator logs-generator --tail=1 --timestamps'
Sep 26 11:36:59.300: INFO: stderr: ""
Sep 26 11:36:59.300: INFO: stdout: "2021-09-26T11:36:59.112622214Z I0926 11:36:59.112505       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/zms 334\n"
Sep 26 11:36:59.300: INFO: got output "2021-09-26T11:36:59.112622214Z I0926 11:36:59.112505       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/zms 334\n"
STEP: restricting to a time range
Sep 26 11:37:01.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-2285 logs logs-generator logs-generator --since=1s'
Sep 26 11:37:01.898: INFO: stderr: ""
Sep 26 11:37:01.898: INFO: stdout: "I0926 11:37:00.912618       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/nqh 504\nI0926 11:37:01.112020       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/kqhk 265\nI0926 11:37:01.312490       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/h8f 347\nI0926 11:37:01.511821       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/m7cr 423\nI0926 11:37:01.712240       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/mmr 344\n"
Sep 26 11:37:01.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-2285 logs logs-generator logs-generator --since=24h'
Sep 26 11:37:02.011: INFO: stderr: ""
Sep 26 11:37:02.011: INFO: stdout: "I0926 11:36:58.111592       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/264 360\nI0926 11:36:58.311810       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/r4q 436\nI0926 11:36:58.512333       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/g8d 257\nI0926 11:36:58.711627       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/ldcz 410\nI0926 11:36:58.912079       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/ldx 324\nI0926 11:36:59.112505       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/zms 334\nI0926 11:36:59.311818       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/7vg 528\nI0926 11:36:59.512199       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/7r9k 219\nI0926 11:36:59.712603       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/fgm 265\nI0926 11:36:59.912176       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/qc2c 313\nI0926 11:37:00.112705       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/ldrf 239\nI0926 11:37:00.312430       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/bgbv 585\nI0926 11:37:00.511688       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/gn9t 440\nI0926 11:37:00.712084       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/5zw 330\nI0926 11:37:00.912618       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/nqh 504\nI0926 11:37:01.112020       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/kqhk 265\nI0926 11:37:01.312490       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/h8f 347\nI0926 11:37:01.511821       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/m7cr 423\nI0926 11:37:01.712240       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/mmr 344\nI0926 11:37:01.912594       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/99c 300\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Sep 26 11:37:02.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-2285 delete pod logs-generator'
Sep 26 11:37:03.370: INFO: stderr: ""
Sep 26 11:37:03.370: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:37:03.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2285" for this suite.

• [SLOW TEST:6.769 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1393
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":232,"skipped":3951,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:37:03.394: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-2e302004-a762-4903-b487-3304b3c4ff2d
STEP: Creating a pod to test consume secrets
Sep 26 11:37:03.462: INFO: Waiting up to 5m0s for pod "pod-secrets-5df9ba77-8807-4892-9cac-b83f4fa0f3aa" in namespace "secrets-4533" to be "Succeeded or Failed"
Sep 26 11:37:03.466: INFO: Pod "pod-secrets-5df9ba77-8807-4892-9cac-b83f4fa0f3aa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.972411ms
Sep 26 11:37:05.472: INFO: Pod "pod-secrets-5df9ba77-8807-4892-9cac-b83f4fa0f3aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01036975s
STEP: Saw pod success
Sep 26 11:37:05.472: INFO: Pod "pod-secrets-5df9ba77-8807-4892-9cac-b83f4fa0f3aa" satisfied condition "Succeeded or Failed"
Sep 26 11:37:05.475: INFO: Trying to get logs from node 10.37.21.194 pod pod-secrets-5df9ba77-8807-4892-9cac-b83f4fa0f3aa container secret-env-test: <nil>
STEP: delete the pod
Sep 26 11:37:05.502: INFO: Waiting for pod pod-secrets-5df9ba77-8807-4892-9cac-b83f4fa0f3aa to disappear
Sep 26 11:37:05.505: INFO: Pod pod-secrets-5df9ba77-8807-4892-9cac-b83f4fa0f3aa no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:37:05.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4533" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":233,"skipped":3971,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:37:05.513: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Sep 26 11:37:05.565: INFO: The status of Pod pod-update-activedeadlineseconds-e1dd8e08-ae64-40df-bebf-8f825999b04e is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:37:07.574: INFO: The status of Pod pod-update-activedeadlineseconds-e1dd8e08-ae64-40df-bebf-8f825999b04e is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 26 11:37:08.097: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e1dd8e08-ae64-40df-bebf-8f825999b04e"
Sep 26 11:37:08.097: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e1dd8e08-ae64-40df-bebf-8f825999b04e" in namespace "pods-2915" to be "terminated due to deadline exceeded"
Sep 26 11:37:08.101: INFO: Pod "pod-update-activedeadlineseconds-e1dd8e08-ae64-40df-bebf-8f825999b04e": Phase="Running", Reason="", readiness=true. Elapsed: 3.205197ms
Sep 26 11:37:10.118: INFO: Pod "pod-update-activedeadlineseconds-e1dd8e08-ae64-40df-bebf-8f825999b04e": Phase="Running", Reason="", readiness=true. Elapsed: 2.020378645s
Sep 26 11:37:12.130: INFO: Pod "pod-update-activedeadlineseconds-e1dd8e08-ae64-40df-bebf-8f825999b04e": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 4.032384002s
Sep 26 11:37:12.130: INFO: Pod "pod-update-activedeadlineseconds-e1dd8e08-ae64-40df-bebf-8f825999b04e" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:37:12.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2915" for this suite.

• [SLOW TEST:6.633 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":234,"skipped":3980,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:37:12.146: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 11:37:12.198: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1aa52ac1-b50c-4cbe-ae1b-5c0558fa36cb" in namespace "projected-6091" to be "Succeeded or Failed"
Sep 26 11:37:12.201: INFO: Pod "downwardapi-volume-1aa52ac1-b50c-4cbe-ae1b-5c0558fa36cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.138257ms
Sep 26 11:37:14.207: INFO: Pod "downwardapi-volume-1aa52ac1-b50c-4cbe-ae1b-5c0558fa36cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008884598s
STEP: Saw pod success
Sep 26 11:37:14.207: INFO: Pod "downwardapi-volume-1aa52ac1-b50c-4cbe-ae1b-5c0558fa36cb" satisfied condition "Succeeded or Failed"
Sep 26 11:37:14.211: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-1aa52ac1-b50c-4cbe-ae1b-5c0558fa36cb container client-container: <nil>
STEP: delete the pod
Sep 26 11:37:14.230: INFO: Waiting for pod downwardapi-volume-1aa52ac1-b50c-4cbe-ae1b-5c0558fa36cb to disappear
Sep 26 11:37:14.234: INFO: Pod downwardapi-volume-1aa52ac1-b50c-4cbe-ae1b-5c0558fa36cb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:37:14.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6091" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":235,"skipped":3997,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:37:14.244: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep 26 11:37:14.288: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep 26 11:37:31.787: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 11:37:40.309: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:37:58.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5748" for this suite.

• [SLOW TEST:44.665 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":236,"skipped":4005,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:37:58.910: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1005.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1005.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1005.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1005.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1005.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1005.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1005.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1005.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1005.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1005.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 26 11:38:01.035: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:01.040: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:01.043: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1005.svc.cluster.local from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:01.046: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1005.svc.cluster.local from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:01.050: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:01.052: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:01.055: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:01.060: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:01.063: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1005.svc.cluster.local from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:01.066: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1005.svc.cluster.local from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:01.070: INFO: Unable to read jessie_udp@PodARecord from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:01.073: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:01.073: INFO: Lookups using dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1005.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1005.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local jessie_udp@dns-test-service-2.dns-1005.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1005.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 26 11:38:06.095: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:06.098: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:06.106: INFO: Unable to read jessie_udp@PodARecord from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:06.108: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31: the server could not find the requested resource (get pods dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31)
Sep 26 11:38:06.108: INFO: Lookups using dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31 failed for: [jessie_udp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1005.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 26 11:38:11.118: INFO: DNS probes using dns-1005/dns-test-d18825b0-ea4f-41d1-8b04-c82a298d9d31 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:38:11.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1005" for this suite.

• [SLOW TEST:12.279 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":237,"skipped":4013,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:38:11.189: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Sep 26 11:38:11.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 create -f -'
Sep 26 11:38:11.414: INFO: stderr: ""
Sep 26 11:38:11.414: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 26 11:38:11.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 26 11:38:11.506: INFO: stderr: ""
Sep 26 11:38:11.506: INFO: stdout: "update-demo-nautilus-c6nm4 update-demo-nautilus-f722f "
Sep 26 11:38:11.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-c6nm4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 26 11:38:11.577: INFO: stderr: ""
Sep 26 11:38:11.577: INFO: stdout: ""
Sep 26 11:38:11.577: INFO: update-demo-nautilus-c6nm4 is created but not running
Sep 26 11:38:16.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 26 11:38:16.661: INFO: stderr: ""
Sep 26 11:38:16.661: INFO: stdout: "update-demo-nautilus-c6nm4 update-demo-nautilus-f722f "
Sep 26 11:38:16.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-c6nm4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 26 11:38:16.737: INFO: stderr: ""
Sep 26 11:38:16.737: INFO: stdout: "true"
Sep 26 11:38:16.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-c6nm4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 26 11:38:16.804: INFO: stderr: ""
Sep 26 11:38:16.804: INFO: stdout: "registry.dahuatech.com/cncf/nautilus:1.4"
Sep 26 11:38:16.805: INFO: validating pod update-demo-nautilus-c6nm4
Sep 26 11:38:16.809: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 26 11:38:16.810: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 26 11:38:16.810: INFO: update-demo-nautilus-c6nm4 is verified up and running
Sep 26 11:38:16.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-f722f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 26 11:38:16.886: INFO: stderr: ""
Sep 26 11:38:16.886: INFO: stdout: "true"
Sep 26 11:38:16.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-f722f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 26 11:38:16.958: INFO: stderr: ""
Sep 26 11:38:16.958: INFO: stdout: "registry.dahuatech.com/cncf/nautilus:1.4"
Sep 26 11:38:16.958: INFO: validating pod update-demo-nautilus-f722f
Sep 26 11:38:16.962: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 26 11:38:16.962: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 26 11:38:16.962: INFO: update-demo-nautilus-f722f is verified up and running
STEP: scaling down the replication controller
Sep 26 11:38:16.964: INFO: scanned /root for discovery docs: <nil>
Sep 26 11:38:16.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep 26 11:38:18.064: INFO: stderr: ""
Sep 26 11:38:18.064: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 26 11:38:18.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 26 11:38:18.138: INFO: stderr: ""
Sep 26 11:38:18.138: INFO: stdout: "update-demo-nautilus-c6nm4 update-demo-nautilus-f722f "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 26 11:38:23.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 26 11:38:23.230: INFO: stderr: ""
Sep 26 11:38:23.230: INFO: stdout: "update-demo-nautilus-c6nm4 "
Sep 26 11:38:23.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-c6nm4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 26 11:38:23.326: INFO: stderr: ""
Sep 26 11:38:23.326: INFO: stdout: "true"
Sep 26 11:38:23.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-c6nm4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 26 11:38:23.410: INFO: stderr: ""
Sep 26 11:38:23.410: INFO: stdout: "registry.dahuatech.com/cncf/nautilus:1.4"
Sep 26 11:38:23.410: INFO: validating pod update-demo-nautilus-c6nm4
Sep 26 11:38:23.415: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 26 11:38:23.415: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 26 11:38:23.415: INFO: update-demo-nautilus-c6nm4 is verified up and running
STEP: scaling up the replication controller
Sep 26 11:38:23.417: INFO: scanned /root for discovery docs: <nil>
Sep 26 11:38:23.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep 26 11:38:24.527: INFO: stderr: ""
Sep 26 11:38:24.527: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 26 11:38:24.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 26 11:38:24.602: INFO: stderr: ""
Sep 26 11:38:24.602: INFO: stdout: "update-demo-nautilus-c6nm4 update-demo-nautilus-zvrgf "
Sep 26 11:38:24.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-c6nm4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 26 11:38:24.684: INFO: stderr: ""
Sep 26 11:38:24.684: INFO: stdout: "true"
Sep 26 11:38:24.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-c6nm4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 26 11:38:24.753: INFO: stderr: ""
Sep 26 11:38:24.753: INFO: stdout: "registry.dahuatech.com/cncf/nautilus:1.4"
Sep 26 11:38:24.753: INFO: validating pod update-demo-nautilus-c6nm4
Sep 26 11:38:24.757: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 26 11:38:24.757: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 26 11:38:24.757: INFO: update-demo-nautilus-c6nm4 is verified up and running
Sep 26 11:38:24.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-zvrgf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 26 11:38:24.834: INFO: stderr: ""
Sep 26 11:38:24.834: INFO: stdout: ""
Sep 26 11:38:24.834: INFO: update-demo-nautilus-zvrgf is created but not running
Sep 26 11:38:29.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 26 11:38:29.931: INFO: stderr: ""
Sep 26 11:38:29.931: INFO: stdout: "update-demo-nautilus-c6nm4 update-demo-nautilus-zvrgf "
Sep 26 11:38:29.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-c6nm4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 26 11:38:29.994: INFO: stderr: ""
Sep 26 11:38:29.994: INFO: stdout: "true"
Sep 26 11:38:29.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-c6nm4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 26 11:38:30.083: INFO: stderr: ""
Sep 26 11:38:30.083: INFO: stdout: "registry.dahuatech.com/cncf/nautilus:1.4"
Sep 26 11:38:30.083: INFO: validating pod update-demo-nautilus-c6nm4
Sep 26 11:38:30.087: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 26 11:38:30.087: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 26 11:38:30.087: INFO: update-demo-nautilus-c6nm4 is verified up and running
Sep 26 11:38:30.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-zvrgf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 26 11:38:30.167: INFO: stderr: ""
Sep 26 11:38:30.167: INFO: stdout: "true"
Sep 26 11:38:30.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods update-demo-nautilus-zvrgf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 26 11:38:30.248: INFO: stderr: ""
Sep 26 11:38:30.248: INFO: stdout: "registry.dahuatech.com/cncf/nautilus:1.4"
Sep 26 11:38:30.248: INFO: validating pod update-demo-nautilus-zvrgf
Sep 26 11:38:30.255: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 26 11:38:30.255: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 26 11:38:30.255: INFO: update-demo-nautilus-zvrgf is verified up and running
STEP: using delete to clean up resources
Sep 26 11:38:30.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 delete --grace-period=0 --force -f -'
Sep 26 11:38:30.333: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 26 11:38:30.333: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 26 11:38:30.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get rc,svc -l name=update-demo --no-headers'
Sep 26 11:38:30.427: INFO: stderr: "No resources found in kubectl-4906 namespace.\n"
Sep 26 11:38:30.427: INFO: stdout: ""
Sep 26 11:38:30.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-4906 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 26 11:38:30.508: INFO: stderr: ""
Sep 26 11:38:30.508: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:38:30.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4906" for this suite.

• [SLOW TEST:19.332 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":238,"skipped":4036,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:38:30.521: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Sep 26 11:38:30.591: INFO: Waiting up to 5m0s for pod "downward-api-6de15ba7-8098-44cd-a8b8-35d26443ba1e" in namespace "downward-api-5681" to be "Succeeded or Failed"
Sep 26 11:38:30.595: INFO: Pod "downward-api-6de15ba7-8098-44cd-a8b8-35d26443ba1e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.841162ms
Sep 26 11:38:32.611: INFO: Pod "downward-api-6de15ba7-8098-44cd-a8b8-35d26443ba1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02035036s
STEP: Saw pod success
Sep 26 11:38:32.611: INFO: Pod "downward-api-6de15ba7-8098-44cd-a8b8-35d26443ba1e" satisfied condition "Succeeded or Failed"
Sep 26 11:38:32.617: INFO: Trying to get logs from node 10.37.21.194 pod downward-api-6de15ba7-8098-44cd-a8b8-35d26443ba1e container dapi-container: <nil>
STEP: delete the pod
Sep 26 11:38:32.647: INFO: Waiting for pod downward-api-6de15ba7-8098-44cd-a8b8-35d26443ba1e to disappear
Sep 26 11:38:32.650: INFO: Pod downward-api-6de15ba7-8098-44cd-a8b8-35d26443ba1e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:38:32.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5681" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":239,"skipped":4043,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:38:32.662: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:38:33.190: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:38:36.223: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:38:36.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4962" for this suite.
STEP: Destroying namespace "webhook-4962-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":240,"skipped":4114,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:38:36.416: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Sep 26 11:38:37.519: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W0926 11:38:37.519936      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 26 11:38:37.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5911" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":241,"skipped":4120,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:38:37.529: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Sep 26 11:38:37.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-497 create -f -'
Sep 26 11:38:37.765: INFO: stderr: ""
Sep 26 11:38:37.765: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 26 11:38:38.772: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 26 11:38:38.772: INFO: Found 0 / 1
Sep 26 11:38:39.774: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 26 11:38:39.774: INFO: Found 1 / 1
Sep 26 11:38:39.774: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 26 11:38:39.777: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 26 11:38:39.777: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 26 11:38:39.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-497 patch pod agnhost-primary-4lqnf -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 26 11:38:39.866: INFO: stderr: ""
Sep 26 11:38:39.866: INFO: stdout: "pod/agnhost-primary-4lqnf patched\n"
STEP: checking annotations
Sep 26 11:38:39.869: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 26 11:38:39.869: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:38:39.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-497" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":242,"skipped":4122,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:38:39.880: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-ea66f75b-8832-4fe8-a79e-aee47e84c9d4
STEP: Creating secret with name s-test-opt-upd-4bcf367b-2b08-4a47-9087-b5cff6b45160
STEP: Creating the pod
Sep 26 11:38:39.944: INFO: The status of Pod pod-projected-secrets-3ec6e1e2-ff01-4637-9610-48c70a14cf32 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:38:41.949: INFO: The status of Pod pod-projected-secrets-3ec6e1e2-ff01-4637-9610-48c70a14cf32 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-ea66f75b-8832-4fe8-a79e-aee47e84c9d4
STEP: Updating secret s-test-opt-upd-4bcf367b-2b08-4a47-9087-b5cff6b45160
STEP: Creating secret with name s-test-opt-create-b7e2acc3-be09-497c-bc80-ecdc35f68d06
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:38:44.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4851" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":243,"skipped":4125,"failed":0}
SS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:38:44.048: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Sep 26 11:38:44.098: INFO: created test-podtemplate-1
Sep 26 11:38:44.102: INFO: created test-podtemplate-2
Sep 26 11:38:44.105: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Sep 26 11:38:44.108: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Sep 26 11:38:44.121: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:38:44.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8248" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":244,"skipped":4127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:38:44.132: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep 26 11:38:54.222: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W0926 11:38:54.222847      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 26 11:38:54.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4288" for this suite.

• [SLOW TEST:10.106 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":245,"skipped":4150,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:38:54.237: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Sep 26 11:38:54.289: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:39:16.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1515" for this suite.

• [SLOW TEST:22.342 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":246,"skipped":4152,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:39:16.580: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Sep 26 11:39:16.660: INFO: PodSpec: initContainers in spec.initContainers
Sep 26 11:40:01.921: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-79726d87-aad0-4157-aaaf-1e225f56b0b2", GenerateName:"", Namespace:"init-container-3378", SelfLink:"", UID:"ac5ec072-cf64-4326-a01d-4b6ad89a81ad", ResourceVersion:"551172", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63768253156, loc:(*time.Location)(0xa09bc80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"660877369"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"1728b0240a9407d169792b45a032ce2728c1b18b1f09c381c620a7f696b37fac", "cni.projectcalico.org/podIP":"192.168.181.104/32", "cni.projectcalico.org/podIPs":"192.168.181.104/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0064573c8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0064573e0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0064573f8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006457410), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc006457428), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006457440), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-2ck7x", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003d102e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.dahuatech.com/cncf/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-2ck7x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.dahuatech.com/cncf/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-2ck7x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.dahuatech.com/cncf/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-2ck7x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0069b8a88), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.37.21.195", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001040000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0069b8b40)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0069b8b70)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0069b8b78), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0069b8b7c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00464e950), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253156, loc:(*time.Location)(0xa09bc80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253156, loc:(*time.Location)(0xa09bc80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253156, loc:(*time.Location)(0xa09bc80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253156, loc:(*time.Location)(0xa09bc80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.37.21.195", PodIP:"192.168.181.104", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.181.104"}}, StartTime:(*v1.Time)(0xc006457470), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0010400e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0010401c0)}, Ready:false, RestartCount:3, Image:"registry.dahuatech.com/cncf/busybox:1.29-1", ImageID:"docker-pullable://registry.dahuatech.com/cncf/busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"docker://fa4c1eb855a36b4d58be8adb7c9c815137729fd1846cfb1360be60ec54c241b9", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003d10540), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.dahuatech.com/cncf/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003d10500), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.dahuatech.com/cncf/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc0069b8c2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:40:01.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3378" for this suite.

• [SLOW TEST:45.382 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":247,"skipped":4171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:40:01.964: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1819
STEP: creating service affinity-clusterip in namespace services-1819
STEP: creating replication controller affinity-clusterip in namespace services-1819
I0926 11:40:02.034157      22 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-1819, replica count: 3
I0926 11:40:05.085858      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 26 11:40:05.094: INFO: Creating new exec pod
Sep 26 11:40:08.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-1819 exec execpod-affinityrnb95 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Sep 26 11:40:08.332: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep 26 11:40:08.332: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:40:08.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-1819 exec execpod-affinityrnb95 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.161.98 80'
Sep 26 11:40:08.558: INFO: stderr: "+ + echonc -v hostName -t -w\n 2 10.254.161.98 80\nConnection to 10.254.161.98 80 port [tcp/http] succeeded!\n"
Sep 26 11:40:08.558: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 11:40:08.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-1819 exec execpod-affinityrnb95 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.161.98:80/ ; done'
Sep 26 11:40:08.862: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.161.98:80/\n"
Sep 26 11:40:08.862: INFO: stdout: "\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr\naffinity-clusterip-6dmxr"
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Received response from host: affinity-clusterip-6dmxr
Sep 26 11:40:08.862: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1819, will wait for the garbage collector to delete the pods
Sep 26 11:40:08.939: INFO: Deleting ReplicationController affinity-clusterip took: 7.092788ms
Sep 26 11:40:09.040: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.957961ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:40:11.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1819" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.909 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":248,"skipped":4278,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:40:11.872: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:40:15.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4458" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":249,"skipped":4279,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:40:15.988: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:40:16.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5179" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":250,"skipped":4286,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:40:16.078: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5625
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-5625
I0926 11:40:16.146446      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5625, replica count: 2
Sep 26 11:40:19.197: INFO: Creating new exec pod
I0926 11:40:19.197320      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 26 11:40:22.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5625 exec execpod6pbxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Sep 26 11:40:22.468: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 26 11:40:22.468: INFO: stdout: ""
Sep 26 11:40:23.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5625 exec execpod6pbxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Sep 26 11:40:23.692: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 26 11:40:23.692: INFO: stdout: "externalname-service-mzmvb"
Sep 26 11:40:23.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5625 exec execpod6pbxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.254.128 80'
Sep 26 11:40:23.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.254.128 80\nConnection to 10.254.254.128 80 port [tcp/http] succeeded!\n"
Sep 26 11:40:23.886: INFO: stdout: "externalname-service-4hmlv"
Sep 26 11:40:23.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5625 exec execpod6pbxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.194 30562'
Sep 26 11:40:24.099: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.194 30562\nConnection to 10.37.21.194 30562 port [tcp/*] succeeded!\n"
Sep 26 11:40:24.099: INFO: stdout: "externalname-service-mzmvb"
Sep 26 11:40:24.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5625 exec execpod6pbxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.193 30562'
Sep 26 11:40:24.311: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.193 30562\nConnection to 10.37.21.193 30562 port [tcp/*] succeeded!\n"
Sep 26 11:40:24.311: INFO: stdout: ""
Sep 26 11:40:25.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-5625 exec execpod6pbxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.193 30562'
Sep 26 11:40:25.550: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.193 30562\nConnection to 10.37.21.193 30562 port [tcp/*] succeeded!\n"
Sep 26 11:40:25.550: INFO: stdout: "externalname-service-4hmlv"
Sep 26 11:40:25.550: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:40:25.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5625" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.528 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":251,"skipped":4302,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:40:25.606: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:40:25.673: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 26 11:40:30.699: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Sep 26 11:40:30.723: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Sep 26 11:40:30.736: INFO: observed ReplicaSet test-rs in namespace replicaset-1334 with ReadyReplicas 1, AvailableReplicas 1
Sep 26 11:40:30.749: INFO: observed ReplicaSet test-rs in namespace replicaset-1334 with ReadyReplicas 1, AvailableReplicas 1
Sep 26 11:40:30.772: INFO: observed ReplicaSet test-rs in namespace replicaset-1334 with ReadyReplicas 1, AvailableReplicas 1
Sep 26 11:40:30.777: INFO: observed ReplicaSet test-rs in namespace replicaset-1334 with ReadyReplicas 1, AvailableReplicas 1
Sep 26 11:40:32.743: INFO: observed ReplicaSet test-rs in namespace replicaset-1334 with ReadyReplicas 2, AvailableReplicas 2
Sep 26 11:40:32.869: INFO: observed Replicaset test-rs in namespace replicaset-1334 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:40:32.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1334" for this suite.

• [SLOW TEST:7.287 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":252,"skipped":4309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:40:32.893: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 26 11:40:32.964: INFO: Number of nodes with available pods: 0
Sep 26 11:40:32.964: INFO: Node 10.37.21.193 is running more than one daemon pod
Sep 26 11:40:33.977: INFO: Number of nodes with available pods: 0
Sep 26 11:40:33.977: INFO: Node 10.37.21.193 is running more than one daemon pod
Sep 26 11:40:34.975: INFO: Number of nodes with available pods: 3
Sep 26 11:40:34.975: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Getting /status
Sep 26 11:40:34.981: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Sep 26 11:40:34.991: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Sep 26 11:40:34.993: INFO: Observed &DaemonSet event: ADDED
Sep 26 11:40:34.993: INFO: Observed &DaemonSet event: MODIFIED
Sep 26 11:40:34.993: INFO: Observed &DaemonSet event: MODIFIED
Sep 26 11:40:34.993: INFO: Observed &DaemonSet event: MODIFIED
Sep 26 11:40:34.993: INFO: Observed &DaemonSet event: MODIFIED
Sep 26 11:40:34.993: INFO: Found daemon set daemon-set in namespace daemonsets-2944 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 26 11:40:34.993: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Sep 26 11:40:35.000: INFO: Observed &DaemonSet event: ADDED
Sep 26 11:40:35.000: INFO: Observed &DaemonSet event: MODIFIED
Sep 26 11:40:35.000: INFO: Observed &DaemonSet event: MODIFIED
Sep 26 11:40:35.001: INFO: Observed &DaemonSet event: MODIFIED
Sep 26 11:40:35.001: INFO: Observed &DaemonSet event: MODIFIED
Sep 26 11:40:35.001: INFO: Observed daemon set daemon-set in namespace daemonsets-2944 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 26 11:40:35.001: INFO: Observed &DaemonSet event: MODIFIED
Sep 26 11:40:35.001: INFO: Found daemon set daemon-set in namespace daemonsets-2944 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Sep 26 11:40:35.001: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2944, will wait for the garbage collector to delete the pods
Sep 26 11:40:35.065: INFO: Deleting DaemonSet.extensions daemon-set took: 7.507567ms
Sep 26 11:40:35.166: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.684266ms
Sep 26 11:40:38.073: INFO: Number of nodes with available pods: 0
Sep 26 11:40:38.073: INFO: Number of running nodes: 0, number of available pods: 0
Sep 26 11:40:38.076: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"551720"},"items":null}

Sep 26 11:40:38.078: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"551720"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:40:38.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2944" for this suite.

• [SLOW TEST:5.209 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":253,"skipped":4338,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:40:38.103: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:40:39.136: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 26 11:40:41.155: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253239, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253239, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253239, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253239, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-657f8c86c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:40:44.187: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:40:44.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2480" for this suite.
STEP: Destroying namespace "webhook-2480-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.185 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":254,"skipped":4350,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:40:44.288: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 26 11:40:44.341: INFO: Waiting up to 5m0s for pod "pod-404f7d2e-1431-41bd-9211-0b1b7e87047c" in namespace "emptydir-2453" to be "Succeeded or Failed"
Sep 26 11:40:44.345: INFO: Pod "pod-404f7d2e-1431-41bd-9211-0b1b7e87047c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.675356ms
Sep 26 11:40:46.356: INFO: Pod "pod-404f7d2e-1431-41bd-9211-0b1b7e87047c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014517683s
STEP: Saw pod success
Sep 26 11:40:46.356: INFO: Pod "pod-404f7d2e-1431-41bd-9211-0b1b7e87047c" satisfied condition "Succeeded or Failed"
Sep 26 11:40:46.362: INFO: Trying to get logs from node 10.37.21.195 pod pod-404f7d2e-1431-41bd-9211-0b1b7e87047c container test-container: <nil>
STEP: delete the pod
Sep 26 11:40:46.397: INFO: Waiting for pod pod-404f7d2e-1431-41bd-9211-0b1b7e87047c to disappear
Sep 26 11:40:46.400: INFO: Pod pod-404f7d2e-1431-41bd-9211-0b1b7e87047c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:40:46.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2453" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":255,"skipped":4370,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:40:46.409: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:40:47.187: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 26 11:40:49.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253247, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253247, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253247, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253247, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-657f8c86c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:40:52.238: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:40:52.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8829" for this suite.
STEP: Destroying namespace "webhook-8829-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.904 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":256,"skipped":4382,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:40:52.313: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:41:52.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8150" for this suite.

• [SLOW TEST:60.089 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":257,"skipped":4402,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:41:52.403: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 26 11:41:52.483: INFO: Waiting up to 5m0s for pod "pod-590f802c-7819-4be4-b6fd-78187e387655" in namespace "emptydir-8386" to be "Succeeded or Failed"
Sep 26 11:41:52.487: INFO: Pod "pod-590f802c-7819-4be4-b6fd-78187e387655": Phase="Pending", Reason="", readiness=false. Elapsed: 4.313843ms
Sep 26 11:41:54.500: INFO: Pod "pod-590f802c-7819-4be4-b6fd-78187e387655": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016579183s
STEP: Saw pod success
Sep 26 11:41:54.500: INFO: Pod "pod-590f802c-7819-4be4-b6fd-78187e387655" satisfied condition "Succeeded or Failed"
Sep 26 11:41:54.503: INFO: Trying to get logs from node 10.37.21.194 pod pod-590f802c-7819-4be4-b6fd-78187e387655 container test-container: <nil>
STEP: delete the pod
Sep 26 11:41:54.547: INFO: Waiting for pod pod-590f802c-7819-4be4-b6fd-78187e387655 to disappear
Sep 26 11:41:54.550: INFO: Pod pod-590f802c-7819-4be4-b6fd-78187e387655 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:41:54.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8386" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":258,"skipped":4406,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:41:54.562: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:41:55.107: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 26 11:41:57.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253315, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253315, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253315, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253315, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-657f8c86c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:42:00.151: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep 26 11:42:04.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=webhook-6866 attach --namespace=webhook-6866 to-be-attached-pod -i -c=container1'
Sep 26 11:42:04.303: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:42:04.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6866" for this suite.
STEP: Destroying namespace "webhook-6866-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.815 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":259,"skipped":4465,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:42:04.378: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:42:04.438: INFO: Creating pod...
Sep 26 11:42:04.451: INFO: Pod Quantity: 1 Status: Pending
Sep 26 11:42:05.460: INFO: Pod Quantity: 1 Status: Pending
Sep 26 11:42:06.461: INFO: Pod Status: Running
Sep 26 11:42:06.461: INFO: Creating service...
Sep 26 11:42:06.472: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/pods/agnhost/proxy/some/path/with/DELETE
Sep 26 11:42:06.477: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep 26 11:42:06.477: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/pods/agnhost/proxy/some/path/with/GET
Sep 26 11:42:06.480: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep 26 11:42:06.480: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/pods/agnhost/proxy/some/path/with/HEAD
Sep 26 11:42:06.484: INFO: http.Client request:HEAD | StatusCode:200
Sep 26 11:42:06.484: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/pods/agnhost/proxy/some/path/with/OPTIONS
Sep 26 11:42:06.489: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep 26 11:42:06.489: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/pods/agnhost/proxy/some/path/with/PATCH
Sep 26 11:42:06.492: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep 26 11:42:06.492: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/pods/agnhost/proxy/some/path/with/POST
Sep 26 11:42:06.495: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep 26 11:42:06.495: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/pods/agnhost/proxy/some/path/with/PUT
Sep 26 11:42:06.499: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep 26 11:42:06.499: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/services/test-service/proxy/some/path/with/DELETE
Sep 26 11:42:06.504: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep 26 11:42:06.504: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/services/test-service/proxy/some/path/with/GET
Sep 26 11:42:06.508: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep 26 11:42:06.508: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/services/test-service/proxy/some/path/with/HEAD
Sep 26 11:42:06.515: INFO: http.Client request:HEAD | StatusCode:200
Sep 26 11:42:06.515: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/services/test-service/proxy/some/path/with/OPTIONS
Sep 26 11:42:06.520: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep 26 11:42:06.520: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/services/test-service/proxy/some/path/with/PATCH
Sep 26 11:42:06.524: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep 26 11:42:06.524: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/services/test-service/proxy/some/path/with/POST
Sep 26 11:42:06.532: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep 26 11:42:06.532: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7430/services/test-service/proxy/some/path/with/PUT
Sep 26 11:42:06.539: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:42:06.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7430" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":260,"skipped":4502,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:42:06.553: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Sep 26 11:42:06.606: INFO: Waiting up to 5m0s for pod "downward-api-66e40e0a-94da-4c5e-a877-6cac7f4df412" in namespace "downward-api-1532" to be "Succeeded or Failed"
Sep 26 11:42:06.610: INFO: Pod "downward-api-66e40e0a-94da-4c5e-a877-6cac7f4df412": Phase="Pending", Reason="", readiness=false. Elapsed: 3.274084ms
Sep 26 11:42:08.618: INFO: Pod "downward-api-66e40e0a-94da-4c5e-a877-6cac7f4df412": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011096574s
STEP: Saw pod success
Sep 26 11:42:08.618: INFO: Pod "downward-api-66e40e0a-94da-4c5e-a877-6cac7f4df412" satisfied condition "Succeeded or Failed"
Sep 26 11:42:08.621: INFO: Trying to get logs from node 10.37.21.194 pod downward-api-66e40e0a-94da-4c5e-a877-6cac7f4df412 container dapi-container: <nil>
STEP: delete the pod
Sep 26 11:42:08.642: INFO: Waiting for pod downward-api-66e40e0a-94da-4c5e-a877-6cac7f4df412 to disappear
Sep 26 11:42:08.645: INFO: Pod downward-api-66e40e0a-94da-4c5e-a877-6cac7f4df412 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:42:08.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1532" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":261,"skipped":4518,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:42:08.655: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:42:08.701: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:42:14.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2725" for this suite.

• [SLOW TEST:6.189 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":262,"skipped":4538,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:42:14.844: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:42:14.904: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5a0e19cb-c0fd-4243-8c83-a16651dbc4d1" in namespace "security-context-test-9152" to be "Succeeded or Failed"
Sep 26 11:42:14.906: INFO: Pod "busybox-user-65534-5a0e19cb-c0fd-4243-8c83-a16651dbc4d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.90871ms
Sep 26 11:42:16.915: INFO: Pod "busybox-user-65534-5a0e19cb-c0fd-4243-8c83-a16651dbc4d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011655127s
Sep 26 11:42:16.915: INFO: Pod "busybox-user-65534-5a0e19cb-c0fd-4243-8c83-a16651dbc4d1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:42:16.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9152" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":263,"skipped":4615,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:42:16.928: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 26 11:42:16.991: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7464  12703476-51b9-4100-bedd-34dd8f571cfa 552415 0 2021-09-26 11:42:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-26 11:42:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 11:42:16.991: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7464  12703476-51b9-4100-bedd-34dd8f571cfa 552416 0 2021-09-26 11:42:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-26 11:42:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 11:42:16.991: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7464  12703476-51b9-4100-bedd-34dd8f571cfa 552417 0 2021-09-26 11:42:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-26 11:42:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 26 11:42:27.023: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7464  12703476-51b9-4100-bedd-34dd8f571cfa 552457 0 2021-09-26 11:42:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-26 11:42:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 11:42:27.023: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7464  12703476-51b9-4100-bedd-34dd8f571cfa 552458 0 2021-09-26 11:42:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-26 11:42:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 11:42:27.023: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7464  12703476-51b9-4100-bedd-34dd8f571cfa 552459 0 2021-09-26 11:42:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-26 11:42:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:42:27.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7464" for this suite.

• [SLOW TEST:10.104 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":264,"skipped":4657,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:42:27.032: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-24a9d54c-2c2d-46e2-a031-d136e8e8909c
STEP: Creating a pod to test consume configMaps
Sep 26 11:42:27.106: INFO: Waiting up to 5m0s for pod "pod-configmaps-7431c4da-f13e-49f9-91fa-e58da67b4093" in namespace "configmap-918" to be "Succeeded or Failed"
Sep 26 11:42:27.110: INFO: Pod "pod-configmaps-7431c4da-f13e-49f9-91fa-e58da67b4093": Phase="Pending", Reason="", readiness=false. Elapsed: 3.651289ms
Sep 26 11:42:29.119: INFO: Pod "pod-configmaps-7431c4da-f13e-49f9-91fa-e58da67b4093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012818288s
STEP: Saw pod success
Sep 26 11:42:29.119: INFO: Pod "pod-configmaps-7431c4da-f13e-49f9-91fa-e58da67b4093" satisfied condition "Succeeded or Failed"
Sep 26 11:42:29.123: INFO: Trying to get logs from node 10.37.21.194 pod pod-configmaps-7431c4da-f13e-49f9-91fa-e58da67b4093 container agnhost-container: <nil>
STEP: delete the pod
Sep 26 11:42:29.147: INFO: Waiting for pod pod-configmaps-7431c4da-f13e-49f9-91fa-e58da67b4093 to disappear
Sep 26 11:42:29.149: INFO: Pod pod-configmaps-7431c4da-f13e-49f9-91fa-e58da67b4093 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:42:29.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-918" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":265,"skipped":4670,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:42:29.158: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:42:29.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-1761 version'
Sep 26 11:42:29.273: INFO: stderr: ""
Sep 26 11:42:29.273: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.2\", GitCommit:\"8b5a19147530eaac9476b0ab82980b4088bbc1b2\", GitTreeState:\"clean\", BuildDate:\"2021-09-15T21:38:50Z\", GoVersion:\"go1.16.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.2\", GitCommit:\"8b5a19147530eaac9476b0ab82980b4088bbc1b2\", GitTreeState:\"clean\", BuildDate:\"2021-09-15T21:32:41Z\", GoVersion:\"go1.16.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:42:29.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1761" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":266,"skipped":4683,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:42:29.285: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:42:29.327: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:42:34.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6771" for this suite.

• [SLOW TEST:5.597 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":267,"skipped":4695,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:42:34.883: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-4a5a0382-dc86-4e19-ba0f-2e3e99a033f4 in namespace container-probe-7697
Sep 26 11:42:36.964: INFO: Started pod busybox-4a5a0382-dc86-4e19-ba0f-2e3e99a033f4 in namespace container-probe-7697
STEP: checking the pod's current state and verifying that restartCount is present
Sep 26 11:42:36.968: INFO: Initial restart count of pod busybox-4a5a0382-dc86-4e19-ba0f-2e3e99a033f4 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:46:38.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7697" for this suite.

• [SLOW TEST:243.821 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":268,"skipped":4708,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:46:38.704: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Sep 26 11:46:38.764: INFO: Major version: 1
STEP: Confirm minor version
Sep 26 11:46:38.764: INFO: cleanMinorVersion: 22
Sep 26 11:46:38.764: INFO: Minor version: 22
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:46:38.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8311" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":269,"skipped":4716,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:46:38.776: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:46:39.543: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:46:42.573: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:46:42.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3125" for this suite.
STEP: Destroying namespace "webhook-3125-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":270,"skipped":4738,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:46:42.706: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 11:46:43.488: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 26 11:46:45.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253603, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253603, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253603, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768253603, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-657f8c86c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 11:46:48.533: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:47:00.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8054" for this suite.
STEP: Destroying namespace "webhook-8054-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.041 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":271,"skipped":4740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:47:00.748: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-j8cq
STEP: Creating a pod to test atomic-volume-subpath
Sep 26 11:47:00.935: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j8cq" in namespace "subpath-1558" to be "Succeeded or Failed"
Sep 26 11:47:00.939: INFO: Pod "pod-subpath-test-configmap-j8cq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.632045ms
Sep 26 11:47:02.952: INFO: Pod "pod-subpath-test-configmap-j8cq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017016747s
Sep 26 11:47:04.964: INFO: Pod "pod-subpath-test-configmap-j8cq": Phase="Running", Reason="", readiness=true. Elapsed: 4.028751549s
Sep 26 11:47:06.979: INFO: Pod "pod-subpath-test-configmap-j8cq": Phase="Running", Reason="", readiness=true. Elapsed: 6.044266312s
Sep 26 11:47:08.988: INFO: Pod "pod-subpath-test-configmap-j8cq": Phase="Running", Reason="", readiness=true. Elapsed: 8.053403443s
Sep 26 11:47:10.998: INFO: Pod "pod-subpath-test-configmap-j8cq": Phase="Running", Reason="", readiness=true. Elapsed: 10.062606741s
Sep 26 11:47:13.010: INFO: Pod "pod-subpath-test-configmap-j8cq": Phase="Running", Reason="", readiness=true. Elapsed: 12.074919642s
Sep 26 11:47:15.023: INFO: Pod "pod-subpath-test-configmap-j8cq": Phase="Running", Reason="", readiness=true. Elapsed: 14.087493003s
Sep 26 11:47:17.039: INFO: Pod "pod-subpath-test-configmap-j8cq": Phase="Running", Reason="", readiness=true. Elapsed: 16.103992823s
Sep 26 11:47:19.055: INFO: Pod "pod-subpath-test-configmap-j8cq": Phase="Running", Reason="", readiness=true. Elapsed: 18.119692375s
Sep 26 11:47:21.100: INFO: Pod "pod-subpath-test-configmap-j8cq": Phase="Running", Reason="", readiness=true. Elapsed: 20.165056839s
Sep 26 11:47:23.114: INFO: Pod "pod-subpath-test-configmap-j8cq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.178640213s
STEP: Saw pod success
Sep 26 11:47:23.114: INFO: Pod "pod-subpath-test-configmap-j8cq" satisfied condition "Succeeded or Failed"
Sep 26 11:47:23.119: INFO: Trying to get logs from node 10.37.21.195 pod pod-subpath-test-configmap-j8cq container test-container-subpath-configmap-j8cq: <nil>
STEP: delete the pod
Sep 26 11:47:23.155: INFO: Waiting for pod pod-subpath-test-configmap-j8cq to disappear
Sep 26 11:47:23.159: INFO: Pod pod-subpath-test-configmap-j8cq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j8cq
Sep 26 11:47:23.159: INFO: Deleting pod "pod-subpath-test-configmap-j8cq" in namespace "subpath-1558"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:47:23.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1558" for this suite.

• [SLOW TEST:22.428 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":272,"skipped":4764,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:47:23.176: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 26 11:47:25.318: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:47:25.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8773" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":273,"skipped":4781,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:47:25.346: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 26 11:47:25.407: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 26 11:48:25.450: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Sep 26 11:48:25.492: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep 26 11:48:25.498: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep 26 11:48:25.520: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep 26 11:48:25.526: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Sep 26 11:48:25.546: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Sep 26 11:48:25.552: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:48:37.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-677" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:72.371 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":274,"skipped":4826,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:48:37.718: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Sep 26 11:48:37.779: INFO: Waiting up to 5m0s for pod "downward-api-e967dadb-1084-4119-a5e9-bdf1d1bb4fb3" in namespace "downward-api-2349" to be "Succeeded or Failed"
Sep 26 11:48:37.783: INFO: Pod "downward-api-e967dadb-1084-4119-a5e9-bdf1d1bb4fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.905704ms
Sep 26 11:48:39.798: INFO: Pod "downward-api-e967dadb-1084-4119-a5e9-bdf1d1bb4fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018388154s
Sep 26 11:48:41.808: INFO: Pod "downward-api-e967dadb-1084-4119-a5e9-bdf1d1bb4fb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02805904s
STEP: Saw pod success
Sep 26 11:48:41.808: INFO: Pod "downward-api-e967dadb-1084-4119-a5e9-bdf1d1bb4fb3" satisfied condition "Succeeded or Failed"
Sep 26 11:48:41.811: INFO: Trying to get logs from node 10.37.21.195 pod downward-api-e967dadb-1084-4119-a5e9-bdf1d1bb4fb3 container dapi-container: <nil>
STEP: delete the pod
Sep 26 11:48:41.914: INFO: Waiting for pod downward-api-e967dadb-1084-4119-a5e9-bdf1d1bb4fb3 to disappear
Sep 26 11:48:41.917: INFO: Pod downward-api-e967dadb-1084-4119-a5e9-bdf1d1bb4fb3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:48:41.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2349" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":4864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:48:41.927: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-6357e3f2-022d-44aa-8731-5d1dfb4501f9
STEP: Creating a pod to test consume secrets
Sep 26 11:48:42.004: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7108a5d1-11db-4724-936d-f0701e93453a" in namespace "projected-6449" to be "Succeeded or Failed"
Sep 26 11:48:42.008: INFO: Pod "pod-projected-secrets-7108a5d1-11db-4724-936d-f0701e93453a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.438197ms
Sep 26 11:48:44.017: INFO: Pod "pod-projected-secrets-7108a5d1-11db-4724-936d-f0701e93453a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012335461s
Sep 26 11:48:46.026: INFO: Pod "pod-projected-secrets-7108a5d1-11db-4724-936d-f0701e93453a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021847602s
STEP: Saw pod success
Sep 26 11:48:46.026: INFO: Pod "pod-projected-secrets-7108a5d1-11db-4724-936d-f0701e93453a" satisfied condition "Succeeded or Failed"
Sep 26 11:48:46.029: INFO: Trying to get logs from node 10.37.21.194 pod pod-projected-secrets-7108a5d1-11db-4724-936d-f0701e93453a container secret-volume-test: <nil>
STEP: delete the pod
Sep 26 11:48:46.057: INFO: Waiting for pod pod-projected-secrets-7108a5d1-11db-4724-936d-f0701e93453a to disappear
Sep 26 11:48:46.060: INFO: Pod pod-projected-secrets-7108a5d1-11db-4724-936d-f0701e93453a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:48:46.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6449" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":4919,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:48:46.077: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 26 11:48:46.149: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep 26 11:48:46.153: INFO: starting watch
STEP: patching
STEP: updating
Sep 26 11:48:46.171: INFO: waiting for watch events with expected annotations
Sep 26 11:48:46.171: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:48:46.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-394" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":277,"skipped":4927,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:48:46.212: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Sep 26 11:48:46.254: INFO: Waiting up to 5m0s for pod "downward-api-82972464-b6ad-437f-92fb-82d075ec8280" in namespace "downward-api-7660" to be "Succeeded or Failed"
Sep 26 11:48:46.257: INFO: Pod "downward-api-82972464-b6ad-437f-92fb-82d075ec8280": Phase="Pending", Reason="", readiness=false. Elapsed: 2.8653ms
Sep 26 11:48:48.277: INFO: Pod "downward-api-82972464-b6ad-437f-92fb-82d075ec8280": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022915931s
Sep 26 11:48:50.290: INFO: Pod "downward-api-82972464-b6ad-437f-92fb-82d075ec8280": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036205628s
STEP: Saw pod success
Sep 26 11:48:50.290: INFO: Pod "downward-api-82972464-b6ad-437f-92fb-82d075ec8280" satisfied condition "Succeeded or Failed"
Sep 26 11:48:50.293: INFO: Trying to get logs from node 10.37.21.195 pod downward-api-82972464-b6ad-437f-92fb-82d075ec8280 container dapi-container: <nil>
STEP: delete the pod
Sep 26 11:48:50.314: INFO: Waiting for pod downward-api-82972464-b6ad-437f-92fb-82d075ec8280 to disappear
Sep 26 11:48:50.317: INFO: Pod downward-api-82972464-b6ad-437f-92fb-82d075ec8280 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:48:50.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7660" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":278,"skipped":4933,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:48:50.326: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-kbsb
STEP: Creating a pod to test atomic-volume-subpath
Sep 26 11:48:50.388: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kbsb" in namespace "subpath-1417" to be "Succeeded or Failed"
Sep 26 11:48:50.391: INFO: Pod "pod-subpath-test-configmap-kbsb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.308957ms
Sep 26 11:48:52.396: INFO: Pod "pod-subpath-test-configmap-kbsb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008239114s
Sep 26 11:48:54.408: INFO: Pod "pod-subpath-test-configmap-kbsb": Phase="Running", Reason="", readiness=true. Elapsed: 4.020262369s
Sep 26 11:48:56.420: INFO: Pod "pod-subpath-test-configmap-kbsb": Phase="Running", Reason="", readiness=true. Elapsed: 6.031796847s
Sep 26 11:48:58.431: INFO: Pod "pod-subpath-test-configmap-kbsb": Phase="Running", Reason="", readiness=true. Elapsed: 8.043490698s
Sep 26 11:49:00.449: INFO: Pod "pod-subpath-test-configmap-kbsb": Phase="Running", Reason="", readiness=true. Elapsed: 10.061435257s
Sep 26 11:49:02.456: INFO: Pod "pod-subpath-test-configmap-kbsb": Phase="Running", Reason="", readiness=true. Elapsed: 12.068134742s
Sep 26 11:49:04.466: INFO: Pod "pod-subpath-test-configmap-kbsb": Phase="Running", Reason="", readiness=true. Elapsed: 14.078182s
Sep 26 11:49:06.475: INFO: Pod "pod-subpath-test-configmap-kbsb": Phase="Running", Reason="", readiness=true. Elapsed: 16.087481811s
Sep 26 11:49:08.488: INFO: Pod "pod-subpath-test-configmap-kbsb": Phase="Running", Reason="", readiness=true. Elapsed: 18.099712599s
Sep 26 11:49:10.500: INFO: Pod "pod-subpath-test-configmap-kbsb": Phase="Running", Reason="", readiness=true. Elapsed: 20.111848856s
Sep 26 11:49:12.507: INFO: Pod "pod-subpath-test-configmap-kbsb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.118841114s
STEP: Saw pod success
Sep 26 11:49:12.507: INFO: Pod "pod-subpath-test-configmap-kbsb" satisfied condition "Succeeded or Failed"
Sep 26 11:49:12.511: INFO: Trying to get logs from node 10.37.21.194 pod pod-subpath-test-configmap-kbsb container test-container-subpath-configmap-kbsb: <nil>
STEP: delete the pod
Sep 26 11:49:12.531: INFO: Waiting for pod pod-subpath-test-configmap-kbsb to disappear
Sep 26 11:49:12.535: INFO: Pod pod-subpath-test-configmap-kbsb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kbsb
Sep 26 11:49:12.535: INFO: Deleting pod "pod-subpath-test-configmap-kbsb" in namespace "subpath-1417"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:49:12.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1417" for this suite.

• [SLOW TEST:22.222 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":279,"skipped":4937,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:49:12.549: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:55:00.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2372" for this suite.

• [SLOW TEST:348.115 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":280,"skipped":5011,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:55:00.665: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Sep 26 11:55:00.745: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:55:02.755: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Sep 26 11:55:02.768: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 26 11:55:04.786: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 26 11:55:04.829: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 26 11:55:04.834: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 26 11:55:06.834: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 26 11:55:06.843: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 26 11:55:08.834: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 26 11:55:08.850: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:55:08.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6106" for this suite.

• [SLOW TEST:8.200 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":281,"skipped":5023,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:55:08.865: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-9162
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9162
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9162
Sep 26 11:55:08.940: INFO: Found 0 stateful pods, waiting for 1
Sep 26 11:55:18.956: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 26 11:55:18.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-9162 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 26 11:55:19.252: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 26 11:55:19.252: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 26 11:55:19.252: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 26 11:55:19.258: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 26 11:55:29.283: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 26 11:55:29.283: INFO: Waiting for statefulset status.replicas updated to 0
Sep 26 11:55:29.347: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999992s
Sep 26 11:55:30.356: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995394985s
Sep 26 11:55:31.368: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985094255s
Sep 26 11:55:32.379: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.974442391s
Sep 26 11:55:33.395: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.962186593s
Sep 26 11:55:34.405: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.947414523s
Sep 26 11:55:35.413: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.937152153s
Sep 26 11:55:36.419: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.929195171s
Sep 26 11:55:37.429: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.922734474s
Sep 26 11:55:38.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 913.625163ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9162
Sep 26 11:55:39.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-9162 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 26 11:55:39.682: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 26 11:55:39.682: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 26 11:55:39.682: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 26 11:55:39.688: INFO: Found 1 stateful pods, waiting for 3
Sep 26 11:55:49.698: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 26 11:55:49.699: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 26 11:55:49.699: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 26 11:55:49.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-9162 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 26 11:55:49.946: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 26 11:55:49.946: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 26 11:55:49.946: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 26 11:55:49.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-9162 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 26 11:55:50.176: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 26 11:55:50.176: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 26 11:55:50.176: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 26 11:55:50.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-9162 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 26 11:55:50.437: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 26 11:55:50.437: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 26 11:55:50.438: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 26 11:55:50.438: INFO: Waiting for statefulset status.replicas updated to 0
Sep 26 11:55:50.443: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 26 11:56:00.465: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 26 11:56:00.465: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 26 11:56:00.465: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 26 11:56:00.485: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999617s
Sep 26 11:56:01.494: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995676125s
Sep 26 11:56:02.503: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987697532s
Sep 26 11:56:03.516: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97843059s
Sep 26 11:56:04.528: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964955941s
Sep 26 11:56:05.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.952531339s
Sep 26 11:56:06.648: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.939859828s
Sep 26 11:56:07.657: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.832913692s
Sep 26 11:56:08.668: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.823417612s
Sep 26 11:56:09.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 813.322721ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9162
Sep 26 11:56:10.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-9162 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 26 11:56:10.936: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 26 11:56:10.936: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 26 11:56:10.936: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 26 11:56:10.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-9162 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 26 11:56:11.170: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 26 11:56:11.170: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 26 11:56:11.170: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 26 11:56:11.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=statefulset-9162 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 26 11:56:11.411: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 26 11:56:11.412: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 26 11:56:11.412: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 26 11:56:11.412: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Sep 26 11:56:21.456: INFO: Deleting all statefulset in ns statefulset-9162
Sep 26 11:56:21.460: INFO: Scaling statefulset ss to 0
Sep 26 11:56:21.480: INFO: Waiting for statefulset status.replicas updated to 0
Sep 26 11:56:21.491: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:56:21.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9162" for this suite.

• [SLOW TEST:72.657 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":282,"skipped":5030,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:56:21.522: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Sep 26 11:58:22.135: INFO: Successfully updated pod "var-expansion-4082c06e-046f-4081-aa49-35afa8abd7dd"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Sep 26 11:58:24.149: INFO: Deleting pod "var-expansion-4082c06e-046f-4081-aa49-35afa8abd7dd" in namespace "var-expansion-9354"
Sep 26 11:58:24.160: INFO: Wait up to 5m0s for pod "var-expansion-4082c06e-046f-4081-aa49-35afa8abd7dd" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:58:56.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9354" for this suite.

• [SLOW TEST:154.672 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":283,"skipped":5047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:58:56.195: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Sep 26 11:58:56.251: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:58:59.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4317" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":284,"skipped":5075,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:58:59.293: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-5e77067b-0f12-4dfd-9a80-aeaa3e81c9a0
STEP: Creating a pod to test consume configMaps
Sep 26 11:58:59.360: INFO: Waiting up to 5m0s for pod "pod-configmaps-a8c55065-8a4f-4623-9bda-6973f1f81afe" in namespace "configmap-3760" to be "Succeeded or Failed"
Sep 26 11:58:59.363: INFO: Pod "pod-configmaps-a8c55065-8a4f-4623-9bda-6973f1f81afe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.761177ms
Sep 26 11:59:01.380: INFO: Pod "pod-configmaps-a8c55065-8a4f-4623-9bda-6973f1f81afe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020497711s
STEP: Saw pod success
Sep 26 11:59:01.380: INFO: Pod "pod-configmaps-a8c55065-8a4f-4623-9bda-6973f1f81afe" satisfied condition "Succeeded or Failed"
Sep 26 11:59:01.384: INFO: Trying to get logs from node 10.37.21.194 pod pod-configmaps-a8c55065-8a4f-4623-9bda-6973f1f81afe container agnhost-container: <nil>
STEP: delete the pod
Sep 26 11:59:01.432: INFO: Waiting for pod pod-configmaps-a8c55065-8a4f-4623-9bda-6973f1f81afe to disappear
Sep 26 11:59:01.436: INFO: Pod pod-configmaps-a8c55065-8a4f-4623-9bda-6973f1f81afe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:59:01.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3760" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":5083,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:59:01.448: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Sep 26 11:59:01.521: INFO: Pod name sample-pod: Found 0 pods out of 3
Sep 26 11:59:06.537: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Sep 26 11:59:06.540: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:59:06.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-296" for this suite.

• [SLOW TEST:5.121 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":286,"skipped":5084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:59:06.569: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-6219
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 11:59:06.658: INFO: Found 0 stateful pods, waiting for 1
Sep 26 11:59:16.671: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Sep 26 11:59:16.690: INFO: Found 1 stateful pods, waiting for 2
Sep 26 11:59:26.710: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 26 11:59:26.710: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Sep 26 11:59:26.736: INFO: Deleting all statefulset in ns statefulset-6219
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 11:59:26.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6219" for this suite.

• [SLOW TEST:20.192 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":287,"skipped":5134,"failed":0}
SSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 11:59:26.762: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4300, will wait for the garbage collector to delete the pods
Sep 26 11:59:28.897: INFO: Deleting Job.batch foo took: 9.962083ms
Sep 26 11:59:28.997: INFO: Terminating Job.batch foo pods took: 100.683057ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:00:01.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4300" for this suite.

• [SLOW TEST:34.968 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":288,"skipped":5140,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:00:01.730: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 12:00:01.804: INFO: The status of Pod pod-secrets-4113fe95-853a-4e5c-80c5-45d30b332e07 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 12:00:03.814: INFO: The status of Pod pod-secrets-4113fe95-853a-4e5c-80c5-45d30b332e07 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:00:03.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2361" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":289,"skipped":5161,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:00:03.860: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 12:00:03.932: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1f69721-325c-4b49-92b3-ead4dee36b52" in namespace "downward-api-5019" to be "Succeeded or Failed"
Sep 26 12:00:03.935: INFO: Pod "downwardapi-volume-d1f69721-325c-4b49-92b3-ead4dee36b52": Phase="Pending", Reason="", readiness=false. Elapsed: 3.324359ms
Sep 26 12:00:05.941: INFO: Pod "downwardapi-volume-d1f69721-325c-4b49-92b3-ead4dee36b52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00890614s
STEP: Saw pod success
Sep 26 12:00:05.941: INFO: Pod "downwardapi-volume-d1f69721-325c-4b49-92b3-ead4dee36b52" satisfied condition "Succeeded or Failed"
Sep 26 12:00:05.944: INFO: Trying to get logs from node 10.37.21.195 pod downwardapi-volume-d1f69721-325c-4b49-92b3-ead4dee36b52 container client-container: <nil>
STEP: delete the pod
Sep 26 12:00:05.971: INFO: Waiting for pod downwardapi-volume-d1f69721-325c-4b49-92b3-ead4dee36b52 to disappear
Sep 26 12:00:05.974: INFO: Pod downwardapi-volume-d1f69721-325c-4b49-92b3-ead4dee36b52 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:00:05.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5019" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":290,"skipped":5178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:00:05.989: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Sep 26 12:00:06.045: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 26 12:00:06.045: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 26 12:00:06.056: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 26 12:00:06.057: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 26 12:00:06.073: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 26 12:00:06.073: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 26 12:00:06.105: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 26 12:00:06.105: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 26 12:00:08.067: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep 26 12:00:08.067: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep 26 12:00:08.071: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Sep 26 12:00:08.085: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 0
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:08.087: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:08.091: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:08.091: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:08.105: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:08.105: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:08.125: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1
Sep 26 12:00:08.125: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1
Sep 26 12:00:09.937: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:09.937: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:09.966: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1
STEP: listing Deployments
Sep 26 12:00:09.972: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Sep 26 12:00:09.989: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Sep 26 12:00:09.996: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 26 12:00:09.999: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 26 12:00:10.013: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 26 12:00:10.030: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 26 12:00:10.036: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 26 12:00:10.046: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 26 12:00:11.993: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 26 12:00:12.331: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Sep 26 12:00:12.371: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 26 12:00:12.388: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 26 12:00:14.425: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Sep 26 12:00:14.464: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1
Sep 26 12:00:14.464: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1
Sep 26 12:00:14.464: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1
Sep 26 12:00:14.464: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1
Sep 26 12:00:14.464: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1
Sep 26 12:00:14.464: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 1
Sep 26 12:00:14.464: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:14.464: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 3
Sep 26 12:00:14.464: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:14.464: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 2
Sep 26 12:00:14.464: INFO: observed Deployment test-deployment in namespace deployment-8865 with ReadyReplicas 3
STEP: deleting the Deployment
Sep 26 12:00:14.473: INFO: observed event type MODIFIED
Sep 26 12:00:14.473: INFO: observed event type MODIFIED
Sep 26 12:00:14.473: INFO: observed event type MODIFIED
Sep 26 12:00:14.473: INFO: observed event type MODIFIED
Sep 26 12:00:14.473: INFO: observed event type MODIFIED
Sep 26 12:00:14.474: INFO: observed event type MODIFIED
Sep 26 12:00:14.474: INFO: observed event type MODIFIED
Sep 26 12:00:14.474: INFO: observed event type MODIFIED
Sep 26 12:00:14.474: INFO: observed event type MODIFIED
Sep 26 12:00:14.474: INFO: observed event type MODIFIED
Sep 26 12:00:14.474: INFO: observed event type MODIFIED
Sep 26 12:00:14.474: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 26 12:00:14.479: INFO: Log out all the ReplicaSets if there is no deployment created
Sep 26 12:00:14.484: INFO: ReplicaSet "test-deployment-59d76c4c4c":
&ReplicaSet{ObjectMeta:{test-deployment-59d76c4c4c  deployment-8865  67b873b8-fe99-4cce-9926-36346ece1ccf 555959 4 2021-09-26 12:00:08 +0000 UTC <nil> <nil> map[pod-template-hash:59d76c4c4c test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e76b5029-f960-4b1f-90ca-ba88d2851638 0xc004552487 0xc004552488}] []  [{kube-controller-manager Update apps/v1 2021-09-26 12:00:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e76b5029-f960-4b1f-90ca-ba88d2851638\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 12:00:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 59d76c4c4c,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:59d76c4c4c test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment registry.dahuatech.com/cncf/pause:3.5 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004552510 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Sep 26 12:00:14.488: INFO: pod: "test-deployment-59d76c4c4c-nq4xt":
&Pod{ObjectMeta:{test-deployment-59d76c4c4c-nq4xt test-deployment-59d76c4c4c- deployment-8865  8403f229-dfbf-4033-a637-55413ebd9d87 555955 0 2021-09-26 12:00:08 +0000 UTC 2021-09-26 12:00:15 +0000 UTC 0xc008abf378 map[pod-template-hash:59d76c4c4c test-deployment-static:true] map[cni.projectcalico.org/containerID:fdfceb1b657bb05f8b2bf61ef3521bbdfe79549abbe8b0a5ebc38c41f0b01c0e cni.projectcalico.org/podIP:192.168.181.126/32 cni.projectcalico.org/podIPs:192.168.181.126/32] [{apps/v1 ReplicaSet test-deployment-59d76c4c4c 67b873b8-fe99-4cce-9926-36346ece1ccf 0xc008abf3a7 0xc008abf3a8}] []  [{kube-controller-manager Update v1 2021-09-26 12:00:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67b873b8-fe99-4cce-9926-36346ece1ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 12:00:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 12:00:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.181.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vchn7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.dahuatech.com/cncf/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vchn7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.195,PodIP:192.168.181.126,StartTime:2021-09-26 12:00:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 12:00:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/pause:3.5,ImageID:docker-pullable://registry.dahuatech.com/cncf/pause@sha256:2f4b437353f90e646504ec8317dacd6123e931152674628289c990a7a05790b0,ContainerID:docker://9de0e67f36e764a406b17ae546a64167d1a10d47bbe18249e8d8153a0ea6425d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.181.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep 26 12:00:14.489: INFO: pod: "test-deployment-59d76c4c4c-td2ms":
&Pod{ObjectMeta:{test-deployment-59d76c4c4c-td2ms test-deployment-59d76c4c4c- deployment-8865  02ce4a24-c438-4f69-bc6b-c1f932615e0f 555945 0 2021-09-26 12:00:09 +0000 UTC 2021-09-26 12:00:13 +0000 UTC 0xc008abf590 map[pod-template-hash:59d76c4c4c test-deployment-static:true] map[cni.projectcalico.org/containerID:eea8d916344aed3f57c043cabbcc3d492d2550bd107d79d9d1ac5aeed369fa1f cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-59d76c4c4c 67b873b8-fe99-4cce-9926-36346ece1ccf 0xc008abf5c7 0xc008abf5c8}] []  [{kube-controller-manager Update v1 2021-09-26 12:00:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67b873b8-fe99-4cce-9926-36346ece1ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 12:00:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 12:00:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.181.66\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-77qnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.dahuatech.com/cncf/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-77qnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.195,PodIP:192.168.181.66,StartTime:2021-09-26 12:00:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 12:00:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/pause:3.5,ImageID:docker-pullable://registry.dahuatech.com/cncf/pause@sha256:2f4b437353f90e646504ec8317dacd6123e931152674628289c990a7a05790b0,ContainerID:docker://fd6b20c1fc3ee85e3634c471f9f2c0148f786ae7d9a78a5a72b2b66289da241e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.181.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep 26 12:00:14.489: INFO: ReplicaSet "test-deployment-5fd94d9bb8":
&ReplicaSet{ObjectMeta:{test-deployment-5fd94d9bb8  deployment-8865  e808cb5f-f1bb-4b4e-a009-0b34dab4fb35 555951 2 2021-09-26 12:00:10 +0000 UTC <nil> <nil> map[pod-template-hash:5fd94d9bb8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e76b5029-f960-4b1f-90ca-ba88d2851638 0xc004552577 0xc004552578}] []  [{kube-controller-manager Update apps/v1 2021-09-26 12:00:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e76b5029-f960-4b1f-90ca-ba88d2851638\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 12:00:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5fd94d9bb8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5fd94d9bb8 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment registry.dahuatech.com/cncf/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004552600 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Sep 26 12:00:14.495: INFO: pod: "test-deployment-5fd94d9bb8-jlbsz":
&Pod{ObjectMeta:{test-deployment-5fd94d9bb8-jlbsz test-deployment-5fd94d9bb8- deployment-8865  5f762170-96cd-4292-98a3-667305dfc9e8 555950 0 2021-09-26 12:00:12 +0000 UTC <nil> <nil> map[pod-template-hash:5fd94d9bb8 test-deployment-static:true] map[cni.projectcalico.org/containerID:568a72fed764a2424b5c2063c7ff65a80afe80c93d1b81915d5c48a1cbe2d15b cni.projectcalico.org/podIP:192.168.50.136/32 cni.projectcalico.org/podIPs:192.168.50.136/32] [{apps/v1 ReplicaSet test-deployment-5fd94d9bb8 e808cb5f-f1bb-4b4e-a009-0b34dab4fb35 0xc0040c5507 0xc0040c5508}] []  [{kube-controller-manager Update v1 2021-09-26 12:00:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e808cb5f-f1bb-4b4e-a009-0b34dab4fb35\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 12:00:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 12:00:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.50.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pgvtg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pgvtg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.194,PodIP:192.168.50.136,StartTime:2021-09-26 12:00:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 12:00:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://f1b6e5be72d04d3682390a3c7fed6e139b28022497b29f9c961d69773d5dceec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.50.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep 26 12:00:14.495: INFO: pod: "test-deployment-5fd94d9bb8-mgqgr":
&Pod{ObjectMeta:{test-deployment-5fd94d9bb8-mgqgr test-deployment-5fd94d9bb8- deployment-8865  09cbbdda-595d-4b93-9a9e-3d03a2c488f5 555914 0 2021-09-26 12:00:10 +0000 UTC <nil> <nil> map[pod-template-hash:5fd94d9bb8 test-deployment-static:true] map[cni.projectcalico.org/containerID:f0812dbb7a2bf1584d29e3ac27c6daa76ee4e904a3d7b03783c0169d05e983bf cni.projectcalico.org/podIP:192.168.50.129/32 cni.projectcalico.org/podIPs:192.168.50.129/32] [{apps/v1 ReplicaSet test-deployment-5fd94d9bb8 e808cb5f-f1bb-4b4e-a009-0b34dab4fb35 0xc0040c5a47 0xc0040c5a48}] []  [{kube-controller-manager Update v1 2021-09-26 12:00:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e808cb5f-f1bb-4b4e-a009-0b34dab4fb35\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 12:00:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 12:00:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.50.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bvs56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bvs56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.194,PodIP:192.168.50.129,StartTime:2021-09-26 12:00:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 12:00:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://a49575a4444ed82fee66c6b14bb01576f0b80ce8b2d148887f554346dbd800bd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.50.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep 26 12:00:14.495: INFO: ReplicaSet "test-deployment-d66cbfb6b":
&ReplicaSet{ObjectMeta:{test-deployment-d66cbfb6b  deployment-8865  b5eb0252-a906-46e2-8364-41f3d5cfb3fe 555844 3 2021-09-26 12:00:06 +0000 UTC <nil> <nil> map[pod-template-hash:d66cbfb6b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e76b5029-f960-4b1f-90ca-ba88d2851638 0xc004552667 0xc004552668}] []  [{kube-controller-manager Update apps/v1 2021-09-26 12:00:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e76b5029-f960-4b1f-90ca-ba88d2851638\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 12:00:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d66cbfb6b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d66cbfb6b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment registry.dahuatech.com/cncf/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0045526f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:00:14.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8865" for this suite.

• [SLOW TEST:8.531 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":291,"skipped":5221,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:00:14.520: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Sep 26 12:00:15.102: INFO: created pod pod-service-account-defaultsa
Sep 26 12:00:15.102: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 26 12:00:15.108: INFO: created pod pod-service-account-mountsa
Sep 26 12:00:15.108: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 26 12:00:15.114: INFO: created pod pod-service-account-nomountsa
Sep 26 12:00:15.114: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 26 12:00:15.135: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 26 12:00:15.135: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 26 12:00:15.144: INFO: created pod pod-service-account-mountsa-mountspec
Sep 26 12:00:15.144: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 26 12:00:15.152: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 26 12:00:15.152: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 26 12:00:15.166: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 26 12:00:15.166: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 26 12:00:15.173: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 26 12:00:15.173: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 26 12:00:15.192: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 26 12:00:15.192: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:00:15.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4523" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":292,"skipped":5229,"failed":0}

------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:00:15.209: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 12:00:15.321: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 26 12:00:15.329: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 26 12:00:20.344: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 26 12:00:20.344: INFO: Creating deployment "test-rolling-update-deployment"
Sep 26 12:00:20.359: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 26 12:00:20.371: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 26 12:00:22.385: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 26 12:00:22.388: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 26 12:00:22.397: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-469  32ace1f9-4e66-4b80-97a8-7b35d775c30d 556227 1 2021-09-26 12:00:20 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-09-26 12:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 12:00:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost registry.dahuatech.com/cncf/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006923fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-26 12:00:20 +0000 UTC,LastTransitionTime:2021-09-26 12:00:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-75b5b5fdb9" has successfully progressed.,LastUpdateTime:2021-09-26 12:00:22 +0000 UTC,LastTransitionTime:2021-09-26 12:00:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 26 12:00:22.400: INFO: New ReplicaSet "test-rolling-update-deployment-75b5b5fdb9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-75b5b5fdb9  deployment-469  68236b54-3787-4c5d-887b-e45282330a21 556217 1 2021-09-26 12:00:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:75b5b5fdb9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 32ace1f9-4e66-4b80-97a8-7b35d775c30d 0xc003e364e7 0xc003e364e8}] []  [{kube-controller-manager Update apps/v1 2021-09-26 12:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32ace1f9-4e66-4b80-97a8-7b35d775c30d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 12:00:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 75b5b5fdb9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:75b5b5fdb9] map[] [] []  []} {[] [] [{agnhost registry.dahuatech.com/cncf/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e365a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 26 12:00:22.400: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 26 12:00:22.400: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-469  1b6a5676-3ee9-4993-9768-95621b2221c1 556226 2 2021-09-26 12:00:15 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 32ace1f9-4e66-4b80-97a8-7b35d775c30d 0xc003e363b7 0xc003e363b8}] []  [{e2e.test Update apps/v1 2021-09-26 12:00:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 12:00:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32ace1f9-4e66-4b80-97a8-7b35d775c30d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2021-09-26 12:00:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd registry.dahuatech.com/cncf/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003e36478 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 26 12:00:22.403: INFO: Pod "test-rolling-update-deployment-75b5b5fdb9-t5mcn" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-75b5b5fdb9-t5mcn test-rolling-update-deployment-75b5b5fdb9- deployment-469  66acaaa0-91eb-4f39-954e-f0f65e3a3278 556216 0 2021-09-26 12:00:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:75b5b5fdb9] map[cni.projectcalico.org/containerID:903d071b5e3674e4dd1123fe69a6124a8a254751067563d1e1c52d8019964dcc cni.projectcalico.org/podIP:192.168.50.137/32 cni.projectcalico.org/podIPs:192.168.50.137/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-75b5b5fdb9 68236b54-3787-4c5d-887b-e45282330a21 0xc003e369f7 0xc003e369f8}] []  [{kube-controller-manager Update v1 2021-09-26 12:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68236b54-3787-4c5d-887b-e45282330a21\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 12:00:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 12:00:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.50.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m44zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.dahuatech.com/cncf/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m44zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:00:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.194,PodIP:192.168.50.137,StartTime:2021-09-26 12:00:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 12:00:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/agnhost:2.32,ImageID:docker-pullable://registry.dahuatech.com/cncf/agnhost@sha256:e76d7fb8b95056e3909026f6024f19c496a908f7471ad3426302c4fb684885f9,ContainerID:docker://0c02865a35b4a6193be7e89d3e1f2c5695826707ed87c2b3f49996ff52f3460e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.50.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:00:22.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-469" for this suite.

• [SLOW TEST:7.204 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":293,"skipped":5229,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:00:22.413: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 12:00:22.468: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f942def2-90fc-4788-9eda-e7780fcc8155" in namespace "downward-api-2322" to be "Succeeded or Failed"
Sep 26 12:00:22.472: INFO: Pod "downwardapi-volume-f942def2-90fc-4788-9eda-e7780fcc8155": Phase="Pending", Reason="", readiness=false. Elapsed: 3.886467ms
Sep 26 12:00:24.481: INFO: Pod "downwardapi-volume-f942def2-90fc-4788-9eda-e7780fcc8155": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012765804s
STEP: Saw pod success
Sep 26 12:00:24.481: INFO: Pod "downwardapi-volume-f942def2-90fc-4788-9eda-e7780fcc8155" satisfied condition "Succeeded or Failed"
Sep 26 12:00:24.485: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-f942def2-90fc-4788-9eda-e7780fcc8155 container client-container: <nil>
STEP: delete the pod
Sep 26 12:00:24.500: INFO: Waiting for pod downwardapi-volume-f942def2-90fc-4788-9eda-e7780fcc8155 to disappear
Sep 26 12:00:24.506: INFO: Pod downwardapi-volume-f942def2-90fc-4788-9eda-e7780fcc8155 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:00:24.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2322" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5230,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:00:24.517: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-gwtw
STEP: Creating a pod to test atomic-volume-subpath
Sep 26 12:00:24.580: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-gwtw" in namespace "subpath-3042" to be "Succeeded or Failed"
Sep 26 12:00:24.586: INFO: Pod "pod-subpath-test-downwardapi-gwtw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.104568ms
Sep 26 12:00:26.599: INFO: Pod "pod-subpath-test-downwardapi-gwtw": Phase="Running", Reason="", readiness=true. Elapsed: 2.018931234s
Sep 26 12:00:28.605: INFO: Pod "pod-subpath-test-downwardapi-gwtw": Phase="Running", Reason="", readiness=true. Elapsed: 4.025409871s
Sep 26 12:00:30.615: INFO: Pod "pod-subpath-test-downwardapi-gwtw": Phase="Running", Reason="", readiness=true. Elapsed: 6.035135922s
Sep 26 12:00:32.626: INFO: Pod "pod-subpath-test-downwardapi-gwtw": Phase="Running", Reason="", readiness=true. Elapsed: 8.046057325s
Sep 26 12:00:34.637: INFO: Pod "pod-subpath-test-downwardapi-gwtw": Phase="Running", Reason="", readiness=true. Elapsed: 10.057117093s
Sep 26 12:00:36.650: INFO: Pod "pod-subpath-test-downwardapi-gwtw": Phase="Running", Reason="", readiness=true. Elapsed: 12.070368306s
Sep 26 12:00:38.671: INFO: Pod "pod-subpath-test-downwardapi-gwtw": Phase="Running", Reason="", readiness=true. Elapsed: 14.090927582s
Sep 26 12:00:40.679: INFO: Pod "pod-subpath-test-downwardapi-gwtw": Phase="Running", Reason="", readiness=true. Elapsed: 16.099336179s
Sep 26 12:00:42.692: INFO: Pod "pod-subpath-test-downwardapi-gwtw": Phase="Running", Reason="", readiness=true. Elapsed: 18.111880404s
Sep 26 12:00:44.696: INFO: Pod "pod-subpath-test-downwardapi-gwtw": Phase="Running", Reason="", readiness=true. Elapsed: 20.116586618s
Sep 26 12:00:46.708: INFO: Pod "pod-subpath-test-downwardapi-gwtw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.127828426s
STEP: Saw pod success
Sep 26 12:00:46.708: INFO: Pod "pod-subpath-test-downwardapi-gwtw" satisfied condition "Succeeded or Failed"
Sep 26 12:00:46.710: INFO: Trying to get logs from node 10.37.21.194 pod pod-subpath-test-downwardapi-gwtw container test-container-subpath-downwardapi-gwtw: <nil>
STEP: delete the pod
Sep 26 12:00:46.735: INFO: Waiting for pod pod-subpath-test-downwardapi-gwtw to disappear
Sep 26 12:00:46.739: INFO: Pod pod-subpath-test-downwardapi-gwtw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-gwtw
Sep 26 12:00:46.739: INFO: Deleting pod "pod-subpath-test-downwardapi-gwtw" in namespace "subpath-3042"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:00:46.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3042" for this suite.

• [SLOW TEST:22.234 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":295,"skipped":5239,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:00:46.752: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-35c4c645-ff0c-4042-be5f-c8d7b42d5463
STEP: Creating a pod to test consume secrets
Sep 26 12:00:46.814: INFO: Waiting up to 5m0s for pod "pod-secrets-f82dce76-9341-46c1-a999-71c669ee79f2" in namespace "secrets-952" to be "Succeeded or Failed"
Sep 26 12:00:46.819: INFO: Pod "pod-secrets-f82dce76-9341-46c1-a999-71c669ee79f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.61346ms
Sep 26 12:00:48.833: INFO: Pod "pod-secrets-f82dce76-9341-46c1-a999-71c669ee79f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018741585s
Sep 26 12:00:50.841: INFO: Pod "pod-secrets-f82dce76-9341-46c1-a999-71c669ee79f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027157185s
STEP: Saw pod success
Sep 26 12:00:50.841: INFO: Pod "pod-secrets-f82dce76-9341-46c1-a999-71c669ee79f2" satisfied condition "Succeeded or Failed"
Sep 26 12:00:50.844: INFO: Trying to get logs from node 10.37.21.194 pod pod-secrets-f82dce76-9341-46c1-a999-71c669ee79f2 container secret-volume-test: <nil>
STEP: delete the pod
Sep 26 12:00:50.862: INFO: Waiting for pod pod-secrets-f82dce76-9341-46c1-a999-71c669ee79f2 to disappear
Sep 26 12:00:50.865: INFO: Pod pod-secrets-f82dce76-9341-46c1-a999-71c669ee79f2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:00:50.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-952" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5241,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:00:50.874: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-5ea07bfd-ef3d-4c66-b88a-12f7d23327de
STEP: Creating a pod to test consume configMaps
Sep 26 12:00:50.927: INFO: Waiting up to 5m0s for pod "pod-configmaps-ff87bef5-025f-4154-b0a9-bbf37ede7984" in namespace "configmap-8295" to be "Succeeded or Failed"
Sep 26 12:00:50.930: INFO: Pod "pod-configmaps-ff87bef5-025f-4154-b0a9-bbf37ede7984": Phase="Pending", Reason="", readiness=false. Elapsed: 3.061425ms
Sep 26 12:00:52.939: INFO: Pod "pod-configmaps-ff87bef5-025f-4154-b0a9-bbf37ede7984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01174579s
STEP: Saw pod success
Sep 26 12:00:52.939: INFO: Pod "pod-configmaps-ff87bef5-025f-4154-b0a9-bbf37ede7984" satisfied condition "Succeeded or Failed"
Sep 26 12:00:52.942: INFO: Trying to get logs from node 10.37.21.194 pod pod-configmaps-ff87bef5-025f-4154-b0a9-bbf37ede7984 container agnhost-container: <nil>
STEP: delete the pod
Sep 26 12:00:52.959: INFO: Waiting for pod pod-configmaps-ff87bef5-025f-4154-b0a9-bbf37ede7984 to disappear
Sep 26 12:00:52.962: INFO: Pod pod-configmaps-ff87bef5-025f-4154-b0a9-bbf37ede7984 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:00:52.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8295" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":297,"skipped":5268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:00:52.970: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Sep 26 12:00:53.024: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Sep 26 12:00:55.045: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Sep 26 12:00:57.059: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:00:59.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5715" for this suite.

• [SLOW TEST:6.115 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":298,"skipped":5303,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:00:59.086: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Sep 26 12:00:59.137: INFO: The status of Pod pod-update-ec576e60-e834-4014-84e1-a59c38ac4ea0 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 12:01:01.146: INFO: The status of Pod pod-update-ec576e60-e834-4014-84e1-a59c38ac4ea0 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 26 12:01:01.670: INFO: Successfully updated pod "pod-update-ec576e60-e834-4014-84e1-a59c38ac4ea0"
STEP: verifying the updated pod is in kubernetes
Sep 26 12:01:01.677: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:01:01.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-289" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5327,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:01:01.686: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-2061/configmap-test-5bc4634d-f60b-4768-8a38-bdf793e0954d
STEP: Creating a pod to test consume configMaps
Sep 26 12:01:01.771: INFO: Waiting up to 5m0s for pod "pod-configmaps-bcd5e83c-d468-4f28-9441-0e64cf6a23c8" in namespace "configmap-2061" to be "Succeeded or Failed"
Sep 26 12:01:01.774: INFO: Pod "pod-configmaps-bcd5e83c-d468-4f28-9441-0e64cf6a23c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.1861ms
Sep 26 12:01:03.783: INFO: Pod "pod-configmaps-bcd5e83c-d468-4f28-9441-0e64cf6a23c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012763565s
STEP: Saw pod success
Sep 26 12:01:03.784: INFO: Pod "pod-configmaps-bcd5e83c-d468-4f28-9441-0e64cf6a23c8" satisfied condition "Succeeded or Failed"
Sep 26 12:01:03.787: INFO: Trying to get logs from node 10.37.21.194 pod pod-configmaps-bcd5e83c-d468-4f28-9441-0e64cf6a23c8 container env-test: <nil>
STEP: delete the pod
Sep 26 12:01:03.804: INFO: Waiting for pod pod-configmaps-bcd5e83c-d468-4f28-9441-0e64cf6a23c8 to disappear
Sep 26 12:01:03.807: INFO: Pod pod-configmaps-bcd5e83c-d468-4f28-9441-0e64cf6a23c8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:01:03.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2061" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":300,"skipped":5343,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:01:03.816: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 12:01:03.867: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-258a0f00-902b-4285-a816-16a5512dcd6f" in namespace "security-context-test-7038" to be "Succeeded or Failed"
Sep 26 12:01:03.872: INFO: Pod "alpine-nnp-false-258a0f00-902b-4285-a816-16a5512dcd6f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.081719ms
Sep 26 12:01:05.880: INFO: Pod "alpine-nnp-false-258a0f00-902b-4285-a816-16a5512dcd6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012389423s
Sep 26 12:01:05.880: INFO: Pod "alpine-nnp-false-258a0f00-902b-4285-a816-16a5512dcd6f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:01:05.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7038" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":301,"skipped":5353,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:01:05.898: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Sep 26 12:01:05.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=kubectl-3649 cluster-info'
Sep 26 12:01:06.017: INFO: stderr: ""
Sep 26 12:01:06.017: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:01:06.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3649" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":302,"skipped":5356,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:01:06.030: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 26 12:01:06.348: INFO: Pod name wrapped-volume-race-3d96e622-0e49-4056-b201-684f88e816f1: Found 4 pods out of 5
Sep 26 12:01:11.361: INFO: Pod name wrapped-volume-race-3d96e622-0e49-4056-b201-684f88e816f1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3d96e622-0e49-4056-b201-684f88e816f1 in namespace emptydir-wrapper-850, will wait for the garbage collector to delete the pods
Sep 26 12:01:21.454: INFO: Deleting ReplicationController wrapped-volume-race-3d96e622-0e49-4056-b201-684f88e816f1 took: 9.22889ms
Sep 26 12:01:21.555: INFO: Terminating ReplicationController wrapped-volume-race-3d96e622-0e49-4056-b201-684f88e816f1 pods took: 100.560443ms
STEP: Creating RC which spawns configmap-volume pods
Sep 26 12:01:25.995: INFO: Pod name wrapped-volume-race-a30d6c4d-8d04-47a3-96a8-d58c092baa67: Found 0 pods out of 5
Sep 26 12:01:31.008: INFO: Pod name wrapped-volume-race-a30d6c4d-8d04-47a3-96a8-d58c092baa67: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a30d6c4d-8d04-47a3-96a8-d58c092baa67 in namespace emptydir-wrapper-850, will wait for the garbage collector to delete the pods
Sep 26 12:01:43.125: INFO: Deleting ReplicationController wrapped-volume-race-a30d6c4d-8d04-47a3-96a8-d58c092baa67 took: 13.094912ms
Sep 26 12:01:43.225: INFO: Terminating ReplicationController wrapped-volume-race-a30d6c4d-8d04-47a3-96a8-d58c092baa67 pods took: 100.345939ms
STEP: Creating RC which spawns configmap-volume pods
Sep 26 12:01:46.254: INFO: Pod name wrapped-volume-race-2b34f930-4c5c-48da-8ea0-4d5dbcc39f14: Found 0 pods out of 5
Sep 26 12:01:51.268: INFO: Pod name wrapped-volume-race-2b34f930-4c5c-48da-8ea0-4d5dbcc39f14: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2b34f930-4c5c-48da-8ea0-4d5dbcc39f14 in namespace emptydir-wrapper-850, will wait for the garbage collector to delete the pods
Sep 26 12:02:01.358: INFO: Deleting ReplicationController wrapped-volume-race-2b34f930-4c5c-48da-8ea0-4d5dbcc39f14 took: 9.000865ms
Sep 26 12:02:01.458: INFO: Terminating ReplicationController wrapped-volume-race-2b34f930-4c5c-48da-8ea0-4d5dbcc39f14 pods took: 100.102077ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:02:05.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-850" for this suite.

• [SLOW TEST:59.860 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":303,"skipped":5359,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:02:05.890: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 26 12:02:05.983: INFO: Number of nodes with available pods: 0
Sep 26 12:02:05.983: INFO: Node 10.37.21.193 is running more than one daemon pod
Sep 26 12:02:06.999: INFO: Number of nodes with available pods: 0
Sep 26 12:02:06.999: INFO: Node 10.37.21.193 is running more than one daemon pod
Sep 26 12:02:07.994: INFO: Number of nodes with available pods: 2
Sep 26 12:02:07.994: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 12:02:08.995: INFO: Number of nodes with available pods: 3
Sep 26 12:02:08.995: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 26 12:02:09.018: INFO: Number of nodes with available pods: 2
Sep 26 12:02:09.018: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 12:02:10.030: INFO: Number of nodes with available pods: 2
Sep 26 12:02:10.030: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 12:02:11.029: INFO: Number of nodes with available pods: 2
Sep 26 12:02:11.029: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 12:02:12.030: INFO: Number of nodes with available pods: 2
Sep 26 12:02:12.031: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 12:02:13.029: INFO: Number of nodes with available pods: 2
Sep 26 12:02:13.029: INFO: Node 10.37.21.195 is running more than one daemon pod
Sep 26 12:02:14.029: INFO: Number of nodes with available pods: 3
Sep 26 12:02:14.029: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8244, will wait for the garbage collector to delete the pods
Sep 26 12:02:14.098: INFO: Deleting DaemonSet.extensions daemon-set took: 11.447821ms
Sep 26 12:02:14.198: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.533489ms
Sep 26 12:02:16.515: INFO: Number of nodes with available pods: 0
Sep 26 12:02:16.515: INFO: Number of running nodes: 0, number of available pods: 0
Sep 26 12:02:16.518: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"557803"},"items":null}

Sep 26 12:02:16.521: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"557803"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:02:16.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8244" for this suite.

• [SLOW TEST:10.654 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":304,"skipped":5372,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:02:16.544: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-8b686137-5a2a-477f-93fa-b278dfe25722
STEP: Creating a pod to test consume configMaps
Sep 26 12:02:16.592: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-830f3c46-96d6-44d5-b1c7-1c5e8404d4b7" in namespace "projected-7107" to be "Succeeded or Failed"
Sep 26 12:02:16.596: INFO: Pod "pod-projected-configmaps-830f3c46-96d6-44d5-b1c7-1c5e8404d4b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.401576ms
Sep 26 12:02:18.610: INFO: Pod "pod-projected-configmaps-830f3c46-96d6-44d5-b1c7-1c5e8404d4b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017078359s
Sep 26 12:02:20.616: INFO: Pod "pod-projected-configmaps-830f3c46-96d6-44d5-b1c7-1c5e8404d4b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023250265s
STEP: Saw pod success
Sep 26 12:02:20.616: INFO: Pod "pod-projected-configmaps-830f3c46-96d6-44d5-b1c7-1c5e8404d4b7" satisfied condition "Succeeded or Failed"
Sep 26 12:02:20.620: INFO: Trying to get logs from node 10.37.21.194 pod pod-projected-configmaps-830f3c46-96d6-44d5-b1c7-1c5e8404d4b7 container agnhost-container: <nil>
STEP: delete the pod
Sep 26 12:02:20.641: INFO: Waiting for pod pod-projected-configmaps-830f3c46-96d6-44d5-b1c7-1c5e8404d4b7 to disappear
Sep 26 12:02:20.644: INFO: Pod pod-projected-configmaps-830f3c46-96d6-44d5-b1c7-1c5e8404d4b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:02:20.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7107" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":305,"skipped":5381,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:02:20.654: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-8ef281c3-b1e2-4756-9ab8-ef236e8418a3
STEP: Creating a pod to test consume configMaps
Sep 26 12:02:20.724: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4cb88006-b603-440d-8eaa-4e1038903e95" in namespace "projected-3791" to be "Succeeded or Failed"
Sep 26 12:02:20.728: INFO: Pod "pod-projected-configmaps-4cb88006-b603-440d-8eaa-4e1038903e95": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980313ms
Sep 26 12:02:22.738: INFO: Pod "pod-projected-configmaps-4cb88006-b603-440d-8eaa-4e1038903e95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014006703s
STEP: Saw pod success
Sep 26 12:02:22.738: INFO: Pod "pod-projected-configmaps-4cb88006-b603-440d-8eaa-4e1038903e95" satisfied condition "Succeeded or Failed"
Sep 26 12:02:22.741: INFO: Trying to get logs from node 10.37.21.195 pod pod-projected-configmaps-4cb88006-b603-440d-8eaa-4e1038903e95 container agnhost-container: <nil>
STEP: delete the pod
Sep 26 12:02:22.777: INFO: Waiting for pod pod-projected-configmaps-4cb88006-b603-440d-8eaa-4e1038903e95 to disappear
Sep 26 12:02:22.780: INFO: Pod pod-projected-configmaps-4cb88006-b603-440d-8eaa-4e1038903e95 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:02:22.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3791" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":306,"skipped":5381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:02:22.789: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-5981
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5981
STEP: Waiting until pod test-pod will start running in namespace statefulset-5981
STEP: Creating statefulset with conflicting port in namespace statefulset-5981
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5981
Sep 26 12:02:26.886: INFO: Observed stateful pod in namespace: statefulset-5981, name: ss-0, uid: 30708ced-6802-448e-ae62-bacbc74ef109, status phase: Pending. Waiting for statefulset controller to delete.
Sep 26 12:02:26.899: INFO: Observed stateful pod in namespace: statefulset-5981, name: ss-0, uid: 30708ced-6802-448e-ae62-bacbc74ef109, status phase: Failed. Waiting for statefulset controller to delete.
Sep 26 12:02:26.906: INFO: Observed stateful pod in namespace: statefulset-5981, name: ss-0, uid: 30708ced-6802-448e-ae62-bacbc74ef109, status phase: Failed. Waiting for statefulset controller to delete.
Sep 26 12:02:26.913: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5981
STEP: Removing pod with conflicting port in namespace statefulset-5981
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5981 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Sep 26 12:02:30.955: INFO: Deleting all statefulset in ns statefulset-5981
Sep 26 12:02:30.959: INFO: Scaling statefulset ss to 0
Sep 26 12:02:40.991: INFO: Waiting for statefulset status.replicas updated to 0
Sep 26 12:02:40.995: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:02:41.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5981" for this suite.

• [SLOW TEST:18.241 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":307,"skipped":5410,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:02:41.031: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-d963ff33-6422-4c11-89e7-222f319d9a1c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:02:45.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8568" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":5436,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:02:45.167: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2427.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2427.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2427.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2427.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2427.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2427.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 26 12:02:49.281: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2427/dns-test-69a0f6cd-c9cb-4d3d-afeb-11b7858aaeec: the server could not find the requested resource (get pods dns-test-69a0f6cd-c9cb-4d3d-afeb-11b7858aaeec)
Sep 26 12:02:49.284: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2427/dns-test-69a0f6cd-c9cb-4d3d-afeb-11b7858aaeec: the server could not find the requested resource (get pods dns-test-69a0f6cd-c9cb-4d3d-afeb-11b7858aaeec)
Sep 26 12:02:49.293: INFO: Unable to read jessie_udp@PodARecord from pod dns-2427/dns-test-69a0f6cd-c9cb-4d3d-afeb-11b7858aaeec: the server could not find the requested resource (get pods dns-test-69a0f6cd-c9cb-4d3d-afeb-11b7858aaeec)
Sep 26 12:02:49.300: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2427/dns-test-69a0f6cd-c9cb-4d3d-afeb-11b7858aaeec: the server could not find the requested resource (get pods dns-test-69a0f6cd-c9cb-4d3d-afeb-11b7858aaeec)
Sep 26 12:02:49.300: INFO: Lookups using dns-2427/dns-test-69a0f6cd-c9cb-4d3d-afeb-11b7858aaeec failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 26 12:02:54.342: INFO: DNS probes using dns-2427/dns-test-69a0f6cd-c9cb-4d3d-afeb-11b7858aaeec succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:02:54.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2427" for this suite.

• [SLOW TEST:9.220 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":309,"skipped":5458,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:02:54.387: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Sep 26 12:02:54.474: INFO: The status of Pod pod-hostip-9e716eaf-204c-4193-8a40-a5def6823908 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 12:02:56.484: INFO: The status of Pod pod-hostip-9e716eaf-204c-4193-8a40-a5def6823908 is Running (Ready = true)
Sep 26 12:02:56.490: INFO: Pod pod-hostip-9e716eaf-204c-4193-8a40-a5def6823908 has hostIP: 10.37.21.194
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:02:56.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5251" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":5462,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:02:56.500: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 12:02:56.568: INFO: Create a RollingUpdate DaemonSet
Sep 26 12:02:56.574: INFO: Check that daemon pods launch on every node of the cluster
Sep 26 12:02:56.582: INFO: Number of nodes with available pods: 0
Sep 26 12:02:56.582: INFO: Node 10.37.21.193 is running more than one daemon pod
Sep 26 12:02:57.594: INFO: Number of nodes with available pods: 0
Sep 26 12:02:57.594: INFO: Node 10.37.21.193 is running more than one daemon pod
Sep 26 12:02:58.595: INFO: Number of nodes with available pods: 3
Sep 26 12:02:58.595: INFO: Number of running nodes: 3, number of available pods: 3
Sep 26 12:02:58.595: INFO: Update the DaemonSet to trigger a rollout
Sep 26 12:02:58.603: INFO: Updating DaemonSet daemon-set
Sep 26 12:03:01.624: INFO: Roll back the DaemonSet before rollout is complete
Sep 26 12:03:01.641: INFO: Updating DaemonSet daemon-set
Sep 26 12:03:01.641: INFO: Make sure DaemonSet rollback is complete
Sep 26 12:03:01.645: INFO: Wrong image for pod: daemon-set-g87pq. Expected: registry.dahuatech.com/cncf/httpd:2.4.38-1, got: foo:non-existent.
Sep 26 12:03:01.645: INFO: Pod daemon-set-g87pq is not available
Sep 26 12:03:04.659: INFO: Pod daemon-set-7qfjg is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1409, will wait for the garbage collector to delete the pods
Sep 26 12:03:04.738: INFO: Deleting DaemonSet.extensions daemon-set took: 12.709473ms
Sep 26 12:03:04.839: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.734069ms
Sep 26 12:03:07.152: INFO: Number of nodes with available pods: 0
Sep 26 12:03:07.152: INFO: Number of running nodes: 0, number of available pods: 0
Sep 26 12:03:07.155: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"558363"},"items":null}

Sep 26 12:03:07.158: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"558363"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:03:07.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1409" for this suite.

• [SLOW TEST:10.683 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":311,"skipped":5484,"failed":0}
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:03:07.183: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Sep 26 12:03:07.232: INFO: The status of Pod labelsupdate1b023c9d-640b-4a1d-816f-8ce17d716fed is Pending, waiting for it to be Running (with Ready = true)
Sep 26 12:03:09.239: INFO: The status of Pod labelsupdate1b023c9d-640b-4a1d-816f-8ce17d716fed is Running (Ready = true)
Sep 26 12:03:09.768: INFO: Successfully updated pod "labelsupdate1b023c9d-640b-4a1d-816f-8ce17d716fed"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:03:11.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2942" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":312,"skipped":5484,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:03:11.812: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 26 12:03:11.878: INFO: Waiting up to 5m0s for pod "pod-c117d4d6-9527-42e7-9eca-3873cf034cf4" in namespace "emptydir-5820" to be "Succeeded or Failed"
Sep 26 12:03:11.881: INFO: Pod "pod-c117d4d6-9527-42e7-9eca-3873cf034cf4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.31726ms
Sep 26 12:03:13.891: INFO: Pod "pod-c117d4d6-9527-42e7-9eca-3873cf034cf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013319345s
STEP: Saw pod success
Sep 26 12:03:13.891: INFO: Pod "pod-c117d4d6-9527-42e7-9eca-3873cf034cf4" satisfied condition "Succeeded or Failed"
Sep 26 12:03:13.894: INFO: Trying to get logs from node 10.37.21.194 pod pod-c117d4d6-9527-42e7-9eca-3873cf034cf4 container test-container: <nil>
STEP: delete the pod
Sep 26 12:03:13.915: INFO: Waiting for pod pod-c117d4d6-9527-42e7-9eca-3873cf034cf4 to disappear
Sep 26 12:03:13.918: INFO: Pod pod-c117d4d6-9527-42e7-9eca-3873cf034cf4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:03:13.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5820" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":313,"skipped":5496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:03:13.929: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:03:16.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7488" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":314,"skipped":5542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:03:16.025: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 26 12:03:16.543: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 12:03:19.568: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 12:03:19.577: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:03:27.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1135" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:11.728 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":315,"skipped":5570,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:03:27.754: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 12:03:27.824: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-40a8c449-489a-400d-ae4a-cf154d0e69bd" in namespace "security-context-test-2189" to be "Succeeded or Failed"
Sep 26 12:03:27.829: INFO: Pod "busybox-privileged-false-40a8c449-489a-400d-ae4a-cf154d0e69bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.313923ms
Sep 26 12:03:29.837: INFO: Pod "busybox-privileged-false-40a8c449-489a-400d-ae4a-cf154d0e69bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012347664s
Sep 26 12:03:29.837: INFO: Pod "busybox-privileged-false-40a8c449-489a-400d-ae4a-cf154d0e69bd" satisfied condition "Succeeded or Failed"
Sep 26 12:03:29.845: INFO: Got logs for pod "busybox-privileged-false-40a8c449-489a-400d-ae4a-cf154d0e69bd": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:03:29.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2189" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":316,"skipped":5578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:03:29.859: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Sep 26 12:03:29.916: INFO: Found Service test-service-hxbf7 in namespace services-1136 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Sep 26 12:03:29.916: INFO: Service test-service-hxbf7 created
STEP: Getting /status
Sep 26 12:03:29.919: INFO: Service test-service-hxbf7 has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Sep 26 12:03:29.928: INFO: observed Service test-service-hxbf7 in namespace services-1136 with annotations: map[] & LoadBalancer: {[]}
Sep 26 12:03:29.928: INFO: Found Service test-service-hxbf7 in namespace services-1136 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Sep 26 12:03:29.928: INFO: Service test-service-hxbf7 has service status patched
STEP: updating the ServiceStatus
Sep 26 12:03:29.939: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Sep 26 12:03:29.941: INFO: Observed Service test-service-hxbf7 in namespace services-1136 with annotations: map[] & Conditions: {[]}
Sep 26 12:03:29.941: INFO: Observed event: &Service{ObjectMeta:{test-service-hxbf7  services-1136  7cd88365-6c9d-49b4-9453-3133204344f2 558645 0 2021-09-26 12:03:29 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2021-09-26 12:03:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2021-09-26 12:03:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.254.121.220,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.254.121.220],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Sep 26 12:03:29.941: INFO: Found Service test-service-hxbf7 in namespace services-1136 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 26 12:03:29.941: INFO: Service test-service-hxbf7 has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Sep 26 12:03:29.962: INFO: observed Service test-service-hxbf7 in namespace services-1136 with labels: map[test-service-static:true]
Sep 26 12:03:29.962: INFO: observed Service test-service-hxbf7 in namespace services-1136 with labels: map[test-service-static:true]
Sep 26 12:03:29.962: INFO: observed Service test-service-hxbf7 in namespace services-1136 with labels: map[test-service-static:true]
Sep 26 12:03:29.962: INFO: Found Service test-service-hxbf7 in namespace services-1136 with labels: map[test-service:patched test-service-static:true]
Sep 26 12:03:29.962: INFO: Service test-service-hxbf7 patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Sep 26 12:03:29.978: INFO: Observed event: ADDED
Sep 26 12:03:29.978: INFO: Observed event: MODIFIED
Sep 26 12:03:29.979: INFO: Observed event: MODIFIED
Sep 26 12:03:29.979: INFO: Observed event: MODIFIED
Sep 26 12:03:29.979: INFO: Found Service test-service-hxbf7 in namespace services-1136 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Sep 26 12:03:29.979: INFO: Service test-service-hxbf7 deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:03:29.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1136" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":317,"skipped":5610,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:03:29.992: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Sep 26 12:03:30.040: INFO: The status of Pod labelsupdate352dd026-3930-4525-8f32-544721a873a5 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 12:03:32.051: INFO: The status of Pod labelsupdate352dd026-3930-4525-8f32-544721a873a5 is Running (Ready = true)
Sep 26 12:03:32.587: INFO: Successfully updated pod "labelsupdate352dd026-3930-4525-8f32-544721a873a5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:03:34.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7327" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":318,"skipped":5612,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:03:34.628: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 12:03:35.169: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 12:03:38.200: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 12:03:38.208: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:03:46.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8840" for this suite.
STEP: Destroying namespace "webhook-8840-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.806 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":319,"skipped":5645,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:03:46.435: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-7422
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 26 12:03:46.505: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 26 12:03:46.539: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 12:03:48.547: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 12:03:50.556: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 12:03:52.553: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 12:03:54.552: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 12:03:56.555: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 12:03:58.555: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 12:04:00.549: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 12:04:02.547: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 12:04:04.548: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 26 12:04:06.551: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 26 12:04:06.562: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 26 12:04:06.568: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 26 12:04:08.600: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep 26 12:04:08.600: INFO: Breadth first check of 192.168.84.71 on host 10.37.21.193...
Sep 26 12:04:08.603: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.50.153:9080/dial?request=hostname&protocol=udp&host=192.168.84.71&port=8081&tries=1'] Namespace:pod-network-test-7422 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 12:04:08.603: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 12:04:08.749: INFO: Waiting for responses: map[]
Sep 26 12:04:08.749: INFO: reached 192.168.84.71 after 0/1 tries
Sep 26 12:04:08.749: INFO: Breadth first check of 192.168.50.134 on host 10.37.21.194...
Sep 26 12:04:08.754: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.50.153:9080/dial?request=hostname&protocol=udp&host=192.168.50.134&port=8081&tries=1'] Namespace:pod-network-test-7422 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 12:04:08.754: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 12:04:08.915: INFO: Waiting for responses: map[]
Sep 26 12:04:08.915: INFO: reached 192.168.50.134 after 0/1 tries
Sep 26 12:04:08.915: INFO: Breadth first check of 192.168.181.91 on host 10.37.21.195...
Sep 26 12:04:08.921: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.50.153:9080/dial?request=hostname&protocol=udp&host=192.168.181.91&port=8081&tries=1'] Namespace:pod-network-test-7422 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 26 12:04:08.921: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
Sep 26 12:04:09.076: INFO: Waiting for responses: map[]
Sep 26 12:04:09.076: INFO: reached 192.168.181.91 after 0/1 tries
Sep 26 12:04:09.076: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:09.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7422" for this suite.

• [SLOW TEST:22.657 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":320,"skipped":5672,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:09.093: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:09.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5591" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":321,"skipped":5684,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:09.189: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 26 12:04:09.239: INFO: Waiting up to 5m0s for pod "pod-a56ed829-323d-417b-b351-1b035c608eb1" in namespace "emptydir-4634" to be "Succeeded or Failed"
Sep 26 12:04:09.242: INFO: Pod "pod-a56ed829-323d-417b-b351-1b035c608eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.987992ms
Sep 26 12:04:11.252: INFO: Pod "pod-a56ed829-323d-417b-b351-1b035c608eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012177558s
Sep 26 12:04:13.268: INFO: Pod "pod-a56ed829-323d-417b-b351-1b035c608eb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028314732s
STEP: Saw pod success
Sep 26 12:04:13.268: INFO: Pod "pod-a56ed829-323d-417b-b351-1b035c608eb1" satisfied condition "Succeeded or Failed"
Sep 26 12:04:13.273: INFO: Trying to get logs from node 10.37.21.193 pod pod-a56ed829-323d-417b-b351-1b035c608eb1 container test-container: <nil>
STEP: delete the pod
Sep 26 12:04:13.313: INFO: Waiting for pod pod-a56ed829-323d-417b-b351-1b035c608eb1 to disappear
Sep 26 12:04:13.318: INFO: Pod pod-a56ed829-323d-417b-b351-1b035c608eb1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:13.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4634" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":322,"skipped":5688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:13.331: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Sep 26 12:04:13.410: INFO: Waiting up to 5m0s for pod "security-context-1cfbd8ed-4a28-4594-88b6-faf90191a0be" in namespace "security-context-4519" to be "Succeeded or Failed"
Sep 26 12:04:13.415: INFO: Pod "security-context-1cfbd8ed-4a28-4594-88b6-faf90191a0be": Phase="Pending", Reason="", readiness=false. Elapsed: 5.234041ms
Sep 26 12:04:15.424: INFO: Pod "security-context-1cfbd8ed-4a28-4594-88b6-faf90191a0be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014543937s
Sep 26 12:04:17.434: INFO: Pod "security-context-1cfbd8ed-4a28-4594-88b6-faf90191a0be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024353569s
STEP: Saw pod success
Sep 26 12:04:17.434: INFO: Pod "security-context-1cfbd8ed-4a28-4594-88b6-faf90191a0be" satisfied condition "Succeeded or Failed"
Sep 26 12:04:17.437: INFO: Trying to get logs from node 10.37.21.193 pod security-context-1cfbd8ed-4a28-4594-88b6-faf90191a0be container test-container: <nil>
STEP: delete the pod
Sep 26 12:04:17.455: INFO: Waiting for pod security-context-1cfbd8ed-4a28-4594-88b6-faf90191a0be to disappear
Sep 26 12:04:17.458: INFO: Pod security-context-1cfbd8ed-4a28-4594-88b6-faf90191a0be no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:17.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4519" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":323,"skipped":5723,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:17.466: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6080
STEP: creating service affinity-nodeport-transition in namespace services-6080
STEP: creating replication controller affinity-nodeport-transition in namespace services-6080
I0926 12:04:17.542157      22 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-6080, replica count: 3
I0926 12:04:20.593577      22 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 26 12:04:20.605: INFO: Creating new exec pod
Sep 26 12:04:23.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-6080 exec execpod-affinitysdfmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Sep 26 12:04:23.829: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep 26 12:04:23.829: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 12:04:23.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-6080 exec execpod-affinitysdfmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.105.126 80'
Sep 26 12:04:24.035: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.105.126 80\nConnection to 10.254.105.126 80 port [tcp/http] succeeded!\n"
Sep 26 12:04:24.035: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 12:04:24.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-6080 exec execpod-affinitysdfmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.194 30765'
Sep 26 12:04:24.223: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.194 30765\nConnection to 10.37.21.194 30765 port [tcp/*] succeeded!\n"
Sep 26 12:04:24.223: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 12:04:24.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-6080 exec execpod-affinitysdfmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.21.193 30765'
Sep 26 12:04:24.436: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.21.193 30765\nConnection to 10.37.21.193 30765 port [tcp/*] succeeded!\n"
Sep 26 12:04:24.436: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 26 12:04:24.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-6080 exec execpod-affinitysdfmf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.37.21.193:30765/ ; done'
Sep 26 12:04:24.742: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n"
Sep 26 12:04:24.742: INFO: stdout: "\naffinity-nodeport-transition-tdpfq\naffinity-nodeport-transition-267nt\naffinity-nodeport-transition-267nt\naffinity-nodeport-transition-267nt\naffinity-nodeport-transition-267nt\naffinity-nodeport-transition-267nt\naffinity-nodeport-transition-267nt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-tdpfq\naffinity-nodeport-transition-267nt\naffinity-nodeport-transition-267nt\naffinity-nodeport-transition-267nt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-267nt\naffinity-nodeport-transition-tdpfq\naffinity-nodeport-transition-267nt"
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-tdpfq
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-267nt
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-267nt
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-267nt
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-267nt
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-267nt
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-267nt
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-tdpfq
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-267nt
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-267nt
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-267nt
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-267nt
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-tdpfq
Sep 26 12:04:24.742: INFO: Received response from host: affinity-nodeport-transition-267nt
Sep 26 12:04:24.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-516931234 --namespace=services-6080 exec execpod-affinitysdfmf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.37.21.193:30765/ ; done'
Sep 26 12:04:25.040: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.21.193:30765/\n"
Sep 26 12:04:25.040: INFO: stdout: "\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt\naffinity-nodeport-transition-v58zt"
Sep 26 12:04:25.040: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.040: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Received response from host: affinity-nodeport-transition-v58zt
Sep 26 12:04:25.041: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6080, will wait for the garbage collector to delete the pods
Sep 26 12:04:25.116: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.142534ms
Sep 26 12:04:25.217: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.248657ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:27.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6080" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.487 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":324,"skipped":5740,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:27.953: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 12:04:28.001: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ace7e2e-c00e-4c03-bf4a-8502fcc4a1cf" in namespace "downward-api-117" to be "Succeeded or Failed"
Sep 26 12:04:28.004: INFO: Pod "downwardapi-volume-3ace7e2e-c00e-4c03-bf4a-8502fcc4a1cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.803927ms
Sep 26 12:04:30.017: INFO: Pod "downwardapi-volume-3ace7e2e-c00e-4c03-bf4a-8502fcc4a1cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015115812s
STEP: Saw pod success
Sep 26 12:04:30.017: INFO: Pod "downwardapi-volume-3ace7e2e-c00e-4c03-bf4a-8502fcc4a1cf" satisfied condition "Succeeded or Failed"
Sep 26 12:04:30.020: INFO: Trying to get logs from node 10.37.21.195 pod downwardapi-volume-3ace7e2e-c00e-4c03-bf4a-8502fcc4a1cf container client-container: <nil>
STEP: delete the pod
Sep 26 12:04:30.042: INFO: Waiting for pod downwardapi-volume-3ace7e2e-c00e-4c03-bf4a-8502fcc4a1cf to disappear
Sep 26 12:04:30.045: INFO: Pod downwardapi-volume-3ace7e2e-c00e-4c03-bf4a-8502fcc4a1cf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:30.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-117" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":325,"skipped":5748,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:30.056: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 12:04:30.114: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca8c32ad-a06e-4cf4-b7e5-3b3d5728b85d" in namespace "projected-7150" to be "Succeeded or Failed"
Sep 26 12:04:30.117: INFO: Pod "downwardapi-volume-ca8c32ad-a06e-4cf4-b7e5-3b3d5728b85d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.700238ms
Sep 26 12:04:32.124: INFO: Pod "downwardapi-volume-ca8c32ad-a06e-4cf4-b7e5-3b3d5728b85d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010044491s
STEP: Saw pod success
Sep 26 12:04:32.124: INFO: Pod "downwardapi-volume-ca8c32ad-a06e-4cf4-b7e5-3b3d5728b85d" satisfied condition "Succeeded or Failed"
Sep 26 12:04:32.127: INFO: Trying to get logs from node 10.37.21.194 pod downwardapi-volume-ca8c32ad-a06e-4cf4-b7e5-3b3d5728b85d container client-container: <nil>
STEP: delete the pod
Sep 26 12:04:32.144: INFO: Waiting for pod downwardapi-volume-ca8c32ad-a06e-4cf4-b7e5-3b3d5728b85d to disappear
Sep 26 12:04:32.148: INFO: Pod downwardapi-volume-ca8c32ad-a06e-4cf4-b7e5-3b3d5728b85d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:32.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7150" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":5763,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:32.158: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Sep 26 12:04:33.268: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:33.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0926 12:04:33.268048      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-4071" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":327,"skipped":5767,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:33.288: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:33.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5785" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":328,"skipped":5795,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:33.445: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5855.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5855.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5855.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5855.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 26 12:04:35.544: INFO: DNS probes using dns-test-49090b1a-3750-408e-9a22-4f59d8d11823 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5855.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5855.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5855.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5855.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 26 12:04:37.604: INFO: DNS probes using dns-test-661dee18-724b-4b51-913d-efccfa3a1d90 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5855.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5855.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5855.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5855.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 26 12:04:39.663: INFO: DNS probes using dns-test-9e39d462-dad9-4901-8784-a0b7d84e8639 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:39.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5855" for this suite.

• [SLOW TEST:6.255 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":329,"skipped":5796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:39.701: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-60783f80-2721-4875-9919-1de5a42f8824
STEP: Creating configMap with name cm-test-opt-upd-21e042f8-a603-4343-9657-3ab16812ec9a
STEP: Creating the pod
Sep 26 12:04:39.770: INFO: The status of Pod pod-projected-configmaps-fe0bf488-09f9-45fd-baf1-8c470ccedbb2 is Pending, waiting for it to be Running (with Ready = true)
Sep 26 12:04:41.776: INFO: The status of Pod pod-projected-configmaps-fe0bf488-09f9-45fd-baf1-8c470ccedbb2 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-60783f80-2721-4875-9919-1de5a42f8824
STEP: Updating configmap cm-test-opt-upd-21e042f8-a603-4343-9657-3ab16812ec9a
STEP: Creating configMap with name cm-test-opt-create-c1f49b13-c1ca-4df8-af19-095293fa800f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:43.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7392" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":330,"skipped":5820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:43.865: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 26 12:04:43.937: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5417  1dcce0ff-0af0-4b1e-86cc-dfe8f6f3ebd3 559554 0 2021-09-26 12:04:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-09-26 12:04:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 26 12:04:43.937: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5417  1dcce0ff-0af0-4b1e-86cc-dfe8f6f3ebd3 559555 0 2021-09-26 12:04:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-09-26 12:04:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:43.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5417" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":331,"skipped":5848,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:43.945: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-7618
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7618
STEP: Deleting pre-stop pod
Sep 26 12:04:55.034: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:04:55.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7618" for this suite.

• [SLOW TEST:11.128 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":332,"skipped":5868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:04:55.074: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2865 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2865;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2865 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2865;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2865.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2865.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2865.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2865.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2865.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2865.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2865.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2865.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2865.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2865.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2865.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2865.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2865.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 109.178.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.178.109_udp@PTR;check="$$(dig +tcp +noall +answer +search 109.178.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.178.109_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2865 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2865;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2865 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2865;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2865.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2865.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2865.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2865.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2865.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2865.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2865.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2865.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2865.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2865.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2865.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2865.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2865.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 109.178.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.178.109_udp@PTR;check="$$(dig +tcp +noall +answer +search 109.178.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.178.109_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 26 12:04:57.181: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.184: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.187: INFO: Unable to read wheezy_udp@dns-test-service.dns-2865 from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.190: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2865 from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.192: INFO: Unable to read wheezy_udp@dns-test-service.dns-2865.svc from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.195: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2865.svc from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.197: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2865.svc from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.200: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2865.svc from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.207: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.210: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.219: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.222: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.224: INFO: Unable to read jessie_udp@dns-test-service.dns-2865 from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.227: INFO: Unable to read jessie_tcp@dns-test-service.dns-2865 from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.230: INFO: Unable to read jessie_udp@dns-test-service.dns-2865.svc from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.233: INFO: Unable to read jessie_tcp@dns-test-service.dns-2865.svc from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.236: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2865.svc from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.239: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2865.svc from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.249: INFO: Unable to read jessie_udp@PodARecord from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.252: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:04:57.258: INFO: Lookups using dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2865 wheezy_tcp@dns-test-service.dns-2865 wheezy_udp@dns-test-service.dns-2865.svc wheezy_tcp@dns-test-service.dns-2865.svc wheezy_udp@_http._tcp.dns-test-service.dns-2865.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2865.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2865 jessie_tcp@dns-test-service.dns-2865 jessie_udp@dns-test-service.dns-2865.svc jessie_tcp@dns-test-service.dns-2865.svc jessie_udp@_http._tcp.dns-test-service.dns-2865.svc jessie_tcp@_http._tcp.dns-test-service.dns-2865.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 26 12:05:02.263: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:05:02.267: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:05:02.270: INFO: Unable to read wheezy_udp@dns-test-service.dns-2865 from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:05:02.273: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2865 from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:05:02.275: INFO: Unable to read wheezy_udp@dns-test-service.dns-2865.svc from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:05:02.278: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2865.svc from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:05:02.303: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:05:02.305: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:05:02.308: INFO: Unable to read jessie_udp@dns-test-service.dns-2865 from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:05:02.311: INFO: Unable to read jessie_tcp@dns-test-service.dns-2865 from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:05:02.313: INFO: Unable to read jessie_udp@dns-test-service.dns-2865.svc from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:05:02.316: INFO: Unable to read jessie_tcp@dns-test-service.dns-2865.svc from pod dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05: the server could not find the requested resource (get pods dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05)
Sep 26 12:05:02.337: INFO: Lookups using dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2865 wheezy_tcp@dns-test-service.dns-2865 wheezy_udp@dns-test-service.dns-2865.svc wheezy_tcp@dns-test-service.dns-2865.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2865 jessie_tcp@dns-test-service.dns-2865 jessie_udp@dns-test-service.dns-2865.svc jessie_tcp@dns-test-service.dns-2865.svc]

Sep 26 12:05:07.355: INFO: DNS probes using dns-2865/dns-test-d2e97a30-e34b-4428-a550-08ae551b2c05 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:05:07.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2865" for this suite.

• [SLOW TEST:12.365 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":333,"skipped":5895,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:05:07.439: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Sep 26 12:05:07.550: INFO: Creating simple deployment test-deployment-v98ww
Sep 26 12:05:07.562: INFO: new replicaset for deployment "test-deployment-v98ww" is yet to be created
STEP: Getting /status
Sep 26 12:05:09.584: INFO: Deployment test-deployment-v98ww has Conditions: [{Available True 2021-09-26 12:05:08 +0000 UTC 2021-09-26 12:05:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2021-09-26 12:05:08 +0000 UTC 2021-09-26 12:05:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v98ww-7889c48c79" has successfully progressed.}]
STEP: updating Deployment Status
Sep 26 12:05:09.595: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768254708, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768254708, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768254708, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768254707, loc:(*time.Location)(0xa09bc80)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-v98ww-7889c48c79\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Sep 26 12:05:09.596: INFO: Observed &Deployment event: ADDED
Sep 26 12:05:09.596: INFO: Observed Deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-09-26 12:05:07 +0000 UTC 2021-09-26 12:05:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-v98ww-7889c48c79"}
Sep 26 12:05:09.596: INFO: Observed &Deployment event: MODIFIED
Sep 26 12:05:09.597: INFO: Observed Deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-09-26 12:05:07 +0000 UTC 2021-09-26 12:05:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-v98ww-7889c48c79"}
Sep 26 12:05:09.597: INFO: Observed Deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-09-26 12:05:07 +0000 UTC 2021-09-26 12:05:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep 26 12:05:09.597: INFO: Observed &Deployment event: MODIFIED
Sep 26 12:05:09.597: INFO: Observed Deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-09-26 12:05:07 +0000 UTC 2021-09-26 12:05:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep 26 12:05:09.597: INFO: Observed Deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-09-26 12:05:07 +0000 UTC 2021-09-26 12:05:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-v98ww-7889c48c79" is progressing.}
Sep 26 12:05:09.597: INFO: Observed &Deployment event: MODIFIED
Sep 26 12:05:09.597: INFO: Observed Deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-09-26 12:05:08 +0000 UTC 2021-09-26 12:05:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep 26 12:05:09.597: INFO: Observed Deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-09-26 12:05:08 +0000 UTC 2021-09-26 12:05:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v98ww-7889c48c79" has successfully progressed.}
Sep 26 12:05:09.597: INFO: Observed &Deployment event: MODIFIED
Sep 26 12:05:09.597: INFO: Observed Deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-09-26 12:05:08 +0000 UTC 2021-09-26 12:05:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep 26 12:05:09.597: INFO: Observed Deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-09-26 12:05:08 +0000 UTC 2021-09-26 12:05:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v98ww-7889c48c79" has successfully progressed.}
Sep 26 12:05:09.597: INFO: Found Deployment test-deployment-v98ww in namespace deployment-194 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep 26 12:05:09.597: INFO: Deployment test-deployment-v98ww has an updated status
STEP: patching the Statefulset Status
Sep 26 12:05:09.597: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep 26 12:05:09.603: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Sep 26 12:05:09.604: INFO: Observed &Deployment event: ADDED
Sep 26 12:05:09.604: INFO: Observed deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-09-26 12:05:07 +0000 UTC 2021-09-26 12:05:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-v98ww-7889c48c79"}
Sep 26 12:05:09.604: INFO: Observed &Deployment event: MODIFIED
Sep 26 12:05:09.604: INFO: Observed deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-09-26 12:05:07 +0000 UTC 2021-09-26 12:05:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-v98ww-7889c48c79"}
Sep 26 12:05:09.604: INFO: Observed deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-09-26 12:05:07 +0000 UTC 2021-09-26 12:05:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep 26 12:05:09.604: INFO: Observed &Deployment event: MODIFIED
Sep 26 12:05:09.604: INFO: Observed deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-09-26 12:05:07 +0000 UTC 2021-09-26 12:05:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep 26 12:05:09.604: INFO: Observed deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-09-26 12:05:07 +0000 UTC 2021-09-26 12:05:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-v98ww-7889c48c79" is progressing.}
Sep 26 12:05:09.604: INFO: Observed &Deployment event: MODIFIED
Sep 26 12:05:09.604: INFO: Observed deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-09-26 12:05:08 +0000 UTC 2021-09-26 12:05:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep 26 12:05:09.604: INFO: Observed deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-09-26 12:05:08 +0000 UTC 2021-09-26 12:05:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v98ww-7889c48c79" has successfully progressed.}
Sep 26 12:05:09.605: INFO: Observed &Deployment event: MODIFIED
Sep 26 12:05:09.605: INFO: Observed deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-09-26 12:05:08 +0000 UTC 2021-09-26 12:05:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep 26 12:05:09.605: INFO: Observed deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-09-26 12:05:08 +0000 UTC 2021-09-26 12:05:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v98ww-7889c48c79" has successfully progressed.}
Sep 26 12:05:09.605: INFO: Observed deployment test-deployment-v98ww in namespace deployment-194 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep 26 12:05:09.605: INFO: Observed &Deployment event: MODIFIED
Sep 26 12:05:09.605: INFO: Found deployment test-deployment-v98ww in namespace deployment-194 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Sep 26 12:05:09.605: INFO: Deployment test-deployment-v98ww has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 26 12:05:09.608: INFO: Deployment "test-deployment-v98ww":
&Deployment{ObjectMeta:{test-deployment-v98ww  deployment-194  80637348-1203-4648-a12e-02b127f919a8 559812 1 2021-09-26 12:05:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-09-26 12:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 12:05:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2021-09-26 12:05:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd registry.dahuatech.com/cncf/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b2f058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 26 12:05:09.612: INFO: New ReplicaSet "test-deployment-v98ww-7889c48c79" of Deployment "test-deployment-v98ww":
&ReplicaSet{ObjectMeta:{test-deployment-v98ww-7889c48c79  deployment-194  23cc9011-eac2-4a9b-af10-2705bb737301 559806 1 2021-09-26 12:05:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:7889c48c79] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-v98ww 80637348-1203-4648-a12e-02b127f919a8 0xc005fb1407 0xc005fb1408}] []  [{kube-controller-manager Update apps/v1 2021-09-26 12:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"80637348-1203-4648-a12e-02b127f919a8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 12:05:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 7889c48c79,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:7889c48c79] map[] [] []  []} {[] [] [{httpd registry.dahuatech.com/cncf/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fb14b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 26 12:05:09.615: INFO: Pod "test-deployment-v98ww-7889c48c79-mq7hx" is available:
&Pod{ObjectMeta:{test-deployment-v98ww-7889c48c79-mq7hx test-deployment-v98ww-7889c48c79- deployment-194  20a99a0b-c348-4150-aab2-b819bbbf1591 559805 0 2021-09-26 12:05:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:7889c48c79] map[cni.projectcalico.org/containerID:4a285ac6762f772c7816845f49b13fc55cd80fab2e6ecf65a003389380fdb99d cni.projectcalico.org/podIP:192.168.50.162/32 cni.projectcalico.org/podIPs:192.168.50.162/32] [{apps/v1 ReplicaSet test-deployment-v98ww-7889c48c79 23cc9011-eac2-4a9b-af10-2705bb737301 0xc005fb1860 0xc005fb1861}] []  [{kube-controller-manager Update v1 2021-09-26 12:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23cc9011-eac2-4a9b-af10-2705bb737301\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 12:05:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 12:05:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.50.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2mc49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2mc49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:05:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:05:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:05:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:05:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.194,PodIP:192.168.50.162,StartTime:2021-09-26 12:05:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 12:05:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://e41d25a4b602754072d14e715a366535ebf092898bd38e712af31e98e681c3ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.50.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:05:09.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-194" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":334,"skipped":5915,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:05:09.630: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 26 12:05:09.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1aa96124-87b2-48d6-97d9-fa1b05017898" in namespace "downward-api-9387" to be "Succeeded or Failed"
Sep 26 12:05:09.686: INFO: Pod "downwardapi-volume-1aa96124-87b2-48d6-97d9-fa1b05017898": Phase="Pending", Reason="", readiness=false. Elapsed: 3.602636ms
Sep 26 12:05:11.695: INFO: Pod "downwardapi-volume-1aa96124-87b2-48d6-97d9-fa1b05017898": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013122189s
STEP: Saw pod success
Sep 26 12:05:11.695: INFO: Pod "downwardapi-volume-1aa96124-87b2-48d6-97d9-fa1b05017898" satisfied condition "Succeeded or Failed"
Sep 26 12:05:11.699: INFO: Trying to get logs from node 10.37.21.195 pod downwardapi-volume-1aa96124-87b2-48d6-97d9-fa1b05017898 container client-container: <nil>
STEP: delete the pod
Sep 26 12:05:11.722: INFO: Waiting for pod downwardapi-volume-1aa96124-87b2-48d6-97d9-fa1b05017898 to disappear
Sep 26 12:05:11.725: INFO: Pod downwardapi-volume-1aa96124-87b2-48d6-97d9-fa1b05017898 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:05:11.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9387" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":335,"skipped":5937,"failed":0}
SSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:05:11.737: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Sep 26 12:05:31.949: INFO: EndpointSlice for Service endpointslice-9986/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:05:41.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9986" for this suite.

• [SLOW TEST:30.273 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":336,"skipped":5941,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:05:42.009: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 26 12:05:42.085: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 26 12:05:47.102: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:05:48.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2890" for this suite.

• [SLOW TEST:6.130 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":337,"skipped":5943,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:05:48.139: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-c5e71071-d7f1-427e-8196-5a418e830285
STEP: Creating a pod to test consume secrets
Sep 26 12:05:48.198: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d6e82d64-8622-410c-ae42-ee0586e53a41" in namespace "projected-7803" to be "Succeeded or Failed"
Sep 26 12:05:48.203: INFO: Pod "pod-projected-secrets-d6e82d64-8622-410c-ae42-ee0586e53a41": Phase="Pending", Reason="", readiness=false. Elapsed: 5.166742ms
Sep 26 12:05:50.214: INFO: Pod "pod-projected-secrets-d6e82d64-8622-410c-ae42-ee0586e53a41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01647391s
STEP: Saw pod success
Sep 26 12:05:50.214: INFO: Pod "pod-projected-secrets-d6e82d64-8622-410c-ae42-ee0586e53a41" satisfied condition "Succeeded or Failed"
Sep 26 12:05:50.217: INFO: Trying to get logs from node 10.37.21.194 pod pod-projected-secrets-d6e82d64-8622-410c-ae42-ee0586e53a41 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 26 12:05:50.237: INFO: Waiting for pod pod-projected-secrets-d6e82d64-8622-410c-ae42-ee0586e53a41 to disappear
Sep 26 12:05:50.241: INFO: Pod pod-projected-secrets-d6e82d64-8622-410c-ae42-ee0586e53a41 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:05:50.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7803" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":338,"skipped":5954,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:05:50.251: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-b1b1bd9d-4e1e-42cb-9aed-dd2959fdfdd5
STEP: Creating a pod to test consume secrets
Sep 26 12:05:50.310: INFO: Waiting up to 5m0s for pod "pod-secrets-a8ae23f7-83d4-453a-b091-b40236aadb85" in namespace "secrets-6244" to be "Succeeded or Failed"
Sep 26 12:05:50.313: INFO: Pod "pod-secrets-a8ae23f7-83d4-453a-b091-b40236aadb85": Phase="Pending", Reason="", readiness=false. Elapsed: 3.333664ms
Sep 26 12:05:52.400: INFO: Pod "pod-secrets-a8ae23f7-83d4-453a-b091-b40236aadb85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.090410574s
STEP: Saw pod success
Sep 26 12:05:52.401: INFO: Pod "pod-secrets-a8ae23f7-83d4-453a-b091-b40236aadb85" satisfied condition "Succeeded or Failed"
Sep 26 12:05:52.404: INFO: Trying to get logs from node 10.37.21.195 pod pod-secrets-a8ae23f7-83d4-453a-b091-b40236aadb85 container secret-volume-test: <nil>
STEP: delete the pod
Sep 26 12:05:52.427: INFO: Waiting for pod pod-secrets-a8ae23f7-83d4-453a-b091-b40236aadb85 to disappear
Sep 26 12:05:52.430: INFO: Pod pod-secrets-a8ae23f7-83d4-453a-b091-b40236aadb85 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:05:52.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6244" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":339,"skipped":5964,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:05:52.439: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Sep 26 12:05:52.499: INFO: Waiting up to 5m0s for pod "var-expansion-e2369e89-c9a5-49c7-9095-737b899a86bf" in namespace "var-expansion-4541" to be "Succeeded or Failed"
Sep 26 12:05:52.502: INFO: Pod "var-expansion-e2369e89-c9a5-49c7-9095-737b899a86bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.432435ms
Sep 26 12:05:54.510: INFO: Pod "var-expansion-e2369e89-c9a5-49c7-9095-737b899a86bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011548752s
STEP: Saw pod success
Sep 26 12:05:54.510: INFO: Pod "var-expansion-e2369e89-c9a5-49c7-9095-737b899a86bf" satisfied condition "Succeeded or Failed"
Sep 26 12:05:54.514: INFO: Trying to get logs from node 10.37.21.195 pod var-expansion-e2369e89-c9a5-49c7-9095-737b899a86bf container dapi-container: <nil>
STEP: delete the pod
Sep 26 12:05:54.530: INFO: Waiting for pod var-expansion-e2369e89-c9a5-49c7-9095-737b899a86bf to disappear
Sep 26 12:05:54.534: INFO: Pod var-expansion-e2369e89-c9a5-49c7-9095-737b899a86bf no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:05:54.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4541" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":340,"skipped":5969,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:05:54.543: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:05:54.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7004" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":341,"skipped":5976,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:05:54.607: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-d8r7
STEP: Creating a pod to test atomic-volume-subpath
Sep 26 12:05:54.712: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-d8r7" in namespace "subpath-4650" to be "Succeeded or Failed"
Sep 26 12:05:54.715: INFO: Pod "pod-subpath-test-projected-d8r7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.444499ms
Sep 26 12:05:56.723: INFO: Pod "pod-subpath-test-projected-d8r7": Phase="Running", Reason="", readiness=true. Elapsed: 2.011416721s
Sep 26 12:05:58.734: INFO: Pod "pod-subpath-test-projected-d8r7": Phase="Running", Reason="", readiness=true. Elapsed: 4.022446276s
Sep 26 12:06:00.744: INFO: Pod "pod-subpath-test-projected-d8r7": Phase="Running", Reason="", readiness=true. Elapsed: 6.032239624s
Sep 26 12:06:02.750: INFO: Pod "pod-subpath-test-projected-d8r7": Phase="Running", Reason="", readiness=true. Elapsed: 8.037750042s
Sep 26 12:06:04.766: INFO: Pod "pod-subpath-test-projected-d8r7": Phase="Running", Reason="", readiness=true. Elapsed: 10.053475387s
Sep 26 12:06:06.780: INFO: Pod "pod-subpath-test-projected-d8r7": Phase="Running", Reason="", readiness=true. Elapsed: 12.068135185s
Sep 26 12:06:08.795: INFO: Pod "pod-subpath-test-projected-d8r7": Phase="Running", Reason="", readiness=true. Elapsed: 14.082859446s
Sep 26 12:06:10.813: INFO: Pod "pod-subpath-test-projected-d8r7": Phase="Running", Reason="", readiness=true. Elapsed: 16.100492783s
Sep 26 12:06:12.819: INFO: Pod "pod-subpath-test-projected-d8r7": Phase="Running", Reason="", readiness=true. Elapsed: 18.106591655s
Sep 26 12:06:14.832: INFO: Pod "pod-subpath-test-projected-d8r7": Phase="Running", Reason="", readiness=true. Elapsed: 20.119976361s
Sep 26 12:06:16.846: INFO: Pod "pod-subpath-test-projected-d8r7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.134122581s
STEP: Saw pod success
Sep 26 12:06:16.846: INFO: Pod "pod-subpath-test-projected-d8r7" satisfied condition "Succeeded or Failed"
Sep 26 12:06:16.850: INFO: Trying to get logs from node 10.37.21.194 pod pod-subpath-test-projected-d8r7 container test-container-subpath-projected-d8r7: <nil>
STEP: delete the pod
Sep 26 12:06:16.873: INFO: Waiting for pod pod-subpath-test-projected-d8r7 to disappear
Sep 26 12:06:16.876: INFO: Pod pod-subpath-test-projected-d8r7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-d8r7
Sep 26 12:06:16.876: INFO: Deleting pod "pod-subpath-test-projected-d8r7" in namespace "subpath-4650"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:06:16.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4650" for this suite.

• [SLOW TEST:22.281 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":342,"skipped":5981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:06:16.889: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-bfb7b790-26cd-483a-9ac9-886a72253844
STEP: Creating a pod to test consume secrets
Sep 26 12:06:16.983: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8b11e487-616b-463d-bb83-9bd1543ca6a5" in namespace "projected-1527" to be "Succeeded or Failed"
Sep 26 12:06:16.987: INFO: Pod "pod-projected-secrets-8b11e487-616b-463d-bb83-9bd1543ca6a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.877717ms
Sep 26 12:06:18.997: INFO: Pod "pod-projected-secrets-8b11e487-616b-463d-bb83-9bd1543ca6a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014071825s
Sep 26 12:06:21.008: INFO: Pod "pod-projected-secrets-8b11e487-616b-463d-bb83-9bd1543ca6a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024737548s
STEP: Saw pod success
Sep 26 12:06:21.008: INFO: Pod "pod-projected-secrets-8b11e487-616b-463d-bb83-9bd1543ca6a5" satisfied condition "Succeeded or Failed"
Sep 26 12:06:21.011: INFO: Trying to get logs from node 10.37.21.194 pod pod-projected-secrets-8b11e487-616b-463d-bb83-9bd1543ca6a5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 26 12:06:21.036: INFO: Waiting for pod pod-projected-secrets-8b11e487-616b-463d-bb83-9bd1543ca6a5 to disappear
Sep 26 12:06:21.039: INFO: Pod pod-projected-secrets-8b11e487-616b-463d-bb83-9bd1543ca6a5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:06:21.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1527" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":343,"skipped":6017,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:06:21.048: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 26 12:06:22.184: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 26 12:06:24.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768254782, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768254782, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768254782, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768254782, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-657f8c86c\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 26 12:06:27.224: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:06:27.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1340" for this suite.
STEP: Destroying namespace "webhook-1340-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.299 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":344,"skipped":6017,"failed":0}
SSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:06:27.348: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep 26 12:06:31.934: INFO: Successfully updated pod "adopt-release--1-mk6kn"
STEP: Checking that the Job readopts the Pod
Sep 26 12:06:31.934: INFO: Waiting up to 15m0s for pod "adopt-release--1-mk6kn" in namespace "job-7373" to be "adopted"
Sep 26 12:06:31.938: INFO: Pod "adopt-release--1-mk6kn": Phase="Running", Reason="", readiness=true. Elapsed: 3.636655ms
Sep 26 12:06:33.951: INFO: Pod "adopt-release--1-mk6kn": Phase="Running", Reason="", readiness=true. Elapsed: 2.017373378s
Sep 26 12:06:33.951: INFO: Pod "adopt-release--1-mk6kn" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep 26 12:06:34.472: INFO: Successfully updated pod "adopt-release--1-mk6kn"
STEP: Checking that the Job releases the Pod
Sep 26 12:06:34.472: INFO: Waiting up to 15m0s for pod "adopt-release--1-mk6kn" in namespace "job-7373" to be "released"
Sep 26 12:06:34.477: INFO: Pod "adopt-release--1-mk6kn": Phase="Running", Reason="", readiness=true. Elapsed: 4.202285ms
Sep 26 12:06:36.486: INFO: Pod "adopt-release--1-mk6kn": Phase="Running", Reason="", readiness=true. Elapsed: 2.013376254s
Sep 26 12:06:36.486: INFO: Pod "adopt-release--1-mk6kn" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:06:36.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7373" for this suite.

• [SLOW TEST:9.150 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":345,"skipped":6023,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 26 12:06:36.498: INFO: >>> kubeConfig: /tmp/kubeconfig-516931234
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 26 12:06:36.548: INFO: Creating simple deployment test-new-deployment
Sep 26 12:06:36.559: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Sep 26 12:06:38.604: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1031  a55bab71-48c5-4d66-8277-98583af4027f 560576 3 2021-09-26 12:06:36 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2021-09-26 12:06:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 12:06:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd registry.dahuatech.com/cncf/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040c52e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-26 12:06:38 +0000 UTC,LastTransitionTime:2021-09-26 12:06:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7485cd56c" has successfully progressed.,LastUpdateTime:2021-09-26 12:06:38 +0000 UTC,LastTransitionTime:2021-09-26 12:06:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 26 12:06:38.609: INFO: New ReplicaSet "test-new-deployment-7485cd56c" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7485cd56c  deployment-1031  c0270ea3-1cb4-48db-a5e8-3488dd6a0a0e 560579 2 2021-09-26 12:06:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment a55bab71-48c5-4d66-8277-98583af4027f 0xc0040c5a37 0xc0040c5a38}] []  [{kube-controller-manager Update apps/v1 2021-09-26 12:06:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a55bab71-48c5-4d66-8277-98583af4027f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-09-26 12:06:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7485cd56c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[] [] []  []} {[] [] [{httpd registry.dahuatech.com/cncf/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040c5c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 26 12:06:38.614: INFO: Pod "test-new-deployment-7485cd56c-sg2j7" is available:
&Pod{ObjectMeta:{test-new-deployment-7485cd56c-sg2j7 test-new-deployment-7485cd56c- deployment-1031  b79fdbb5-b41d-4ba1-8eef-9562943eddf1 560571 0 2021-09-26 12:06:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[cni.projectcalico.org/containerID:fa471a4bee44111b450f4d45654c51381b0d67b503587bba829f2e9957b02874 cni.projectcalico.org/podIP:192.168.181.83/32 cni.projectcalico.org/podIPs:192.168.181.83/32] [{apps/v1 ReplicaSet test-new-deployment-7485cd56c c0270ea3-1cb4-48db-a5e8-3488dd6a0a0e 0xc006922057 0xc006922058}] []  [{kube-controller-manager Update v1 2021-09-26 12:06:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c0270ea3-1cb4-48db-a5e8-3488dd6a0a0e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-09-26 12:06:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-09-26 12:06:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.181.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pktjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pktjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.195,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:06:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:06:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:06:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:06:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.37.21.195,PodIP:192.168.181.83,StartTime:2021-09-26 12:06:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-26 12:06:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,ImageID:docker-pullable://registry.dahuatech.com/cncf/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://d50d5060c21bb88ea4cb33eb141754f0a998c31112d58d5b0239282ff971ba26,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.181.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 26 12:06:38.614: INFO: Pod "test-new-deployment-7485cd56c-zh7sj" is not available:
&Pod{ObjectMeta:{test-new-deployment-7485cd56c-zh7sj test-new-deployment-7485cd56c- deployment-1031  c984a197-bc5d-41c6-a91d-d3a5030449a8 560581 0 2021-09-26 12:06:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7485cd56c] map[] [{apps/v1 ReplicaSet test-new-deployment-7485cd56c c0270ea3-1cb4-48db-a5e8-3488dd6a0a0e 0xc006922267 0xc006922268}] []  [{kube-controller-manager Update v1 2021-09-26 12:06:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c0270ea3-1cb4-48db-a5e8-3488dd6a0a0e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhjlb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.dahuatech.com/cncf/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhjlb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.37.21.194,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-26 12:06:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 26 12:06:38.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1031" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":346,"skipped":6029,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSep 26 12:06:38.628: INFO: Running AfterSuite actions on all nodes
Sep 26 12:06:38.628: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Sep 26 12:06:38.628: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Sep 26 12:06:38.628: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Sep 26 12:06:38.628: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Sep 26 12:06:38.628: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Sep 26 12:06:38.628: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Sep 26 12:06:38.628: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Sep 26 12:06:38.628: INFO: Running AfterSuite actions on node 1
Sep 26 12:06:38.628: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6086,"failed":0}

Ran 346 of 6432 Specs in 5590.974 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6086 Skipped
PASS

Ginkgo ran 1 suite in 1h33m13.148027939s
Test Suite Passed
