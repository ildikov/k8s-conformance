I0822 09:27:57.195953      21 e2e.go:129] Starting e2e run "d263fab8-6dd8-43ca-b25a-4719ad4b36e6" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1629624477 - Will randomize all specs
Will run 346 of 6432 specs

Aug 22 09:27:58.692: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 09:27:58.694: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 22 09:27:58.704: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 22 09:27:58.721: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 22 09:27:58.721: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Aug 22 09:27:58.721: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 22 09:27:58.725: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Aug 22 09:27:58.725: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 22 09:27:58.725: INFO: e2e test version: v1.22.1
Aug 22 09:27:58.726: INFO: kube-apiserver version: v1.22.1
Aug 22 09:27:58.726: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 09:27:58.729: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:27:58.729: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
W0822 09:27:58.745042      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Aug 22 09:27:58.745: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Aug 22 09:27:58.748: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-5a5c81a1-3146-46e7-88b1-22d7a4b0653d
STEP: Creating a pod to test consume secrets
Aug 22 09:27:58.754: INFO: Waiting up to 5m0s for pod "pod-secrets-6318fcc1-61fd-40f2-b389-33093dc5ae04" in namespace "secrets-4759" to be "Succeeded or Failed"
Aug 22 09:27:58.756: INFO: Pod "pod-secrets-6318fcc1-61fd-40f2-b389-33093dc5ae04": Phase="Pending", Reason="", readiness=false. Elapsed: 1.39357ms
Aug 22 09:28:00.760: INFO: Pod "pod-secrets-6318fcc1-61fd-40f2-b389-33093dc5ae04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006080264s
STEP: Saw pod success
Aug 22 09:28:00.760: INFO: Pod "pod-secrets-6318fcc1-61fd-40f2-b389-33093dc5ae04" satisfied condition "Succeeded or Failed"
Aug 22 09:28:00.762: INFO: Trying to get logs from node 172.23.79.103 pod pod-secrets-6318fcc1-61fd-40f2-b389-33093dc5ae04 container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 09:28:00.775: INFO: Waiting for pod pod-secrets-6318fcc1-61fd-40f2-b389-33093dc5ae04 to disappear
Aug 22 09:28:00.776: INFO: Pod pod-secrets-6318fcc1-61fd-40f2-b389-33093dc5ae04 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:28:00.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4759" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":1,"skipped":27,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:28:00.780: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:28:16.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5285" for this suite.

• [SLOW TEST:16.091 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":2,"skipped":43,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:28:16.871: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 22 09:28:17.428: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 09:28:20.442: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:28:20.445: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:28:23.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4359" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.727 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":3,"skipped":47,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:28:23.598: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-9835
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 22 09:28:23.626: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 22 09:28:23.636: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:28:25.640: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:28:27.640: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:28:29.642: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:28:31.643: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:28:33.640: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:28:35.643: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:28:37.641: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:28:39.641: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:28:41.644: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:28:43.641: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:28:45.643: INFO: The status of Pod netserver-0 is Running (Ready = true)
STEP: Creating test pods
Aug 22 09:28:47.667: INFO: Setting MaxTries for pod polling to 31 for networking test based on endpoint count 1
Aug 22 09:28:47.667: INFO: Going to poll 10.244.1.13 on port 8081 at least 0 times, with a maximum of 31 tries before failing
Aug 22 09:28:47.669: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9835 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 09:28:47.669: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 09:28:48.730: INFO: Found all 1 expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:28:48.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9835" for this suite.

• [SLOW TEST:25.140 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":54,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:28:48.738: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Aug 22 09:28:50.763: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2298 PodName:var-expansion-82379081-56c6-4e27-a6fa-34a33a75fb23 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 09:28:50.763: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: test for file in mounted path
Aug 22 09:28:50.821: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2298 PodName:var-expansion-82379081-56c6-4e27-a6fa-34a33a75fb23 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 09:28:50.821: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: updating the annotation value
Aug 22 09:28:51.381: INFO: Successfully updated pod "var-expansion-82379081-56c6-4e27-a6fa-34a33a75fb23"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Aug 22 09:28:51.383: INFO: Deleting pod "var-expansion-82379081-56c6-4e27-a6fa-34a33a75fb23" in namespace "var-expansion-2298"
Aug 22 09:28:51.385: INFO: Wait up to 5m0s for pod "var-expansion-82379081-56c6-4e27-a6fa-34a33a75fb23" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:25.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2298" for this suite.

• [SLOW TEST:36.659 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":5,"skipped":84,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:25.397: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 09:29:25.918: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 09:29:28.931: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:29:28.934: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9842-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:31.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1753" for this suite.
STEP: Destroying namespace "webhook-1753-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.671 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":6,"skipped":93,"failed":0}
S
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:32.068: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Aug 22 09:29:32.102: INFO: Waiting up to 5m0s for pod "security-context-41d55a85-31a9-442c-b3d3-779d86bdd65c" in namespace "security-context-2738" to be "Succeeded or Failed"
Aug 22 09:29:32.104: INFO: Pod "security-context-41d55a85-31a9-442c-b3d3-779d86bdd65c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.374276ms
Aug 22 09:29:34.108: INFO: Pod "security-context-41d55a85-31a9-442c-b3d3-779d86bdd65c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006015474s
STEP: Saw pod success
Aug 22 09:29:34.109: INFO: Pod "security-context-41d55a85-31a9-442c-b3d3-779d86bdd65c" satisfied condition "Succeeded or Failed"
Aug 22 09:29:34.110: INFO: Trying to get logs from node 172.23.79.103 pod security-context-41d55a85-31a9-442c-b3d3-779d86bdd65c container test-container: <nil>
STEP: delete the pod
Aug 22 09:29:34.123: INFO: Waiting for pod security-context-41d55a85-31a9-442c-b3d3-779d86bdd65c to disappear
Aug 22 09:29:34.124: INFO: Pod security-context-41d55a85-31a9-442c-b3d3-779d86bdd65c no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:34.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2738" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":7,"skipped":94,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:34.128: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-74e853b6-0907-4d15-98a4-4444f7cb6eed
STEP: Creating a pod to test consume configMaps
Aug 22 09:29:34.148: INFO: Waiting up to 5m0s for pod "pod-configmaps-60996e6b-7861-4ea3-9970-4454f6ea705f" in namespace "configmap-9209" to be "Succeeded or Failed"
Aug 22 09:29:34.150: INFO: Pod "pod-configmaps-60996e6b-7861-4ea3-9970-4454f6ea705f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.80978ms
Aug 22 09:29:36.156: INFO: Pod "pod-configmaps-60996e6b-7861-4ea3-9970-4454f6ea705f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007215435s
STEP: Saw pod success
Aug 22 09:29:36.156: INFO: Pod "pod-configmaps-60996e6b-7861-4ea3-9970-4454f6ea705f" satisfied condition "Succeeded or Failed"
Aug 22 09:29:36.157: INFO: Trying to get logs from node 172.23.79.103 pod pod-configmaps-60996e6b-7861-4ea3-9970-4454f6ea705f container agnhost-container: <nil>
STEP: delete the pod
Aug 22 09:29:36.166: INFO: Waiting for pod pod-configmaps-60996e6b-7861-4ea3-9970-4454f6ea705f to disappear
Aug 22 09:29:36.167: INFO: Pod pod-configmaps-60996e6b-7861-4ea3-9970-4454f6ea705f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:36.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9209" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":8,"skipped":136,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:36.172: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 09:29:36.192: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc45a3ee-cfcc-4389-a966-84750df53a4e" in namespace "projected-5402" to be "Succeeded or Failed"
Aug 22 09:29:36.194: INFO: Pod "downwardapi-volume-fc45a3ee-cfcc-4389-a966-84750df53a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.655358ms
Aug 22 09:29:38.199: INFO: Pod "downwardapi-volume-fc45a3ee-cfcc-4389-a966-84750df53a4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006458958s
STEP: Saw pod success
Aug 22 09:29:38.199: INFO: Pod "downwardapi-volume-fc45a3ee-cfcc-4389-a966-84750df53a4e" satisfied condition "Succeeded or Failed"
Aug 22 09:29:38.200: INFO: Trying to get logs from node 172.23.79.103 pod downwardapi-volume-fc45a3ee-cfcc-4389-a966-84750df53a4e container client-container: <nil>
STEP: delete the pod
Aug 22 09:29:38.209: INFO: Waiting for pod downwardapi-volume-fc45a3ee-cfcc-4389-a966-84750df53a4e to disappear
Aug 22 09:29:38.210: INFO: Pod downwardapi-volume-fc45a3ee-cfcc-4389-a966-84750df53a4e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:38.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5402" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":9,"skipped":139,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:38.213: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:29:38.230: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 22 09:29:38.236: INFO: The status of Pod pod-logs-websocket-4d16a7f3-983c-44b9-855b-1eb38b6d0eb2 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:29:40.240: INFO: The status of Pod pod-logs-websocket-4d16a7f3-983c-44b9-855b-1eb38b6d0eb2 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:40.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5277" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":144,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:40.253: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Aug 22 09:29:40.273: INFO: Waiting up to 5m0s for pod "downward-api-9787c5fd-f059-46a5-908c-c289a71bdd2c" in namespace "downward-api-6894" to be "Succeeded or Failed"
Aug 22 09:29:40.275: INFO: Pod "downward-api-9787c5fd-f059-46a5-908c-c289a71bdd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.279506ms
Aug 22 09:29:42.279: INFO: Pod "downward-api-9787c5fd-f059-46a5-908c-c289a71bdd2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0060004s
STEP: Saw pod success
Aug 22 09:29:42.279: INFO: Pod "downward-api-9787c5fd-f059-46a5-908c-c289a71bdd2c" satisfied condition "Succeeded or Failed"
Aug 22 09:29:42.281: INFO: Trying to get logs from node 172.23.79.103 pod downward-api-9787c5fd-f059-46a5-908c-c289a71bdd2c container dapi-container: <nil>
STEP: delete the pod
Aug 22 09:29:42.289: INFO: Waiting for pod downward-api-9787c5fd-f059-46a5-908c-c289a71bdd2c to disappear
Aug 22 09:29:42.291: INFO: Pod downward-api-9787c5fd-f059-46a5-908c-c289a71bdd2c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:42.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6894" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":11,"skipped":157,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:42.294: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:42.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8033" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":12,"skipped":177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:42.321: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:42.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9229" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":13,"skipped":222,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:42.349: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Aug 22 09:29:42.367: INFO: The status of Pod labelsupdate0fab6baf-253e-49e9-a7ad-c8c0b715bd5e is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:29:44.370: INFO: The status of Pod labelsupdate0fab6baf-253e-49e9-a7ad-c8c0b715bd5e is Running (Ready = true)
Aug 22 09:29:44.883: INFO: Successfully updated pod "labelsupdate0fab6baf-253e-49e9-a7ad-c8c0b715bd5e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:48.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1542" for this suite.

• [SLOW TEST:6.552 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":14,"skipped":245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:48.902: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Aug 22 09:29:48.920: INFO: Waiting up to 5m0s for pod "security-context-bf01156a-8b46-4ca5-b5a4-744890802269" in namespace "security-context-2099" to be "Succeeded or Failed"
Aug 22 09:29:48.924: INFO: Pod "security-context-bf01156a-8b46-4ca5-b5a4-744890802269": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272682ms
Aug 22 09:29:50.928: INFO: Pod "security-context-bf01156a-8b46-4ca5-b5a4-744890802269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008729015s
STEP: Saw pod success
Aug 22 09:29:50.928: INFO: Pod "security-context-bf01156a-8b46-4ca5-b5a4-744890802269" satisfied condition "Succeeded or Failed"
Aug 22 09:29:50.930: INFO: Trying to get logs from node 172.23.79.103 pod security-context-bf01156a-8b46-4ca5-b5a4-744890802269 container test-container: <nil>
STEP: delete the pod
Aug 22 09:29:50.938: INFO: Waiting for pod security-context-bf01156a-8b46-4ca5-b5a4-744890802269 to disappear
Aug 22 09:29:50.939: INFO: Pod security-context-bf01156a-8b46-4ca5-b5a4-744890802269 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:50.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2099" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":15,"skipped":274,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:50.943: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Aug 22 09:29:50.960: INFO: Waiting up to 5m0s for pod "downward-api-40a42be4-ce47-4799-b6fc-f1fb2d76922f" in namespace "downward-api-2374" to be "Succeeded or Failed"
Aug 22 09:29:50.961: INFO: Pod "downward-api-40a42be4-ce47-4799-b6fc-f1fb2d76922f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.381488ms
Aug 22 09:29:52.965: INFO: Pod "downward-api-40a42be4-ce47-4799-b6fc-f1fb2d76922f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005885616s
STEP: Saw pod success
Aug 22 09:29:52.965: INFO: Pod "downward-api-40a42be4-ce47-4799-b6fc-f1fb2d76922f" satisfied condition "Succeeded or Failed"
Aug 22 09:29:52.971: INFO: Trying to get logs from node 172.23.79.103 pod downward-api-40a42be4-ce47-4799-b6fc-f1fb2d76922f container dapi-container: <nil>
STEP: delete the pod
Aug 22 09:29:52.979: INFO: Waiting for pod downward-api-40a42be4-ce47-4799-b6fc-f1fb2d76922f to disappear
Aug 22 09:29:52.980: INFO: Pod downward-api-40a42be4-ce47-4799-b6fc-f1fb2d76922f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:52.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2374" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":16,"skipped":280,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:52.983: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Aug 22 09:29:52.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9416 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Aug 22 09:29:53.135: INFO: stderr: ""
Aug 22 09:29:53.135: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Aug 22 09:29:53.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9416 delete pods e2e-test-httpd-pod'
Aug 22 09:29:55.719: INFO: stderr: ""
Aug 22 09:29:55.719: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:55.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9416" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":17,"skipped":291,"failed":0}
SSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:55.726: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Aug 22 09:29:55.748: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:55.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9908" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":18,"skipped":294,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:55.758: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:55.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1445" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":19,"skipped":314,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:55.776: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 22 09:29:55.799: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:29:55.800: INFO: Number of nodes with available pods: 0
Aug 22 09:29:55.801: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 09:29:56.804: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:29:56.805: INFO: Number of nodes with available pods: 1
Aug 22 09:29:56.805: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 22 09:29:56.815: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:29:56.816: INFO: Number of nodes with available pods: 1
Aug 22 09:29:56.816: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1475, will wait for the garbage collector to delete the pods
Aug 22 09:29:56.874: INFO: Deleting DaemonSet.extensions daemon-set took: 2.723348ms
Aug 22 09:29:56.974: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.440897ms
Aug 22 09:29:58.777: INFO: Number of nodes with available pods: 0
Aug 22 09:29:58.777: INFO: Number of running nodes: 0, number of available pods: 0
Aug 22 09:29:58.780: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2896"},"items":null}

Aug 22 09:29:58.781: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2896"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:29:58.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1475" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":20,"skipped":319,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:29:58.788: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Aug 22 09:30:38.837: INFO: The status of Pod kube-controller-manager-172.23.79.104 is Running (Ready = true)
Aug 22 09:30:38.921: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 22 09:30:38.921: INFO: Deleting pod "simpletest.rc-2vls9" in namespace "gc-1471"
Aug 22 09:30:38.925: INFO: Deleting pod "simpletest.rc-7phpx" in namespace "gc-1471"
Aug 22 09:30:38.929: INFO: Deleting pod "simpletest.rc-7xbbg" in namespace "gc-1471"
Aug 22 09:30:38.933: INFO: Deleting pod "simpletest.rc-9dts8" in namespace "gc-1471"
Aug 22 09:30:38.938: INFO: Deleting pod "simpletest.rc-9mn8f" in namespace "gc-1471"
Aug 22 09:30:38.942: INFO: Deleting pod "simpletest.rc-d5w9t" in namespace "gc-1471"
Aug 22 09:30:38.948: INFO: Deleting pod "simpletest.rc-g29zp" in namespace "gc-1471"
Aug 22 09:30:38.951: INFO: Deleting pod "simpletest.rc-ppx6j" in namespace "gc-1471"
Aug 22 09:30:38.954: INFO: Deleting pod "simpletest.rc-prq9f" in namespace "gc-1471"
Aug 22 09:30:38.958: INFO: Deleting pod "simpletest.rc-vrx7t" in namespace "gc-1471"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:30:38.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1471" for this suite.

• [SLOW TEST:40.178 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":21,"skipped":342,"failed":0}
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:30:38.965: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-d24071bd-e9b2-49c2-af55-c8b364b4eb34 in namespace container-probe-4660
Aug 22 09:30:44.990: INFO: Started pod test-webserver-d24071bd-e9b2-49c2-af55-c8b364b4eb34 in namespace container-probe-4660
STEP: checking the pod's current state and verifying that restartCount is present
Aug 22 09:30:44.992: INFO: Initial restart count of pod test-webserver-d24071bd-e9b2-49c2-af55-c8b364b4eb34 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:34:45.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4660" for this suite.

• [SLOW TEST:246.850 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":22,"skipped":342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:34:45.815: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Aug 22 09:34:45.838: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 22 09:34:45.838: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 22 09:34:45.841: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 22 09:34:45.841: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 22 09:34:45.847: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 22 09:34:45.847: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 22 09:34:45.855: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 22 09:34:45.855: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 22 09:34:46.793: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 22 09:34:46.793: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 22 09:34:46.800: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Aug 22 09:34:46.806: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Aug 22 09:34:46.807: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0
Aug 22 09:34:46.807: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0
Aug 22 09:34:46.807: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0
Aug 22 09:34:46.807: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0
Aug 22 09:34:46.808: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0
Aug 22 09:34:46.808: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0
Aug 22 09:34:46.808: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0
Aug 22 09:34:46.808: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 0
Aug 22 09:34:46.808: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
Aug 22 09:34:46.808: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
Aug 22 09:34:46.808: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:46.808: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:46.808: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:46.808: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:46.811: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:46.811: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:46.820: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:46.820: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:46.828: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
Aug 22 09:34:46.828: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
Aug 22 09:34:46.834: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
Aug 22 09:34:46.834: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
Aug 22 09:34:47.822: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:47.822: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:47.834: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
STEP: listing Deployments
Aug 22 09:34:47.836: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Aug 22 09:34:47.842: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Aug 22 09:34:47.848: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 22 09:34:47.848: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 22 09:34:47.855: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 22 09:34:47.862: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 22 09:34:47.864: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 22 09:34:49.214: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 22 09:34:50.415: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Aug 22 09:34:50.426: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 22 09:34:50.432: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 22 09:34:53.014: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Aug 22 09:34:53.032: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
Aug 22 09:34:53.032: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
Aug 22 09:34:53.032: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
Aug 22 09:34:53.033: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
Aug 22 09:34:53.033: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 1
Aug 22 09:34:53.033: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:53.033: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 3
Aug 22 09:34:53.033: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:53.033: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 2
Aug 22 09:34:53.033: INFO: observed Deployment test-deployment in namespace deployment-6909 with ReadyReplicas 3
STEP: deleting the Deployment
Aug 22 09:34:53.036: INFO: observed event type MODIFIED
Aug 22 09:34:53.036: INFO: observed event type MODIFIED
Aug 22 09:34:53.036: INFO: observed event type MODIFIED
Aug 22 09:34:53.036: INFO: observed event type MODIFIED
Aug 22 09:34:53.036: INFO: observed event type MODIFIED
Aug 22 09:34:53.036: INFO: observed event type MODIFIED
Aug 22 09:34:53.037: INFO: observed event type MODIFIED
Aug 22 09:34:53.037: INFO: observed event type MODIFIED
Aug 22 09:34:53.037: INFO: observed event type MODIFIED
Aug 22 09:34:53.037: INFO: observed event type MODIFIED
Aug 22 09:34:53.037: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 22 09:34:53.040: INFO: Log out all the ReplicaSets if there is no deployment created
Aug 22 09:34:53.042: INFO: ReplicaSet "test-deployment-56c98d85f9":
&ReplicaSet{ObjectMeta:{test-deployment-56c98d85f9  deployment-6909  a8a3961d-588b-4e14-ac16-1f975f8fce9a 3742 4 2021-08-22 09:34:46 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment ed2e55ff-2806-44dd-8e59-b2632bb69652 0xc002f6a897 0xc002f6a898}] []  [{kube-controller-manager Update apps/v1 2021-08-22 09:34:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed2e55ff-2806-44dd-8e59-b2632bb69652\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 09:34:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 56c98d85f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.5 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002f6a920 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 22 09:34:53.045: INFO: pod: "test-deployment-56c98d85f9-942md":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-942md test-deployment-56c98d85f9- deployment-6909  df08f708-0e70-4d78-bcba-cf60a33ef519 3738 0 2021-08-22 09:34:46 +0000 UTC 2021-08-22 09:34:54 +0000 UTC 0xc003fbed60 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 a8a3961d-588b-4e14-ac16-1f975f8fce9a 0xc003fbed97 0xc003fbed98}] []  [{kube-controller-manager Update v1 2021-08-22 09:34:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8a3961d-588b-4e14-ac16-1f975f8fce9a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:34:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dzcwc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dzcwc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:34:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:34:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:34:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:34:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.103,PodIP:10.244.1.42,StartTime:2021-08-22 09:34:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:34:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:docker-pullable://k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:docker://82f1de9bf22c73375bf7c6cc691dbb0127935a6335af66c2e76c2c4de8dcbce9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 22 09:34:53.045: INFO: ReplicaSet "test-deployment-855f7994f9":
&ReplicaSet{ObjectMeta:{test-deployment-855f7994f9  deployment-6909  9460c1fd-3da3-4a25-bd3d-6db6a3fbdb8e 3646 3 2021-08-22 09:34:45 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment ed2e55ff-2806-44dd-8e59-b2632bb69652 0xc002f6a987 0xc002f6a988}] []  [{kube-controller-manager Update apps/v1 2021-08-22 09:34:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed2e55ff-2806-44dd-8e59-b2632bb69652\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 09:34:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 855f7994f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002f6aa10 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 22 09:34:53.046: INFO: ReplicaSet "test-deployment-d4dfddfbf":
&ReplicaSet{ObjectMeta:{test-deployment-d4dfddfbf  deployment-6909  4b12e31e-b41d-4217-8106-96d9f837611f 3734 2 2021-08-22 09:34:47 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment ed2e55ff-2806-44dd-8e59-b2632bb69652 0xc002f6aa77 0xc002f6aa78}] []  [{kube-controller-manager Update apps/v1 2021-08-22 09:34:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed2e55ff-2806-44dd-8e59-b2632bb69652\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 09:34:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d4dfddfbf,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002f6ab00 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Aug 22 09:34:53.048: INFO: pod: "test-deployment-d4dfddfbf-6587n":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-6587n test-deployment-d4dfddfbf- deployment-6909  8f79e100-b263-49ba-9468-e4ccb83c804a 3689 0 2021-08-22 09:34:47 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 4b12e31e-b41d-4217-8106-96d9f837611f 0xc002f6af97 0xc002f6af98}] []  [{kube-controller-manager Update v1 2021-08-22 09:34:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b12e31e-b41d-4217-8106-96d9f837611f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:34:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rdphb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rdphb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:34:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:34:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:34:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:34:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.103,PodIP:10.244.1.44,StartTime:2021-08-22 09:34:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:34:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://e8a414c99a9e37ebe3147fc1a18a43a821112cc01c6ccc1edfd79ef50445337e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 22 09:34:53.048: INFO: pod: "test-deployment-d4dfddfbf-hntbj":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-hntbj test-deployment-d4dfddfbf- deployment-6909  13c8bef7-e164-43d3-b78f-920532c4eaf1 3733 0 2021-08-22 09:34:50 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 4b12e31e-b41d-4217-8106-96d9f837611f 0xc002f6b3f7 0xc002f6b3f8}] []  [{kube-controller-manager Update v1 2021-08-22 09:34:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b12e31e-b41d-4217-8106-96d9f837611f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:34:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cbfjs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cbfjs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:34:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:34:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:34:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:34:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.103,PodIP:10.244.1.45,StartTime:2021-08-22 09:34:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:34:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://fb703d9c94f1415c357d963c3846e2d001b4384806dc25dcb26c530c45df2e70,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:34:53.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6909" for this suite.

• [SLOW TEST:7.238 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":23,"skipped":401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:34:53.054: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:34:53.068: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:34:56.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3183" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":24,"skipped":453,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:34:56.237: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Aug 22 09:34:56.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-7862 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 22 09:34:56.307: INFO: stderr: ""
Aug 22 09:34:56.307: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Aug 22 09:34:56.307: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 22 09:34:56.307: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7862" to be "running and ready, or succeeded"
Aug 22 09:34:56.309: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.361892ms
Aug 22 09:34:58.314: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006681839s
Aug 22 09:34:58.314: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 22 09:34:58.314: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Aug 22 09:34:58.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-7862 logs logs-generator logs-generator'
Aug 22 09:34:58.372: INFO: stderr: ""
Aug 22 09:34:58.372: INFO: stdout: "I0822 09:34:57.006525       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/6q47 427\nI0822 09:34:57.206661       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/vkp 260\nI0822 09:34:57.407087       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/4zz 253\nI0822 09:34:57.607369       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/gwp 318\nI0822 09:34:57.806598       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/j2k 284\nI0822 09:34:58.006789       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/748p 501\nI0822 09:34:58.207085       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/p6vb 371\n"
STEP: limiting log lines
Aug 22 09:34:58.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-7862 logs logs-generator logs-generator --tail=1'
Aug 22 09:34:58.420: INFO: stderr: ""
Aug 22 09:34:58.420: INFO: stdout: "I0822 09:34:58.407371       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/dzr 289\n"
Aug 22 09:34:58.420: INFO: got output "I0822 09:34:58.407371       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/dzr 289\n"
STEP: limiting log bytes
Aug 22 09:34:58.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-7862 logs logs-generator logs-generator --limit-bytes=1'
Aug 22 09:34:58.468: INFO: stderr: ""
Aug 22 09:34:58.468: INFO: stdout: "I"
Aug 22 09:34:58.468: INFO: got output "I"
STEP: exposing timestamps
Aug 22 09:34:58.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-7862 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 22 09:34:58.516: INFO: stderr: ""
Aug 22 09:34:58.516: INFO: stdout: "2021-08-22T09:34:58.407440774Z I0822 09:34:58.407371       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/dzr 289\n"
Aug 22 09:34:58.516: INFO: got output "2021-08-22T09:34:58.407440774Z I0822 09:34:58.407371       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/dzr 289\n"
STEP: restricting to a time range
Aug 22 09:35:01.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-7862 logs logs-generator logs-generator --since=1s'
Aug 22 09:35:01.077: INFO: stderr: ""
Aug 22 09:35:01.077: INFO: stdout: "I0822 09:35:00.206605       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/8w7 299\nI0822 09:35:00.406889       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/4x64 557\nI0822 09:35:00.607176       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/vp4 439\nI0822 09:35:00.807464       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/zcr 239\nI0822 09:35:01.006700       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/7bbh 362\n"
Aug 22 09:35:01.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-7862 logs logs-generator logs-generator --since=24h'
Aug 22 09:35:01.126: INFO: stderr: ""
Aug 22 09:35:01.126: INFO: stdout: "I0822 09:34:57.006525       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/6q47 427\nI0822 09:34:57.206661       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/vkp 260\nI0822 09:34:57.407087       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/4zz 253\nI0822 09:34:57.607369       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/gwp 318\nI0822 09:34:57.806598       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/j2k 284\nI0822 09:34:58.006789       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/748p 501\nI0822 09:34:58.207085       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/p6vb 371\nI0822 09:34:58.407371       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/dzr 289\nI0822 09:34:58.606600       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/wrk 538\nI0822 09:34:58.806891       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/kg82 252\nI0822 09:34:59.007171       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/8p4 336\nI0822 09:34:59.207462       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/nltr 486\nI0822 09:34:59.406689       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/qqt 358\nI0822 09:34:59.606976       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/9hj 243\nI0822 09:34:59.807250       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/bsmw 394\nI0822 09:35:00.007423       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/dnn 214\nI0822 09:35:00.206605       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/8w7 299\nI0822 09:35:00.406889       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/4x64 557\nI0822 09:35:00.607176       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/vp4 439\nI0822 09:35:00.807464       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/zcr 239\nI0822 09:35:01.006700       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/7bbh 362\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Aug 22 09:35:01.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-7862 delete pod logs-generator'
Aug 22 09:35:02.073: INFO: stderr: ""
Aug 22 09:35:02.073: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:35:02.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7862" for this suite.

• [SLOW TEST:5.840 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1393
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":25,"skipped":453,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:35:02.077: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-e2aa5d7b-9ac6-45f6-97a2-50c4d8293e34
STEP: Creating a pod to test consume configMaps
Aug 22 09:35:02.097: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ff0af95-4447-4120-8399-619b210839ae" in namespace "configmap-9506" to be "Succeeded or Failed"
Aug 22 09:35:02.099: INFO: Pod "pod-configmaps-7ff0af95-4447-4120-8399-619b210839ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1.249836ms
Aug 22 09:35:04.105: INFO: Pod "pod-configmaps-7ff0af95-4447-4120-8399-619b210839ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007276597s
STEP: Saw pod success
Aug 22 09:35:04.105: INFO: Pod "pod-configmaps-7ff0af95-4447-4120-8399-619b210839ae" satisfied condition "Succeeded or Failed"
Aug 22 09:35:04.106: INFO: Trying to get logs from node 172.23.79.103 pod pod-configmaps-7ff0af95-4447-4120-8399-619b210839ae container agnhost-container: <nil>
STEP: delete the pod
Aug 22 09:35:04.115: INFO: Waiting for pod pod-configmaps-7ff0af95-4447-4120-8399-619b210839ae to disappear
Aug 22 09:35:04.117: INFO: Pod pod-configmaps-7ff0af95-4447-4120-8399-619b210839ae no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:35:04.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9506" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":26,"skipped":467,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:35:04.120: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Aug 22 09:35:04.137: INFO: Creating simple deployment test-deployment-5t7kt
Aug 22 09:35:04.142: INFO: deployment "test-deployment-5t7kt" doesn't have the required revision set
STEP: Getting /status
Aug 22 09:35:06.152: INFO: Deployment test-deployment-5t7kt has Conditions: [{Available True 2021-08-22 09:35:05 +0000 UTC 2021-08-22 09:35:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2021-08-22 09:35:05 +0000 UTC 2021-08-22 09:35:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5t7kt-794dd694d8" has successfully progressed.}]
STEP: updating Deployment Status
Aug 22 09:35:06.156: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765221705, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765221705, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765221705, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765221704, loc:(*time.Location)(0xa09cc60)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-5t7kt-794dd694d8\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Aug 22 09:35:06.157: INFO: Observed &Deployment event: ADDED
Aug 22 09:35:06.157: INFO: Observed Deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-08-22 09:35:04 +0000 UTC 2021-08-22 09:35:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5t7kt-794dd694d8"}
Aug 22 09:35:06.158: INFO: Observed &Deployment event: MODIFIED
Aug 22 09:35:06.158: INFO: Observed Deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-08-22 09:35:04 +0000 UTC 2021-08-22 09:35:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5t7kt-794dd694d8"}
Aug 22 09:35:06.158: INFO: Observed Deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-08-22 09:35:04 +0000 UTC 2021-08-22 09:35:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 22 09:35:06.158: INFO: Observed &Deployment event: MODIFIED
Aug 22 09:35:06.158: INFO: Observed Deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-08-22 09:35:04 +0000 UTC 2021-08-22 09:35:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 22 09:35:06.158: INFO: Observed Deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-08-22 09:35:04 +0000 UTC 2021-08-22 09:35:04 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5t7kt-794dd694d8" is progressing.}
Aug 22 09:35:06.158: INFO: Observed &Deployment event: MODIFIED
Aug 22 09:35:06.158: INFO: Observed Deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-08-22 09:35:05 +0000 UTC 2021-08-22 09:35:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 22 09:35:06.158: INFO: Observed Deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-08-22 09:35:05 +0000 UTC 2021-08-22 09:35:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5t7kt-794dd694d8" has successfully progressed.}
Aug 22 09:35:06.158: INFO: Observed &Deployment event: MODIFIED
Aug 22 09:35:06.158: INFO: Observed Deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-08-22 09:35:05 +0000 UTC 2021-08-22 09:35:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 22 09:35:06.158: INFO: Observed Deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-08-22 09:35:05 +0000 UTC 2021-08-22 09:35:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5t7kt-794dd694d8" has successfully progressed.}
Aug 22 09:35:06.158: INFO: Found Deployment test-deployment-5t7kt in namespace deployment-5423 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 22 09:35:06.158: INFO: Deployment test-deployment-5t7kt has an updated status
STEP: patching the Statefulset Status
Aug 22 09:35:06.158: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 22 09:35:06.161: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Aug 22 09:35:06.163: INFO: Observed &Deployment event: ADDED
Aug 22 09:35:06.163: INFO: Observed deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-08-22 09:35:04 +0000 UTC 2021-08-22 09:35:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5t7kt-794dd694d8"}
Aug 22 09:35:06.163: INFO: Observed &Deployment event: MODIFIED
Aug 22 09:35:06.163: INFO: Observed deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-08-22 09:35:04 +0000 UTC 2021-08-22 09:35:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5t7kt-794dd694d8"}
Aug 22 09:35:06.163: INFO: Observed deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-08-22 09:35:04 +0000 UTC 2021-08-22 09:35:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 22 09:35:06.163: INFO: Observed &Deployment event: MODIFIED
Aug 22 09:35:06.163: INFO: Observed deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-08-22 09:35:04 +0000 UTC 2021-08-22 09:35:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 22 09:35:06.163: INFO: Observed deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-08-22 09:35:04 +0000 UTC 2021-08-22 09:35:04 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5t7kt-794dd694d8" is progressing.}
Aug 22 09:35:06.163: INFO: Observed &Deployment event: MODIFIED
Aug 22 09:35:06.163: INFO: Observed deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-08-22 09:35:05 +0000 UTC 2021-08-22 09:35:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 22 09:35:06.163: INFO: Observed deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-08-22 09:35:05 +0000 UTC 2021-08-22 09:35:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5t7kt-794dd694d8" has successfully progressed.}
Aug 22 09:35:06.164: INFO: Observed &Deployment event: MODIFIED
Aug 22 09:35:06.164: INFO: Observed deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-08-22 09:35:05 +0000 UTC 2021-08-22 09:35:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 22 09:35:06.164: INFO: Observed deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-08-22 09:35:05 +0000 UTC 2021-08-22 09:35:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5t7kt-794dd694d8" has successfully progressed.}
Aug 22 09:35:06.164: INFO: Observed deployment test-deployment-5t7kt in namespace deployment-5423 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 22 09:35:06.164: INFO: Observed &Deployment event: MODIFIED
Aug 22 09:35:06.164: INFO: Found deployment test-deployment-5t7kt in namespace deployment-5423 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 22 09:35:06.164: INFO: Deployment test-deployment-5t7kt has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 22 09:35:06.166: INFO: Deployment "test-deployment-5t7kt":
&Deployment{ObjectMeta:{test-deployment-5t7kt  deployment-5423  297ab0bc-4059-4dcc-8747-983ace607feb 3927 1 2021-08-22 09:35:04 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-08-22 09:35:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2021-08-22 09:35:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2021-08-22 09:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00359d7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-5t7kt-794dd694d8",LastUpdateTime:2021-08-22 09:35:06 +0000 UTC,LastTransitionTime:2021-08-22 09:35:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 22 09:35:06.168: INFO: New ReplicaSet "test-deployment-5t7kt-794dd694d8" of Deployment "test-deployment-5t7kt":
&ReplicaSet{ObjectMeta:{test-deployment-5t7kt-794dd694d8  deployment-5423  34eb8244-6816-433f-92cf-afabacd06fb3 3921 1 2021-08-22 09:35:04 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-5t7kt 297ab0bc-4059-4dcc-8747-983ace607feb 0xc003266c87 0xc003266c88}] []  [{kube-controller-manager Update apps/v1 2021-08-22 09:35:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"297ab0bc-4059-4dcc-8747-983ace607feb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 09:35:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 794dd694d8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003266d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 22 09:35:06.170: INFO: Pod "test-deployment-5t7kt-794dd694d8-d6lfq" is available:
&Pod{ObjectMeta:{test-deployment-5t7kt-794dd694d8-d6lfq test-deployment-5t7kt-794dd694d8- deployment-5423  c3fba30e-432e-4c0d-b0f3-d1dca5c75cd9 3920 0 2021-08-22 09:35:04 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [{apps/v1 ReplicaSet test-deployment-5t7kt-794dd694d8 34eb8244-6816-433f-92cf-afabacd06fb3 0xc003267397 0xc003267398}] []  [{kube-controller-manager Update v1 2021-08-22 09:35:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34eb8244-6816-433f-92cf-afabacd06fb3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:35:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kbhxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kbhxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:35:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.103,PodIP:10.244.1.48,StartTime:2021-08-22 09:35:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:35:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://da073809b1b29b3a2b1e4efed903d4eb7d0aed02e3e9dc0dcfe195380b3abeed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:35:06.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5423" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":27,"skipped":511,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:35:06.174: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9992 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9992;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9992 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9992;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9992.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9992.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9992.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9992.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9992.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9992.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9992.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9992.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9992.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9992.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9992.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9992.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9992.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 45.153.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.153.45_udp@PTR;check="$$(dig +tcp +noall +answer +search 45.153.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.153.45_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9992 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9992;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9992 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9992;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9992.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9992.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9992.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9992.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9992.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9992.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9992.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9992.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9992.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9992.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9992.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9992.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9992.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 45.153.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.153.45_udp@PTR;check="$$(dig +tcp +noall +answer +search 45.153.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.153.45_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 09:35:24.211: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.213: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.215: INFO: Unable to read wheezy_udp@dns-test-service.dns-9992 from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.216: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9992 from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.218: INFO: Unable to read wheezy_udp@dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.219: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.220: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.222: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.232: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.233: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.234: INFO: Unable to read jessie_udp@dns-test-service.dns-9992 from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.236: INFO: Unable to read jessie_tcp@dns-test-service.dns-9992 from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.237: INFO: Unable to read jessie_udp@dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.239: INFO: Unable to read jessie_tcp@dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.240: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.241: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:24.250: INFO: Lookups using dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9992 wheezy_tcp@dns-test-service.dns-9992 wheezy_udp@dns-test-service.dns-9992.svc wheezy_tcp@dns-test-service.dns-9992.svc wheezy_udp@_http._tcp.dns-test-service.dns-9992.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9992.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9992 jessie_tcp@dns-test-service.dns-9992 jessie_udp@dns-test-service.dns-9992.svc jessie_tcp@dns-test-service.dns-9992.svc jessie_udp@_http._tcp.dns-test-service.dns-9992.svc jessie_tcp@_http._tcp.dns-test-service.dns-9992.svc]

Aug 22 09:35:29.253: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.255: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.257: INFO: Unable to read wheezy_udp@dns-test-service.dns-9992 from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.258: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9992 from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.260: INFO: Unable to read wheezy_udp@dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.261: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.263: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.264: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.274: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.275: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.276: INFO: Unable to read jessie_udp@dns-test-service.dns-9992 from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.278: INFO: Unable to read jessie_tcp@dns-test-service.dns-9992 from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.279: INFO: Unable to read jessie_udp@dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.281: INFO: Unable to read jessie_tcp@dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.282: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.283: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:29.292: INFO: Lookups using dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9992 wheezy_tcp@dns-test-service.dns-9992 wheezy_udp@dns-test-service.dns-9992.svc wheezy_tcp@dns-test-service.dns-9992.svc wheezy_udp@_http._tcp.dns-test-service.dns-9992.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9992.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9992 jessie_tcp@dns-test-service.dns-9992 jessie_udp@dns-test-service.dns-9992.svc jessie_tcp@dns-test-service.dns-9992.svc jessie_udp@_http._tcp.dns-test-service.dns-9992.svc jessie_tcp@_http._tcp.dns-test-service.dns-9992.svc]

Aug 22 09:35:34.254: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.255: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.257: INFO: Unable to read wheezy_udp@dns-test-service.dns-9992 from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.259: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9992 from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.260: INFO: Unable to read wheezy_udp@dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.262: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.263: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.264: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.274: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.276: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.277: INFO: Unable to read jessie_udp@dns-test-service.dns-9992 from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.278: INFO: Unable to read jessie_tcp@dns-test-service.dns-9992 from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.280: INFO: Unable to read jessie_udp@dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.281: INFO: Unable to read jessie_tcp@dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.283: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.284: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9992.svc from pod dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda: the server could not find the requested resource (get pods dns-test-d2160102-1a22-4608-8463-e18fa78b4bda)
Aug 22 09:35:34.292: INFO: Lookups using dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9992 wheezy_tcp@dns-test-service.dns-9992 wheezy_udp@dns-test-service.dns-9992.svc wheezy_tcp@dns-test-service.dns-9992.svc wheezy_udp@_http._tcp.dns-test-service.dns-9992.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9992.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9992 jessie_tcp@dns-test-service.dns-9992 jessie_udp@dns-test-service.dns-9992.svc jessie_tcp@dns-test-service.dns-9992.svc jessie_udp@_http._tcp.dns-test-service.dns-9992.svc jessie_tcp@_http._tcp.dns-test-service.dns-9992.svc]

Aug 22 09:35:39.293: INFO: DNS probes using dns-9992/dns-test-d2160102-1a22-4608-8463-e18fa78b4bda succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:35:39.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9992" for this suite.

• [SLOW TEST:33.168 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":28,"skipped":517,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:35:39.342: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 22 09:35:39.362: INFO: Waiting up to 5m0s for pod "pod-1d00b216-5886-47c3-a3aa-571a3dd1b1e7" in namespace "emptydir-9842" to be "Succeeded or Failed"
Aug 22 09:35:39.365: INFO: Pod "pod-1d00b216-5886-47c3-a3aa-571a3dd1b1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.669316ms
Aug 22 09:35:41.370: INFO: Pod "pod-1d00b216-5886-47c3-a3aa-571a3dd1b1e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007536887s
STEP: Saw pod success
Aug 22 09:35:41.370: INFO: Pod "pod-1d00b216-5886-47c3-a3aa-571a3dd1b1e7" satisfied condition "Succeeded or Failed"
Aug 22 09:35:41.372: INFO: Trying to get logs from node 172.23.79.103 pod pod-1d00b216-5886-47c3-a3aa-571a3dd1b1e7 container test-container: <nil>
STEP: delete the pod
Aug 22 09:35:41.382: INFO: Waiting for pod pod-1d00b216-5886-47c3-a3aa-571a3dd1b1e7 to disappear
Aug 22 09:35:41.385: INFO: Pod pod-1d00b216-5886-47c3-a3aa-571a3dd1b1e7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:35:41.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9842" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":29,"skipped":523,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:35:41.389: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 09:35:41.868: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 22 09:35:43.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765221741, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765221741, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765221741, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765221741, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 09:35:46.890: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:35:56.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3346" for this suite.
STEP: Destroying namespace "webhook-3346-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.596 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":30,"skipped":531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:35:56.986: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:35:59.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2094" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":31,"skipped":603,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:35:59.665: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7685
STEP: creating service affinity-nodeport in namespace services-7685
STEP: creating replication controller affinity-nodeport in namespace services-7685
I0822 09:35:59.691284      21 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-7685, replica count: 3
I0822 09:36:02.743056      21 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 09:36:02.751: INFO: Creating new exec pod
Aug 22 09:36:05.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-7685 exec execpod-affinityr4tlq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Aug 22 09:36:05.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 22 09:36:05.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 09:36:05.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-7685 exec execpod-affinityr4tlq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.164.59 80'
Aug 22 09:36:05.972: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.164.59 80\nConnection to 10.110.164.59 80 port [tcp/http] succeeded!\n"
Aug 22 09:36:05.972: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 09:36:05.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-7685 exec execpod-affinityr4tlq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.23.79.103 32674'
Aug 22 09:36:06.077: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.23.79.103 32674\nConnection to 172.23.79.103 32674 port [tcp/*] succeeded!\n"
Aug 22 09:36:06.077: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 09:36:06.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-7685 exec execpod-affinityr4tlq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.23.79.103:32674/ ; done'
Aug 22 09:36:06.232: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:32674/\n"
Aug 22 09:36:06.232: INFO: stdout: "\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv\naffinity-nodeport-w42hv"
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Received response from host: affinity-nodeport-w42hv
Aug 22 09:36:06.232: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7685, will wait for the garbage collector to delete the pods
Aug 22 09:36:06.293: INFO: Deleting ReplicationController affinity-nodeport took: 2.902449ms
Aug 22 09:36:06.393: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.348982ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:36:08.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7685" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.055 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":32,"skipped":605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:36:08.721: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 22 09:36:08.740: INFO: Waiting up to 5m0s for pod "pod-d3e362f0-16c9-40c8-827a-40fdc636f514" in namespace "emptydir-2697" to be "Succeeded or Failed"
Aug 22 09:36:08.742: INFO: Pod "pod-d3e362f0-16c9-40c8-827a-40fdc636f514": Phase="Pending", Reason="", readiness=false. Elapsed: 1.827747ms
Aug 22 09:36:10.746: INFO: Pod "pod-d3e362f0-16c9-40c8-827a-40fdc636f514": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005792024s
Aug 22 09:36:12.751: INFO: Pod "pod-d3e362f0-16c9-40c8-827a-40fdc636f514": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010543546s
STEP: Saw pod success
Aug 22 09:36:12.751: INFO: Pod "pod-d3e362f0-16c9-40c8-827a-40fdc636f514" satisfied condition "Succeeded or Failed"
Aug 22 09:36:12.752: INFO: Trying to get logs from node 172.23.79.103 pod pod-d3e362f0-16c9-40c8-827a-40fdc636f514 container test-container: <nil>
STEP: delete the pod
Aug 22 09:36:12.760: INFO: Waiting for pod pod-d3e362f0-16c9-40c8-827a-40fdc636f514 to disappear
Aug 22 09:36:12.761: INFO: Pod pod-d3e362f0-16c9-40c8-827a-40fdc636f514 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:36:12.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2697" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":639,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:36:12.764: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Aug 22 09:36:14.797: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:36:16.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5187" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":34,"skipped":658,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:36:16.820: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:36:20.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1429" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":35,"skipped":671,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:36:20.869: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 22 09:36:20.886: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:36:25.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9608" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":36,"skipped":719,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:36:25.064: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:36:25.084: INFO: Waiting up to 5m0s for pod "busybox-user-65534-8c97f896-79fa-4925-a87a-9f8fb659c7f9" in namespace "security-context-test-5354" to be "Succeeded or Failed"
Aug 22 09:36:25.085: INFO: Pod "busybox-user-65534-8c97f896-79fa-4925-a87a-9f8fb659c7f9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.1558ms
Aug 22 09:36:27.088: INFO: Pod "busybox-user-65534-8c97f896-79fa-4925-a87a-9f8fb659c7f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004409974s
Aug 22 09:36:27.088: INFO: Pod "busybox-user-65534-8c97f896-79fa-4925-a87a-9f8fb659c7f9" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:36:27.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5354" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":37,"skipped":734,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:36:27.092: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:36:27.111: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3e1f7b4c-3da8-416d-a980-670a4b0d7c6f" in namespace "security-context-test-3388" to be "Succeeded or Failed"
Aug 22 09:36:27.112: INFO: Pod "busybox-readonly-false-3e1f7b4c-3da8-416d-a980-670a4b0d7c6f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.389978ms
Aug 22 09:36:29.117: INFO: Pod "busybox-readonly-false-3e1f7b4c-3da8-416d-a980-670a4b0d7c6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005904832s
Aug 22 09:36:29.117: INFO: Pod "busybox-readonly-false-3e1f7b4c-3da8-416d-a980-670a4b0d7c6f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:36:29.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3388" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":741,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:36:29.122: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:36:29.144: INFO: The status of Pod test-webserver-2dd6d041-53ec-4895-a567-d2b06aab4396 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:36:31.147: INFO: The status of Pod test-webserver-2dd6d041-53ec-4895-a567-d2b06aab4396 is Running (Ready = false)
Aug 22 09:36:33.149: INFO: The status of Pod test-webserver-2dd6d041-53ec-4895-a567-d2b06aab4396 is Running (Ready = false)
Aug 22 09:36:35.149: INFO: The status of Pod test-webserver-2dd6d041-53ec-4895-a567-d2b06aab4396 is Running (Ready = false)
Aug 22 09:36:37.147: INFO: The status of Pod test-webserver-2dd6d041-53ec-4895-a567-d2b06aab4396 is Running (Ready = false)
Aug 22 09:36:39.149: INFO: The status of Pod test-webserver-2dd6d041-53ec-4895-a567-d2b06aab4396 is Running (Ready = false)
Aug 22 09:36:41.149: INFO: The status of Pod test-webserver-2dd6d041-53ec-4895-a567-d2b06aab4396 is Running (Ready = false)
Aug 22 09:36:43.150: INFO: The status of Pod test-webserver-2dd6d041-53ec-4895-a567-d2b06aab4396 is Running (Ready = false)
Aug 22 09:36:45.150: INFO: The status of Pod test-webserver-2dd6d041-53ec-4895-a567-d2b06aab4396 is Running (Ready = false)
Aug 22 09:36:47.148: INFO: The status of Pod test-webserver-2dd6d041-53ec-4895-a567-d2b06aab4396 is Running (Ready = false)
Aug 22 09:36:49.156: INFO: The status of Pod test-webserver-2dd6d041-53ec-4895-a567-d2b06aab4396 is Running (Ready = false)
Aug 22 09:36:51.150: INFO: The status of Pod test-webserver-2dd6d041-53ec-4895-a567-d2b06aab4396 is Running (Ready = true)
Aug 22 09:36:51.151: INFO: Container started at 2021-08-22 09:36:29 +0000 UTC, pod became ready at 2021-08-22 09:36:49 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:36:51.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6472" for this suite.

• [SLOW TEST:22.034 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":39,"skipped":769,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:36:51.155: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Aug 22 09:36:52.209: INFO: The status of Pod kube-controller-manager-172.23.79.104 is Running (Ready = true)
Aug 22 09:36:52.299: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:36:52.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8005" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":40,"skipped":771,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:36:52.304: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-08f6f91e-be2c-41a4-9e09-58c0e0f77902
STEP: Creating secret with name s-test-opt-upd-ec55a577-7cca-4993-a106-9df41e6ee463
STEP: Creating the pod
Aug 22 09:36:52.329: INFO: The status of Pod pod-projected-secrets-81ba2d93-4071-4a58-bb65-9d3ffa729dbc is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:36:54.334: INFO: The status of Pod pod-projected-secrets-81ba2d93-4071-4a58-bb65-9d3ffa729dbc is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:36:56.334: INFO: The status of Pod pod-projected-secrets-81ba2d93-4071-4a58-bb65-9d3ffa729dbc is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-08f6f91e-be2c-41a4-9e09-58c0e0f77902
STEP: Updating secret s-test-opt-upd-ec55a577-7cca-4993-a106-9df41e6ee463
STEP: Creating secret with name s-test-opt-create-4c2f3f92-8bef-4814-b270-4aa1b4f970e8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:38:06.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3397" for this suite.

• [SLOW TEST:74.281 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":41,"skipped":772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:38:06.585: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Aug 22 09:38:06.603: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:38:08.607: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Aug 22 09:38:08.614: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:38:10.617: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Aug 22 09:38:10.620: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 22 09:38:10.622: INFO: Pod pod-with-prestop-http-hook still exists
Aug 22 09:38:12.623: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 22 09:38:12.627: INFO: Pod pod-with-prestop-http-hook still exists
Aug 22 09:38:14.623: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 22 09:38:14.627: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:38:14.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5577" for this suite.

• [SLOW TEST:8.049 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":42,"skipped":800,"failed":0}
SS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:38:14.635: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:38:16.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3697" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":43,"skipped":802,"failed":0}

------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:38:16.667: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:38:16.687: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-14e640a6-ad56-4bf3-a1a2-c4b266904daf" in namespace "security-context-test-3926" to be "Succeeded or Failed"
Aug 22 09:38:16.690: INFO: Pod "alpine-nnp-false-14e640a6-ad56-4bf3-a1a2-c4b266904daf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.97798ms
Aug 22 09:38:18.695: INFO: Pod "alpine-nnp-false-14e640a6-ad56-4bf3-a1a2-c4b266904daf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008631733s
Aug 22 09:38:20.699: INFO: Pod "alpine-nnp-false-14e640a6-ad56-4bf3-a1a2-c4b266904daf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011773675s
Aug 22 09:38:20.699: INFO: Pod "alpine-nnp-false-14e640a6-ad56-4bf3-a1a2-c4b266904daf" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:38:20.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3926" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":802,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:38:20.707: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:38:33.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1280" for this suite.

• [SLOW TEST:13.065 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":45,"skipped":834,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:38:33.772: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 22 09:38:33.804: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:38:33.806: INFO: Number of nodes with available pods: 0
Aug 22 09:38:33.806: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 09:38:34.810: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:38:34.812: INFO: Number of nodes with available pods: 0
Aug 22 09:38:34.812: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 09:38:35.810: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:38:35.812: INFO: Number of nodes with available pods: 1
Aug 22 09:38:35.812: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Getting /status
Aug 22 09:38:35.815: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Aug 22 09:38:35.819: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Aug 22 09:38:35.820: INFO: Observed &DaemonSet event: ADDED
Aug 22 09:38:35.820: INFO: Observed &DaemonSet event: MODIFIED
Aug 22 09:38:35.820: INFO: Observed &DaemonSet event: MODIFIED
Aug 22 09:38:35.820: INFO: Found daemon set daemon-set in namespace daemonsets-1965 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 22 09:38:35.820: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Aug 22 09:38:35.824: INFO: Observed &DaemonSet event: ADDED
Aug 22 09:38:35.824: INFO: Observed &DaemonSet event: MODIFIED
Aug 22 09:38:35.824: INFO: Observed &DaemonSet event: MODIFIED
Aug 22 09:38:35.824: INFO: Observed daemon set daemon-set in namespace daemonsets-1965 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 22 09:38:35.824: INFO: Observed &DaemonSet event: MODIFIED
Aug 22 09:38:35.824: INFO: Found daemon set daemon-set in namespace daemonsets-1965 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 22 09:38:35.824: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1965, will wait for the garbage collector to delete the pods
Aug 22 09:38:35.880: INFO: Deleting DaemonSet.extensions daemon-set took: 2.535913ms
Aug 22 09:38:35.980: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.329672ms
Aug 22 09:38:38.185: INFO: Number of nodes with available pods: 0
Aug 22 09:38:38.185: INFO: Number of running nodes: 0, number of available pods: 0
Aug 22 09:38:38.187: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5104"},"items":null}

Aug 22 09:38:38.188: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5104"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:38:38.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1965" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":46,"skipped":835,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:38:38.194: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 22 09:38:38.211: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 22 09:38:38.214: INFO: Waiting for terminating namespaces to be deleted...
Aug 22 09:38:38.216: INFO: 
Logging pods the apiserver thinks is on node 172.23.79.103 before test
Aug 22 09:38:38.218: INFO: kube-flannel-ds-s57gz from kube-system started at 2021-08-22 09:15:14 +0000 UTC (1 container statuses recorded)
Aug 22 09:38:38.219: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 09:38:38.219: INFO: kube-proxy-r9sql from kube-system started at 2021-08-22 09:14:45 +0000 UTC (1 container statuses recorded)
Aug 22 09:38:38.219: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 22 09:38:38.219: INFO: sonobuoy from sonobuoy started at 2021-08-22 09:27:55 +0000 UTC (1 container statuses recorded)
Aug 22 09:38:38.219: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 22 09:38:38.219: INFO: sonobuoy-e2e-job-75357183cd8142cd from sonobuoy started at 2021-08-22 09:27:56 +0000 UTC (2 container statuses recorded)
Aug 22 09:38:38.219: INFO: 	Container e2e ready: true, restart count 0
Aug 22 09:38:38.219: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 09:38:38.219: INFO: sonobuoy-systemd-logs-daemon-set-d3d945f99a484f42-ggzhp from sonobuoy started at 2021-08-22 09:27:56 +0000 UTC (2 container statuses recorded)
Aug 22 09:38:38.219: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 09:38:38.219: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a6b6bcb1-6924-493c-a6f8-47443095423b 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.23.79.103 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-a6b6bcb1-6924-493c-a6f8-47443095423b off the node 172.23.79.103
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a6b6bcb1-6924-493c-a6f8-47443095423b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:43:42.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5268" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.082 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":47,"skipped":849,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:43:42.276: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Aug 22 09:43:42.291: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2958 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:43:42.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2958" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":48,"skipped":850,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:43:42.331: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-288v
STEP: Creating a pod to test atomic-volume-subpath
Aug 22 09:43:42.350: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-288v" in namespace "subpath-5825" to be "Succeeded or Failed"
Aug 22 09:43:42.352: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Pending", Reason="", readiness=false. Elapsed: 1.691741ms
Aug 22 09:43:44.357: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006444481s
Aug 22 09:43:46.362: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012057097s
Aug 22 09:43:48.366: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016202901s
Aug 22 09:43:50.373: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Running", Reason="", readiness=true. Elapsed: 8.022616087s
Aug 22 09:43:52.376: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Running", Reason="", readiness=true. Elapsed: 10.026260369s
Aug 22 09:43:54.380: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Running", Reason="", readiness=true. Elapsed: 12.029823694s
Aug 22 09:43:56.386: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Running", Reason="", readiness=true. Elapsed: 14.035613728s
Aug 22 09:43:58.390: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Running", Reason="", readiness=true. Elapsed: 16.040315577s
Aug 22 09:44:00.396: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Running", Reason="", readiness=true. Elapsed: 18.045597642s
Aug 22 09:44:02.399: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Running", Reason="", readiness=true. Elapsed: 20.048789594s
Aug 22 09:44:04.405: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Running", Reason="", readiness=true. Elapsed: 22.054552647s
Aug 22 09:44:06.411: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Running", Reason="", readiness=true. Elapsed: 24.061157009s
Aug 22 09:44:08.416: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Running", Reason="", readiness=true. Elapsed: 26.066100082s
Aug 22 09:44:10.421: INFO: Pod "pod-subpath-test-downwardapi-288v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.070599687s
STEP: Saw pod success
Aug 22 09:44:10.421: INFO: Pod "pod-subpath-test-downwardapi-288v" satisfied condition "Succeeded or Failed"
Aug 22 09:44:10.422: INFO: Trying to get logs from node 172.23.79.105 pod pod-subpath-test-downwardapi-288v container test-container-subpath-downwardapi-288v: <nil>
STEP: delete the pod
Aug 22 09:44:10.437: INFO: Waiting for pod pod-subpath-test-downwardapi-288v to disappear
Aug 22 09:44:10.438: INFO: Pod pod-subpath-test-downwardapi-288v no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-288v
Aug 22 09:44:10.438: INFO: Deleting pod "pod-subpath-test-downwardapi-288v" in namespace "subpath-5825"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:44:10.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5825" for this suite.

• [SLOW TEST:28.112 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":49,"skipped":853,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:44:10.442: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:44:10.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1574 version'
Aug 22 09:44:10.501: INFO: stderr: ""
Aug 22 09:44:10.501: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.1\", GitCommit:\"632ed300f2c34f6d6d15ca4cef3d3c7073412212\", GitTreeState:\"clean\", BuildDate:\"2021-08-19T15:45:37Z\", GoVersion:\"go1.16.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.1\", GitCommit:\"632ed300f2c34f6d6d15ca4cef3d3c7073412212\", GitTreeState:\"clean\", BuildDate:\"2021-08-19T15:39:34Z\", GoVersion:\"go1.16.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:44:10.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1574" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":50,"skipped":858,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:44:10.505: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4c0bee8b-c499-49c0-842c-db4e8b519c17
STEP: Creating the pod
Aug 22 09:44:10.526: INFO: The status of Pod pod-projected-configmaps-5422c47c-4cd2-4d4c-a34b-79e9253818fb is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:44:12.530: INFO: The status of Pod pod-projected-configmaps-5422c47c-4cd2-4d4c-a34b-79e9253818fb is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-4c0bee8b-c499-49c0-842c-db4e8b519c17
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:44:14.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9793" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":51,"skipped":862,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:44:14.554: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Aug 22 09:44:14.573: INFO: Waiting up to 5m0s for pod "var-expansion-019edeaf-c1b5-4994-ad78-7299c24c9f4d" in namespace "var-expansion-6340" to be "Succeeded or Failed"
Aug 22 09:44:14.574: INFO: Pod "var-expansion-019edeaf-c1b5-4994-ad78-7299c24c9f4d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.137841ms
Aug 22 09:44:16.580: INFO: Pod "var-expansion-019edeaf-c1b5-4994-ad78-7299c24c9f4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006647775s
Aug 22 09:44:18.585: INFO: Pod "var-expansion-019edeaf-c1b5-4994-ad78-7299c24c9f4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011633239s
STEP: Saw pod success
Aug 22 09:44:18.585: INFO: Pod "var-expansion-019edeaf-c1b5-4994-ad78-7299c24c9f4d" satisfied condition "Succeeded or Failed"
Aug 22 09:44:18.586: INFO: Trying to get logs from node 172.23.79.105 pod var-expansion-019edeaf-c1b5-4994-ad78-7299c24c9f4d container dapi-container: <nil>
STEP: delete the pod
Aug 22 09:44:18.595: INFO: Waiting for pod var-expansion-019edeaf-c1b5-4994-ad78-7299c24c9f4d to disappear
Aug 22 09:44:18.596: INFO: Pod var-expansion-019edeaf-c1b5-4994-ad78-7299c24c9f4d no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:44:18.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6340" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":52,"skipped":885,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:44:18.600: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:46:00.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3540" for this suite.

• [SLOW TEST:102.038 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":53,"skipped":925,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:46:00.639: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5804.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5804.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5804.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5804.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5804.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5804.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 09:46:18.688: INFO: DNS probes using dns-5804/dns-test-ffdbb3d4-80e4-49ec-b5de-a9905fc6b74d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:46:18.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5804" for this suite.

• [SLOW TEST:18.057 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":54,"skipped":932,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:46:18.696: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:46:18.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9919" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":55,"skipped":939,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:46:18.735: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-13711e9a-5d5d-4b83-8ab2-8c895b623e40
STEP: Creating a pod to test consume secrets
Aug 22 09:46:18.758: INFO: Waiting up to 5m0s for pod "pod-secrets-778b71dd-b477-4fe5-a227-d744a0efcb6c" in namespace "secrets-4487" to be "Succeeded or Failed"
Aug 22 09:46:18.760: INFO: Pod "pod-secrets-778b71dd-b477-4fe5-a227-d744a0efcb6c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.731972ms
Aug 22 09:46:20.765: INFO: Pod "pod-secrets-778b71dd-b477-4fe5-a227-d744a0efcb6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006829718s
STEP: Saw pod success
Aug 22 09:46:20.765: INFO: Pod "pod-secrets-778b71dd-b477-4fe5-a227-d744a0efcb6c" satisfied condition "Succeeded or Failed"
Aug 22 09:46:20.767: INFO: Trying to get logs from node 172.23.79.105 pod pod-secrets-778b71dd-b477-4fe5-a227-d744a0efcb6c container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 09:46:20.779: INFO: Waiting for pod pod-secrets-778b71dd-b477-4fe5-a227-d744a0efcb6c to disappear
Aug 22 09:46:20.780: INFO: Pod pod-secrets-778b71dd-b477-4fe5-a227-d744a0efcb6c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:46:20.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4487" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":56,"skipped":956,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:46:20.784: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:46:20.804: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 22 09:46:25.808: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 22 09:46:29.816: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 22 09:46:31.821: INFO: Creating deployment "test-rollover-deployment"
Aug 22 09:46:31.825: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 22 09:46:33.833: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 22 09:46:33.836: INFO: Ensure that both replica sets have 1 created replica
Aug 22 09:46:33.838: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 22 09:46:33.842: INFO: Updating deployment test-rollover-deployment
Aug 22 09:46:33.842: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 22 09:46:35.850: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 22 09:46:35.853: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 22 09:46:35.856: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 09:46:35.856: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222394, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 09:46:37.861: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 09:46:37.861: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222394, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 09:46:39.864: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 09:46:39.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222394, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 09:46:41.864: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 09:46:41.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222394, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 09:46:43.862: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 09:46:43.862: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222394, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765222391, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 09:46:45.863: INFO: 
Aug 22 09:46:45.863: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 22 09:46:45.867: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8819  6bcd4675-55b4-4c74-a334-181b316e7e6e 6349 2 2021-08-22 09:46:31 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-08-22 09:46:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 09:46:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036a1ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-08-22 09:46:31 +0000 UTC,LastTransitionTime:2021-08-22 09:46:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-08-22 09:46:44 +0000 UTC,LastTransitionTime:2021-08-22 09:46:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 22 09:46:45.869: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-8819  10ebc54d-bc36-49f8-b24a-b4eff6f8913a 6339 2 2021-08-22 09:46:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 6bcd4675-55b4-4c74-a334-181b316e7e6e 0xc00410d1f0 0xc00410d1f1}] []  [{kube-controller-manager Update apps/v1 2021-08-22 09:46:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6bcd4675-55b4-4c74-a334-181b316e7e6e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 09:46:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00410d2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 22 09:46:45.869: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 22 09:46:45.869: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8819  7e139d4b-357a-4e5c-b77d-0f5d2bb33894 6348 2 2021-08-22 09:46:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 6bcd4675-55b4-4c74-a334-181b316e7e6e 0xc00410ce8f 0xc00410cea0}] []  [{e2e.test Update apps/v1 2021-08-22 09:46:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 09:46:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6bcd4675-55b4-4c74-a334-181b316e7e6e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2021-08-22 09:46:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00410cff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 22 09:46:45.869: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-8819  368747df-c662-4a65-b7e4-8b76ed8f054d 6313 2 2021-08-22 09:46:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 6bcd4675-55b4-4c74-a334-181b316e7e6e 0xc00410d057 0xc00410d058}] []  [{kube-controller-manager Update apps/v1 2021-08-22 09:46:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6bcd4675-55b4-4c74-a334-181b316e7e6e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 09:46:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00410d108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 22 09:46:45.871: INFO: Pod "test-rollover-deployment-98c5f4599-4mnkd" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-4mnkd test-rollover-deployment-98c5f4599- deployment-8819  02e85c82-a34d-4fec-b2cb-8a1fa6fe67b4 6320 0 2021-08-22 09:46:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 10ebc54d-bc36-49f8-b24a-b4eff6f8913a 0xc00410df30 0xc00410df31}] []  [{kube-controller-manager Update v1 2021-08-22 09:46:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10ebc54d-bc36-49f8-b24a-b4eff6f8913a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:46:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5vsd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5vsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:46:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:46:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:46:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:46:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:10.244.2.11,StartTime:2021-08-22 09:46:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:46:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://02ac6965bb0e12d710e6dea54898ce70b5919a1217a72c175674e7f77accb37d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:46:45.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8819" for this suite.

• [SLOW TEST:25.092 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":57,"skipped":958,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:46:45.876: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1118.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1118.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1118.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 09:46:47.910: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:47.912: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:47.913: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:47.914: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:47.918: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:47.920: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:47.921: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:47.923: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:47.925: INFO: Lookups using dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local]

Aug 22 09:46:52.928: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:52.930: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:52.931: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:52.933: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:52.937: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:52.938: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:52.939: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:52.941: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:52.944: INFO: Lookups using dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local]

Aug 22 09:46:57.928: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:57.930: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:57.931: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:57.933: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:57.937: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:57.938: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:57.940: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:57.941: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:46:57.943: INFO: Lookups using dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local]

Aug 22 09:47:02.928: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:02.930: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:02.931: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:02.933: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:02.937: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:02.938: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:02.940: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:02.941: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:02.944: INFO: Lookups using dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local]

Aug 22 09:47:07.929: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:07.930: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:07.932: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:07.933: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:07.938: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:07.939: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:07.940: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:07.942: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:07.944: INFO: Lookups using dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local]

Aug 22 09:47:12.928: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:12.930: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:12.931: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:12.933: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:12.937: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:12.939: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:12.940: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:12.942: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local from pod dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6: the server could not find the requested resource (get pods dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6)
Aug 22 09:47:12.944: INFO: Lookups using dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1118.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1118.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1118.svc.cluster.local jessie_udp@dns-test-service-2.dns-1118.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1118.svc.cluster.local]

Aug 22 09:47:17.943: INFO: DNS probes using dns-1118/dns-test-674ceaca-a42a-4e23-a06d-3c834055f6c6 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:47:17.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1118" for this suite.

• [SLOW TEST:32.129 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":58,"skipped":979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:47:18.005: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-98f140cc-8f60-45e5-bd66-e5ba2aed43bc in namespace container-probe-8556
Aug 22 09:47:20.035: INFO: Started pod busybox-98f140cc-8f60-45e5-bd66-e5ba2aed43bc in namespace container-probe-8556
STEP: checking the pod's current state and verifying that restartCount is present
Aug 22 09:47:20.036: INFO: Initial restart count of pod busybox-98f140cc-8f60-45e5-bd66-e5ba2aed43bc is 0
Aug 22 09:48:10.170: INFO: Restart count of pod container-probe-8556/busybox-98f140cc-8f60-45e5-bd66-e5ba2aed43bc is now 1 (50.133497237s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:48:10.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8556" for this suite.

• [SLOW TEST:52.174 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":59,"skipped":1033,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:48:10.179: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 22 09:48:10.200: INFO: Waiting up to 5m0s for pod "pod-8379e5db-3825-4d2f-b337-5f2dc408fb4e" in namespace "emptydir-9511" to be "Succeeded or Failed"
Aug 22 09:48:10.202: INFO: Pod "pod-8379e5db-3825-4d2f-b337-5f2dc408fb4e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.329611ms
Aug 22 09:48:12.207: INFO: Pod "pod-8379e5db-3825-4d2f-b337-5f2dc408fb4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006826649s
STEP: Saw pod success
Aug 22 09:48:12.207: INFO: Pod "pod-8379e5db-3825-4d2f-b337-5f2dc408fb4e" satisfied condition "Succeeded or Failed"
Aug 22 09:48:12.209: INFO: Trying to get logs from node 172.23.79.105 pod pod-8379e5db-3825-4d2f-b337-5f2dc408fb4e container test-container: <nil>
STEP: delete the pod
Aug 22 09:48:12.222: INFO: Waiting for pod pod-8379e5db-3825-4d2f-b337-5f2dc408fb4e to disappear
Aug 22 09:48:12.223: INFO: Pod pod-8379e5db-3825-4d2f-b337-5f2dc408fb4e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:48:12.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9511" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":60,"skipped":1033,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:48:12.227: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Aug 22 09:48:12.243: INFO: namespace kubectl-9413
Aug 22 09:48:12.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9413 create -f -'
Aug 22 09:48:12.474: INFO: stderr: ""
Aug 22 09:48:12.474: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 22 09:48:13.478: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 22 09:48:13.478: INFO: Found 0 / 1
Aug 22 09:48:14.478: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 22 09:48:14.478: INFO: Found 1 / 1
Aug 22 09:48:14.478: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 22 09:48:14.480: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 22 09:48:14.480: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 22 09:48:14.480: INFO: wait on agnhost-primary startup in kubectl-9413 
Aug 22 09:48:14.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9413 logs agnhost-primary-tlr29 agnhost-primary'
Aug 22 09:48:14.528: INFO: stderr: ""
Aug 22 09:48:14.528: INFO: stdout: "Paused\n"
STEP: exposing RC
Aug 22 09:48:14.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9413 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 22 09:48:14.587: INFO: stderr: ""
Aug 22 09:48:14.587: INFO: stdout: "service/rm2 exposed\n"
Aug 22 09:48:14.593: INFO: Service rm2 in namespace kubectl-9413 found.
STEP: exposing service
Aug 22 09:48:16.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9413 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 22 09:48:16.652: INFO: stderr: ""
Aug 22 09:48:16.652: INFO: stdout: "service/rm3 exposed\n"
Aug 22 09:48:16.660: INFO: Service rm3 in namespace kubectl-9413 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:48:18.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9413" for this suite.

• [SLOW TEST:6.445 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1233
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":61,"skipped":1039,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:48:18.672: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:48:18.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5638" for this suite.
STEP: Destroying namespace "nspatchtest-28d8a3ff-8006-48c9-8ea4-837192a2c8c0-7665" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":62,"skipped":1049,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:48:18.708: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 09:48:19.060: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 09:48:22.073: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:48:22.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3772" for this suite.
STEP: Destroying namespace "webhook-3772-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":63,"skipped":1052,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:48:22.138: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:48:30.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1353" for this suite.

• [SLOW TEST:8.036 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":64,"skipped":1062,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:48:30.174: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:48:30.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-4449 create -f -'
Aug 22 09:48:30.304: INFO: stderr: ""
Aug 22 09:48:30.304: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 22 09:48:30.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-4449 create -f -'
Aug 22 09:48:30.417: INFO: stderr: ""
Aug 22 09:48:30.417: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 22 09:48:31.421: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 22 09:48:31.421: INFO: Found 1 / 1
Aug 22 09:48:31.421: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 22 09:48:31.422: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 22 09:48:31.422: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 22 09:48:31.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-4449 describe pod agnhost-primary-cs55r'
Aug 22 09:48:31.478: INFO: stderr: ""
Aug 22 09:48:31.478: INFO: stdout: "Name:         agnhost-primary-cs55r\nNamespace:    kubectl-4449\nPriority:     0\nNode:         172.23.79.105/172.23.79.105\nStart Time:   Sun, 22 Aug 2021 09:48:30 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.244.2.21\nIPs:\n  IP:           10.244.2.21\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://1e72c2ec8281090701b76c0222731f3bdfeb61d8847ff11fba56b554234ff8be\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 22 Aug 2021 09:48:31 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f7b8v (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-f7b8v:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-4449/agnhost-primary-cs55r to 172.23.79.105\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Aug 22 09:48:31.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-4449 describe rc agnhost-primary'
Aug 22 09:48:31.537: INFO: stderr: ""
Aug 22 09:48:31.537: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4449\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-cs55r\n"
Aug 22 09:48:31.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-4449 describe service agnhost-primary'
Aug 22 09:48:31.584: INFO: stderr: ""
Aug 22 09:48:31.584: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4449\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.103.129.43\nIPs:               10.103.129.43\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.2.21:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 22 09:48:31.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-4449 describe node 172.23.79.103'
Aug 22 09:48:31.645: INFO: stderr: ""
Aug 22 09:48:31.645: INFO: stdout: "Name:               172.23.79.103\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=172.23.79.103\n                    kubernetes.io/os=linux\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"9e:45:bb:f4:a1:76\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.23.79.103\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 22 Aug 2021 09:14:44 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  172.23.79.103\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 22 Aug 2021 09:48:24 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sun, 22 Aug 2021 09:15:16 +0000   Sun, 22 Aug 2021 09:15:16 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Sun, 22 Aug 2021 09:43:51 +0000   Sun, 22 Aug 2021 09:14:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sun, 22 Aug 2021 09:43:51 +0000   Sun, 22 Aug 2021 09:14:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sun, 22 Aug 2021 09:43:51 +0000   Sun, 22 Aug 2021 09:14:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sun, 22 Aug 2021 09:43:51 +0000   Sun, 22 Aug 2021 09:14:54 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.23.79.103\n  Hostname:    172.23.79.103\nCapacity:\n  cpu:                4\n  ephemeral-storage:  41152812Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15990528Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  37926431477\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15888128Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 20210623111207095151419199170789\n  System UUID:                0E0E2F58-AB36-4D3A-9CBA-7F0D6380387E\n  Boot ID:                    5d4ba79b-40e9-46d7-b9df-afa36c5a369f\n  Kernel Version:             3.10.0-1160.31.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.8\n  Kubelet Version:            v1.22.1\n  Kube-Proxy Version:         v1.22.1\nPodCIDR:                      10.244.1.0/24\nPodCIDRs:                     10.244.1.0/24\nNon-terminated Pods:          (5 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 kube-flannel-ds-s57gz                                      100m (2%)     100m (2%)   50Mi (0%)        50Mi (0%)      33m\n  kube-system                 kube-proxy-r9sql                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  sonobuoy                    sonobuoy-e2e-job-75357183cd8142cd                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-d3d945f99a484f42-ggzhp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                100m (2%)  100m (2%)\n  memory             50Mi (0%)  50Mi (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:\n  Type    Reason                   Age                From     Message\n  ----    ------                   ----               ----     -------\n  Normal  Starting                 33m                kubelet  Starting kubelet.\n  Normal  NodeHasSufficientMemory  33m (x2 over 33m)  kubelet  Node 172.23.79.103 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    33m (x2 over 33m)  kubelet  Node 172.23.79.103 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     33m (x2 over 33m)  kubelet  Node 172.23.79.103 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  33m                kubelet  Updated Node Allocatable limit across pods\n  Normal  NodeReady                33m                kubelet  Node 172.23.79.103 status is now: NodeReady\n"
Aug 22 09:48:31.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-4449 describe namespace kubectl-4449'
Aug 22 09:48:31.691: INFO: stderr: ""
Aug 22 09:48:31.691: INFO: stdout: "Name:         kubectl-4449\nLabels:       e2e-framework=kubectl\n              e2e-run=d263fab8-6dd8-43ca-b25a-4719ad4b36e6\n              kubernetes.io/metadata.name=kubectl-4449\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:48:31.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4449" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":65,"skipped":1102,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:48:31.696: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 22 09:48:31.714: INFO: Waiting up to 5m0s for pod "pod-ea337789-63a0-4d8a-8f6f-47db7da75aee" in namespace "emptydir-6245" to be "Succeeded or Failed"
Aug 22 09:48:31.715: INFO: Pod "pod-ea337789-63a0-4d8a-8f6f-47db7da75aee": Phase="Pending", Reason="", readiness=false. Elapsed: 1.359883ms
Aug 22 09:48:33.719: INFO: Pod "pod-ea337789-63a0-4d8a-8f6f-47db7da75aee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004629647s
STEP: Saw pod success
Aug 22 09:48:33.719: INFO: Pod "pod-ea337789-63a0-4d8a-8f6f-47db7da75aee" satisfied condition "Succeeded or Failed"
Aug 22 09:48:33.720: INFO: Trying to get logs from node 172.23.79.105 pod pod-ea337789-63a0-4d8a-8f6f-47db7da75aee container test-container: <nil>
STEP: delete the pod
Aug 22 09:48:33.729: INFO: Waiting for pod pod-ea337789-63a0-4d8a-8f6f-47db7da75aee to disappear
Aug 22 09:48:33.730: INFO: Pod pod-ea337789-63a0-4d8a-8f6f-47db7da75aee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:48:33.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6245" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":66,"skipped":1115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:48:33.734: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 09:48:33.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc233e4d-de60-43b6-a706-c947c7ee11b4" in namespace "downward-api-3381" to be "Succeeded or Failed"
Aug 22 09:48:33.754: INFO: Pod "downwardapi-volume-dc233e4d-de60-43b6-a706-c947c7ee11b4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.65722ms
Aug 22 09:48:35.757: INFO: Pod "downwardapi-volume-dc233e4d-de60-43b6-a706-c947c7ee11b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004045515s
STEP: Saw pod success
Aug 22 09:48:35.757: INFO: Pod "downwardapi-volume-dc233e4d-de60-43b6-a706-c947c7ee11b4" satisfied condition "Succeeded or Failed"
Aug 22 09:48:35.758: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-dc233e4d-de60-43b6-a706-c947c7ee11b4 container client-container: <nil>
STEP: delete the pod
Aug 22 09:48:35.767: INFO: Waiting for pod downwardapi-volume-dc233e4d-de60-43b6-a706-c947c7ee11b4 to disappear
Aug 22 09:48:35.768: INFO: Pod downwardapi-volume-dc233e4d-de60-43b6-a706-c947c7ee11b4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:48:35.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3381" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":67,"skipped":1174,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:48:35.772: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5228
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 22 09:48:35.787: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 22 09:48:35.798: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:48:37.802: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:48:39.803: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:48:41.802: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:48:43.802: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:48:45.803: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:48:47.802: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:48:49.805: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:48:51.803: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:48:53.802: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:48:55.805: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 09:48:57.802: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 22 09:48:57.804: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Aug 22 09:48:59.829: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 22 09:48:59.829: INFO: Going to poll 10.244.1.74 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Aug 22 09:48:59.830: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.74:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5228 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 09:48:59.830: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 09:48:59.886: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 22 09:48:59.886: INFO: Going to poll 10.244.2.24 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Aug 22 09:48:59.888: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.24:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5228 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 09:48:59.888: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 09:48:59.941: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:48:59.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5228" for this suite.

• [SLOW TEST:24.174 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":68,"skipped":1175,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:48:59.946: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:48:59.962: INFO: Creating deployment "webserver-deployment"
Aug 22 09:48:59.964: INFO: Waiting for observed generation 1
Aug 22 09:49:01.970: INFO: Waiting for all required pods to come up
Aug 22 09:49:01.973: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 22 09:49:07.979: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 22 09:49:07.982: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 22 09:49:07.986: INFO: Updating deployment webserver-deployment
Aug 22 09:49:07.986: INFO: Waiting for observed generation 2
Aug 22 09:49:09.992: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 22 09:49:09.993: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 22 09:49:09.995: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 22 09:49:09.998: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 22 09:49:09.998: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 22 09:49:09.999: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 22 09:49:10.002: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 22 09:49:10.002: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 22 09:49:10.006: INFO: Updating deployment webserver-deployment
Aug 22 09:49:10.006: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 22 09:49:10.008: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 22 09:49:10.011: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 22 09:49:10.027: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-348  4d307d51-fe63-417c-a18a-7dee48b365d9 7352 3 2021-08-22 09:48:59 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-08-22 09:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 09:49:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043c6d48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-08-22 09:49:08 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-08-22 09:49:10 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 22 09:49:10.040: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-348  6d8e3494-b175-49f1-9feb-dd6824531466 7332 3 2021-08-22 09:49:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4d307d51-fe63-417c-a18a-7dee48b365d9 0xc0043c7167 0xc0043c7168}] []  [{kube-controller-manager Update apps/v1 2021-08-22 09:49:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d307d51-fe63-417c-a18a-7dee48b365d9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 09:49:08 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043c7208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 22 09:49:10.040: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 22 09:49:10.040: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-348  e059d1a9-5458-4d6b-a2c1-040d875a37dc 7330 3 2021-08-22 09:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4d307d51-fe63-417c-a18a-7dee48b365d9 0xc0043c7267 0xc0043c7268}] []  [{kube-controller-manager Update apps/v1 2021-08-22 09:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d307d51-fe63-417c-a18a-7dee48b365d9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 09:49:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043c72f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 22 09:49:10.053: INFO: Pod "webserver-deployment-795d758f88-59qhk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-59qhk webserver-deployment-795d758f88- deployment-348  05ad196e-c07b-4e33-976c-a5516eba455a 7377 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc0043c77c0 0xc0043c77c1}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-29ldf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-29ldf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.053: INFO: Pod "webserver-deployment-795d758f88-79jmr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-79jmr webserver-deployment-795d758f88- deployment-348  8d82d3fe-a72f-4e59-93b6-9cf7bce73a8b 7295 0 2021-08-22 09:49:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc0043c7930 0xc0043c7931}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:49:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2bgdf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2bgdf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.103,PodIP:,StartTime:2021-08-22 09:49:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.053: INFO: Pod "webserver-deployment-795d758f88-8sfc5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8sfc5 webserver-deployment-795d758f88- deployment-348  d9fb886e-0dd1-4116-8c47-8c15240d04c3 7380 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc0043c7b17 0xc0043c7b18}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x562m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x562m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.054: INFO: Pod "webserver-deployment-795d758f88-8vqrk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8vqrk webserver-deployment-795d758f88- deployment-348  aae133a6-1006-4e59-a48c-79ef64a43d4c 7359 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc0043c7c90 0xc0043c7c91}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgn8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgn8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.054: INFO: Pod "webserver-deployment-795d758f88-bz9dx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bz9dx webserver-deployment-795d758f88- deployment-348  5aa0e8ac-9e57-48c9-8495-38a2a545ac37 7354 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc0043c7e00 0xc0043c7e01}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h474x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h474x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.054: INFO: Pod "webserver-deployment-795d758f88-c46sg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c46sg webserver-deployment-795d758f88- deployment-348  c5bca906-1f06-4195-be41-4f628c49cc50 7315 0 2021-08-22 09:49:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc0043c7f70 0xc0043c7f71}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bl4v7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bl4v7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.054: INFO: Pod "webserver-deployment-795d758f88-cbtjt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cbtjt webserver-deployment-795d758f88- deployment-348  a843ce63-c7b7-4b4e-97bf-ecb314d71fd6 7306 0 2021-08-22 09:49:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc0045300e0 0xc0045300e1}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lg7f2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lg7f2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.054: INFO: Pod "webserver-deployment-795d758f88-clh4t" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-clh4t webserver-deployment-795d758f88- deployment-348  e866fd0f-b914-49d4-9adb-c1290d3805ea 7373 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc004530250 0xc004530251}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m6q8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m6q8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.103,PodIP:,StartTime:2021-08-22 09:49:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.054: INFO: Pod "webserver-deployment-795d758f88-hfsvv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hfsvv webserver-deployment-795d758f88- deployment-348  7132bcf2-ae7a-466b-a89d-0c6206c02f9b 7385 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc004530437 0xc004530438}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbtkd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbtkd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.055: INFO: Pod "webserver-deployment-795d758f88-jls2d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jls2d webserver-deployment-795d758f88- deployment-348  849f5689-7609-4a2a-92e3-6c356c09177f 7374 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc0045305c0 0xc0045305c1}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6657f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6657f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.055: INFO: Pod "webserver-deployment-795d758f88-l46dq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-l46dq webserver-deployment-795d758f88- deployment-348  c1d5d64e-3fb8-43a5-8f0a-214b7f1a92b1 7277 0 2021-08-22 09:49:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc004530730 0xc004530731}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-74hq6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-74hq6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.055: INFO: Pod "webserver-deployment-795d758f88-mhfsv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mhfsv webserver-deployment-795d758f88- deployment-348  a685e35f-c806-4a3a-a633-f22fdb46417d 7287 0 2021-08-22 09:49:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc0045308d0 0xc0045308d1}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-txzdj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-txzdj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.055: INFO: Pod "webserver-deployment-795d758f88-nfzl4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nfzl4 webserver-deployment-795d758f88- deployment-348  623bc586-dae3-4266-9e08-5a516f77ded0 7384 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6d8e3494-b175-49f1-9feb-dd6824531466 0xc004530a40 0xc004530a41}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d8e3494-b175-49f1-9feb-dd6824531466\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-whw5s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-whw5s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.055: INFO: Pod "webserver-deployment-847dcfb7fb-47f89" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-47f89 webserver-deployment-847dcfb7fb- deployment-348  9974f30f-f170-49e6-b1cc-2b711d556346 7379 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc004530bb0 0xc004530bb1}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bp55x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bp55x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.055: INFO: Pod "webserver-deployment-847dcfb7fb-4zgmq" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-4zgmq webserver-deployment-847dcfb7fb- deployment-348  fe47d4b1-bcaa-40bd-a74c-969df5583369 7261 0 2021-08-22 09:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc004530d10 0xc004530d11}] []  [{kube-controller-manager Update v1 2021-08-22 09:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:49:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5hk4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5hk4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:10.244.2.32,StartTime:2021-08-22 09:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:49:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://6b2355de6e157be9f62c83948b0891938e12c8f0f9dc7f3c21df60e63f3a259a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.055: INFO: Pod "webserver-deployment-847dcfb7fb-7grsf" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-7grsf webserver-deployment-847dcfb7fb- deployment-348  df705ce5-577e-4f9b-9664-01589cf922aa 7356 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc004530ef0 0xc004530ef1}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzpgt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzpgt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.055: INFO: Pod "webserver-deployment-847dcfb7fb-855q7" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-855q7 webserver-deployment-847dcfb7fb- deployment-348  f5ee5e89-50b3-494d-bf22-8eb4d02ad7a7 7185 0 2021-08-22 09:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc004531050 0xc004531051}] []  [{kube-controller-manager Update v1 2021-08-22 09:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:49:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7pxf7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7pxf7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.103,PodIP:10.244.1.75,StartTime:2021-08-22 09:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:49:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://b01676e1c26c1994822ff6a95c01409df6b9be994ac3a6bd570b9c0d4eb6522b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.056: INFO: Pod "webserver-deployment-847dcfb7fb-8xm7w" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8xm7w webserver-deployment-847dcfb7fb- deployment-348  03b9c370-1fe1-4b98-a00a-0f598192c1b2 7378 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc004531230 0xc004531231}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjznt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjznt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.056: INFO: Pod "webserver-deployment-847dcfb7fb-d5l9q" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-d5l9q webserver-deployment-847dcfb7fb- deployment-348  ad13990a-5e6b-43d5-805e-8fa60d27f931 7368 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc004531390 0xc004531391}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgxm9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgxm9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.056: INFO: Pod "webserver-deployment-847dcfb7fb-g7j4m" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-g7j4m webserver-deployment-847dcfb7fb- deployment-348  e95999c3-b532-4a44-8055-9886dd9fb780 7207 0 2021-08-22 09:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc0045314f0 0xc0045314f1}] []  [{kube-controller-manager Update v1 2021-08-22 09:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:49:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwsqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwsqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:10.244.2.26,StartTime:2021-08-22 09:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:49:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://7b893c4c078b7d317fb2bbc6dee33d917a826c9af7f03dfe9f97ff4a15f2b6f3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.056: INFO: Pod "webserver-deployment-847dcfb7fb-gn6kk" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-gn6kk webserver-deployment-847dcfb7fb- deployment-348  925584e2-4c45-4e3d-b8c0-6b2b05e084c3 7355 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc0045316d0 0xc0045316d1}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l4xb7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l4xb7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.056: INFO: Pod "webserver-deployment-847dcfb7fb-jmkl6" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-jmkl6 webserver-deployment-847dcfb7fb- deployment-348  2ec53857-ec8a-41ae-ac1c-e18a9bd58f10 7372 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc004531830 0xc004531831}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gnrtj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gnrtj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.057: INFO: Pod "webserver-deployment-847dcfb7fb-lnmt6" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-lnmt6 webserver-deployment-847dcfb7fb- deployment-348  dcabdb15-d5e1-40b1-a7f0-a8164413b5fc 7334 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc0045319b0 0xc0045319b1}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9phwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9phwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.057: INFO: Pod "webserver-deployment-847dcfb7fb-mcccj" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mcccj webserver-deployment-847dcfb7fb- deployment-348  1112ec4a-266b-4733-a815-e3f5b0561f79 7216 0 2021-08-22 09:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc004531b20 0xc004531b21}] []  [{kube-controller-manager Update v1 2021-08-22 09:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:49:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72pw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72pw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:10.244.2.27,StartTime:2021-08-22 09:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:49:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://803850f9feec3f4c01e348e74730012b03750b1230fa39e32470fd70ebcbf6f4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.057: INFO: Pod "webserver-deployment-847dcfb7fb-nh9th" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-nh9th webserver-deployment-847dcfb7fb- deployment-348  7d4c736a-6888-4e27-befd-19190629c7d6 7351 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc004531d00 0xc004531d01}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gsdp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gsdp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.057: INFO: Pod "webserver-deployment-847dcfb7fb-q2q28" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-q2q28 webserver-deployment-847dcfb7fb- deployment-348  f89bee67-21af-4f3b-8606-3d3f5a5d6ef7 7343 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc004531e60 0xc004531e61}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lzd4z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lzd4z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.057: INFO: Pod "webserver-deployment-847dcfb7fb-q8mgp" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-q8mgp webserver-deployment-847dcfb7fb- deployment-348  e48838d1-ffa3-402b-b05e-7e16c1ff999b 7254 0 2021-08-22 09:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc004531fc0 0xc004531fc1}] []  [{kube-controller-manager Update v1 2021-08-22 09:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:49:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dp7jq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dp7jq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:10.244.2.33,StartTime:2021-08-22 09:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:49:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://cbcf75cae23d646deb66d03582a539c556391b3b1adea24f7386c2bbd10d915f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.057: INFO: Pod "webserver-deployment-847dcfb7fb-r4ht9" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-r4ht9 webserver-deployment-847dcfb7fb- deployment-348  3e9932b3-697b-453f-ac46-57bc0e64ff4e 7211 0 2021-08-22 09:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc00469c1c0 0xc00469c1c1}] []  [{kube-controller-manager Update v1 2021-08-22 09:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:49:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-594r2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-594r2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:10.244.2.28,StartTime:2021-08-22 09:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:49:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://cd29133bbba6592c573174c5ba0e51beb6e67752b5bb6eaa0d8b8af084fd6baf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.057: INFO: Pod "webserver-deployment-847dcfb7fb-sdzw2" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-sdzw2 webserver-deployment-847dcfb7fb- deployment-348  7dbc9898-005b-4cde-b36e-394f67168c08 7348 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc00469c3a0 0xc00469c3a1}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8b2lm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8b2lm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.058: INFO: Pod "webserver-deployment-847dcfb7fb-t8nmg" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-t8nmg webserver-deployment-847dcfb7fb- deployment-348  84491e0d-1f6f-470a-9a28-a26a8aca4cd2 7376 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc00469c500 0xc00469c501}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6qpk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6qpk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.058: INFO: Pod "webserver-deployment-847dcfb7fb-td49q" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-td49q webserver-deployment-847dcfb7fb- deployment-348  7ab569b8-87ed-4ba3-ad71-cd3b3a6c1bd3 7375 0 2021-08-22 09:49:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc00469c660 0xc00469c661}] []  [{kube-controller-manager Update v1 2021-08-22 09:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4l4bw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4l4bw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.058: INFO: Pod "webserver-deployment-847dcfb7fb-vr2zb" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-vr2zb webserver-deployment-847dcfb7fb- deployment-348  33468509-441a-4d0a-bdb4-c396539528a0 7264 0 2021-08-22 09:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc00469c7c0 0xc00469c7c1}] []  [{kube-controller-manager Update v1 2021-08-22 09:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:49:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qnlmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qnlmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:10.244.2.29,StartTime:2021-08-22 09:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:49:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://6ab3448bb1538e9d925da03abc7a1aaa21cdaa87e1d7d8c35a51e8036c779ab3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 09:49:10.058: INFO: Pod "webserver-deployment-847dcfb7fb-zq2gz" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-zq2gz webserver-deployment-847dcfb7fb- deployment-348  232b538b-9bec-4c19-8e82-60c7e9e3d6db 7257 0 2021-08-22 09:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb e059d1a9-5458-4d6b-a2c1-040d875a37dc 0xc00469c9a0 0xc00469c9a1}] []  [{kube-controller-manager Update v1 2021-08-22 09:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e059d1a9-5458-4d6b-a2c1-040d875a37dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 09:49:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gc97r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc97r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:49:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 09:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:10.244.2.34,StartTime:2021-08-22 09:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 09:49:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://69aa8fccda45189416c9cb8f2052b44dba461be70b17de09dfef015303c87342,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:49:10.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-348" for this suite.

• [SLOW TEST:10.118 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":69,"skipped":1191,"failed":0}
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:49:10.065: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Aug 22 09:49:10.089: INFO: created test-event-1
Aug 22 09:49:10.090: INFO: created test-event-2
Aug 22 09:49:10.092: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Aug 22 09:49:10.093: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Aug 22 09:49:10.099: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:49:10.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5965" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":70,"skipped":1201,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:49:10.104: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 09:49:10.121: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24f61dae-f48a-4877-88d3-bb7f8e7a090d" in namespace "downward-api-4607" to be "Succeeded or Failed"
Aug 22 09:49:10.122: INFO: Pod "downwardapi-volume-24f61dae-f48a-4877-88d3-bb7f8e7a090d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.178969ms
Aug 22 09:49:12.127: INFO: Pod "downwardapi-volume-24f61dae-f48a-4877-88d3-bb7f8e7a090d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006219349s
Aug 22 09:49:14.131: INFO: Pod "downwardapi-volume-24f61dae-f48a-4877-88d3-bb7f8e7a090d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010061398s
Aug 22 09:49:16.135: INFO: Pod "downwardapi-volume-24f61dae-f48a-4877-88d3-bb7f8e7a090d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014473986s
Aug 22 09:49:18.139: INFO: Pod "downwardapi-volume-24f61dae-f48a-4877-88d3-bb7f8e7a090d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018558303s
Aug 22 09:49:20.143: INFO: Pod "downwardapi-volume-24f61dae-f48a-4877-88d3-bb7f8e7a090d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.021823358s
STEP: Saw pod success
Aug 22 09:49:20.143: INFO: Pod "downwardapi-volume-24f61dae-f48a-4877-88d3-bb7f8e7a090d" satisfied condition "Succeeded or Failed"
Aug 22 09:49:20.144: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-24f61dae-f48a-4877-88d3-bb7f8e7a090d container client-container: <nil>
STEP: delete the pod
Aug 22 09:49:21.578: INFO: Waiting for pod downwardapi-volume-24f61dae-f48a-4877-88d3-bb7f8e7a090d to disappear
Aug 22 09:49:21.579: INFO: Pod downwardapi-volume-24f61dae-f48a-4877-88d3-bb7f8e7a090d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:49:21.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4607" for this suite.

• [SLOW TEST:11.480 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":71,"skipped":1216,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:49:21.584: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-b8d55ee0-588f-4eaf-a5c4-68bf513b8a75
STEP: Creating a pod to test consume secrets
Aug 22 09:49:21.608: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-facf50ca-d690-4f53-af1a-f72910dcf554" in namespace "projected-2613" to be "Succeeded or Failed"
Aug 22 09:49:21.609: INFO: Pod "pod-projected-secrets-facf50ca-d690-4f53-af1a-f72910dcf554": Phase="Pending", Reason="", readiness=false. Elapsed: 1.22404ms
Aug 22 09:49:23.612: INFO: Pod "pod-projected-secrets-facf50ca-d690-4f53-af1a-f72910dcf554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00432868s
Aug 22 09:49:25.617: INFO: Pod "pod-projected-secrets-facf50ca-d690-4f53-af1a-f72910dcf554": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009402975s
Aug 22 09:49:27.621: INFO: Pod "pod-projected-secrets-facf50ca-d690-4f53-af1a-f72910dcf554": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01330493s
Aug 22 09:49:29.625: INFO: Pod "pod-projected-secrets-facf50ca-d690-4f53-af1a-f72910dcf554": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016765934s
STEP: Saw pod success
Aug 22 09:49:29.625: INFO: Pod "pod-projected-secrets-facf50ca-d690-4f53-af1a-f72910dcf554" satisfied condition "Succeeded or Failed"
Aug 22 09:49:29.626: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-secrets-facf50ca-d690-4f53-af1a-f72910dcf554 container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 09:49:29.633: INFO: Waiting for pod pod-projected-secrets-facf50ca-d690-4f53-af1a-f72910dcf554 to disappear
Aug 22 09:49:29.635: INFO: Pod pod-projected-secrets-facf50ca-d690-4f53-af1a-f72910dcf554 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:49:29.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2613" for this suite.

• [SLOW TEST:8.054 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1230,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:49:29.639: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:49:29.658: INFO: Got root ca configmap in namespace "svcaccounts-4528"
Aug 22 09:49:29.660: INFO: Deleted root ca configmap in namespace "svcaccounts-4528"
STEP: waiting for a new root ca configmap created
Aug 22 09:49:30.164: INFO: Recreated root ca configmap in namespace "svcaccounts-4528"
Aug 22 09:49:30.166: INFO: Updated root ca configmap in namespace "svcaccounts-4528"
STEP: waiting for the root ca configmap reconciled
Aug 22 09:49:30.669: INFO: Reconciled root ca configmap in namespace "svcaccounts-4528"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:49:30.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4528" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":73,"skipped":1244,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:49:30.673: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 09:49:30.692: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ecfef7ab-0f13-449d-8e1f-ca03864e2655" in namespace "downward-api-974" to be "Succeeded or Failed"
Aug 22 09:49:30.693: INFO: Pod "downwardapi-volume-ecfef7ab-0f13-449d-8e1f-ca03864e2655": Phase="Pending", Reason="", readiness=false. Elapsed: 1.291076ms
Aug 22 09:49:32.699: INFO: Pod "downwardapi-volume-ecfef7ab-0f13-449d-8e1f-ca03864e2655": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006426691s
STEP: Saw pod success
Aug 22 09:49:32.699: INFO: Pod "downwardapi-volume-ecfef7ab-0f13-449d-8e1f-ca03864e2655" satisfied condition "Succeeded or Failed"
Aug 22 09:49:32.700: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-ecfef7ab-0f13-449d-8e1f-ca03864e2655 container client-container: <nil>
STEP: delete the pod
Aug 22 09:49:32.708: INFO: Waiting for pod downwardapi-volume-ecfef7ab-0f13-449d-8e1f-ca03864e2655 to disappear
Aug 22 09:49:32.709: INFO: Pod downwardapi-volume-ecfef7ab-0f13-449d-8e1f-ca03864e2655 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:49:32.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-974" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:49:32.713: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:49:32.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1337" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":75,"skipped":1269,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:49:32.745: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-e43ce9af-1936-45de-b971-6d2de1ab8030
Aug 22 09:49:32.762: INFO: Pod name my-hostname-basic-e43ce9af-1936-45de-b971-6d2de1ab8030: Found 0 pods out of 1
Aug 22 09:49:37.765: INFO: Pod name my-hostname-basic-e43ce9af-1936-45de-b971-6d2de1ab8030: Found 1 pods out of 1
Aug 22 09:49:37.765: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e43ce9af-1936-45de-b971-6d2de1ab8030" are running
Aug 22 09:49:37.767: INFO: Pod "my-hostname-basic-e43ce9af-1936-45de-b971-6d2de1ab8030-f852p" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-08-22 09:49:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-08-22 09:49:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-08-22 09:49:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-08-22 09:49:32 +0000 UTC Reason: Message:}])
Aug 22 09:49:37.767: INFO: Trying to dial the pod
Aug 22 09:49:42.774: INFO: Controller my-hostname-basic-e43ce9af-1936-45de-b971-6d2de1ab8030: Got expected result from replica 1 [my-hostname-basic-e43ce9af-1936-45de-b971-6d2de1ab8030-f852p]: "my-hostname-basic-e43ce9af-1936-45de-b971-6d2de1ab8030-f852p", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:49:42.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4733" for this suite.

• [SLOW TEST:10.033 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":76,"skipped":1291,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:49:42.778: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Aug 22 09:49:42.804: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:49:55.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2903" for this suite.

• [SLOW TEST:12.670 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":77,"skipped":1302,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:49:55.449: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 22 09:49:55.472: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9875  8a92c8ae-815e-4fb1-bd14-ada015637ab8 7821 0 2021-08-22 09:49:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-08-22 09:49:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 09:49:55.472: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9875  8a92c8ae-815e-4fb1-bd14-ada015637ab8 7822 0 2021-08-22 09:49:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-08-22 09:49:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 22 09:49:55.477: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9875  8a92c8ae-815e-4fb1-bd14-ada015637ab8 7823 0 2021-08-22 09:49:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-08-22 09:49:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 09:49:55.477: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9875  8a92c8ae-815e-4fb1-bd14-ada015637ab8 7824 0 2021-08-22 09:49:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-08-22 09:49:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:49:55.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9875" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":78,"skipped":1307,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:49:55.481: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-8513
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:49:55.501: INFO: Found 0 stateful pods, waiting for 1
Aug 22 09:50:05.506: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Aug 22 09:50:05.521: INFO: Found 1 stateful pods, waiting for 2
Aug 22 09:50:15.528: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 09:50:15.528: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Aug 22 09:50:15.540: INFO: Deleting all statefulset in ns statefulset-8513
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:50:15.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8513" for this suite.

• [SLOW TEST:20.085 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":79,"skipped":1404,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:50:15.566: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7886.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7886.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7886.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7886.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7886.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7886.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 09:50:19.610: INFO: DNS probes using dns-7886/dns-test-6e834484-36c4-41ec-8043-e4102997cd1d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:50:19.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7886" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":80,"skipped":1416,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:50:19.629: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-ece77f91-6ab2-4790-9ef5-896f2864708e
STEP: Creating the pod
Aug 22 09:50:19.650: INFO: The status of Pod pod-configmaps-2addd3ff-c27f-4b3f-ac01-3ba4224660dd is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:50:21.655: INFO: The status of Pod pod-configmaps-2addd3ff-c27f-4b3f-ac01-3ba4224660dd is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-ece77f91-6ab2-4790-9ef5-896f2864708e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:50:23.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8548" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":81,"skipped":1421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:50:23.673: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:50:23.695: INFO: Create a RollingUpdate DaemonSet
Aug 22 09:50:23.697: INFO: Check that daemon pods launch on every node of the cluster
Aug 22 09:50:23.699: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:50:23.701: INFO: Number of nodes with available pods: 0
Aug 22 09:50:23.701: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 09:50:24.705: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:50:24.707: INFO: Number of nodes with available pods: 1
Aug 22 09:50:24.707: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 09:50:25.704: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:50:25.706: INFO: Number of nodes with available pods: 2
Aug 22 09:50:25.706: INFO: Number of running nodes: 2, number of available pods: 2
Aug 22 09:50:25.706: INFO: Update the DaemonSet to trigger a rollout
Aug 22 09:50:25.710: INFO: Updating DaemonSet daemon-set
Aug 22 09:50:27.719: INFO: Roll back the DaemonSet before rollout is complete
Aug 22 09:50:27.723: INFO: Updating DaemonSet daemon-set
Aug 22 09:50:27.723: INFO: Make sure DaemonSet rollback is complete
Aug 22 09:50:27.725: INFO: Wrong image for pod: daemon-set-2bhsw. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Aug 22 09:50:27.725: INFO: Pod daemon-set-2bhsw is not available
Aug 22 09:50:27.727: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:50:28.732: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:50:29.733: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:50:30.733: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:50:31.733: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:50:32.732: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:50:33.731: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:50:34.731: INFO: Pod daemon-set-5flw4 is not available
Aug 22 09:50:34.733: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8009, will wait for the garbage collector to delete the pods
Aug 22 09:50:34.792: INFO: Deleting DaemonSet.extensions daemon-set took: 2.830051ms
Aug 22 09:50:34.892: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.314742ms
Aug 22 09:50:37.796: INFO: Number of nodes with available pods: 0
Aug 22 09:50:37.796: INFO: Number of running nodes: 0, number of available pods: 0
Aug 22 09:50:37.797: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8182"},"items":null}

Aug 22 09:50:37.798: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8182"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:50:37.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8009" for this suite.

• [SLOW TEST:14.133 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":82,"skipped":1455,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:50:37.806: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Aug 22 09:50:37.828: INFO: The status of Pod labelsupdatec75a6763-ef1f-4368-8038-acb535b3ce3d is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:50:39.833: INFO: The status of Pod labelsupdatec75a6763-ef1f-4368-8038-acb535b3ce3d is Running (Ready = true)
Aug 22 09:50:40.347: INFO: Successfully updated pod "labelsupdatec75a6763-ef1f-4368-8038-acb535b3ce3d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:50:44.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6919" for this suite.

• [SLOW TEST:6.560 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1466,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:50:44.367: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-4866
STEP: creating replication controller nodeport-test in namespace services-4866
I0822 09:50:44.391126      21 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-4866, replica count: 2
I0822 09:50:47.441995      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 09:50:47.442: INFO: Creating new exec pod
Aug 22 09:50:50.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-4866 exec execpodbb8lc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Aug 22 09:50:50.559: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 22 09:50:50.559: INFO: stdout: "nodeport-test-bnlrz"
Aug 22 09:50:50.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-4866 exec execpodbb8lc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.160.104 80'
Aug 22 09:50:50.657: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.160.104 80\nConnection to 10.106.160.104 80 port [tcp/http] succeeded!\n"
Aug 22 09:50:50.657: INFO: stdout: "nodeport-test-6dqfs"
Aug 22 09:50:50.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-4866 exec execpodbb8lc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.23.79.103 31094'
Aug 22 09:50:50.754: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.23.79.103 31094\nConnection to 172.23.79.103 31094 port [tcp/*] succeeded!\n"
Aug 22 09:50:50.754: INFO: stdout: "nodeport-test-6dqfs"
Aug 22 09:50:50.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-4866 exec execpodbb8lc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.23.79.105 31094'
Aug 22 09:50:50.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.23.79.105 31094\nConnection to 172.23.79.105 31094 port [tcp/*] succeeded!\n"
Aug 22 09:50:50.851: INFO: stdout: "nodeport-test-bnlrz"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:50:50.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4866" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:6.491 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":84,"skipped":1486,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:50:50.857: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:50:50.888: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ae88334c-9de5-44fe-9327-3f1ccc6fdd21", Controller:(*bool)(0xc005352e66), BlockOwnerDeletion:(*bool)(0xc005352e67)}}
Aug 22 09:50:50.891: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a51bcf05-599f-4553-9504-0d0b911ffda3", Controller:(*bool)(0xc00537a1fe), BlockOwnerDeletion:(*bool)(0xc00537a1ff)}}
Aug 22 09:50:50.893: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"862df076-a1c9-47f7-9a44-6f26a134f485", Controller:(*bool)(0xc00531a76e), BlockOwnerDeletion:(*bool)(0xc00531a76f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:50:55.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5110" for this suite.

• [SLOW TEST:5.049 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":85,"skipped":1499,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:50:55.906: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Aug 22 09:50:55.924: INFO: Waiting up to 5m0s for pod "downward-api-e4f123ea-e040-4d00-83d7-0e0afb166ddc" in namespace "downward-api-8814" to be "Succeeded or Failed"
Aug 22 09:50:55.926: INFO: Pod "downward-api-e4f123ea-e040-4d00-83d7-0e0afb166ddc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.969961ms
Aug 22 09:50:57.930: INFO: Pod "downward-api-e4f123ea-e040-4d00-83d7-0e0afb166ddc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005944155s
STEP: Saw pod success
Aug 22 09:50:57.930: INFO: Pod "downward-api-e4f123ea-e040-4d00-83d7-0e0afb166ddc" satisfied condition "Succeeded or Failed"
Aug 22 09:50:57.931: INFO: Trying to get logs from node 172.23.79.105 pod downward-api-e4f123ea-e040-4d00-83d7-0e0afb166ddc container dapi-container: <nil>
STEP: delete the pod
Aug 22 09:50:57.940: INFO: Waiting for pod downward-api-e4f123ea-e040-4d00-83d7-0e0afb166ddc to disappear
Aug 22 09:50:57.941: INFO: Pod downward-api-e4f123ea-e040-4d00-83d7-0e0afb166ddc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:50:57.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8814" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":86,"skipped":1523,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:50:57.945: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-8f81801c-d208-4986-aded-146e9b64fa4f
STEP: Creating a pod to test consume configMaps
Aug 22 09:50:57.963: INFO: Waiting up to 5m0s for pod "pod-configmaps-14f97bad-25a3-43b2-b63e-a8ab54df63e0" in namespace "configmap-4418" to be "Succeeded or Failed"
Aug 22 09:50:57.965: INFO: Pod "pod-configmaps-14f97bad-25a3-43b2-b63e-a8ab54df63e0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.188459ms
Aug 22 09:50:59.969: INFO: Pod "pod-configmaps-14f97bad-25a3-43b2-b63e-a8ab54df63e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005337657s
STEP: Saw pod success
Aug 22 09:50:59.969: INFO: Pod "pod-configmaps-14f97bad-25a3-43b2-b63e-a8ab54df63e0" satisfied condition "Succeeded or Failed"
Aug 22 09:50:59.970: INFO: Trying to get logs from node 172.23.79.105 pod pod-configmaps-14f97bad-25a3-43b2-b63e-a8ab54df63e0 container agnhost-container: <nil>
STEP: delete the pod
Aug 22 09:50:59.980: INFO: Waiting for pod pod-configmaps-14f97bad-25a3-43b2-b63e-a8ab54df63e0 to disappear
Aug 22 09:50:59.981: INFO: Pod pod-configmaps-14f97bad-25a3-43b2-b63e-a8ab54df63e0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:50:59.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4418" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:50:59.985: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:51:00.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1729" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":88,"skipped":1568,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:51:00.004: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 22 09:51:00.312: INFO: Pod name wrapped-volume-race-fe747579-d8f0-4074-a179-01f1821adda7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fe747579-d8f0-4074-a179-01f1821adda7 in namespace emptydir-wrapper-3689, will wait for the garbage collector to delete the pods
Aug 22 09:51:16.427: INFO: Deleting ReplicationController wrapped-volume-race-fe747579-d8f0-4074-a179-01f1821adda7 took: 3.221067ms
Aug 22 09:51:16.528: INFO: Terminating ReplicationController wrapped-volume-race-fe747579-d8f0-4074-a179-01f1821adda7 pods took: 100.793237ms
STEP: Creating RC which spawns configmap-volume pods
Aug 22 09:51:20.541: INFO: Pod name wrapped-volume-race-9e0c594a-63aa-44c0-8072-6dd87225cb83: Found 0 pods out of 5
Aug 22 09:51:25.549: INFO: Pod name wrapped-volume-race-9e0c594a-63aa-44c0-8072-6dd87225cb83: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9e0c594a-63aa-44c0-8072-6dd87225cb83 in namespace emptydir-wrapper-3689, will wait for the garbage collector to delete the pods
Aug 22 09:51:37.621: INFO: Deleting ReplicationController wrapped-volume-race-9e0c594a-63aa-44c0-8072-6dd87225cb83 took: 3.477482ms
Aug 22 09:51:37.722: INFO: Terminating ReplicationController wrapped-volume-race-9e0c594a-63aa-44c0-8072-6dd87225cb83 pods took: 100.383346ms
STEP: Creating RC which spawns configmap-volume pods
Aug 22 09:51:40.539: INFO: Pod name wrapped-volume-race-79d57e21-d1c1-4e59-b007-ddfc92af8049: Found 0 pods out of 5
Aug 22 09:51:45.548: INFO: Pod name wrapped-volume-race-79d57e21-d1c1-4e59-b007-ddfc92af8049: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-79d57e21-d1c1-4e59-b007-ddfc92af8049 in namespace emptydir-wrapper-3689, will wait for the garbage collector to delete the pods
Aug 22 09:51:57.615: INFO: Deleting ReplicationController wrapped-volume-race-79d57e21-d1c1-4e59-b007-ddfc92af8049 took: 3.109019ms
Aug 22 09:51:57.716: INFO: Terminating ReplicationController wrapped-volume-race-79d57e21-d1c1-4e59-b007-ddfc92af8049 pods took: 100.724678ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:52:01.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3689" for this suite.

• [SLOW TEST:61.015 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":89,"skipped":1581,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:52:01.019: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4400.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4400.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4400.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4400.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 09:52:03.053: INFO: DNS probes using dns-test-cca50246-ce51-4ad2-90e3-8af13138d52e succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4400.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4400.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4400.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4400.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 09:52:05.074: INFO: File wheezy_udp@dns-test-service-3.dns-4400.svc.cluster.local from pod  dns-4400/dns-test-f1b2f8e4-fcaf-4f44-9b43-8e9a0791d819 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 22 09:52:05.075: INFO: Lookups using dns-4400/dns-test-f1b2f8e4-fcaf-4f44-9b43-8e9a0791d819 failed for: [wheezy_udp@dns-test-service-3.dns-4400.svc.cluster.local]

Aug 22 09:52:10.080: INFO: File wheezy_udp@dns-test-service-3.dns-4400.svc.cluster.local from pod  dns-4400/dns-test-f1b2f8e4-fcaf-4f44-9b43-8e9a0791d819 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 22 09:52:10.082: INFO: File jessie_udp@dns-test-service-3.dns-4400.svc.cluster.local from pod  dns-4400/dns-test-f1b2f8e4-fcaf-4f44-9b43-8e9a0791d819 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 22 09:52:10.082: INFO: Lookups using dns-4400/dns-test-f1b2f8e4-fcaf-4f44-9b43-8e9a0791d819 failed for: [wheezy_udp@dns-test-service-3.dns-4400.svc.cluster.local jessie_udp@dns-test-service-3.dns-4400.svc.cluster.local]

Aug 22 09:52:15.081: INFO: DNS probes using dns-test-f1b2f8e4-fcaf-4f44-9b43-8e9a0791d819 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4400.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4400.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4400.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4400.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 09:52:17.112: INFO: DNS probes using dns-test-5ded75cd-06d3-4273-99b8-0529bc59f62b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:52:17.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4400" for this suite.

• [SLOW TEST:16.106 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":90,"skipped":1584,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:52:17.126: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Aug 22 09:52:17.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1205 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 22 09:52:17.193: INFO: stderr: ""
Aug 22 09:52:17.193: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Aug 22 09:52:22.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1205 get pod e2e-test-httpd-pod -o json'
Aug 22 09:52:22.292: INFO: stderr: ""
Aug 22 09:52:22.292: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-08-22T09:52:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1205\",\n        \"resourceVersion\": \"9470\",\n        \"uid\": \"798ec87e-bcd1-457a-9302-3adb45353937\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-rj5mb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"172.23.79.105\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-rj5mb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-08-22T09:52:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-08-22T09:52:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-08-22T09:52:18Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-08-22T09:52:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://892e63a012a8d48825322f6e081a661879032bc1cc73ba94dc44c17bc8cce15a\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-08-22T09:52:17Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.23.79.105\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.2.70\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.2.70\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-08-22T09:52:17Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 22 09:52:22.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1205 replace -f -'
Aug 22 09:52:22.409: INFO: stderr: ""
Aug 22 09:52:22.409: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Aug 22 09:52:22.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1205 delete pods e2e-test-httpd-pod'
Aug 22 09:52:23.986: INFO: stderr: ""
Aug 22 09:52:23.986: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:52:23.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1205" for this suite.

• [SLOW TEST:6.864 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1555
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":91,"skipped":1589,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:52:23.991: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 22 09:52:24.015: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:52:24.018: INFO: Number of nodes with available pods: 0
Aug 22 09:52:24.018: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 09:52:25.021: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:52:25.023: INFO: Number of nodes with available pods: 1
Aug 22 09:52:25.023: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 09:52:26.022: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:52:26.024: INFO: Number of nodes with available pods: 2
Aug 22 09:52:26.024: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 22 09:52:26.031: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:52:26.032: INFO: Number of nodes with available pods: 1
Aug 22 09:52:26.032: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 09:52:27.036: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:52:27.038: INFO: Number of nodes with available pods: 1
Aug 22 09:52:27.038: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 09:52:28.037: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:52:28.039: INFO: Number of nodes with available pods: 1
Aug 22 09:52:28.039: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 09:52:29.035: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:52:29.038: INFO: Number of nodes with available pods: 1
Aug 22 09:52:29.038: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 09:52:30.036: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 09:52:30.038: INFO: Number of nodes with available pods: 2
Aug 22 09:52:30.038: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7987, will wait for the garbage collector to delete the pods
Aug 22 09:52:30.094: INFO: Deleting DaemonSet.extensions daemon-set took: 3.017147ms
Aug 22 09:52:30.195: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.264886ms
Aug 22 09:52:32.500: INFO: Number of nodes with available pods: 0
Aug 22 09:52:32.500: INFO: Number of running nodes: 0, number of available pods: 0
Aug 22 09:52:32.501: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9597"},"items":null}

Aug 22 09:52:32.502: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9597"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:52:32.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7987" for this suite.

• [SLOW TEST:8.519 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":92,"skipped":1657,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:52:32.510: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Aug 22 09:52:34.533: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3842 PodName:pod-sharedvolume-e6f653e7-c946-4ff8-b443-b7dda2592929 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 09:52:34.533: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 09:52:34.585: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:52:34.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3842" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":93,"skipped":1660,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:52:34.590: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:53:02.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2635" for this suite.

• [SLOW TEST:28.054 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":94,"skipped":1703,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:53:02.644: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-35d9e279-589f-4b05-a468-1148013dc2b1 in namespace container-probe-1179
Aug 22 09:53:04.669: INFO: Started pod liveness-35d9e279-589f-4b05-a468-1148013dc2b1 in namespace container-probe-1179
STEP: checking the pod's current state and verifying that restartCount is present
Aug 22 09:53:04.671: INFO: Initial restart count of pod liveness-35d9e279-589f-4b05-a468-1148013dc2b1 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:05.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1179" for this suite.

• [SLOW TEST:242.642 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":95,"skipped":1720,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:05.287: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-a8774aed-384b-401c-8993-d062c453eddb
STEP: Creating a pod to test consume secrets
Aug 22 09:57:05.306: INFO: Waiting up to 5m0s for pod "pod-secrets-874b0502-3113-4622-a278-98447e365a55" in namespace "secrets-2695" to be "Succeeded or Failed"
Aug 22 09:57:05.308: INFO: Pod "pod-secrets-874b0502-3113-4622-a278-98447e365a55": Phase="Pending", Reason="", readiness=false. Elapsed: 1.305735ms
Aug 22 09:57:07.310: INFO: Pod "pod-secrets-874b0502-3113-4622-a278-98447e365a55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003915051s
STEP: Saw pod success
Aug 22 09:57:07.310: INFO: Pod "pod-secrets-874b0502-3113-4622-a278-98447e365a55" satisfied condition "Succeeded or Failed"
Aug 22 09:57:07.312: INFO: Trying to get logs from node 172.23.79.105 pod pod-secrets-874b0502-3113-4622-a278-98447e365a55 container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 09:57:07.327: INFO: Waiting for pod pod-secrets-874b0502-3113-4622-a278-98447e365a55 to disappear
Aug 22 09:57:07.328: INFO: Pod pod-secrets-874b0502-3113-4622-a278-98447e365a55 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:07.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2695" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":96,"skipped":1721,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:07.331: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Aug 22 09:57:07.354: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Aug 22 09:57:09.364: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Aug 22 09:57:11.375: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:13.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-8422" for this suite.

• [SLOW TEST:6.054 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":97,"skipped":1732,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:13.386: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-f29d439d-144c-48c9-8de6-ae48df5db498
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:13.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6094" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":98,"skipped":1743,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:13.405: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-8523/secret-test-df8e8874-38aa-4722-9bd6-b202a8676bac
STEP: Creating a pod to test consume secrets
Aug 22 09:57:13.423: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf406645-3b3e-44c0-8688-249927f65fb9" in namespace "secrets-8523" to be "Succeeded or Failed"
Aug 22 09:57:13.425: INFO: Pod "pod-configmaps-bf406645-3b3e-44c0-8688-249927f65fb9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.309939ms
Aug 22 09:57:15.428: INFO: Pod "pod-configmaps-bf406645-3b3e-44c0-8688-249927f65fb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00486409s
STEP: Saw pod success
Aug 22 09:57:15.428: INFO: Pod "pod-configmaps-bf406645-3b3e-44c0-8688-249927f65fb9" satisfied condition "Succeeded or Failed"
Aug 22 09:57:15.430: INFO: Trying to get logs from node 172.23.79.105 pod pod-configmaps-bf406645-3b3e-44c0-8688-249927f65fb9 container env-test: <nil>
STEP: delete the pod
Aug 22 09:57:15.439: INFO: Waiting for pod pod-configmaps-bf406645-3b3e-44c0-8688-249927f65fb9 to disappear
Aug 22 09:57:15.440: INFO: Pod pod-configmaps-bf406645-3b3e-44c0-8688-249927f65fb9 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:15.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8523" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":99,"skipped":1748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:15.443: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-cbe9c391-173e-4dca-ac30-d4dfcd3e6e57
STEP: Creating a pod to test consume secrets
Aug 22 09:57:15.462: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6bc06c36-2642-44a8-b0a9-dfa1bbc95614" in namespace "projected-9725" to be "Succeeded or Failed"
Aug 22 09:57:15.463: INFO: Pod "pod-projected-secrets-6bc06c36-2642-44a8-b0a9-dfa1bbc95614": Phase="Pending", Reason="", readiness=false. Elapsed: 1.209163ms
Aug 22 09:57:17.466: INFO: Pod "pod-projected-secrets-6bc06c36-2642-44a8-b0a9-dfa1bbc95614": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004148476s
STEP: Saw pod success
Aug 22 09:57:17.466: INFO: Pod "pod-projected-secrets-6bc06c36-2642-44a8-b0a9-dfa1bbc95614" satisfied condition "Succeeded or Failed"
Aug 22 09:57:17.468: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-secrets-6bc06c36-2642-44a8-b0a9-dfa1bbc95614 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 22 09:57:17.476: INFO: Waiting for pod pod-projected-secrets-6bc06c36-2642-44a8-b0a9-dfa1bbc95614 to disappear
Aug 22 09:57:17.477: INFO: Pod pod-projected-secrets-6bc06c36-2642-44a8-b0a9-dfa1bbc95614 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:17.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9725" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":100,"skipped":1798,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:17.481: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 22 09:57:17.500: INFO: Waiting up to 5m0s for pod "pod-cb6647ba-43d4-46a2-b4a6-4132afb152b5" in namespace "emptydir-8688" to be "Succeeded or Failed"
Aug 22 09:57:17.502: INFO: Pod "pod-cb6647ba-43d4-46a2-b4a6-4132afb152b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.86726ms
Aug 22 09:57:19.508: INFO: Pod "pod-cb6647ba-43d4-46a2-b4a6-4132afb152b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007339136s
STEP: Saw pod success
Aug 22 09:57:19.508: INFO: Pod "pod-cb6647ba-43d4-46a2-b4a6-4132afb152b5" satisfied condition "Succeeded or Failed"
Aug 22 09:57:19.509: INFO: Trying to get logs from node 172.23.79.105 pod pod-cb6647ba-43d4-46a2-b4a6-4132afb152b5 container test-container: <nil>
STEP: delete the pod
Aug 22 09:57:19.518: INFO: Waiting for pod pod-cb6647ba-43d4-46a2-b4a6-4132afb152b5 to disappear
Aug 22 09:57:19.520: INFO: Pod pod-cb6647ba-43d4-46a2-b4a6-4132afb152b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:19.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8688" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":101,"skipped":1834,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:19.525: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Aug 22 09:57:20.053: INFO: created pod pod-service-account-defaultsa
Aug 22 09:57:20.053: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 22 09:57:20.056: INFO: created pod pod-service-account-mountsa
Aug 22 09:57:20.056: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 22 09:57:20.059: INFO: created pod pod-service-account-nomountsa
Aug 22 09:57:20.059: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 22 09:57:20.072: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 22 09:57:20.072: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 22 09:57:20.074: INFO: created pod pod-service-account-mountsa-mountspec
Aug 22 09:57:20.074: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 22 09:57:20.076: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 22 09:57:20.076: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 22 09:57:20.080: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 22 09:57:20.080: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 22 09:57:20.087: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 22 09:57:20.087: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 22 09:57:20.090: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 22 09:57:20.090: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:20.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3134" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":102,"skipped":1839,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:20.096: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Aug 22 09:57:20.110: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 22 09:57:20.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2258 create -f -'
Aug 22 09:57:20.235: INFO: stderr: ""
Aug 22 09:57:20.235: INFO: stdout: "service/agnhost-replica created\n"
Aug 22 09:57:20.235: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 22 09:57:20.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2258 create -f -'
Aug 22 09:57:20.357: INFO: stderr: ""
Aug 22 09:57:20.357: INFO: stdout: "service/agnhost-primary created\n"
Aug 22 09:57:20.357: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 22 09:57:20.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2258 create -f -'
Aug 22 09:57:20.475: INFO: stderr: ""
Aug 22 09:57:20.475: INFO: stdout: "service/frontend created\n"
Aug 22 09:57:20.475: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 22 09:57:20.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2258 create -f -'
Aug 22 09:57:20.589: INFO: stderr: ""
Aug 22 09:57:20.589: INFO: stdout: "deployment.apps/frontend created\n"
Aug 22 09:57:20.589: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 22 09:57:20.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2258 create -f -'
Aug 22 09:57:20.742: INFO: stderr: ""
Aug 22 09:57:20.742: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 22 09:57:20.742: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 22 09:57:20.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2258 create -f -'
Aug 22 09:57:20.857: INFO: stderr: ""
Aug 22 09:57:20.857: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Aug 22 09:57:20.857: INFO: Waiting for all frontend pods to be Running.
Aug 22 09:57:30.909: INFO: Waiting for frontend to serve content.
Aug 22 09:57:30.914: INFO: Trying to add a new entry to the guestbook.
Aug 22 09:57:30.919: INFO: Verifying that added entry can be retrieved.
Aug 22 09:57:30.922: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Aug 22 09:57:35.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2258 delete --grace-period=0 --force -f -'
Aug 22 09:57:35.982: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 09:57:35.982: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Aug 22 09:57:35.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2258 delete --grace-period=0 --force -f -'
Aug 22 09:57:36.036: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 09:57:36.036: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 22 09:57:36.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2258 delete --grace-period=0 --force -f -'
Aug 22 09:57:36.081: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 09:57:36.081: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 22 09:57:36.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2258 delete --grace-period=0 --force -f -'
Aug 22 09:57:36.125: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 09:57:36.125: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 22 09:57:36.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2258 delete --grace-period=0 --force -f -'
Aug 22 09:57:36.172: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 09:57:36.172: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 22 09:57:36.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-2258 delete --grace-period=0 --force -f -'
Aug 22 09:57:36.219: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 09:57:36.219: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:36.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2258" for this suite.

• [SLOW TEST:16.128 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":103,"skipped":1856,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:36.223: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 09:57:36.244: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a4f29ee-56db-4779-9db4-ad0debb91212" in namespace "projected-3463" to be "Succeeded or Failed"
Aug 22 09:57:36.245: INFO: Pod "downwardapi-volume-0a4f29ee-56db-4779-9db4-ad0debb91212": Phase="Pending", Reason="", readiness=false. Elapsed: 1.201288ms
Aug 22 09:57:38.250: INFO: Pod "downwardapi-volume-0a4f29ee-56db-4779-9db4-ad0debb91212": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005954071s
Aug 22 09:57:40.259: INFO: Pod "downwardapi-volume-0a4f29ee-56db-4779-9db4-ad0debb91212": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015024716s
Aug 22 09:57:42.265: INFO: Pod "downwardapi-volume-0a4f29ee-56db-4779-9db4-ad0debb91212": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020799262s
STEP: Saw pod success
Aug 22 09:57:42.265: INFO: Pod "downwardapi-volume-0a4f29ee-56db-4779-9db4-ad0debb91212" satisfied condition "Succeeded or Failed"
Aug 22 09:57:42.266: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-0a4f29ee-56db-4779-9db4-ad0debb91212 container client-container: <nil>
STEP: delete the pod
Aug 22 09:57:42.276: INFO: Waiting for pod downwardapi-volume-0a4f29ee-56db-4779-9db4-ad0debb91212 to disappear
Aug 22 09:57:42.277: INFO: Pod downwardapi-volume-0a4f29ee-56db-4779-9db4-ad0debb91212 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:42.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3463" for this suite.

• [SLOW TEST:6.057 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":104,"skipped":1857,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:42.281: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 09:57:42.299: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e54f5d3e-ef91-46b0-b654-aba9cbe57626" in namespace "downward-api-2517" to be "Succeeded or Failed"
Aug 22 09:57:42.300: INFO: Pod "downwardapi-volume-e54f5d3e-ef91-46b0-b654-aba9cbe57626": Phase="Pending", Reason="", readiness=false. Elapsed: 1.794688ms
Aug 22 09:57:44.306: INFO: Pod "downwardapi-volume-e54f5d3e-ef91-46b0-b654-aba9cbe57626": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00681603s
STEP: Saw pod success
Aug 22 09:57:44.306: INFO: Pod "downwardapi-volume-e54f5d3e-ef91-46b0-b654-aba9cbe57626" satisfied condition "Succeeded or Failed"
Aug 22 09:57:44.307: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-e54f5d3e-ef91-46b0-b654-aba9cbe57626 container client-container: <nil>
STEP: delete the pod
Aug 22 09:57:44.315: INFO: Waiting for pod downwardapi-volume-e54f5d3e-ef91-46b0-b654-aba9cbe57626 to disappear
Aug 22 09:57:44.317: INFO: Pod downwardapi-volume-e54f5d3e-ef91-46b0-b654-aba9cbe57626 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:44.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2517" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":105,"skipped":1865,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:44.320: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-3914
STEP: creating service affinity-clusterip-transition in namespace services-3914
STEP: creating replication controller affinity-clusterip-transition in namespace services-3914
I0822 09:57:44.346839      21 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-3914, replica count: 3
I0822 09:57:47.398292      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 09:57:47.401: INFO: Creating new exec pod
Aug 22 09:57:50.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-3914 exec execpod-affinitylbnvk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Aug 22 09:57:50.516: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 22 09:57:50.516: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 09:57:50.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-3914 exec execpod-affinitylbnvk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.155.36 80'
Aug 22 09:57:50.610: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.155.36 80\nConnection to 10.96.155.36 80 port [tcp/http] succeeded!\n"
Aug 22 09:57:50.610: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 09:57:50.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-3914 exec execpod-affinitylbnvk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.155.36:80/ ; done'
Aug 22 09:57:50.761: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n"
Aug 22 09:57:50.761: INFO: stdout: "\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-b7pgg\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-j5hng\naffinity-clusterip-transition-b7pgg\naffinity-clusterip-transition-b7pgg\naffinity-clusterip-transition-b7pgg\naffinity-clusterip-transition-j5hng\naffinity-clusterip-transition-j5hng\naffinity-clusterip-transition-b7pgg\naffinity-clusterip-transition-j5hng\naffinity-clusterip-transition-b7pgg\naffinity-clusterip-transition-j5hng\naffinity-clusterip-transition-j5hng"
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-b7pgg
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-j5hng
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-b7pgg
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-b7pgg
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-b7pgg
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-j5hng
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-j5hng
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-b7pgg
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-j5hng
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-b7pgg
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-j5hng
Aug 22 09:57:50.761: INFO: Received response from host: affinity-clusterip-transition-j5hng
Aug 22 09:57:50.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-3914 exec execpod-affinitylbnvk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.155.36:80/ ; done'
Aug 22 09:57:50.913: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.155.36:80/\n"
Aug 22 09:57:50.913: INFO: stdout: "\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8\naffinity-clusterip-transition-qgzw8"
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Received response from host: affinity-clusterip-transition-qgzw8
Aug 22 09:57:50.913: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3914, will wait for the garbage collector to delete the pods
Aug 22 09:57:50.973: INFO: Deleting ReplicationController affinity-clusterip-transition took: 2.780825ms
Aug 22 09:57:51.074: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.088167ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:53.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3914" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.889 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":106,"skipped":1868,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:53.209: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 22 09:57:53.247: INFO: Waiting up to 5m0s for pod "pod-1ac2a7be-df42-429f-9830-df6048b485cb" in namespace "emptydir-7674" to be "Succeeded or Failed"
Aug 22 09:57:53.249: INFO: Pod "pod-1ac2a7be-df42-429f-9830-df6048b485cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.276288ms
Aug 22 09:57:55.252: INFO: Pod "pod-1ac2a7be-df42-429f-9830-df6048b485cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004204841s
STEP: Saw pod success
Aug 22 09:57:55.252: INFO: Pod "pod-1ac2a7be-df42-429f-9830-df6048b485cb" satisfied condition "Succeeded or Failed"
Aug 22 09:57:55.253: INFO: Trying to get logs from node 172.23.79.105 pod pod-1ac2a7be-df42-429f-9830-df6048b485cb container test-container: <nil>
STEP: delete the pod
Aug 22 09:57:55.260: INFO: Waiting for pod pod-1ac2a7be-df42-429f-9830-df6048b485cb to disappear
Aug 22 09:57:55.262: INFO: Pod pod-1ac2a7be-df42-429f-9830-df6048b485cb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:57:55.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7674" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":107,"skipped":1886,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:57:55.265: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:57:55.284: INFO: created pod
Aug 22 09:57:55.284: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-9545" to be "Succeeded or Failed"
Aug 22 09:57:55.286: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.912342ms
Aug 22 09:57:57.288: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003953677s
STEP: Saw pod success
Aug 22 09:57:57.288: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 22 09:58:27.289: INFO: polling logs
Aug 22 09:58:27.293: INFO: Pod logs: 
2021/08/22 09:57:56 OK: Got token
2021/08/22 09:57:56 validating with in-cluster discovery
2021/08/22 09:57:56 OK: got issuer https://kubernetes.default.svc.cluster.local
2021/08/22 09:57:56 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9545:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1629626875, NotBefore:1629626275, IssuedAt:1629626275, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9545", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"880473c9-be0e-4d30-aa81-ce3682f36c43"}}}
2021/08/22 09:57:56 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2021/08/22 09:57:56 OK: Validated signature on JWT
2021/08/22 09:57:56 OK: Got valid claims from token!
2021/08/22 09:57:56 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9545:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1629626875, NotBefore:1629626275, IssuedAt:1629626275, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9545", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"880473c9-be0e-4d30-aa81-ce3682f36c43"}}}

Aug 22 09:58:27.293: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:58:27.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9545" for this suite.

• [SLOW TEST:32.034 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":108,"skipped":1911,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:58:27.299: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Aug 22 09:58:27.321: INFO: Waiting up to 5m0s for pod "var-expansion-44713bb9-0c21-4ed7-96e4-599e1af8a17b" in namespace "var-expansion-1787" to be "Succeeded or Failed"
Aug 22 09:58:27.323: INFO: Pod "var-expansion-44713bb9-0c21-4ed7-96e4-599e1af8a17b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025921ms
Aug 22 09:58:29.325: INFO: Pod "var-expansion-44713bb9-0c21-4ed7-96e4-599e1af8a17b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004855569s
STEP: Saw pod success
Aug 22 09:58:29.325: INFO: Pod "var-expansion-44713bb9-0c21-4ed7-96e4-599e1af8a17b" satisfied condition "Succeeded or Failed"
Aug 22 09:58:29.327: INFO: Trying to get logs from node 172.23.79.105 pod var-expansion-44713bb9-0c21-4ed7-96e4-599e1af8a17b container dapi-container: <nil>
STEP: delete the pod
Aug 22 09:58:29.337: INFO: Waiting for pod var-expansion-44713bb9-0c21-4ed7-96e4-599e1af8a17b to disappear
Aug 22 09:58:29.338: INFO: Pod var-expansion-44713bb9-0c21-4ed7-96e4-599e1af8a17b no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:58:29.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1787" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":109,"skipped":1931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:58:29.342: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 22 09:58:30.370: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:58:30.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4008" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":110,"skipped":1965,"failed":0}
SSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:58:30.379: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Aug 22 09:58:30.394: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Aug 22 09:58:30.652: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 22 09:58:32.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 09:58:34.682: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 09:58:36.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 09:58:38.681: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 09:58:40.682: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 09:58:42.680: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 09:58:44.682: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765223110, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 09:58:51.294: INFO: Waited 4.609970131s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Aug 22 09:58:51.325: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:58:51.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7839" for this suite.

• [SLOW TEST:21.358 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":111,"skipped":1968,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:58:51.737: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 09:58:52.439: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 09:58:55.453: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:58:55.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5533" for this suite.
STEP: Destroying namespace "webhook-5533-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":112,"skipped":1975,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:58:55.500: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:58:55.528: INFO: The status of Pod server-envvars-05960def-00d3-4a4e-b0c4-0c20b083bc55 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 09:58:57.531: INFO: The status of Pod server-envvars-05960def-00d3-4a4e-b0c4-0c20b083bc55 is Running (Ready = true)
Aug 22 09:58:57.542: INFO: Waiting up to 5m0s for pod "client-envvars-d6e63e79-6b5b-4ac2-816d-7f6f051c743e" in namespace "pods-9086" to be "Succeeded or Failed"
Aug 22 09:58:57.544: INFO: Pod "client-envvars-d6e63e79-6b5b-4ac2-816d-7f6f051c743e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103638ms
Aug 22 09:58:59.549: INFO: Pod "client-envvars-d6e63e79-6b5b-4ac2-816d-7f6f051c743e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006899431s
STEP: Saw pod success
Aug 22 09:58:59.549: INFO: Pod "client-envvars-d6e63e79-6b5b-4ac2-816d-7f6f051c743e" satisfied condition "Succeeded or Failed"
Aug 22 09:58:59.550: INFO: Trying to get logs from node 172.23.79.105 pod client-envvars-d6e63e79-6b5b-4ac2-816d-7f6f051c743e container env3cont: <nil>
STEP: delete the pod
Aug 22 09:58:59.557: INFO: Waiting for pod client-envvars-d6e63e79-6b5b-4ac2-816d-7f6f051c743e to disappear
Aug 22 09:58:59.558: INFO: Pod client-envvars-d6e63e79-6b5b-4ac2-816d-7f6f051c743e no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:58:59.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9086" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":2039,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:58:59.562: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Aug 22 09:58:59.577: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:59:11.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6956" for this suite.

• [SLOW TEST:11.758 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":114,"skipped":2042,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:59:11.320: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-151.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-151.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-151.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-151.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-151.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-151.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-151.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-151.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-151.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-151.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-151.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 202.172.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.172.202_udp@PTR;check="$$(dig +tcp +noall +answer +search 202.172.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.172.202_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-151.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-151.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-151.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-151.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-151.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-151.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-151.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-151.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-151.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-151.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-151.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 202.172.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.172.202_udp@PTR;check="$$(dig +tcp +noall +answer +search 202.172.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.172.202_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 09:59:13.360: INFO: Unable to read wheezy_udp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:13.361: INFO: Unable to read wheezy_tcp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:13.363: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:13.364: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:13.374: INFO: Unable to read jessie_udp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:13.375: INFO: Unable to read jessie_tcp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:13.376: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:13.378: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:13.386: INFO: Lookups using dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75 failed for: [wheezy_udp@dns-test-service.dns-151.svc.cluster.local wheezy_tcp@dns-test-service.dns-151.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local jessie_udp@dns-test-service.dns-151.svc.cluster.local jessie_tcp@dns-test-service.dns-151.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local]

Aug 22 09:59:18.389: INFO: Unable to read wheezy_udp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:18.390: INFO: Unable to read wheezy_tcp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:18.392: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:18.393: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:18.403: INFO: Unable to read jessie_udp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:18.404: INFO: Unable to read jessie_tcp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:18.405: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:18.407: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:18.414: INFO: Lookups using dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75 failed for: [wheezy_udp@dns-test-service.dns-151.svc.cluster.local wheezy_tcp@dns-test-service.dns-151.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local jessie_udp@dns-test-service.dns-151.svc.cluster.local jessie_tcp@dns-test-service.dns-151.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local]

Aug 22 09:59:23.389: INFO: Unable to read wheezy_udp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:23.390: INFO: Unable to read wheezy_tcp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:23.392: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:23.393: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:23.403: INFO: Unable to read jessie_udp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:23.404: INFO: Unable to read jessie_tcp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:23.406: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:23.407: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:23.415: INFO: Lookups using dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75 failed for: [wheezy_udp@dns-test-service.dns-151.svc.cluster.local wheezy_tcp@dns-test-service.dns-151.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local jessie_udp@dns-test-service.dns-151.svc.cluster.local jessie_tcp@dns-test-service.dns-151.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local]

Aug 22 09:59:28.389: INFO: Unable to read wheezy_udp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:28.390: INFO: Unable to read wheezy_tcp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:28.392: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:28.393: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:28.403: INFO: Unable to read jessie_udp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:28.404: INFO: Unable to read jessie_tcp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:28.405: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:28.407: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:28.415: INFO: Lookups using dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75 failed for: [wheezy_udp@dns-test-service.dns-151.svc.cluster.local wheezy_tcp@dns-test-service.dns-151.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local jessie_udp@dns-test-service.dns-151.svc.cluster.local jessie_tcp@dns-test-service.dns-151.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local]

Aug 22 09:59:33.388: INFO: Unable to read wheezy_udp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:33.390: INFO: Unable to read wheezy_tcp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:33.392: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:33.393: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:33.403: INFO: Unable to read jessie_udp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:33.404: INFO: Unable to read jessie_tcp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:33.406: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:33.407: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:33.415: INFO: Lookups using dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75 failed for: [wheezy_udp@dns-test-service.dns-151.svc.cluster.local wheezy_tcp@dns-test-service.dns-151.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local jessie_udp@dns-test-service.dns-151.svc.cluster.local jessie_tcp@dns-test-service.dns-151.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local]

Aug 22 09:59:38.389: INFO: Unable to read wheezy_udp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:38.390: INFO: Unable to read wheezy_tcp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:38.392: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:38.393: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:38.402: INFO: Unable to read jessie_udp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:38.404: INFO: Unable to read jessie_tcp@dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:38.405: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:38.406: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local from pod dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75: the server could not find the requested resource (get pods dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75)
Aug 22 09:59:38.414: INFO: Lookups using dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75 failed for: [wheezy_udp@dns-test-service.dns-151.svc.cluster.local wheezy_tcp@dns-test-service.dns-151.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local jessie_udp@dns-test-service.dns-151.svc.cluster.local jessie_tcp@dns-test-service.dns-151.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-151.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-151.svc.cluster.local]

Aug 22 09:59:43.415: INFO: DNS probes using dns-151/dns-test-cf914011-170c-4574-ab10-7d3b7cc95f75 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:59:43.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-151" for this suite.

• [SLOW TEST:32.156 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":115,"skipped":2044,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:59:43.476: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 09:59:43.791: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 09:59:46.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:59:46.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5439" for this suite.
STEP: Destroying namespace "webhook-5439-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":116,"skipped":2045,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:59:46.829: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 09:59:46.850: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-521c857b-d423-4dfb-ac0d-91665c0915c8" in namespace "security-context-test-515" to be "Succeeded or Failed"
Aug 22 09:59:46.853: INFO: Pod "busybox-privileged-false-521c857b-d423-4dfb-ac0d-91665c0915c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.207219ms
Aug 22 09:59:48.857: INFO: Pod "busybox-privileged-false-521c857b-d423-4dfb-ac0d-91665c0915c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007744865s
Aug 22 09:59:48.857: INFO: Pod "busybox-privileged-false-521c857b-d423-4dfb-ac0d-91665c0915c8" satisfied condition "Succeeded or Failed"
Aug 22 09:59:48.861: INFO: Got logs for pod "busybox-privileged-false-521c857b-d423-4dfb-ac0d-91665c0915c8": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:59:48.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-515" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":117,"skipped":2060,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:59:48.865: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Aug 22 09:59:48.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-6629 create -f -'
Aug 22 09:59:49.115: INFO: stderr: ""
Aug 22 09:59:49.115: INFO: stdout: "pod/pause created\n"
Aug 22 09:59:49.115: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 22 09:59:49.115: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6629" to be "running and ready"
Aug 22 09:59:49.117: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.792116ms
Aug 22 09:59:51.121: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.0058152s
Aug 22 09:59:51.121: INFO: Pod "pause" satisfied condition "running and ready"
Aug 22 09:59:51.121: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 22 09:59:51.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-6629 label pods pause testing-label=testing-label-value'
Aug 22 09:59:51.168: INFO: stderr: ""
Aug 22 09:59:51.168: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 22 09:59:51.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-6629 get pod pause -L testing-label'
Aug 22 09:59:51.209: INFO: stderr: ""
Aug 22 09:59:51.209: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 22 09:59:51.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-6629 label pods pause testing-label-'
Aug 22 09:59:51.254: INFO: stderr: ""
Aug 22 09:59:51.254: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 22 09:59:51.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-6629 get pod pause -L testing-label'
Aug 22 09:59:51.294: INFO: stderr: ""
Aug 22 09:59:51.294: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Aug 22 09:59:51.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-6629 delete --grace-period=0 --force -f -'
Aug 22 09:59:51.342: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 09:59:51.342: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 22 09:59:51.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-6629 get rc,svc -l name=pause --no-headers'
Aug 22 09:59:51.385: INFO: stderr: "No resources found in kubectl-6629 namespace.\n"
Aug 22 09:59:51.385: INFO: stdout: ""
Aug 22 09:59:51.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-6629 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 22 09:59:51.426: INFO: stderr: ""
Aug 22 09:59:51.426: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 09:59:51.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6629" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":118,"skipped":2080,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 09:59:51.431: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Aug 22 09:59:51.449: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 22 10:00:51.469: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:00:51.471: INFO: Starting informer...
STEP: Starting pod...
Aug 22 10:00:51.678: INFO: Pod is running on 172.23.79.105. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Aug 22 10:00:51.687: INFO: Pod wasn't evicted. Proceeding
Aug 22 10:00:51.687: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Aug 22 10:02:06.700: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:02:06.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5109" for this suite.

• [SLOW TEST:135.276 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":119,"skipped":2109,"failed":0}
SSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:02:06.707: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:07:06.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7170" for this suite.

• [SLOW TEST:300.037 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":120,"skipped":2116,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:07:06.744: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-7ff0a5f8-1a82-4edd-b0b2-8d5a3775f59f
STEP: Creating a pod to test consume secrets
Aug 22 10:07:06.769: INFO: Waiting up to 5m0s for pod "pod-secrets-fd9e1c73-66b2-4861-8b49-94ecfe0948db" in namespace "secrets-7502" to be "Succeeded or Failed"
Aug 22 10:07:06.770: INFO: Pod "pod-secrets-fd9e1c73-66b2-4861-8b49-94ecfe0948db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.206813ms
Aug 22 10:07:08.775: INFO: Pod "pod-secrets-fd9e1c73-66b2-4861-8b49-94ecfe0948db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005765957s
STEP: Saw pod success
Aug 22 10:07:08.775: INFO: Pod "pod-secrets-fd9e1c73-66b2-4861-8b49-94ecfe0948db" satisfied condition "Succeeded or Failed"
Aug 22 10:07:08.777: INFO: Trying to get logs from node 172.23.79.105 pod pod-secrets-fd9e1c73-66b2-4861-8b49-94ecfe0948db container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 10:07:08.792: INFO: Waiting for pod pod-secrets-fd9e1c73-66b2-4861-8b49-94ecfe0948db to disappear
Aug 22 10:07:08.794: INFO: Pod pod-secrets-fd9e1c73-66b2-4861-8b49-94ecfe0948db no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:07:08.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7502" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":121,"skipped":2149,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:07:08.797: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Aug 22 10:07:08.820: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:07:11.402: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:07:21.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-515" for this suite.

• [SLOW TEST:12.517 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":122,"skipped":2151,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:07:21.314: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:07:21.655: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:07:24.667: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:07:24.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-703" for this suite.
STEP: Destroying namespace "webhook-703-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":123,"skipped":2160,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:07:24.765: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-07920795-02d7-41ec-8ab1-604bc24b82d6
STEP: Creating secret with name secret-projected-all-test-volume-5a361f1b-d746-4f5c-9d15-b5cbb5a614bf
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 22 10:07:24.789: INFO: Waiting up to 5m0s for pod "projected-volume-718920d8-281e-4166-9191-1dc6b58cc82a" in namespace "projected-198" to be "Succeeded or Failed"
Aug 22 10:07:24.791: INFO: Pod "projected-volume-718920d8-281e-4166-9191-1dc6b58cc82a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.696688ms
Aug 22 10:07:26.796: INFO: Pod "projected-volume-718920d8-281e-4166-9191-1dc6b58cc82a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007187208s
STEP: Saw pod success
Aug 22 10:07:26.796: INFO: Pod "projected-volume-718920d8-281e-4166-9191-1dc6b58cc82a" satisfied condition "Succeeded or Failed"
Aug 22 10:07:26.798: INFO: Trying to get logs from node 172.23.79.105 pod projected-volume-718920d8-281e-4166-9191-1dc6b58cc82a container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 22 10:07:26.806: INFO: Waiting for pod projected-volume-718920d8-281e-4166-9191-1dc6b58cc82a to disappear
Aug 22 10:07:26.807: INFO: Pod projected-volume-718920d8-281e-4166-9191-1dc6b58cc82a no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:07:26.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-198" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":124,"skipped":2169,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:07:26.811: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 22 10:07:26.828: INFO: Waiting up to 5m0s for pod "pod-ed47505f-1a4c-4573-a8cd-9563ddcaa569" in namespace "emptydir-4504" to be "Succeeded or Failed"
Aug 22 10:07:26.829: INFO: Pod "pod-ed47505f-1a4c-4573-a8cd-9563ddcaa569": Phase="Pending", Reason="", readiness=false. Elapsed: 1.204347ms
Aug 22 10:07:28.834: INFO: Pod "pod-ed47505f-1a4c-4573-a8cd-9563ddcaa569": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005773955s
STEP: Saw pod success
Aug 22 10:07:28.834: INFO: Pod "pod-ed47505f-1a4c-4573-a8cd-9563ddcaa569" satisfied condition "Succeeded or Failed"
Aug 22 10:07:28.835: INFO: Trying to get logs from node 172.23.79.105 pod pod-ed47505f-1a4c-4573-a8cd-9563ddcaa569 container test-container: <nil>
STEP: delete the pod
Aug 22 10:07:28.843: INFO: Waiting for pod pod-ed47505f-1a4c-4573-a8cd-9563ddcaa569 to disappear
Aug 22 10:07:28.845: INFO: Pod pod-ed47505f-1a4c-4573-a8cd-9563ddcaa569 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:07:28.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4504" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2185,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:07:28.848: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9455
Aug 22 10:07:28.869: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:07:30.873: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Aug 22 10:07:30.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-9455 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 22 10:07:30.977: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Aug 22 10:07:30.977: INFO: stdout: "iptables"
Aug 22 10:07:30.977: INFO: proxyMode: iptables
Aug 22 10:07:30.982: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 22 10:07:30.983: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-9455
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9455
I0822 10:07:30.994507      21 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9455, replica count: 3
I0822 10:07:34.046119      21 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 10:07:34.050: INFO: Creating new exec pod
Aug 22 10:07:37.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-9455 exec execpod-affinitylzpxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Aug 22 10:07:37.159: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Aug 22 10:07:37.159: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:07:37.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-9455 exec execpod-affinitylzpxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.219.19 80'
Aug 22 10:07:37.260: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.219.19 80\nConnection to 10.105.219.19 80 port [tcp/http] succeeded!\n"
Aug 22 10:07:37.260: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:07:37.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-9455 exec execpod-affinitylzpxx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.219.19:80/ ; done'
Aug 22 10:07:37.408: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n"
Aug 22 10:07:37.408: INFO: stdout: "\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d\naffinity-clusterip-timeout-nql4d"
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Received response from host: affinity-clusterip-timeout-nql4d
Aug 22 10:07:37.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-9455 exec execpod-affinitylzpxx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.105.219.19:80/'
Aug 22 10:07:37.513: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n"
Aug 22 10:07:37.513: INFO: stdout: "affinity-clusterip-timeout-nql4d"
Aug 22 10:07:57.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-9455 exec execpod-affinitylzpxx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.105.219.19:80/'
Aug 22 10:07:57.616: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.105.219.19:80/\n"
Aug 22 10:07:57.616: INFO: stdout: "affinity-clusterip-timeout-4xftm"
Aug 22 10:07:57.616: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9455, will wait for the garbage collector to delete the pods
Aug 22 10:07:57.677: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 2.980489ms
Aug 22 10:07:57.778: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.566669ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:08:00.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9455" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:31.443 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":126,"skipped":2189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:08:00.292: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-05b5d84d-5681-4491-aeb2-30b022c3799d
STEP: Creating a pod to test consume secrets
Aug 22 10:08:00.314: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dfe0b4ec-c9d1-45bc-8d0a-b9ac316b25f7" in namespace "projected-6693" to be "Succeeded or Failed"
Aug 22 10:08:00.316: INFO: Pod "pod-projected-secrets-dfe0b4ec-c9d1-45bc-8d0a-b9ac316b25f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.3686ms
Aug 22 10:08:02.321: INFO: Pod "pod-projected-secrets-dfe0b4ec-c9d1-45bc-8d0a-b9ac316b25f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006505039s
STEP: Saw pod success
Aug 22 10:08:02.321: INFO: Pod "pod-projected-secrets-dfe0b4ec-c9d1-45bc-8d0a-b9ac316b25f7" satisfied condition "Succeeded or Failed"
Aug 22 10:08:02.322: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-secrets-dfe0b4ec-c9d1-45bc-8d0a-b9ac316b25f7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 22 10:08:02.330: INFO: Waiting for pod pod-projected-secrets-dfe0b4ec-c9d1-45bc-8d0a-b9ac316b25f7 to disappear
Aug 22 10:08:02.331: INFO: Pod pod-projected-secrets-dfe0b4ec-c9d1-45bc-8d0a-b9ac316b25f7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:08:02.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6693" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":127,"skipped":2241,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:08:02.335: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 22 10:08:02.351: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 22 10:08:02.354: INFO: Waiting for terminating namespaces to be deleted...
Aug 22 10:08:02.355: INFO: 
Logging pods the apiserver thinks is on node 172.23.79.103 before test
Aug 22 10:08:02.358: INFO: kube-flannel-ds-s57gz from kube-system started at 2021-08-22 09:15:14 +0000 UTC (1 container statuses recorded)
Aug 22 10:08:02.358: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 10:08:02.358: INFO: kube-proxy-r9sql from kube-system started at 2021-08-22 09:14:45 +0000 UTC (1 container statuses recorded)
Aug 22 10:08:02.358: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 22 10:08:02.358: INFO: sonobuoy from sonobuoy started at 2021-08-22 09:27:55 +0000 UTC (1 container statuses recorded)
Aug 22 10:08:02.358: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 22 10:08:02.358: INFO: sonobuoy-e2e-job-75357183cd8142cd from sonobuoy started at 2021-08-22 09:27:56 +0000 UTC (2 container statuses recorded)
Aug 22 10:08:02.358: INFO: 	Container e2e ready: true, restart count 0
Aug 22 10:08:02.358: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 10:08:02.358: INFO: sonobuoy-systemd-logs-daemon-set-d3d945f99a484f42-ggzhp from sonobuoy started at 2021-08-22 09:27:56 +0000 UTC (2 container statuses recorded)
Aug 22 10:08:02.358: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 10:08:02.358: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 22 10:08:02.358: INFO: 
Logging pods the apiserver thinks is on node 172.23.79.105 before test
Aug 22 10:08:02.360: INFO: kube-flannel-ds-bzqtg from kube-system started at 2021-08-22 10:00:52 +0000 UTC (1 container statuses recorded)
Aug 22 10:08:02.360: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 10:08:02.360: INFO: kube-proxy-zhmln from kube-system started at 2021-08-22 09:39:54 +0000 UTC (1 container statuses recorded)
Aug 22 10:08:02.360: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 22 10:08:02.360: INFO: sonobuoy-systemd-logs-daemon-set-d3d945f99a484f42-8k4l9 from sonobuoy started at 2021-08-22 09:39:54 +0000 UTC (2 container statuses recorded)
Aug 22 10:08:02.360: INFO: 	Container sonobuoy-worker ready: false, restart count 10
Aug 22 10:08:02.360: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8a309d10-630a-495e-aecf-9beb80c4c660 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-8a309d10-630a-495e-aecf-9beb80c4c660 off the node 172.23.79.105
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8a309d10-630a-495e-aecf-9beb80c4c660
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:08:06.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2062" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":128,"skipped":2257,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:08:06.401: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:08:06.416: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:08:06.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5476" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":129,"skipped":2260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:08:06.996: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 22 10:08:09.025: INFO: &Pod{ObjectMeta:{send-events-cb9b9455-5a98-418b-9df8-77865226d4d4  events-4539  d7761cd4-35cf-4f07-93e8-5be9279111ee 12784 0 2021-08-22 10:08:07 +0000 UTC <nil> <nil> map[name:foo time:12783886] map[] [] []  [{e2e.test Update v1 2021-08-22 10:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 10:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6v7sp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6v7sp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:10.244.2.117,StartTime:2021-08-22 10:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 10:08:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://6c7f62ff873e47754528d51951c60fd55206ae33478156f866a362fdb79b15da,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Aug 22 10:08:11.029: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 22 10:08:13.035: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:08:13.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4539" for this suite.

• [SLOW TEST:6.048 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":130,"skipped":2294,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:08:13.044: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:08:24.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5143" for this suite.

• [SLOW TEST:11.048 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":131,"skipped":2304,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:08:24.092: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 10:08:24.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75706ac5-7eda-479c-bdc1-dc9e20a6e3db" in namespace "projected-1041" to be "Succeeded or Failed"
Aug 22 10:08:24.112: INFO: Pod "downwardapi-volume-75706ac5-7eda-479c-bdc1-dc9e20a6e3db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481736ms
Aug 22 10:08:26.117: INFO: Pod "downwardapi-volume-75706ac5-7eda-479c-bdc1-dc9e20a6e3db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007144088s
STEP: Saw pod success
Aug 22 10:08:26.117: INFO: Pod "downwardapi-volume-75706ac5-7eda-479c-bdc1-dc9e20a6e3db" satisfied condition "Succeeded or Failed"
Aug 22 10:08:26.119: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-75706ac5-7eda-479c-bdc1-dc9e20a6e3db container client-container: <nil>
STEP: delete the pod
Aug 22 10:08:26.127: INFO: Waiting for pod downwardapi-volume-75706ac5-7eda-479c-bdc1-dc9e20a6e3db to disappear
Aug 22 10:08:26.128: INFO: Pod downwardapi-volume-75706ac5-7eda-479c-bdc1-dc9e20a6e3db no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:08:26.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1041" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":132,"skipped":2311,"failed":0}

------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:08:26.132: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Aug 22 10:08:28.663: INFO: Successfully updated pod "adopt-release--1-hs8dw"
STEP: Checking that the Job readopts the Pod
Aug 22 10:08:28.663: INFO: Waiting up to 15m0s for pod "adopt-release--1-hs8dw" in namespace "job-8324" to be "adopted"
Aug 22 10:08:28.666: INFO: Pod "adopt-release--1-hs8dw": Phase="Running", Reason="", readiness=true. Elapsed: 2.696807ms
Aug 22 10:08:30.669: INFO: Pod "adopt-release--1-hs8dw": Phase="Running", Reason="", readiness=true. Elapsed: 2.006415769s
Aug 22 10:08:30.669: INFO: Pod "adopt-release--1-hs8dw" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Aug 22 10:08:31.175: INFO: Successfully updated pod "adopt-release--1-hs8dw"
STEP: Checking that the Job releases the Pod
Aug 22 10:08:31.175: INFO: Waiting up to 15m0s for pod "adopt-release--1-hs8dw" in namespace "job-8324" to be "released"
Aug 22 10:08:31.177: INFO: Pod "adopt-release--1-hs8dw": Phase="Running", Reason="", readiness=true. Elapsed: 1.390894ms
Aug 22 10:08:33.181: INFO: Pod "adopt-release--1-hs8dw": Phase="Running", Reason="", readiness=true. Elapsed: 2.005933623s
Aug 22 10:08:33.181: INFO: Pod "adopt-release--1-hs8dw" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:08:33.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8324" for this suite.

• [SLOW TEST:7.055 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":133,"skipped":2311,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:08:33.187: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-8200
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Aug 22 10:08:33.218: INFO: Found 0 stateful pods, waiting for 3
Aug 22 10:08:43.228: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 10:08:43.228: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 10:08:43.228: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Aug 22 10:08:43.249: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 22 10:08:53.281: INFO: Updating stateful set ss2
Aug 22 10:08:53.294: INFO: Waiting for Pod statefulset-8200/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Aug 22 10:09:03.330: INFO: Found 1 stateful pods, waiting for 3
Aug 22 10:09:13.336: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 10:09:13.336: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 10:09:13.336: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 22 10:09:13.353: INFO: Updating stateful set ss2
Aug 22 10:09:13.356: INFO: Waiting for Pod statefulset-8200/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Aug 22 10:09:23.379: INFO: Updating stateful set ss2
Aug 22 10:09:23.382: INFO: Waiting for StatefulSet statefulset-8200/ss2 to complete update
Aug 22 10:09:23.382: INFO: Waiting for Pod statefulset-8200/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Aug 22 10:09:33.393: INFO: Deleting all statefulset in ns statefulset-8200
Aug 22 10:09:33.395: INFO: Scaling statefulset ss2 to 0
Aug 22 10:09:43.411: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 10:09:43.413: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:09:43.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8200" for this suite.

• [SLOW TEST:70.242 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":134,"skipped":2319,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:09:43.429: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:09:43.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2280" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":135,"skipped":2326,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:09:43.461: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-8ef1cfd5-51a4-4c04-ab7e-f087feba3005 in namespace container-probe-1236
Aug 22 10:09:45.483: INFO: Started pod busybox-8ef1cfd5-51a4-4c04-ab7e-f087feba3005 in namespace container-probe-1236
STEP: checking the pod's current state and verifying that restartCount is present
Aug 22 10:09:45.485: INFO: Initial restart count of pod busybox-8ef1cfd5-51a4-4c04-ab7e-f087feba3005 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:13:46.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1236" for this suite.

• [SLOW TEST:242.697 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":136,"skipped":2330,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:13:46.158: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 22 10:13:46.192: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 22 10:13:46.193: INFO: starting watch
STEP: patching
STEP: updating
Aug 22 10:13:46.199: INFO: waiting for watch events with expected annotations
Aug 22 10:13:46.200: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:13:46.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9113" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":137,"skipped":2344,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:13:46.212: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Aug 22 10:13:46.225: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 22 10:14:46.239: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:14:46.240: INFO: Starting informer...
STEP: Starting pods...
Aug 22 10:14:46.450: INFO: Pod1 is running on 172.23.79.105. Tainting Node
Aug 22 10:14:48.663: INFO: Pod2 is running on 172.23.79.105. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Aug 22 10:14:54.913: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 22 10:15:14.322: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:15:14.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-3275" for this suite.

• [SLOW TEST:88.125 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":138,"skipped":2370,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:15:14.337: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 22 10:15:14.364: INFO: The status of Pod pod-update-d812a68e-6dbb-46a5-bc2c-b9b4379e12be is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:15:16.368: INFO: The status of Pod pod-update-d812a68e-6dbb-46a5-bc2c-b9b4379e12be is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:15:18.369: INFO: The status of Pod pod-update-d812a68e-6dbb-46a5-bc2c-b9b4379e12be is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 22 10:15:18.879: INFO: Successfully updated pod "pod-update-d812a68e-6dbb-46a5-bc2c-b9b4379e12be"
STEP: verifying the updated pod is in kubernetes
Aug 22 10:15:18.881: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:15:18.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8831" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":139,"skipped":2384,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:15:18.885: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Aug 22 10:15:18.905: INFO: Waiting up to 5m0s for pod "downward-api-7d5bb28c-ce37-4660-82a5-73150ac2ec67" in namespace "downward-api-7062" to be "Succeeded or Failed"
Aug 22 10:15:18.907: INFO: Pod "downward-api-7d5bb28c-ce37-4660-82a5-73150ac2ec67": Phase="Pending", Reason="", readiness=false. Elapsed: 1.950646ms
Aug 22 10:15:20.911: INFO: Pod "downward-api-7d5bb28c-ce37-4660-82a5-73150ac2ec67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005416344s
STEP: Saw pod success
Aug 22 10:15:20.911: INFO: Pod "downward-api-7d5bb28c-ce37-4660-82a5-73150ac2ec67" satisfied condition "Succeeded or Failed"
Aug 22 10:15:20.912: INFO: Trying to get logs from node 172.23.79.105 pod downward-api-7d5bb28c-ce37-4660-82a5-73150ac2ec67 container dapi-container: <nil>
STEP: delete the pod
Aug 22 10:15:20.927: INFO: Waiting for pod downward-api-7d5bb28c-ce37-4660-82a5-73150ac2ec67 to disappear
Aug 22 10:15:20.928: INFO: Pod downward-api-7d5bb28c-ce37-4660-82a5-73150ac2ec67 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:15:20.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7062" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":140,"skipped":2394,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:15:20.931: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:15:20.949: INFO: Creating deployment "test-recreate-deployment"
Aug 22 10:15:20.951: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 22 10:15:20.954: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 22 10:15:22.961: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 22 10:15:22.962: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 22 10:15:22.966: INFO: Updating deployment test-recreate-deployment
Aug 22 10:15:22.966: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 22 10:15:23.003: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2642  734218a1-e9f6-4942-b7d6-956aebb28c63 14101 2 2021-08-22 10:15:20 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-08-22 10:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 10:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004aa0958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-08-22 10:15:22 +0000 UTC,LastTransitionTime:2021-08-22 10:15:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-08-22 10:15:22 +0000 UTC,LastTransitionTime:2021-08-22 10:15:20 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 22 10:15:23.004: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-2642  000e1e3a-1f2a-456a-8417-22ab94049e49 14100 1 2021-08-22 10:15:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 734218a1-e9f6-4942-b7d6-956aebb28c63 0xc004aa0e30 0xc004aa0e31}] []  [{kube-controller-manager Update apps/v1 2021-08-22 10:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"734218a1-e9f6-4942-b7d6-956aebb28c63\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 10:15:22 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004aa0ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 22 10:15:23.004: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 22 10:15:23.004: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-2642  2509f9ed-7025-4c56-b27d-153fad651cac 14090 2 2021-08-22 10:15:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 734218a1-e9f6-4942-b7d6-956aebb28c63 0xc004aa0d07 0xc004aa0d08}] []  [{kube-controller-manager Update apps/v1 2021-08-22 10:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"734218a1-e9f6-4942-b7d6-956aebb28c63\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 10:15:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004aa0db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 22 10:15:23.006: INFO: Pod "test-recreate-deployment-85d47dcb4-mn5qh" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-mn5qh test-recreate-deployment-85d47dcb4- deployment-2642  7a92b1da-20fe-4153-8024-9856952a80bb 14102 0 2021-08-22 10:15:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 000e1e3a-1f2a-456a-8417-22ab94049e49 0xc004b80b10 0xc004b80b11}] []  [{kube-controller-manager Update v1 2021-08-22 10:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"000e1e3a-1f2a-456a-8417-22ab94049e49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 10:15:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nbp2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nbp2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:15:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:15:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:15:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:15:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:,StartTime:2021-08-22 10:15:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:15:23.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2642" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":141,"skipped":2396,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:15:23.009: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:15:23.025: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 22 10:15:23.028: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 22 10:15:28.035: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 22 10:15:28.035: INFO: Creating deployment "test-rolling-update-deployment"
Aug 22 10:15:28.037: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 22 10:15:28.040: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 22 10:15:30.046: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 22 10:15:30.047: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 22 10:15:30.051: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3407  a526e278-6cd2-4bd8-8ccc-66977d4e8f9e 14210 1 2021-08-22 10:15:28 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-08-22 10:15:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 10:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048d5428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-08-22 10:15:28 +0000 UTC,LastTransitionTime:2021-08-22 10:15:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-08-22 10:15:29 +0000 UTC,LastTransitionTime:2021-08-22 10:15:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 22 10:15:30.053: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-3407  2cb8e3ea-a26f-4d94-acfc-dc3045154141 14200 1 2021-08-22 10:15:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a526e278-6cd2-4bd8-8ccc-66977d4e8f9e 0xc0048d5917 0xc0048d5918}] []  [{kube-controller-manager Update apps/v1 2021-08-22 10:15:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a526e278-6cd2-4bd8-8ccc-66977d4e8f9e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 10:15:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048d59c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 22 10:15:30.053: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 22 10:15:30.053: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3407  8ced0c8b-92b7-44a5-9b46-8e7c55c8d4d4 14209 2 2021-08-22 10:15:23 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a526e278-6cd2-4bd8-8ccc-66977d4e8f9e 0xc0048d57e7 0xc0048d57e8}] []  [{e2e.test Update apps/v1 2021-08-22 10:15:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 10:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a526e278-6cd2-4bd8-8ccc-66977d4e8f9e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2021-08-22 10:15:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0048d58a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 22 10:15:30.054: INFO: Pod "test-rolling-update-deployment-585b757574-pljsc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-pljsc test-rolling-update-deployment-585b757574- deployment-3407  cf047ac7-fe2f-4e61-a020-50085a515823 14199 0 2021-08-22 10:15:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 2cb8e3ea-a26f-4d94-acfc-dc3045154141 0xc0048d5e17 0xc0048d5e18}] []  [{kube-controller-manager Update v1 2021-08-22 10:15:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2cb8e3ea-a26f-4d94-acfc-dc3045154141\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 10:15:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lxz55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lxz55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:15:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:15:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:10.244.2.137,StartTime:2021-08-22 10:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 10:15:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://3fd789840bf252a99c58758f873b148bdbd68e006c50e1dca3b6adb348856b0f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:15:30.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3407" for this suite.

• [SLOW TEST:7.049 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":142,"skipped":2397,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:15:30.059: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:15:30.235: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:15:33.251: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:15:33.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5111" for this suite.
STEP: Destroying namespace "webhook-5111-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":143,"skipped":2413,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:15:33.306: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Aug 22 10:15:33.330: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 22 10:16:33.346: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:16:33.347: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:16:33.375: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Aug 22 10:16:33.377: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:16:33.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9638" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:16:33.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4353" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.098 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":144,"skipped":2418,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:16:33.404: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-ffd0ffdd-7485-453b-a94d-9071046b3503
STEP: Creating configMap with name cm-test-opt-upd-9af70c68-7d4b-4095-8933-816f2cf2f9f5
STEP: Creating the pod
Aug 22 10:16:33.427: INFO: The status of Pod pod-projected-configmaps-005fa2f8-13d3-46b7-9d41-f227db0bbedc is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:16:35.433: INFO: The status of Pod pod-projected-configmaps-005fa2f8-13d3-46b7-9d41-f227db0bbedc is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-ffd0ffdd-7485-453b-a94d-9071046b3503
STEP: Updating configmap cm-test-opt-upd-9af70c68-7d4b-4095-8933-816f2cf2f9f5
STEP: Creating configMap with name cm-test-opt-create-38dad95b-d55c-457e-a2db-3a142bb2349c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:16:37.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8476" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":145,"skipped":2420,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:16:37.475: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 10:16:37.500: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f1e553c-55ce-4245-ad9e-00ef64f8b635" in namespace "downward-api-970" to be "Succeeded or Failed"
Aug 22 10:16:37.501: INFO: Pod "downwardapi-volume-1f1e553c-55ce-4245-ad9e-00ef64f8b635": Phase="Pending", Reason="", readiness=false. Elapsed: 1.209267ms
Aug 22 10:16:39.503: INFO: Pod "downwardapi-volume-1f1e553c-55ce-4245-ad9e-00ef64f8b635": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003368938s
STEP: Saw pod success
Aug 22 10:16:39.503: INFO: Pod "downwardapi-volume-1f1e553c-55ce-4245-ad9e-00ef64f8b635" satisfied condition "Succeeded or Failed"
Aug 22 10:16:39.505: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-1f1e553c-55ce-4245-ad9e-00ef64f8b635 container client-container: <nil>
STEP: delete the pod
Aug 22 10:16:39.514: INFO: Waiting for pod downwardapi-volume-1f1e553c-55ce-4245-ad9e-00ef64f8b635 to disappear
Aug 22 10:16:39.515: INFO: Pod downwardapi-volume-1f1e553c-55ce-4245-ad9e-00ef64f8b635 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:16:39.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-970" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2431,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:16:39.518: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4644.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4644.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 10:16:41.556: INFO: DNS probes using dns-4644/dns-test-add3d80a-577a-4630-8d9e-0b44bfdd8a54 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:16:41.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4644" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":147,"skipped":2438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:16:41.565: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Aug 22 10:16:41.586: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:16:43.590: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Aug 22 10:16:43.597: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:16:45.600: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 22 10:16:45.607: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 22 10:16:45.609: INFO: Pod pod-with-poststart-http-hook still exists
Aug 22 10:16:47.609: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 22 10:16:47.613: INFO: Pod pod-with-poststart-http-hook still exists
Aug 22 10:16:49.609: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 22 10:16:49.612: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:16:49.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2741" for this suite.

• [SLOW TEST:8.051 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":148,"skipped":2464,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:16:49.616: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Aug 22 10:18:50.149: INFO: Successfully updated pod "var-expansion-5a4d5aec-5b24-415e-be76-d48166c28ad0"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Aug 22 10:18:52.154: INFO: Deleting pod "var-expansion-5a4d5aec-5b24-415e-be76-d48166c28ad0" in namespace "var-expansion-4491"
Aug 22 10:18:52.156: INFO: Wait up to 5m0s for pod "var-expansion-5a4d5aec-5b24-415e-be76-d48166c28ad0" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:19:24.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4491" for this suite.

• [SLOW TEST:154.552 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":149,"skipped":2471,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:19:24.168: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-34d71554-4e43-45d2-aaf2-28ca58c4804d
STEP: Creating secret with name s-test-opt-upd-941be3b8-dcf9-4638-8c8f-037c8f73ec24
STEP: Creating the pod
Aug 22 10:19:24.192: INFO: The status of Pod pod-secrets-cb31e4ff-5b36-44b1-9439-89043785a982 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:19:26.196: INFO: The status of Pod pod-secrets-cb31e4ff-5b36-44b1-9439-89043785a982 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-34d71554-4e43-45d2-aaf2-28ca58c4804d
STEP: Updating secret s-test-opt-upd-941be3b8-dcf9-4638-8c8f-037c8f73ec24
STEP: Creating secret with name s-test-opt-create-1a3e1fc5-d429-41fc-b901-23f1d4beeb0f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:19:30.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1656" for this suite.

• [SLOW TEST:6.069 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":150,"skipped":2521,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:19:30.237: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:19:30.255: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 22 10:19:35.263: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 22 10:19:35.263: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 22 10:19:35.275: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9284  8c1cdb75-0276-48e4-b124-a581657b0f60 14961 1 2021-08-22 10:19:35 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-08-22 10:19:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a18928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 22 10:19:35.279: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-9284  4a7bfb7a-2a58-4726-9d8b-fd611571bb6e 14965 1 2021-08-22 10:19:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8c1cdb75-0276-48e4-b124-a581657b0f60 0xc004aa1d77 0xc004aa1d78}] []  [{kube-controller-manager Update apps/v1 2021-08-22 10:19:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c1cdb75-0276-48e4-b124-a581657b0f60\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004aa1e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 22 10:19:35.279: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 22 10:19:35.279: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9284  b71f4ac0-0a68-44ee-bce1-0db77349564d 14964 1 2021-08-22 10:19:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 8c1cdb75-0276-48e4-b124-a581657b0f60 0xc004aa1c37 0xc004aa1c38}] []  [{e2e.test Update apps/v1 2021-08-22 10:19:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 10:19:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2021-08-22 10:19:35 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"8c1cdb75-0276-48e4-b124-a581657b0f60\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004aa1cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 22 10:19:35.287: INFO: Pod "test-cleanup-controller-xhv5d" is available:
&Pod{ObjectMeta:{test-cleanup-controller-xhv5d test-cleanup-controller- deployment-9284  ee465ca3-725a-418f-b49f-4a8e95e6dd43 14949 0 2021-08-22 10:19:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller b71f4ac0-0a68-44ee-bce1-0db77349564d 0xc0059a6567 0xc0059a6568}] []  [{kube-controller-manager Update v1 2021-08-22 10:19:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b71f4ac0-0a68-44ee-bce1-0db77349564d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 10:19:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sjm9f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sjm9f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:19:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:19:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:19:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:19:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:10.244.2.146,StartTime:2021-08-22 10:19:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 10:19:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://e5f836c515cba168ed1370cbac4a45b334214108634ec254281f29c65e7047cb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 10:19:35.287: INFO: Pod "test-cleanup-deployment-5b4d99b59b-9fjr5" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-9fjr5 test-cleanup-deployment-5b4d99b59b- deployment-9284  10305d06-0b81-44bc-a01d-72b9206752a9 14970 0 2021-08-22 10:19:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b 4a7bfb7a-2a58-4726-9d8b-fd611571bb6e 0xc0059a6767 0xc0059a6768}] []  [{kube-controller-manager Update v1 2021-08-22 10:19:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a7bfb7a-2a58-4726-9d8b-fd611571bb6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-22f96,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-22f96,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:19:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:19:35.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9284" for this suite.

• [SLOW TEST:5.059 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":151,"skipped":2523,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:19:35.296: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-28
Aug 22 10:19:35.317: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:19:37.323: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Aug 22 10:19:37.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-28 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 22 10:19:37.516: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Aug 22 10:19:37.517: INFO: stdout: "iptables"
Aug 22 10:19:37.517: INFO: proxyMode: iptables
Aug 22 10:19:37.520: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 22 10:19:37.522: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-28
STEP: creating replication controller affinity-nodeport-timeout in namespace services-28
I0822 10:19:37.531763      21 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-28, replica count: 3
I0822 10:19:40.582881      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 10:19:40.588: INFO: Creating new exec pod
Aug 22 10:19:43.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-28 exec execpod-affinitywcttp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Aug 22 10:19:43.699: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Aug 22 10:19:43.699: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:19:43.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-28 exec execpod-affinitywcttp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.182.226 80'
Aug 22 10:19:43.803: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.182.226 80\nConnection to 10.102.182.226 80 port [tcp/http] succeeded!\n"
Aug 22 10:19:43.803: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:19:43.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-28 exec execpod-affinitywcttp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.23.79.103 31234'
Aug 22 10:19:43.903: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.23.79.103 31234\nConnection to 172.23.79.103 31234 port [tcp/*] succeeded!\n"
Aug 22 10:19:43.903: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:19:43.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-28 exec execpod-affinitywcttp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.23.79.105 31234'
Aug 22 10:19:44.000: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.23.79.105 31234\nConnection to 172.23.79.105 31234 port [tcp/*] succeeded!\n"
Aug 22 10:19:44.000: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:19:44.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-28 exec execpod-affinitywcttp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.23.79.103:31234/ ; done'
Aug 22 10:19:44.160: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n"
Aug 22 10:19:44.160: INFO: stdout: "\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf\naffinity-nodeport-timeout-cfncf"
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Received response from host: affinity-nodeport-timeout-cfncf
Aug 22 10:19:44.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-28 exec execpod-affinitywcttp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.23.79.103:31234/'
Aug 22 10:19:44.263: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n"
Aug 22 10:19:44.263: INFO: stdout: "affinity-nodeport-timeout-cfncf"
Aug 22 10:20:04.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-28 exec execpod-affinitywcttp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.23.79.103:31234/'
Aug 22 10:20:04.382: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n"
Aug 22 10:20:04.382: INFO: stdout: "affinity-nodeport-timeout-cfncf"
Aug 22 10:20:24.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-28 exec execpod-affinitywcttp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.23.79.103:31234/'
Aug 22 10:20:24.487: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.23.79.103:31234/\n"
Aug 22 10:20:24.487: INFO: stdout: "affinity-nodeport-timeout-fzhbb"
Aug 22 10:20:24.487: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-28, will wait for the garbage collector to delete the pods
Aug 22 10:20:24.553: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 2.632303ms
Aug 22 10:20:24.653: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.7895ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:20:26.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-28" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:51.273 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":152,"skipped":2531,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:20:26.569: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-c1510c30-1439-4325-8aa6-4b6664540623
STEP: Creating a pod to test consume configMaps
Aug 22 10:20:26.587: INFO: Waiting up to 5m0s for pod "pod-configmaps-4078c379-e66b-439f-9ef4-6a54c1d74b15" in namespace "configmap-1999" to be "Succeeded or Failed"
Aug 22 10:20:26.590: INFO: Pod "pod-configmaps-4078c379-e66b-439f-9ef4-6a54c1d74b15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.828435ms
Aug 22 10:20:28.595: INFO: Pod "pod-configmaps-4078c379-e66b-439f-9ef4-6a54c1d74b15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007709367s
STEP: Saw pod success
Aug 22 10:20:28.595: INFO: Pod "pod-configmaps-4078c379-e66b-439f-9ef4-6a54c1d74b15" satisfied condition "Succeeded or Failed"
Aug 22 10:20:28.597: INFO: Trying to get logs from node 172.23.79.105 pod pod-configmaps-4078c379-e66b-439f-9ef4-6a54c1d74b15 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 10:20:28.610: INFO: Waiting for pod pod-configmaps-4078c379-e66b-439f-9ef4-6a54c1d74b15 to disappear
Aug 22 10:20:28.611: INFO: Pod pod-configmaps-4078c379-e66b-439f-9ef4-6a54c1d74b15 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:20:28.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1999" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":153,"skipped":2538,"failed":0}
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:20:28.615: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:20:28.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5876" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":154,"skipped":2541,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:20:28.637: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-f2b2295a-59d6-40f5-9329-1c2a7db5ddc7
STEP: Creating configMap with name cm-test-opt-upd-1a70b88d-d8d4-4cc9-b50e-6cbef8f2e9a8
STEP: Creating the pod
Aug 22 10:20:28.661: INFO: The status of Pod pod-configmaps-1c9a41f8-d9c5-409a-a605-8725e99eb7e8 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:20:30.664: INFO: The status of Pod pod-configmaps-1c9a41f8-d9c5-409a-a605-8725e99eb7e8 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:20:32.666: INFO: The status of Pod pod-configmaps-1c9a41f8-d9c5-409a-a605-8725e99eb7e8 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-f2b2295a-59d6-40f5-9329-1c2a7db5ddc7
STEP: Updating configmap cm-test-opt-upd-1a70b88d-d8d4-4cc9-b50e-6cbef8f2e9a8
STEP: Creating configMap with name cm-test-opt-create-04a69221-532f-4e3d-84dc-8220d0ff1beb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:22:00.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3891" for this suite.

• [SLOW TEST:92.295 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":155,"skipped":2543,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:22:00.933: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-2129
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2129
STEP: Waiting until pod test-pod will start running in namespace statefulset-2129
STEP: Creating statefulset with conflicting port in namespace statefulset-2129
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2129
Aug 22 10:22:02.970: INFO: Observed stateful pod in namespace: statefulset-2129, name: ss-0, uid: 4dca0df8-8691-47e1-b25c-74771fc5225f, status phase: Pending. Waiting for statefulset controller to delete.
Aug 22 10:22:02.976: INFO: Observed stateful pod in namespace: statefulset-2129, name: ss-0, uid: 4dca0df8-8691-47e1-b25c-74771fc5225f, status phase: Failed. Waiting for statefulset controller to delete.
Aug 22 10:22:02.979: INFO: Observed stateful pod in namespace: statefulset-2129, name: ss-0, uid: 4dca0df8-8691-47e1-b25c-74771fc5225f, status phase: Failed. Waiting for statefulset controller to delete.
Aug 22 10:22:02.980: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2129
STEP: Removing pod with conflicting port in namespace statefulset-2129
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2129 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Aug 22 10:22:04.998: INFO: Deleting all statefulset in ns statefulset-2129
Aug 22 10:22:05.000: INFO: Scaling statefulset ss to 0
Aug 22 10:22:15.011: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 10:22:15.013: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:22:15.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2129" for this suite.

• [SLOW TEST:14.102 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":156,"skipped":2604,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:22:15.035: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 10:22:15.059: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66b610a7-4d72-46d6-8f71-2987813e53ea" in namespace "projected-6898" to be "Succeeded or Failed"
Aug 22 10:22:15.061: INFO: Pod "downwardapi-volume-66b610a7-4d72-46d6-8f71-2987813e53ea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.571304ms
Aug 22 10:22:17.066: INFO: Pod "downwardapi-volume-66b610a7-4d72-46d6-8f71-2987813e53ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006570548s
STEP: Saw pod success
Aug 22 10:22:17.066: INFO: Pod "downwardapi-volume-66b610a7-4d72-46d6-8f71-2987813e53ea" satisfied condition "Succeeded or Failed"
Aug 22 10:22:17.067: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-66b610a7-4d72-46d6-8f71-2987813e53ea container client-container: <nil>
STEP: delete the pod
Aug 22 10:22:17.075: INFO: Waiting for pod downwardapi-volume-66b610a7-4d72-46d6-8f71-2987813e53ea to disappear
Aug 22 10:22:17.077: INFO: Pod downwardapi-volume-66b610a7-4d72-46d6-8f71-2987813e53ea no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:22:17.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6898" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":157,"skipped":2607,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:22:17.081: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Aug 22 10:22:17.101: INFO: Waiting up to 5m0s for pod "downward-api-d5211328-e558-444b-aff5-a591fcd14b19" in namespace "downward-api-4932" to be "Succeeded or Failed"
Aug 22 10:22:17.102: INFO: Pod "downward-api-d5211328-e558-444b-aff5-a591fcd14b19": Phase="Pending", Reason="", readiness=false. Elapsed: 1.224457ms
Aug 22 10:22:19.107: INFO: Pod "downward-api-d5211328-e558-444b-aff5-a591fcd14b19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006244978s
STEP: Saw pod success
Aug 22 10:22:19.107: INFO: Pod "downward-api-d5211328-e558-444b-aff5-a591fcd14b19" satisfied condition "Succeeded or Failed"
Aug 22 10:22:19.108: INFO: Trying to get logs from node 172.23.79.105 pod downward-api-d5211328-e558-444b-aff5-a591fcd14b19 container dapi-container: <nil>
STEP: delete the pod
Aug 22 10:22:19.116: INFO: Waiting for pod downward-api-d5211328-e558-444b-aff5-a591fcd14b19 to disappear
Aug 22 10:22:19.117: INFO: Pod downward-api-d5211328-e558-444b-aff5-a591fcd14b19 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:22:19.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4932" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":158,"skipped":2645,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:22:19.120: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-cxnz
STEP: Creating a pod to test atomic-volume-subpath
Aug 22 10:22:19.143: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-cxnz" in namespace "subpath-1127" to be "Succeeded or Failed"
Aug 22 10:22:19.145: INFO: Pod "pod-subpath-test-secret-cxnz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.872545ms
Aug 22 10:22:21.149: INFO: Pod "pod-subpath-test-secret-cxnz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005369804s
Aug 22 10:22:23.153: INFO: Pod "pod-subpath-test-secret-cxnz": Phase="Running", Reason="", readiness=true. Elapsed: 4.009444203s
Aug 22 10:22:25.158: INFO: Pod "pod-subpath-test-secret-cxnz": Phase="Running", Reason="", readiness=true. Elapsed: 6.014913901s
Aug 22 10:22:27.163: INFO: Pod "pod-subpath-test-secret-cxnz": Phase="Running", Reason="", readiness=true. Elapsed: 8.019433573s
Aug 22 10:22:29.166: INFO: Pod "pod-subpath-test-secret-cxnz": Phase="Running", Reason="", readiness=true. Elapsed: 10.022322219s
Aug 22 10:22:31.168: INFO: Pod "pod-subpath-test-secret-cxnz": Phase="Running", Reason="", readiness=true. Elapsed: 12.025038858s
Aug 22 10:22:33.173: INFO: Pod "pod-subpath-test-secret-cxnz": Phase="Running", Reason="", readiness=true. Elapsed: 14.029723614s
Aug 22 10:22:35.183: INFO: Pod "pod-subpath-test-secret-cxnz": Phase="Running", Reason="", readiness=true. Elapsed: 16.039184042s
Aug 22 10:22:37.188: INFO: Pod "pod-subpath-test-secret-cxnz": Phase="Running", Reason="", readiness=true. Elapsed: 18.044256048s
Aug 22 10:22:39.193: INFO: Pod "pod-subpath-test-secret-cxnz": Phase="Running", Reason="", readiness=true. Elapsed: 20.049754672s
Aug 22 10:22:41.197: INFO: Pod "pod-subpath-test-secret-cxnz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.053365593s
STEP: Saw pod success
Aug 22 10:22:41.197: INFO: Pod "pod-subpath-test-secret-cxnz" satisfied condition "Succeeded or Failed"
Aug 22 10:22:41.198: INFO: Trying to get logs from node 172.23.79.105 pod pod-subpath-test-secret-cxnz container test-container-subpath-secret-cxnz: <nil>
STEP: delete the pod
Aug 22 10:22:41.207: INFO: Waiting for pod pod-subpath-test-secret-cxnz to disappear
Aug 22 10:22:41.208: INFO: Pod pod-subpath-test-secret-cxnz no longer exists
STEP: Deleting pod pod-subpath-test-secret-cxnz
Aug 22 10:22:41.208: INFO: Deleting pod "pod-subpath-test-secret-cxnz" in namespace "subpath-1127"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:22:41.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1127" for this suite.

• [SLOW TEST:22.092 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":159,"skipped":2651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:22:41.213: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-b838d24c-f44a-40f2-850b-3487d50e924e
STEP: Creating a pod to test consume configMaps
Aug 22 10:22:41.231: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98a500cc-79a8-4a61-a41c-8e4d53d4ca3c" in namespace "projected-3882" to be "Succeeded or Failed"
Aug 22 10:22:41.233: INFO: Pod "pod-projected-configmaps-98a500cc-79a8-4a61-a41c-8e4d53d4ca3c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.979194ms
Aug 22 10:22:43.236: INFO: Pod "pod-projected-configmaps-98a500cc-79a8-4a61-a41c-8e4d53d4ca3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005432899s
STEP: Saw pod success
Aug 22 10:22:43.236: INFO: Pod "pod-projected-configmaps-98a500cc-79a8-4a61-a41c-8e4d53d4ca3c" satisfied condition "Succeeded or Failed"
Aug 22 10:22:43.238: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-configmaps-98a500cc-79a8-4a61-a41c-8e4d53d4ca3c container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:22:43.246: INFO: Waiting for pod pod-projected-configmaps-98a500cc-79a8-4a61-a41c-8e4d53d4ca3c to disappear
Aug 22 10:22:43.247: INFO: Pod pod-projected-configmaps-98a500cc-79a8-4a61-a41c-8e4d53d4ca3c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:22:43.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3882" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":160,"skipped":2681,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:22:43.251: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-937
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-937
I0822 10:22:43.278323      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-937, replica count: 2
Aug 22 10:22:46.329: INFO: Creating new exec pod
I0822 10:22:46.329718      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 10:22:49.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-937 exec execpodt2s65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 22 10:22:49.444: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 22 10:22:49.444: INFO: stdout: "externalname-service-8kjfz"
Aug 22 10:22:49.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-937 exec execpodt2s65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.60.42 80'
Aug 22 10:22:49.538: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.60.42 80\nConnection to 10.99.60.42 80 port [tcp/http] succeeded!\n"
Aug 22 10:22:49.538: INFO: stdout: "externalname-service-8kjfz"
Aug 22 10:22:49.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-937 exec execpodt2s65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.23.79.103 30953'
Aug 22 10:22:49.637: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.23.79.103 30953\nConnection to 172.23.79.103 30953 port [tcp/*] succeeded!\n"
Aug 22 10:22:49.637: INFO: stdout: ""
Aug 22 10:22:50.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-937 exec execpodt2s65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.23.79.103 30953'
Aug 22 10:22:50.740: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.23.79.103 30953\nConnection to 172.23.79.103 30953 port [tcp/*] succeeded!\n"
Aug 22 10:22:50.740: INFO: stdout: "externalname-service-2l7pp"
Aug 22 10:22:50.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-937 exec execpodt2s65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.23.79.105 30953'
Aug 22 10:22:50.839: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.23.79.105 30953\nConnection to 172.23.79.105 30953 port [tcp/*] succeeded!\n"
Aug 22 10:22:50.839: INFO: stdout: "externalname-service-2l7pp"
Aug 22 10:22:50.839: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:22:50.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-937" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.624 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":161,"skipped":2683,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:22:50.876: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 22 10:22:51.912: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:22:51.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9238" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":162,"skipped":2734,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:22:51.921: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 22 10:22:51.938: INFO: Waiting up to 5m0s for pod "pod-7a9c6518-361d-4948-812d-4eb4ca97b648" in namespace "emptydir-8026" to be "Succeeded or Failed"
Aug 22 10:22:51.939: INFO: Pod "pod-7a9c6518-361d-4948-812d-4eb4ca97b648": Phase="Pending", Reason="", readiness=false. Elapsed: 1.284366ms
Aug 22 10:22:53.942: INFO: Pod "pod-7a9c6518-361d-4948-812d-4eb4ca97b648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004493808s
STEP: Saw pod success
Aug 22 10:22:53.942: INFO: Pod "pod-7a9c6518-361d-4948-812d-4eb4ca97b648" satisfied condition "Succeeded or Failed"
Aug 22 10:22:53.944: INFO: Trying to get logs from node 172.23.79.105 pod pod-7a9c6518-361d-4948-812d-4eb4ca97b648 container test-container: <nil>
STEP: delete the pod
Aug 22 10:22:53.951: INFO: Waiting for pod pod-7a9c6518-361d-4948-812d-4eb4ca97b648 to disappear
Aug 22 10:22:53.952: INFO: Pod pod-7a9c6518-361d-4948-812d-4eb4ca97b648 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:22:53.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8026" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":163,"skipped":2765,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:22:53.956: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Aug 22 10:22:53.976: INFO: The status of Pod pod-hostip-c25e7361-6d6f-421c-b916-47bd931f92d1 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:22:55.980: INFO: The status of Pod pod-hostip-c25e7361-6d6f-421c-b916-47bd931f92d1 is Running (Ready = true)
Aug 22 10:22:55.982: INFO: Pod pod-hostip-c25e7361-6d6f-421c-b916-47bd931f92d1 has hostIP: 172.23.79.105
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:22:55.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1245" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":164,"skipped":2795,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:22:55.986: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Aug 22 10:22:56.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 create -f -'
Aug 22 10:22:56.117: INFO: stderr: ""
Aug 22 10:22:56.117: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 22 10:22:56.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 22 10:22:56.160: INFO: stderr: ""
Aug 22 10:22:56.160: INFO: stdout: "update-demo-nautilus-9jvgd update-demo-nautilus-whjch "
Aug 22 10:22:56.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 get pods update-demo-nautilus-9jvgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 22 10:22:56.201: INFO: stderr: ""
Aug 22 10:22:56.201: INFO: stdout: ""
Aug 22 10:22:56.201: INFO: update-demo-nautilus-9jvgd is created but not running
Aug 22 10:23:01.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 22 10:23:01.247: INFO: stderr: ""
Aug 22 10:23:01.247: INFO: stdout: "update-demo-nautilus-9jvgd update-demo-nautilus-whjch "
Aug 22 10:23:01.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 get pods update-demo-nautilus-9jvgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 22 10:23:01.289: INFO: stderr: ""
Aug 22 10:23:01.289: INFO: stdout: ""
Aug 22 10:23:01.289: INFO: update-demo-nautilus-9jvgd is created but not running
Aug 22 10:23:06.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 22 10:23:06.339: INFO: stderr: ""
Aug 22 10:23:06.339: INFO: stdout: "update-demo-nautilus-9jvgd update-demo-nautilus-whjch "
Aug 22 10:23:06.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 get pods update-demo-nautilus-9jvgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 22 10:23:06.380: INFO: stderr: ""
Aug 22 10:23:06.380: INFO: stdout: "true"
Aug 22 10:23:06.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 get pods update-demo-nautilus-9jvgd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 22 10:23:06.422: INFO: stderr: ""
Aug 22 10:23:06.422: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Aug 22 10:23:06.422: INFO: validating pod update-demo-nautilus-9jvgd
Aug 22 10:23:06.425: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 10:23:06.425: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 10:23:06.425: INFO: update-demo-nautilus-9jvgd is verified up and running
Aug 22 10:23:06.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 get pods update-demo-nautilus-whjch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 22 10:23:06.466: INFO: stderr: ""
Aug 22 10:23:06.466: INFO: stdout: "true"
Aug 22 10:23:06.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 get pods update-demo-nautilus-whjch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 22 10:23:06.507: INFO: stderr: ""
Aug 22 10:23:06.507: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Aug 22 10:23:06.507: INFO: validating pod update-demo-nautilus-whjch
Aug 22 10:23:06.510: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 10:23:06.510: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 10:23:06.510: INFO: update-demo-nautilus-whjch is verified up and running
STEP: using delete to clean up resources
Aug 22 10:23:06.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 delete --grace-period=0 --force -f -'
Aug 22 10:23:06.553: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 10:23:06.553: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 22 10:23:06.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 get rc,svc -l name=update-demo --no-headers'
Aug 22 10:23:06.597: INFO: stderr: "No resources found in kubectl-9916 namespace.\n"
Aug 22 10:23:06.597: INFO: stdout: ""
Aug 22 10:23:06.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9916 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 22 10:23:06.641: INFO: stderr: ""
Aug 22 10:23:06.641: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:23:06.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9916" for this suite.

• [SLOW TEST:10.660 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":165,"skipped":2802,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:23:06.646: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:23:07.097: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:23:10.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:23:10.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5890" for this suite.
STEP: Destroying namespace "webhook-5890-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":166,"skipped":2810,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:23:10.163: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-523
STEP: creating service affinity-clusterip in namespace services-523
STEP: creating replication controller affinity-clusterip in namespace services-523
I0822 10:23:10.191780      21 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-523, replica count: 3
I0822 10:23:13.241976      21 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 10:23:13.248: INFO: Creating new exec pod
Aug 22 10:23:16.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-523 exec execpod-affinitys75qr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Aug 22 10:23:16.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 22 10:23:16.359: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:23:16.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-523 exec execpod-affinitys75qr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.140.205 80'
Aug 22 10:23:16.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.140.205 80\nConnection to 10.103.140.205 80 port [tcp/http] succeeded!\n"
Aug 22 10:23:16.461: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:23:16.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-523 exec execpod-affinitys75qr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.140.205:80/ ; done'
Aug 22 10:23:16.602: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.140.205:80/\n"
Aug 22 10:23:16.602: INFO: stdout: "\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94\naffinity-clusterip-2px94"
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Received response from host: affinity-clusterip-2px94
Aug 22 10:23:16.602: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-523, will wait for the garbage collector to delete the pods
Aug 22 10:23:16.665: INFO: Deleting ReplicationController affinity-clusterip took: 2.52038ms
Aug 22 10:23:16.766: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.751252ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:23:18.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-523" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.115 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":167,"skipped":2814,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:23:18.278: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:23:20.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6226" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":168,"skipped":2825,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:23:20.325: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:23:20.349: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 22 10:23:20.353: INFO: Number of nodes with available pods: 0
Aug 22 10:23:20.354: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 22 10:23:20.364: INFO: Number of nodes with available pods: 0
Aug 22 10:23:20.364: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 10:23:21.367: INFO: Number of nodes with available pods: 1
Aug 22 10:23:21.367: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 22 10:23:21.377: INFO: Number of nodes with available pods: 1
Aug 22 10:23:21.377: INFO: Number of running nodes: 0, number of available pods: 1
Aug 22 10:23:22.381: INFO: Number of nodes with available pods: 0
Aug 22 10:23:22.381: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 22 10:23:22.395: INFO: Number of nodes with available pods: 0
Aug 22 10:23:22.395: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 10:23:23.398: INFO: Number of nodes with available pods: 0
Aug 22 10:23:23.398: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 10:23:24.398: INFO: Number of nodes with available pods: 0
Aug 22 10:23:24.398: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 10:23:25.398: INFO: Number of nodes with available pods: 1
Aug 22 10:23:25.398: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3211, will wait for the garbage collector to delete the pods
Aug 22 10:23:25.455: INFO: Deleting DaemonSet.extensions daemon-set took: 2.606562ms
Aug 22 10:23:25.556: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.915528ms
Aug 22 10:23:28.461: INFO: Number of nodes with available pods: 0
Aug 22 10:23:28.461: INFO: Number of running nodes: 0, number of available pods: 0
Aug 22 10:23:28.462: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16325"},"items":null}

Aug 22 10:23:28.463: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16325"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:23:28.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3211" for this suite.

• [SLOW TEST:8.151 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":169,"skipped":2845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:23:28.476: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-b1d0f5ad-9360-4ef9-9f05-2b15cd7ef64d
STEP: Creating a pod to test consume configMaps
Aug 22 10:23:28.499: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ef393713-864c-4851-858f-ecf73e966dca" in namespace "projected-6069" to be "Succeeded or Failed"
Aug 22 10:23:28.500: INFO: Pod "pod-projected-configmaps-ef393713-864c-4851-858f-ecf73e966dca": Phase="Pending", Reason="", readiness=false. Elapsed: 1.36635ms
Aug 22 10:23:30.506: INFO: Pod "pod-projected-configmaps-ef393713-864c-4851-858f-ecf73e966dca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006754011s
STEP: Saw pod success
Aug 22 10:23:30.506: INFO: Pod "pod-projected-configmaps-ef393713-864c-4851-858f-ecf73e966dca" satisfied condition "Succeeded or Failed"
Aug 22 10:23:30.507: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-configmaps-ef393713-864c-4851-858f-ecf73e966dca container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:23:30.516: INFO: Waiting for pod pod-projected-configmaps-ef393713-864c-4851-858f-ecf73e966dca to disappear
Aug 22 10:23:30.517: INFO: Pod pod-projected-configmaps-ef393713-864c-4851-858f-ecf73e966dca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:23:30.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6069" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":170,"skipped":2880,"failed":0}

------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:23:30.521: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Aug 22 10:23:30.543: INFO: Waiting up to 5m0s for pod "test-pod-0d9f45b5-fcf9-45d4-9796-e178f8f180f1" in namespace "svcaccounts-6275" to be "Succeeded or Failed"
Aug 22 10:23:30.545: INFO: Pod "test-pod-0d9f45b5-fcf9-45d4-9796-e178f8f180f1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.538476ms
Aug 22 10:23:32.549: INFO: Pod "test-pod-0d9f45b5-fcf9-45d4-9796-e178f8f180f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005199808s
STEP: Saw pod success
Aug 22 10:23:32.549: INFO: Pod "test-pod-0d9f45b5-fcf9-45d4-9796-e178f8f180f1" satisfied condition "Succeeded or Failed"
Aug 22 10:23:32.550: INFO: Trying to get logs from node 172.23.79.105 pod test-pod-0d9f45b5-fcf9-45d4-9796-e178f8f180f1 container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:23:32.558: INFO: Waiting for pod test-pod-0d9f45b5-fcf9-45d4-9796-e178f8f180f1 to disappear
Aug 22 10:23:32.560: INFO: Pod test-pod-0d9f45b5-fcf9-45d4-9796-e178f8f180f1 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:23:32.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6275" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":171,"skipped":2880,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:23:32.564: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:23:32.580: INFO: Creating simple deployment test-new-deployment
Aug 22 10:23:32.586: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Aug 22 10:23:34.614: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-7706  d728d7c9-7fad-4484-a8e7-1a6516b71c97 16425 3 2021-08-22 10:23:32 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2021-08-22 10:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 10:23:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004746528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-08-22 10:23:33 +0000 UTC,LastTransitionTime:2021-08-22 10:23:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-08-22 10:23:33 +0000 UTC,LastTransitionTime:2021-08-22 10:23:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 22 10:23:34.618: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-7706  6133a70a-c705-42a1-b05d-0cc4b180947c 16433 3 2021-08-22 10:23:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d728d7c9-7fad-4484-a8e7-1a6516b71c97 0xc004746987 0xc004746988}] []  [{kube-controller-manager Update apps/v1 2021-08-22 10:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d728d7c9-7fad-4484-a8e7-1a6516b71c97\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-08-22 10:23:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004746a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 22 10:23:34.621: INFO: Pod "test-new-deployment-847dcfb7fb-9qwpb" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-9qwpb test-new-deployment-847dcfb7fb- deployment-7706  bebe10c8-4e9c-4bae-839d-c5949eb8c276 16401 0 2021-08-22 10:23:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 6133a70a-c705-42a1-b05d-0cc4b180947c 0xc004746dc7 0xc004746dc8}] []  [{kube-controller-manager Update v1 2021-08-22 10:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6133a70a-c705-42a1-b05d-0cc4b180947c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-08-22 10:23:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhxh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhxh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:23:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:23:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:23:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.79.105,PodIP:10.244.2.174,StartTime:2021-08-22 10:23:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-08-22 10:23:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://6c868978f924d629d07abbde88d8a6b0025e46e831c8b48f3f0f657b47d79f1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 10:23:34.621: INFO: Pod "test-new-deployment-847dcfb7fb-pdwxx" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-pdwxx test-new-deployment-847dcfb7fb- deployment-7706  9b27f35d-1058-451f-b972-1a4dc5e9056f 16428 0 2021-08-22 10:23:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 6133a70a-c705-42a1-b05d-0cc4b180947c 0xc004746fb7 0xc004746fb8}] []  [{kube-controller-manager Update v1 2021-08-22 10:23:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6133a70a-c705-42a1-b05d-0cc4b180947c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6w48n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6w48n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:172.23.79.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-08-22 10:23:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 10:23:34.622: INFO: Pod "test-new-deployment-847dcfb7fb-pn8zz" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-pn8zz test-new-deployment-847dcfb7fb- deployment-7706  7dfbef66-cce3-424b-b03b-7902bf2babd7 16435 0 2021-08-22 10:23:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 6133a70a-c705-42a1-b05d-0cc4b180947c 0xc004747120 0xc004747121}] []  [{kube-controller-manager Update v1 2021-08-22 10:23:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6133a70a-c705-42a1-b05d-0cc4b180947c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hj4bt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hj4bt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:23:34.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7706" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":172,"skipped":2881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:23:34.633: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-cf699134-f74e-4410-9986-d8bd43ca62d1
STEP: Creating a pod to test consume configMaps
Aug 22 10:23:34.653: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d716e671-93fb-4d4f-818d-541207bd49af" in namespace "projected-9425" to be "Succeeded or Failed"
Aug 22 10:23:34.654: INFO: Pod "pod-projected-configmaps-d716e671-93fb-4d4f-818d-541207bd49af": Phase="Pending", Reason="", readiness=false. Elapsed: 1.266805ms
Aug 22 10:23:36.659: INFO: Pod "pod-projected-configmaps-d716e671-93fb-4d4f-818d-541207bd49af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005881279s
STEP: Saw pod success
Aug 22 10:23:36.659: INFO: Pod "pod-projected-configmaps-d716e671-93fb-4d4f-818d-541207bd49af" satisfied condition "Succeeded or Failed"
Aug 22 10:23:36.660: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-configmaps-d716e671-93fb-4d4f-818d-541207bd49af container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:23:36.668: INFO: Waiting for pod pod-projected-configmaps-d716e671-93fb-4d4f-818d-541207bd49af to disappear
Aug 22 10:23:36.669: INFO: Pod pod-projected-configmaps-d716e671-93fb-4d4f-818d-541207bd49af no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:23:36.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9425" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":173,"skipped":2929,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:23:36.673: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-a8306bc4-aa60-4f08-bdfb-0e80833a5f10
STEP: Creating a pod to test consume configMaps
Aug 22 10:23:36.694: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-706c25c2-53e3-43a6-a751-a4b95c9b3cef" in namespace "projected-2076" to be "Succeeded or Failed"
Aug 22 10:23:36.695: INFO: Pod "pod-projected-configmaps-706c25c2-53e3-43a6-a751-a4b95c9b3cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.235855ms
Aug 22 10:23:38.700: INFO: Pod "pod-projected-configmaps-706c25c2-53e3-43a6-a751-a4b95c9b3cef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005965202s
STEP: Saw pod success
Aug 22 10:23:38.700: INFO: Pod "pod-projected-configmaps-706c25c2-53e3-43a6-a751-a4b95c9b3cef" satisfied condition "Succeeded or Failed"
Aug 22 10:23:38.701: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-configmaps-706c25c2-53e3-43a6-a751-a4b95c9b3cef container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:23:38.709: INFO: Waiting for pod pod-projected-configmaps-706c25c2-53e3-43a6-a751-a4b95c9b3cef to disappear
Aug 22 10:23:38.710: INFO: Pod pod-projected-configmaps-706c25c2-53e3-43a6-a751-a4b95c9b3cef no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:23:38.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2076" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":174,"skipped":2934,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:23:38.714: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:23:38.734: INFO: Creating ReplicaSet my-hostname-basic-eb65d3f7-a7e6-41ed-bd46-a081dfa4df5d
Aug 22 10:23:38.738: INFO: Pod name my-hostname-basic-eb65d3f7-a7e6-41ed-bd46-a081dfa4df5d: Found 0 pods out of 1
Aug 22 10:23:43.744: INFO: Pod name my-hostname-basic-eb65d3f7-a7e6-41ed-bd46-a081dfa4df5d: Found 1 pods out of 1
Aug 22 10:23:43.744: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-eb65d3f7-a7e6-41ed-bd46-a081dfa4df5d" is running
Aug 22 10:23:43.745: INFO: Pod "my-hostname-basic-eb65d3f7-a7e6-41ed-bd46-a081dfa4df5d-w7rdw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-08-22 10:23:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-08-22 10:23:40 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-08-22 10:23:40 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-08-22 10:23:38 +0000 UTC Reason: Message:}])
Aug 22 10:23:43.745: INFO: Trying to dial the pod
Aug 22 10:23:48.755: INFO: Controller my-hostname-basic-eb65d3f7-a7e6-41ed-bd46-a081dfa4df5d: Got expected result from replica 1 [my-hostname-basic-eb65d3f7-a7e6-41ed-bd46-a081dfa4df5d-w7rdw]: "my-hostname-basic-eb65d3f7-a7e6-41ed-bd46-a081dfa4df5d-w7rdw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:23:48.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7653" for this suite.

• [SLOW TEST:10.046 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":175,"skipped":2937,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:23:48.760: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-466
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-466 to expose endpoints map[]
Aug 22 10:23:48.797: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Aug 22 10:23:49.803: INFO: successfully validated that service multi-endpoint-test in namespace services-466 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-466
Aug 22 10:23:49.809: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:23:51.812: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-466 to expose endpoints map[pod1:[100]]
Aug 22 10:23:51.818: INFO: successfully validated that service multi-endpoint-test in namespace services-466 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-466
Aug 22 10:23:51.823: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:23:53.828: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-466 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 22 10:23:53.836: INFO: successfully validated that service multi-endpoint-test in namespace services-466 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Aug 22 10:23:53.836: INFO: Creating new exec pod
Aug 22 10:23:56.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-466 exec execpodw66mg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Aug 22 10:23:56.945: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 22 10:23:56.945: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:23:56.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-466 exec execpodw66mg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.239.62 80'
Aug 22 10:23:57.042: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.239.62 80\nConnection to 10.102.239.62 80 port [tcp/http] succeeded!\n"
Aug 22 10:23:57.042: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:23:57.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-466 exec execpodw66mg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Aug 22 10:23:57.139: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 22 10:23:57.139: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:23:57.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-466 exec execpodw66mg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.239.62 81'
Aug 22 10:23:57.235: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.239.62 81\nConnection to 10.102.239.62 81 port [tcp/*] succeeded!\n"
Aug 22 10:23:57.235: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-466
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-466 to expose endpoints map[pod2:[101]]
Aug 22 10:23:57.252: INFO: successfully validated that service multi-endpoint-test in namespace services-466 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-466
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-466 to expose endpoints map[]
Aug 22 10:23:57.264: INFO: successfully validated that service multi-endpoint-test in namespace services-466 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:23:57.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-466" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.515 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":176,"skipped":2945,"failed":0}
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:23:57.276: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:24:01.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5867" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":2947,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:24:01.305: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2641, will wait for the garbage collector to delete the pods
Aug 22 10:24:03.383: INFO: Deleting Job.batch foo took: 3.551866ms
Aug 22 10:24:03.484: INFO: Terminating Job.batch foo pods took: 100.453454ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:24:36.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2641" for this suite.

• [SLOW TEST:34.991 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":178,"skipped":2950,"failed":0}
S
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:24:36.296: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Aug 22 10:24:38.339: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:24:40.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-171" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":179,"skipped":2951,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:24:40.349: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 10:24:40.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e810b0c2-df06-4670-a18b-679f2f84851b" in namespace "downward-api-4045" to be "Succeeded or Failed"
Aug 22 10:24:40.369: INFO: Pod "downwardapi-volume-e810b0c2-df06-4670-a18b-679f2f84851b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.603304ms
Aug 22 10:24:42.373: INFO: Pod "downwardapi-volume-e810b0c2-df06-4670-a18b-679f2f84851b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005852719s
STEP: Saw pod success
Aug 22 10:24:42.374: INFO: Pod "downwardapi-volume-e810b0c2-df06-4670-a18b-679f2f84851b" satisfied condition "Succeeded or Failed"
Aug 22 10:24:42.375: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-e810b0c2-df06-4670-a18b-679f2f84851b container client-container: <nil>
STEP: delete the pod
Aug 22 10:24:42.383: INFO: Waiting for pod downwardapi-volume-e810b0c2-df06-4670-a18b-679f2f84851b to disappear
Aug 22 10:24:42.385: INFO: Pod downwardapi-volume-e810b0c2-df06-4670-a18b-679f2f84851b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:24:42.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4045" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":180,"skipped":2970,"failed":0}

------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:24:42.388: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Aug 22 10:24:42.406: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Aug 22 10:24:42.408: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 22 10:24:42.408: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Aug 22 10:24:42.412: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 22 10:24:42.412: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Aug 22 10:24:42.417: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 22 10:24:42.417: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Aug 22 10:24:49.446: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:24:49.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4010" for this suite.

• [SLOW TEST:7.068 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":181,"skipped":2970,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:24:49.457: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:24:55.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9635" for this suite.
STEP: Destroying namespace "nsdeletetest-8544" for this suite.
Aug 22 10:24:55.516: INFO: Namespace nsdeletetest-8544 was already deleted
STEP: Destroying namespace "nsdeletetest-9178" for this suite.

• [SLOW TEST:6.060 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":182,"skipped":3023,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:24:55.517: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:24:57.541: INFO: Deleting pod "var-expansion-e107f7c4-9450-4f8c-90e2-9a0e7f9c7853" in namespace "var-expansion-4634"
Aug 22 10:24:57.543: INFO: Wait up to 5m0s for pod "var-expansion-e107f7c4-9450-4f8c-90e2-9a0e7f9c7853" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:24:59.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4634" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":183,"skipped":3056,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:24:59.553: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:24:59.571: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 22 10:24:59.576: INFO: The status of Pod pod-exec-websocket-a7bae28d-4889-4d3f-b9f8-ae0c9ddc47ee is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:25:01.578: INFO: The status of Pod pod-exec-websocket-a7bae28d-4889-4d3f-b9f8-ae0c9ddc47ee is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:01.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5039" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":184,"skipped":3106,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:01.636: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 10:25:01.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-486ad64f-4407-4862-8992-3432cb46b95c" in namespace "downward-api-1965" to be "Succeeded or Failed"
Aug 22 10:25:01.654: INFO: Pod "downwardapi-volume-486ad64f-4407-4862-8992-3432cb46b95c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.121602ms
Aug 22 10:25:03.659: INFO: Pod "downwardapi-volume-486ad64f-4407-4862-8992-3432cb46b95c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005653611s
STEP: Saw pod success
Aug 22 10:25:03.659: INFO: Pod "downwardapi-volume-486ad64f-4407-4862-8992-3432cb46b95c" satisfied condition "Succeeded or Failed"
Aug 22 10:25:03.660: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-486ad64f-4407-4862-8992-3432cb46b95c container client-container: <nil>
STEP: delete the pod
Aug 22 10:25:03.669: INFO: Waiting for pod downwardapi-volume-486ad64f-4407-4862-8992-3432cb46b95c to disappear
Aug 22 10:25:03.670: INFO: Pod downwardapi-volume-486ad64f-4407-4862-8992-3432cb46b95c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:03.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1965" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":185,"skipped":3131,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:03.673: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-26e7ec33-ff61-491c-a32f-5beff769607a
STEP: Creating a pod to test consume secrets
Aug 22 10:25:03.695: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dbf32b6f-530e-4ec7-a4e4-0f92ec9c2d22" in namespace "projected-3242" to be "Succeeded or Failed"
Aug 22 10:25:03.697: INFO: Pod "pod-projected-secrets-dbf32b6f-530e-4ec7-a4e4-0f92ec9c2d22": Phase="Pending", Reason="", readiness=false. Elapsed: 1.371857ms
Aug 22 10:25:05.700: INFO: Pod "pod-projected-secrets-dbf32b6f-530e-4ec7-a4e4-0f92ec9c2d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004735416s
STEP: Saw pod success
Aug 22 10:25:05.700: INFO: Pod "pod-projected-secrets-dbf32b6f-530e-4ec7-a4e4-0f92ec9c2d22" satisfied condition "Succeeded or Failed"
Aug 22 10:25:05.701: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-secrets-dbf32b6f-530e-4ec7-a4e4-0f92ec9c2d22 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 22 10:25:05.710: INFO: Waiting for pod pod-projected-secrets-dbf32b6f-530e-4ec7-a4e4-0f92ec9c2d22 to disappear
Aug 22 10:25:05.711: INFO: Pod pod-projected-secrets-dbf32b6f-530e-4ec7-a4e4-0f92ec9c2d22 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:05.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3242" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":186,"skipped":3151,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:05.715: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 22 10:25:06.742: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:06.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7574" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":187,"skipped":3157,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:06.750: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6392
STEP: creating service affinity-nodeport-transition in namespace services-6392
STEP: creating replication controller affinity-nodeport-transition in namespace services-6392
I0822 10:25:06.772114      21 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-6392, replica count: 3
I0822 10:25:09.823110      21 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 10:25:09.831: INFO: Creating new exec pod
Aug 22 10:25:12.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-6392 exec execpod-affinityjbvwf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Aug 22 10:25:12.948: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 22 10:25:12.948: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:25:12.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-6392 exec execpod-affinityjbvwf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.217.181 80'
Aug 22 10:25:13.046: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.217.181 80\nConnection to 10.111.217.181 80 port [tcp/http] succeeded!\n"
Aug 22 10:25:13.046: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:25:13.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-6392 exec execpod-affinityjbvwf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.23.79.103 30737'
Aug 22 10:25:13.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.23.79.103 30737\nConnection to 172.23.79.103 30737 port [tcp/*] succeeded!\n"
Aug 22 10:25:13.145: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:25:13.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-6392 exec execpod-affinityjbvwf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.23.79.105 30737'
Aug 22 10:25:13.243: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.23.79.105 30737\nConnection to 172.23.79.105 30737 port [tcp/*] succeeded!\n"
Aug 22 10:25:13.243: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:25:13.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-6392 exec execpod-affinityjbvwf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.23.79.103:30737/ ; done'
Aug 22 10:25:13.408: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n"
Aug 22 10:25:13.409: INFO: stdout: "\naffinity-nodeport-transition-r2g8q\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-r2g8q\naffinity-nodeport-transition-hnvvq\naffinity-nodeport-transition-hnvvq\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-r2g8q\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-r2g8q\naffinity-nodeport-transition-r2g8q\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-r2g8q\naffinity-nodeport-transition-r2g8q\naffinity-nodeport-transition-r2g8q\naffinity-nodeport-transition-r2g8q\naffinity-nodeport-transition-r2g8q"
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-r2g8q
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-r2g8q
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-hnvvq
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-hnvvq
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-r2g8q
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-r2g8q
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-r2g8q
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-r2g8q
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-r2g8q
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-r2g8q
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-r2g8q
Aug 22 10:25:13.409: INFO: Received response from host: affinity-nodeport-transition-r2g8q
Aug 22 10:25:13.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-6392 exec execpod-affinityjbvwf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.23.79.103:30737/ ; done'
Aug 22 10:25:13.569: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.23.79.103:30737/\n"
Aug 22 10:25:13.569: INFO: stdout: "\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg\naffinity-nodeport-transition-p2dzg"
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Received response from host: affinity-nodeport-transition-p2dzg
Aug 22 10:25:13.569: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6392, will wait for the garbage collector to delete the pods
Aug 22 10:25:13.629: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.813293ms
Aug 22 10:25:13.730: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.94245ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:16.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6392" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.397 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":188,"skipped":3160,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:16.147: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 22 10:25:18.175: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:18.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8450" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":189,"skipped":3176,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:18.184: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:20.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1171" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":190,"skipped":3217,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:20.216: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-615
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-615
I0822 10:25:20.243979      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-615, replica count: 2
Aug 22 10:25:23.295: INFO: Creating new exec pod
I0822 10:25:23.295190      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 10:25:26.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-615 exec execpod2w9vw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 22 10:25:26.409: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 22 10:25:26.409: INFO: stdout: "externalname-service-rm9mw"
Aug 22 10:25:26.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-615 exec execpod2w9vw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.245.164 80'
Aug 22 10:25:26.511: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.245.164 80\nConnection to 10.99.245.164 80 port [tcp/http] succeeded!\n"
Aug 22 10:25:26.511: INFO: stdout: "externalname-service-9zxcg"
Aug 22 10:25:26.511: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:26.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-615" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:6.307 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":191,"skipped":3233,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:26.523: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 22 10:25:26.540: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 22 10:25:26.544: INFO: Waiting for terminating namespaces to be deleted...
Aug 22 10:25:26.545: INFO: 
Logging pods the apiserver thinks is on node 172.23.79.103 before test
Aug 22 10:25:26.548: INFO: kube-flannel-ds-s57gz from kube-system started at 2021-08-22 09:15:14 +0000 UTC (1 container statuses recorded)
Aug 22 10:25:26.548: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 10:25:26.548: INFO: kube-proxy-r9sql from kube-system started at 2021-08-22 09:14:45 +0000 UTC (1 container statuses recorded)
Aug 22 10:25:26.548: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 22 10:25:26.548: INFO: sonobuoy from sonobuoy started at 2021-08-22 09:27:55 +0000 UTC (1 container statuses recorded)
Aug 22 10:25:26.548: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 22 10:25:26.548: INFO: sonobuoy-e2e-job-75357183cd8142cd from sonobuoy started at 2021-08-22 09:27:56 +0000 UTC (2 container statuses recorded)
Aug 22 10:25:26.548: INFO: 	Container e2e ready: true, restart count 0
Aug 22 10:25:26.548: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 10:25:26.548: INFO: sonobuoy-systemd-logs-daemon-set-d3d945f99a484f42-ggzhp from sonobuoy started at 2021-08-22 09:27:56 +0000 UTC (2 container statuses recorded)
Aug 22 10:25:26.548: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 10:25:26.548: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 22 10:25:26.548: INFO: 
Logging pods the apiserver thinks is on node 172.23.79.105 before test
Aug 22 10:25:26.551: INFO: kube-flannel-ds-q9ltg from kube-system started at 2021-08-22 10:15:14 +0000 UTC (1 container statuses recorded)
Aug 22 10:25:26.551: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 10:25:26.551: INFO: kube-proxy-zhmln from kube-system started at 2021-08-22 09:39:54 +0000 UTC (1 container statuses recorded)
Aug 22 10:25:26.551: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 22 10:25:26.551: INFO: pod-exec-websocket-a7bae28d-4889-4d3f-b9f8-ae0c9ddc47ee from pods-5039 started at 2021-08-22 10:24:59 +0000 UTC (1 container statuses recorded)
Aug 22 10:25:26.551: INFO: 	Container main ready: true, restart count 0
Aug 22 10:25:26.551: INFO: execpod2w9vw from services-615 started at 2021-08-22 10:25:23 +0000 UTC (1 container statuses recorded)
Aug 22 10:25:26.551: INFO: 	Container agnhost-container ready: true, restart count 0
Aug 22 10:25:26.551: INFO: externalname-service-9zxcg from services-615 started at 2021-08-22 10:25:20 +0000 UTC (1 container statuses recorded)
Aug 22 10:25:26.551: INFO: 	Container externalname-service ready: true, restart count 0
Aug 22 10:25:26.551: INFO: externalname-service-rm9mw from services-615 started at 2021-08-22 10:25:20 +0000 UTC (1 container statuses recorded)
Aug 22 10:25:26.551: INFO: 	Container externalname-service ready: true, restart count 0
Aug 22 10:25:26.551: INFO: sonobuoy-systemd-logs-daemon-set-d3d945f99a484f42-8k4l9 from sonobuoy started at 2021-08-22 09:39:54 +0000 UTC (2 container statuses recorded)
Aug 22 10:25:26.551: INFO: 	Container sonobuoy-worker ready: false, restart count 13
Aug 22 10:25:26.551: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node 172.23.79.103
STEP: verifying the node has the label node 172.23.79.105
Aug 22 10:25:26.570: INFO: Pod kube-flannel-ds-q9ltg requesting resource cpu=100m on Node 172.23.79.105
Aug 22 10:25:26.570: INFO: Pod kube-flannel-ds-s57gz requesting resource cpu=100m on Node 172.23.79.103
Aug 22 10:25:26.570: INFO: Pod kube-proxy-r9sql requesting resource cpu=0m on Node 172.23.79.103
Aug 22 10:25:26.570: INFO: Pod kube-proxy-zhmln requesting resource cpu=0m on Node 172.23.79.105
Aug 22 10:25:26.570: INFO: Pod pod-exec-websocket-a7bae28d-4889-4d3f-b9f8-ae0c9ddc47ee requesting resource cpu=0m on Node 172.23.79.105
Aug 22 10:25:26.570: INFO: Pod execpod2w9vw requesting resource cpu=0m on Node 172.23.79.105
Aug 22 10:25:26.570: INFO: Pod externalname-service-9zxcg requesting resource cpu=0m on Node 172.23.79.105
Aug 22 10:25:26.570: INFO: Pod externalname-service-rm9mw requesting resource cpu=0m on Node 172.23.79.105
Aug 22 10:25:26.570: INFO: Pod sonobuoy requesting resource cpu=0m on Node 172.23.79.103
Aug 22 10:25:26.570: INFO: Pod sonobuoy-e2e-job-75357183cd8142cd requesting resource cpu=0m on Node 172.23.79.103
Aug 22 10:25:26.570: INFO: Pod sonobuoy-systemd-logs-daemon-set-d3d945f99a484f42-8k4l9 requesting resource cpu=0m on Node 172.23.79.105
Aug 22 10:25:26.570: INFO: Pod sonobuoy-systemd-logs-daemon-set-d3d945f99a484f42-ggzhp requesting resource cpu=0m on Node 172.23.79.103
STEP: Starting Pods to consume most of the cluster CPU.
Aug 22 10:25:26.570: INFO: Creating a pod which consumes cpu=2730m on Node 172.23.79.103
Aug 22 10:25:26.575: INFO: Creating a pod which consumes cpu=5530m on Node 172.23.79.105
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-062f6777-80a5-4456-8063-9efaad462981.169d99f70fc7bdf9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8785/filler-pod-062f6777-80a5-4456-8063-9efaad462981 to 172.23.79.103]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-062f6777-80a5-4456-8063-9efaad462981.169d99f7324069c4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-062f6777-80a5-4456-8063-9efaad462981.169d99f73522d395], Reason = [Created], Message = [Created container filler-pod-062f6777-80a5-4456-8063-9efaad462981]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-062f6777-80a5-4456-8063-9efaad462981.169d99f73acada3a], Reason = [Started], Message = [Started container filler-pod-062f6777-80a5-4456-8063-9efaad462981]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c501a481-7bc2-4ccf-b110-9ad8f3a88e07.169d99f70fe76c00], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8785/filler-pod-c501a481-7bc2-4ccf-b110-9ad8f3a88e07 to 172.23.79.105]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c501a481-7bc2-4ccf-b110-9ad8f3a88e07.169d99f732b1239e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c501a481-7bc2-4ccf-b110-9ad8f3a88e07.169d99f735885c83], Reason = [Created], Message = [Created container filler-pod-c501a481-7bc2-4ccf-b110-9ad8f3a88e07]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c501a481-7bc2-4ccf-b110-9ad8f3a88e07.169d99f73a912d4e], Reason = [Started], Message = [Started container filler-pod-c501a481-7bc2-4ccf-b110-9ad8f3a88e07]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.169d99f787d447d2], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node 172.23.79.103
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 172.23.79.105
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:29.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8785" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":192,"skipped":3247,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:29.613: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 10:25:29.630: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c85468f-dcc5-451a-8747-129e074e33ed" in namespace "downward-api-2408" to be "Succeeded or Failed"
Aug 22 10:25:29.632: INFO: Pod "downwardapi-volume-3c85468f-dcc5-451a-8747-129e074e33ed": Phase="Pending", Reason="", readiness=false. Elapsed: 1.307497ms
Aug 22 10:25:31.634: INFO: Pod "downwardapi-volume-3c85468f-dcc5-451a-8747-129e074e33ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003855598s
STEP: Saw pod success
Aug 22 10:25:31.634: INFO: Pod "downwardapi-volume-3c85468f-dcc5-451a-8747-129e074e33ed" satisfied condition "Succeeded or Failed"
Aug 22 10:25:31.636: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-3c85468f-dcc5-451a-8747-129e074e33ed container client-container: <nil>
STEP: delete the pod
Aug 22 10:25:31.646: INFO: Waiting for pod downwardapi-volume-3c85468f-dcc5-451a-8747-129e074e33ed to disappear
Aug 22 10:25:31.647: INFO: Pod downwardapi-volume-3c85468f-dcc5-451a-8747-129e074e33ed no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:31.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2408" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":193,"skipped":3283,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:31.651: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-1b96f202-18cc-44a0-9ecd-08a6c643f0c9
STEP: Creating a pod to test consume configMaps
Aug 22 10:25:31.670: INFO: Waiting up to 5m0s for pod "pod-configmaps-8d7c4fee-9e8a-44b0-b8a6-0db1c205dce4" in namespace "configmap-489" to be "Succeeded or Failed"
Aug 22 10:25:31.671: INFO: Pod "pod-configmaps-8d7c4fee-9e8a-44b0-b8a6-0db1c205dce4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.494302ms
Aug 22 10:25:33.673: INFO: Pod "pod-configmaps-8d7c4fee-9e8a-44b0-b8a6-0db1c205dce4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003797949s
STEP: Saw pod success
Aug 22 10:25:33.673: INFO: Pod "pod-configmaps-8d7c4fee-9e8a-44b0-b8a6-0db1c205dce4" satisfied condition "Succeeded or Failed"
Aug 22 10:25:33.675: INFO: Trying to get logs from node 172.23.79.105 pod pod-configmaps-8d7c4fee-9e8a-44b0-b8a6-0db1c205dce4 container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:25:33.683: INFO: Waiting for pod pod-configmaps-8d7c4fee-9e8a-44b0-b8a6-0db1c205dce4 to disappear
Aug 22 10:25:33.684: INFO: Pod pod-configmaps-8d7c4fee-9e8a-44b0-b8a6-0db1c205dce4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:33.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-489" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":194,"skipped":3328,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:33.688: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 10:25:33.705: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72fc2d61-bcc8-4641-bd92-a72bf2ced8c8" in namespace "projected-9787" to be "Succeeded or Failed"
Aug 22 10:25:33.707: INFO: Pod "downwardapi-volume-72fc2d61-bcc8-4641-bd92-a72bf2ced8c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.683197ms
Aug 22 10:25:35.712: INFO: Pod "downwardapi-volume-72fc2d61-bcc8-4641-bd92-a72bf2ced8c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006788708s
STEP: Saw pod success
Aug 22 10:25:35.712: INFO: Pod "downwardapi-volume-72fc2d61-bcc8-4641-bd92-a72bf2ced8c8" satisfied condition "Succeeded or Failed"
Aug 22 10:25:35.714: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-72fc2d61-bcc8-4641-bd92-a72bf2ced8c8 container client-container: <nil>
STEP: delete the pod
Aug 22 10:25:35.722: INFO: Waiting for pod downwardapi-volume-72fc2d61-bcc8-4641-bd92-a72bf2ced8c8 to disappear
Aug 22 10:25:35.723: INFO: Pod downwardapi-volume-72fc2d61-bcc8-4641-bd92-a72bf2ced8c8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:35.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9787" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":195,"skipped":3333,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:35.727: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:25:35.745: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 22 10:25:40.753: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Aug 22 10:25:40.757: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Aug 22 10:25:40.762: INFO: observed ReplicaSet test-rs in namespace replicaset-6325 with ReadyReplicas 1, AvailableReplicas 1
Aug 22 10:25:40.768: INFO: observed ReplicaSet test-rs in namespace replicaset-6325 with ReadyReplicas 1, AvailableReplicas 1
Aug 22 10:25:40.774: INFO: observed ReplicaSet test-rs in namespace replicaset-6325 with ReadyReplicas 1, AvailableReplicas 1
Aug 22 10:25:40.781: INFO: observed ReplicaSet test-rs in namespace replicaset-6325 with ReadyReplicas 1, AvailableReplicas 1
Aug 22 10:25:42.403: INFO: observed ReplicaSet test-rs in namespace replicaset-6325 with ReadyReplicas 2, AvailableReplicas 2
Aug 22 10:25:42.409: INFO: observed Replicaset test-rs in namespace replicaset-6325 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:42.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6325" for this suite.

• [SLOW TEST:6.688 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":196,"skipped":3347,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:42.415: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Aug 22 10:25:42.432: INFO: The status of Pod annotationupdate49b769bc-e4a7-4fbd-a428-68019e65b0e3 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:25:44.438: INFO: The status of Pod annotationupdate49b769bc-e4a7-4fbd-a428-68019e65b0e3 is Running (Ready = true)
Aug 22 10:25:44.951: INFO: Successfully updated pod "annotationupdate49b769bc-e4a7-4fbd-a428-68019e65b0e3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:48.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7546" for this suite.

• [SLOW TEST:6.561 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":197,"skipped":3348,"failed":0}
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:48.976: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:25:49.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6779" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":198,"skipped":3348,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:25:49.006: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-4434
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4434
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4434
Aug 22 10:25:49.028: INFO: Found 0 stateful pods, waiting for 1
Aug 22 10:25:59.035: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 22 10:25:59.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-4434 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 22 10:25:59.147: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 22 10:25:59.147: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 22 10:25:59.147: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 22 10:25:59.148: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 22 10:26:09.155: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 10:26:09.155: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 10:26:09.171: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999783s
Aug 22 10:26:10.175: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.989771666s
Aug 22 10:26:11.179: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985899991s
Aug 22 10:26:12.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980842036s
Aug 22 10:26:13.189: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976009611s
Aug 22 10:26:14.192: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.972041437s
Aug 22 10:26:15.197: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.968537948s
Aug 22 10:26:16.202: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.962927951s
Aug 22 10:26:17.205: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.958727496s
Aug 22 10:26:18.208: INFO: Verifying statefulset ss doesn't scale past 1 for another 956.137344ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4434
Aug 22 10:26:19.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-4434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 22 10:26:19.310: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 22 10:26:19.310: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 22 10:26:19.310: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 22 10:26:19.312: INFO: Found 1 stateful pods, waiting for 3
Aug 22 10:26:29.321: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 10:26:29.321: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 10:26:29.321: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 22 10:26:29.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-4434 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 22 10:26:29.418: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 22 10:26:29.418: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 22 10:26:29.418: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 22 10:26:29.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-4434 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 22 10:26:29.515: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 22 10:26:29.515: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 22 10:26:29.515: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 22 10:26:29.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-4434 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 22 10:26:29.612: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 22 10:26:29.612: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 22 10:26:29.612: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 22 10:26:29.612: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 10:26:29.614: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 22 10:26:39.629: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 10:26:39.629: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 10:26:39.629: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 10:26:39.641: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999794s
Aug 22 10:26:40.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992354881s
Aug 22 10:26:41.645: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990064222s
Aug 22 10:26:42.649: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987671826s
Aug 22 10:26:43.653: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984063574s
Aug 22 10:26:44.657: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979700594s
Aug 22 10:26:45.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975744888s
Aug 22 10:26:46.665: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971662815s
Aug 22 10:26:47.669: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967844068s
Aug 22 10:26:48.672: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.162756ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4434
Aug 22 10:26:49.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-4434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 22 10:26:49.778: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 22 10:26:49.778: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 22 10:26:49.778: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 22 10:26:49.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-4434 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 22 10:26:49.878: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 22 10:26:49.878: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 22 10:26:49.878: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 22 10:26:49.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-4434 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 22 10:26:49.972: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 22 10:26:49.972: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 22 10:26:49.972: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 22 10:26:49.972: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Aug 22 10:26:59.985: INFO: Deleting all statefulset in ns statefulset-4434
Aug 22 10:26:59.987: INFO: Scaling statefulset ss to 0
Aug 22 10:26:59.992: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 10:26:59.993: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:27:00.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4434" for this suite.

• [SLOW TEST:71.005 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":199,"skipped":3376,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:27:00.012: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-d494
STEP: Creating a pod to test atomic-volume-subpath
Aug 22 10:27:00.033: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d494" in namespace "subpath-9863" to be "Succeeded or Failed"
Aug 22 10:27:00.036: INFO: Pod "pod-subpath-test-configmap-d494": Phase="Pending", Reason="", readiness=false. Elapsed: 3.112534ms
Aug 22 10:27:02.040: INFO: Pod "pod-subpath-test-configmap-d494": Phase="Running", Reason="", readiness=true. Elapsed: 2.006697622s
Aug 22 10:27:04.045: INFO: Pod "pod-subpath-test-configmap-d494": Phase="Running", Reason="", readiness=true. Elapsed: 4.012125231s
Aug 22 10:27:06.049: INFO: Pod "pod-subpath-test-configmap-d494": Phase="Running", Reason="", readiness=true. Elapsed: 6.016466994s
Aug 22 10:27:08.053: INFO: Pod "pod-subpath-test-configmap-d494": Phase="Running", Reason="", readiness=true. Elapsed: 8.020584859s
Aug 22 10:27:10.059: INFO: Pod "pod-subpath-test-configmap-d494": Phase="Running", Reason="", readiness=true. Elapsed: 10.0261001s
Aug 22 10:27:12.063: INFO: Pod "pod-subpath-test-configmap-d494": Phase="Running", Reason="", readiness=true. Elapsed: 12.029845313s
Aug 22 10:27:14.068: INFO: Pod "pod-subpath-test-configmap-d494": Phase="Running", Reason="", readiness=true. Elapsed: 14.035092588s
Aug 22 10:27:16.072: INFO: Pod "pod-subpath-test-configmap-d494": Phase="Running", Reason="", readiness=true. Elapsed: 16.038879055s
Aug 22 10:27:18.076: INFO: Pod "pod-subpath-test-configmap-d494": Phase="Running", Reason="", readiness=true. Elapsed: 18.043390743s
Aug 22 10:27:20.082: INFO: Pod "pod-subpath-test-configmap-d494": Phase="Running", Reason="", readiness=true. Elapsed: 20.048830073s
Aug 22 10:27:22.086: INFO: Pod "pod-subpath-test-configmap-d494": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.052837007s
STEP: Saw pod success
Aug 22 10:27:22.086: INFO: Pod "pod-subpath-test-configmap-d494" satisfied condition "Succeeded or Failed"
Aug 22 10:27:22.087: INFO: Trying to get logs from node 172.23.79.105 pod pod-subpath-test-configmap-d494 container test-container-subpath-configmap-d494: <nil>
STEP: delete the pod
Aug 22 10:27:22.100: INFO: Waiting for pod pod-subpath-test-configmap-d494 to disappear
Aug 22 10:27:22.101: INFO: Pod pod-subpath-test-configmap-d494 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-d494
Aug 22 10:27:22.101: INFO: Deleting pod "pod-subpath-test-configmap-d494" in namespace "subpath-9863"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:27:22.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9863" for this suite.

• [SLOW TEST:22.094 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":200,"skipped":3422,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:27:22.106: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5145
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 22 10:27:22.120: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 22 10:27:22.132: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:27:24.137: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:27:26.138: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:27:28.137: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:27:30.138: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:27:32.135: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:27:34.138: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 22 10:27:34.140: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Aug 22 10:27:36.158: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 22 10:27:36.158: INFO: Breadth first check of 10.244.1.90 on host 172.23.79.103...
Aug 22 10:27:36.160: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.214:9080/dial?request=hostname&protocol=udp&host=10.244.1.90&port=8081&tries=1'] Namespace:pod-network-test-5145 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:27:36.160: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:27:36.212: INFO: Waiting for responses: map[]
Aug 22 10:27:36.212: INFO: reached 10.244.1.90 after 0/1 tries
Aug 22 10:27:36.212: INFO: Breadth first check of 10.244.2.213 on host 172.23.79.105...
Aug 22 10:27:36.214: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.214:9080/dial?request=hostname&protocol=udp&host=10.244.2.213&port=8081&tries=1'] Namespace:pod-network-test-5145 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:27:36.214: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:27:36.273: INFO: Waiting for responses: map[]
Aug 22 10:27:36.273: INFO: reached 10.244.2.213 after 0/1 tries
Aug 22 10:27:36.273: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:27:36.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5145" for this suite.

• [SLOW TEST:14.172 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":201,"skipped":3432,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:27:36.278: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Aug 22 10:27:36.299: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 22 10:27:41.307: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Aug 22 10:27:41.309: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:27:41.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1975" for this suite.

• [SLOW TEST:5.045 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":202,"skipped":3433,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:27:41.323: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Aug 22 10:27:41.349: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:27:43.354: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Aug 22 10:27:43.362: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:27:45.367: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 22 10:27:45.369: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4759 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:27:45.369: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:27:45.425: INFO: Exec stderr: ""
Aug 22 10:27:45.425: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4759 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:27:45.425: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:27:45.482: INFO: Exec stderr: ""
Aug 22 10:27:45.482: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4759 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:27:45.482: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:27:45.531: INFO: Exec stderr: ""
Aug 22 10:27:45.531: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4759 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:27:45.531: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:27:45.583: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 22 10:27:45.583: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4759 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:27:45.583: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:27:45.635: INFO: Exec stderr: ""
Aug 22 10:27:45.635: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4759 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:27:45.635: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:27:45.690: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 22 10:27:45.690: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4759 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:27:45.690: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:27:45.739: INFO: Exec stderr: ""
Aug 22 10:27:45.739: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4759 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:27:45.739: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:27:45.804: INFO: Exec stderr: ""
Aug 22 10:27:45.804: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4759 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:27:45.804: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:27:45.853: INFO: Exec stderr: ""
Aug 22 10:27:45.853: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4759 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:27:45.853: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:27:45.904: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:27:45.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4759" for this suite.
•{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":203,"skipped":3443,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:27:45.912: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 22 10:27:45.934: INFO: Waiting up to 5m0s for pod "pod-58090080-0d5d-4deb-9005-c770da061a6d" in namespace "emptydir-4444" to be "Succeeded or Failed"
Aug 22 10:27:45.935: INFO: Pod "pod-58090080-0d5d-4deb-9005-c770da061a6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.206602ms
Aug 22 10:27:47.941: INFO: Pod "pod-58090080-0d5d-4deb-9005-c770da061a6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00694505s
STEP: Saw pod success
Aug 22 10:27:47.941: INFO: Pod "pod-58090080-0d5d-4deb-9005-c770da061a6d" satisfied condition "Succeeded or Failed"
Aug 22 10:27:47.942: INFO: Trying to get logs from node 172.23.79.105 pod pod-58090080-0d5d-4deb-9005-c770da061a6d container test-container: <nil>
STEP: delete the pod
Aug 22 10:27:47.951: INFO: Waiting for pod pod-58090080-0d5d-4deb-9005-c770da061a6d to disappear
Aug 22 10:27:47.952: INFO: Pod pod-58090080-0d5d-4deb-9005-c770da061a6d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:27:47.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4444" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":204,"skipped":3456,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:27:47.956: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-8e7a1c29-750b-44c4-8bdc-885eaeb5ea3a
STEP: Creating a pod to test consume secrets
Aug 22 10:27:47.977: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-78a651a7-ee01-4300-a660-1b795998871a" in namespace "projected-3463" to be "Succeeded or Failed"
Aug 22 10:27:47.979: INFO: Pod "pod-projected-secrets-78a651a7-ee01-4300-a660-1b795998871a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.960717ms
Aug 22 10:27:49.985: INFO: Pod "pod-projected-secrets-78a651a7-ee01-4300-a660-1b795998871a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007207576s
STEP: Saw pod success
Aug 22 10:27:49.985: INFO: Pod "pod-projected-secrets-78a651a7-ee01-4300-a660-1b795998871a" satisfied condition "Succeeded or Failed"
Aug 22 10:27:49.986: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-secrets-78a651a7-ee01-4300-a660-1b795998871a container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 22 10:27:49.995: INFO: Waiting for pod pod-projected-secrets-78a651a7-ee01-4300-a660-1b795998871a to disappear
Aug 22 10:27:49.996: INFO: Pod pod-projected-secrets-78a651a7-ee01-4300-a660-1b795998871a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:27:49.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3463" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":205,"skipped":3484,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:27:49.999: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:27:50.017: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 22 10:27:52.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-769 --namespace=crd-publish-openapi-769 create -f -'
Aug 22 10:27:52.824: INFO: stderr: ""
Aug 22 10:27:52.824: INFO: stdout: "e2e-test-crd-publish-openapi-4841-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 22 10:27:52.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-769 --namespace=crd-publish-openapi-769 delete e2e-test-crd-publish-openapi-4841-crds test-cr'
Aug 22 10:27:52.869: INFO: stderr: ""
Aug 22 10:27:52.869: INFO: stdout: "e2e-test-crd-publish-openapi-4841-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 22 10:27:52.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-769 --namespace=crd-publish-openapi-769 apply -f -'
Aug 22 10:27:52.982: INFO: stderr: ""
Aug 22 10:27:52.982: INFO: stdout: "e2e-test-crd-publish-openapi-4841-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 22 10:27:52.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-769 --namespace=crd-publish-openapi-769 delete e2e-test-crd-publish-openapi-4841-crds test-cr'
Aug 22 10:27:53.027: INFO: stderr: ""
Aug 22 10:27:53.027: INFO: stdout: "e2e-test-crd-publish-openapi-4841-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 22 10:27:53.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-769 explain e2e-test-crd-publish-openapi-4841-crds'
Aug 22 10:27:53.135: INFO: stderr: ""
Aug 22 10:27:53.135: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4841-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:27:55.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-769" for this suite.

• [SLOW TEST:5.714 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":206,"skipped":3521,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:27:55.714: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug 22 10:27:56.765: INFO: The status of Pod kube-controller-manager-172.23.79.104 is Running (Ready = true)
Aug 22 10:27:56.909: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:27:56.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9557" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":207,"skipped":3542,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:27:56.913: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-5514
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-5514
Aug 22 10:27:56.932: INFO: Found 0 stateful pods, waiting for 1
Aug 22 10:28:06.941: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Aug 22 10:28:06.959: INFO: Deleting all statefulset in ns statefulset-5514
Aug 22 10:28:06.962: INFO: Scaling statefulset ss to 0
Aug 22 10:28:16.975: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 10:28:16.977: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:28:16.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5514" for this suite.

• [SLOW TEST:20.090 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":208,"skipped":3548,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:28:17.003: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Aug 22 10:28:17.022: INFO: Found Service test-service-66hv7 in namespace services-1013 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 22 10:28:17.022: INFO: Service test-service-66hv7 created
STEP: Getting /status
Aug 22 10:28:17.024: INFO: Service test-service-66hv7 has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Aug 22 10:28:17.027: INFO: observed Service test-service-66hv7 in namespace services-1013 with annotations: map[] & LoadBalancer: {[]}
Aug 22 10:28:17.027: INFO: Found Service test-service-66hv7 in namespace services-1013 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 22 10:28:17.027: INFO: Service test-service-66hv7 has service status patched
STEP: updating the ServiceStatus
Aug 22 10:28:17.030: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Aug 22 10:28:17.031: INFO: Observed Service test-service-66hv7 in namespace services-1013 with annotations: map[] & Conditions: {[]}
Aug 22 10:28:17.031: INFO: Observed event: &Service{ObjectMeta:{test-service-66hv7  services-1013  be82065c-3cf5-4f58-afa8-02915f769ea2 18739 0 2021-08-22 10:28:17 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2021-08-22 10:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2021-08-22 10:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.100.46.244,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.100.46.244],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 22 10:28:17.031: INFO: Found Service test-service-66hv7 in namespace services-1013 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 22 10:28:17.031: INFO: Service test-service-66hv7 has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Aug 22 10:28:17.040: INFO: observed Service test-service-66hv7 in namespace services-1013 with labels: map[test-service-static:true]
Aug 22 10:28:17.040: INFO: observed Service test-service-66hv7 in namespace services-1013 with labels: map[test-service-static:true]
Aug 22 10:28:17.040: INFO: observed Service test-service-66hv7 in namespace services-1013 with labels: map[test-service-static:true]
Aug 22 10:28:17.040: INFO: Found Service test-service-66hv7 in namespace services-1013 with labels: map[test-service:patched test-service-static:true]
Aug 22 10:28:17.040: INFO: Service test-service-66hv7 patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Aug 22 10:28:17.046: INFO: Observed event: ADDED
Aug 22 10:28:17.046: INFO: Observed event: MODIFIED
Aug 22 10:28:17.046: INFO: Observed event: MODIFIED
Aug 22 10:28:17.046: INFO: Observed event: MODIFIED
Aug 22 10:28:17.046: INFO: Found Service test-service-66hv7 in namespace services-1013 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 22 10:28:17.046: INFO: Service test-service-66hv7 deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:28:17.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1013" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":209,"skipped":3563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:28:17.050: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 22 10:28:17.071: INFO: Waiting up to 5m0s for pod "pod-93c1a440-e63d-4660-b430-b65f99cfe11a" in namespace "emptydir-9057" to be "Succeeded or Failed"
Aug 22 10:28:17.073: INFO: Pod "pod-93c1a440-e63d-4660-b430-b65f99cfe11a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058808ms
Aug 22 10:28:19.078: INFO: Pod "pod-93c1a440-e63d-4660-b430-b65f99cfe11a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007296885s
STEP: Saw pod success
Aug 22 10:28:19.078: INFO: Pod "pod-93c1a440-e63d-4660-b430-b65f99cfe11a" satisfied condition "Succeeded or Failed"
Aug 22 10:28:19.079: INFO: Trying to get logs from node 172.23.79.105 pod pod-93c1a440-e63d-4660-b430-b65f99cfe11a container test-container: <nil>
STEP: delete the pod
Aug 22 10:28:19.088: INFO: Waiting for pod pod-93c1a440-e63d-4660-b430-b65f99cfe11a to disappear
Aug 22 10:28:19.089: INFO: Pod pod-93c1a440-e63d-4660-b430-b65f99cfe11a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:28:19.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9057" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":210,"skipped":3600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:28:19.093: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-6489593e-5508-437b-8846-c1266a68b679
STEP: Creating a pod to test consume configMaps
Aug 22 10:28:19.114: INFO: Waiting up to 5m0s for pod "pod-configmaps-75ccc0e7-4425-4a97-9ebb-12f37709e950" in namespace "configmap-7786" to be "Succeeded or Failed"
Aug 22 10:28:19.116: INFO: Pod "pod-configmaps-75ccc0e7-4425-4a97-9ebb-12f37709e950": Phase="Pending", Reason="", readiness=false. Elapsed: 1.80711ms
Aug 22 10:28:21.122: INFO: Pod "pod-configmaps-75ccc0e7-4425-4a97-9ebb-12f37709e950": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007977968s
STEP: Saw pod success
Aug 22 10:28:21.122: INFO: Pod "pod-configmaps-75ccc0e7-4425-4a97-9ebb-12f37709e950" satisfied condition "Succeeded or Failed"
Aug 22 10:28:21.124: INFO: Trying to get logs from node 172.23.79.105 pod pod-configmaps-75ccc0e7-4425-4a97-9ebb-12f37709e950 container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:28:21.132: INFO: Waiting for pod pod-configmaps-75ccc0e7-4425-4a97-9ebb-12f37709e950 to disappear
Aug 22 10:28:21.133: INFO: Pod pod-configmaps-75ccc0e7-4425-4a97-9ebb-12f37709e950 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:28:21.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7786" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":211,"skipped":3642,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:28:21.137: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-37e0c8b6-6e0d-431c-9c14-9272ee08171c
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:28:21.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3727" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":212,"skipped":3649,"failed":0}
SS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:28:21.159: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Aug 22 10:28:21.172: INFO: PodSpec: initContainers in spec.initContainers
Aug 22 10:29:05.549: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-64c6523f-6b68-446f-a699-93246410724f", GenerateName:"", Namespace:"init-container-2537", SelfLink:"", UID:"080758c5-3773-40e0-b2c1-6e3173dac954", ResourceVersion:"18952", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63765224901, loc:(*time.Location)(0xa09cc60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"172955948"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004e89488), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e894a0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004e894b8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e894d0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-zv7fz", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003571920), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zv7fz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zv7fz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zv7fz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005d55be8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"172.23.79.105", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc007e2db90), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005d55c70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005d55c90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005d55c98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005d55c9c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0091888d0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765224901, loc:(*time.Location)(0xa09cc60)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765224901, loc:(*time.Location)(0xa09cc60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765224901, loc:(*time.Location)(0xa09cc60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765224901, loc:(*time.Location)(0xa09cc60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.23.79.105", PodIP:"10.244.2.227", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.2.227"}}, StartTime:(*v1.Time)(0xc004e89500), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc007e2dc70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc007e2dce0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"docker-pullable://k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"docker://cb20247f3294d8a429a32e48f99e75321dc7d7794e95289df7c6d827bc9697ed", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0035719a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003571980), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc005d55d1f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:05.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2537" for this suite.

• [SLOW TEST:44.396 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":213,"skipped":3651,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:05.555: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:29:05.940: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:29:08.953: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:29:08.956: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3800-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:12.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8120" for this suite.
STEP: Destroying namespace "webhook-8120-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.561 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":214,"skipped":3652,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:12.117: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 10:29:12.149: INFO: Waiting up to 5m0s for pod "downwardapi-volume-059ec81c-1a11-43a2-9307-c07d86cf439e" in namespace "projected-7518" to be "Succeeded or Failed"
Aug 22 10:29:12.151: INFO: Pod "downwardapi-volume-059ec81c-1a11-43a2-9307-c07d86cf439e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.653ms
Aug 22 10:29:14.155: INFO: Pod "downwardapi-volume-059ec81c-1a11-43a2-9307-c07d86cf439e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006221169s
STEP: Saw pod success
Aug 22 10:29:14.155: INFO: Pod "downwardapi-volume-059ec81c-1a11-43a2-9307-c07d86cf439e" satisfied condition "Succeeded or Failed"
Aug 22 10:29:14.156: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-059ec81c-1a11-43a2-9307-c07d86cf439e container client-container: <nil>
STEP: delete the pod
Aug 22 10:29:14.165: INFO: Waiting for pod downwardapi-volume-059ec81c-1a11-43a2-9307-c07d86cf439e to disappear
Aug 22 10:29:14.166: INFO: Pod downwardapi-volume-059ec81c-1a11-43a2-9307-c07d86cf439e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:14.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7518" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":215,"skipped":3659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:14.170: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:14.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7835" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":216,"skipped":3694,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:14.202: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Aug 22 10:29:14.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-482 create -f -'
Aug 22 10:29:14.361: INFO: stderr: ""
Aug 22 10:29:14.361: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 22 10:29:15.365: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 22 10:29:15.365: INFO: Found 0 / 1
Aug 22 10:29:16.365: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 22 10:29:16.365: INFO: Found 1 / 1
Aug 22 10:29:16.365: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 22 10:29:16.366: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 22 10:29:16.366: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 22 10:29:16.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-482 patch pod agnhost-primary-vnpt4 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 22 10:29:16.412: INFO: stderr: ""
Aug 22 10:29:16.412: INFO: stdout: "pod/agnhost-primary-vnpt4 patched\n"
STEP: checking annotations
Aug 22 10:29:16.413: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 22 10:29:16.413: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:16.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-482" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":217,"skipped":3696,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:16.417: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-1012/configmap-test-dc11a6a2-9777-478e-b708-d630c6b49b0d
STEP: Creating a pod to test consume configMaps
Aug 22 10:29:16.437: INFO: Waiting up to 5m0s for pod "pod-configmaps-95bfa9a5-017b-466e-9f98-ff257f689367" in namespace "configmap-1012" to be "Succeeded or Failed"
Aug 22 10:29:16.438: INFO: Pod "pod-configmaps-95bfa9a5-017b-466e-9f98-ff257f689367": Phase="Pending", Reason="", readiness=false. Elapsed: 1.305095ms
Aug 22 10:29:18.443: INFO: Pod "pod-configmaps-95bfa9a5-017b-466e-9f98-ff257f689367": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005699148s
STEP: Saw pod success
Aug 22 10:29:18.443: INFO: Pod "pod-configmaps-95bfa9a5-017b-466e-9f98-ff257f689367" satisfied condition "Succeeded or Failed"
Aug 22 10:29:18.444: INFO: Trying to get logs from node 172.23.79.105 pod pod-configmaps-95bfa9a5-017b-466e-9f98-ff257f689367 container env-test: <nil>
STEP: delete the pod
Aug 22 10:29:18.451: INFO: Waiting for pod pod-configmaps-95bfa9a5-017b-466e-9f98-ff257f689367 to disappear
Aug 22 10:29:18.453: INFO: Pod pod-configmaps-95bfa9a5-017b-466e-9f98-ff257f689367 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:18.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1012" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":218,"skipped":3725,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:18.457: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:29:18.476: INFO: The status of Pod busybox-host-aliasese63ad875-fb1e-4dbf-b3a1-580c457cd105 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:29:20.480: INFO: The status of Pod busybox-host-aliasese63ad875-fb1e-4dbf-b3a1-580c457cd105 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:20.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7066" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":219,"skipped":3745,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:20.490: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Aug 22 10:29:20.509: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:29:22.512: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:23.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6197" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":220,"skipped":3755,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:23.525: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:29:23.819: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:29:26.832: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:26.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2092" for this suite.
STEP: Destroying namespace "webhook-2092-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":221,"skipped":3803,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:26.880: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-59e7dc58-2f94-46df-a4c2-a27dda46618e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:28.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8760" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":222,"skipped":3810,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:28.927: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Aug 22 10:29:28.944: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6893  3a97e008-30c4-42fc-afc1-cfd4b534325c 19331 0 2021-08-22 10:29:28 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-08-22 10:29:28 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c6mlb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c6mlb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 22 10:29:28.945: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:29:30.948: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Aug 22 10:29:30.948: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6893 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:29:30.948: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Verifying customized DNS server is configured on pod...
Aug 22 10:29:31.011: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6893 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:29:31.011: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:29:31.075: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:31.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6893" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":223,"skipped":3824,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:31.086: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 10:29:31.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff603951-2c61-4052-9a3d-982ed52d88f5" in namespace "projected-3432" to be "Succeeded or Failed"
Aug 22 10:29:31.107: INFO: Pod "downwardapi-volume-ff603951-2c61-4052-9a3d-982ed52d88f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.778426ms
Aug 22 10:29:33.111: INFO: Pod "downwardapi-volume-ff603951-2c61-4052-9a3d-982ed52d88f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005230246s
STEP: Saw pod success
Aug 22 10:29:33.111: INFO: Pod "downwardapi-volume-ff603951-2c61-4052-9a3d-982ed52d88f5" satisfied condition "Succeeded or Failed"
Aug 22 10:29:33.112: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-ff603951-2c61-4052-9a3d-982ed52d88f5 container client-container: <nil>
STEP: delete the pod
Aug 22 10:29:33.119: INFO: Waiting for pod downwardapi-volume-ff603951-2c61-4052-9a3d-982ed52d88f5 to disappear
Aug 22 10:29:33.121: INFO: Pod downwardapi-volume-ff603951-2c61-4052-9a3d-982ed52d88f5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:33.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3432" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":224,"skipped":3842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:33.124: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-1b9ca220-b0f8-456d-9024-391f98f79ed1
STEP: Creating a pod to test consume secrets
Aug 22 10:29:33.145: INFO: Waiting up to 5m0s for pod "pod-secrets-f5e06b4f-b91e-4e51-9224-3dcaff9b4d81" in namespace "secrets-9422" to be "Succeeded or Failed"
Aug 22 10:29:33.146: INFO: Pod "pod-secrets-f5e06b4f-b91e-4e51-9224-3dcaff9b4d81": Phase="Pending", Reason="", readiness=false. Elapsed: 1.550314ms
Aug 22 10:29:35.152: INFO: Pod "pod-secrets-f5e06b4f-b91e-4e51-9224-3dcaff9b4d81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006814112s
STEP: Saw pod success
Aug 22 10:29:35.152: INFO: Pod "pod-secrets-f5e06b4f-b91e-4e51-9224-3dcaff9b4d81" satisfied condition "Succeeded or Failed"
Aug 22 10:29:35.153: INFO: Trying to get logs from node 172.23.79.105 pod pod-secrets-f5e06b4f-b91e-4e51-9224-3dcaff9b4d81 container secret-env-test: <nil>
STEP: delete the pod
Aug 22 10:29:35.160: INFO: Waiting for pod pod-secrets-f5e06b4f-b91e-4e51-9224-3dcaff9b4d81 to disappear
Aug 22 10:29:35.162: INFO: Pod pod-secrets-f5e06b4f-b91e-4e51-9224-3dcaff9b4d81 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:35.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9422" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":3877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:35.165: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Aug 22 10:29:35.190: INFO: Waiting up to 5m0s for pod "client-containers-554e8b2f-10dd-466d-a79e-d8c8c2c90e77" in namespace "containers-6032" to be "Succeeded or Failed"
Aug 22 10:29:35.191: INFO: Pod "client-containers-554e8b2f-10dd-466d-a79e-d8c8c2c90e77": Phase="Pending", Reason="", readiness=false. Elapsed: 1.578239ms
Aug 22 10:29:37.197: INFO: Pod "client-containers-554e8b2f-10dd-466d-a79e-d8c8c2c90e77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007284904s
STEP: Saw pod success
Aug 22 10:29:37.197: INFO: Pod "client-containers-554e8b2f-10dd-466d-a79e-d8c8c2c90e77" satisfied condition "Succeeded or Failed"
Aug 22 10:29:37.199: INFO: Trying to get logs from node 172.23.79.105 pod client-containers-554e8b2f-10dd-466d-a79e-d8c8c2c90e77 container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:29:37.206: INFO: Waiting for pod client-containers-554e8b2f-10dd-466d-a79e-d8c8c2c90e77 to disappear
Aug 22 10:29:37.207: INFO: Pod client-containers-554e8b2f-10dd-466d-a79e-d8c8c2c90e77 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:37.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6032" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":226,"skipped":3934,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:37.211: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-3404105c-d9a6-4b9f-bbc4-42e01cdd48b1
STEP: Creating a pod to test consume secrets
Aug 22 10:29:37.231: INFO: Waiting up to 5m0s for pod "pod-secrets-5f92cc00-27b7-4135-a89c-aaac08ab2594" in namespace "secrets-5" to be "Succeeded or Failed"
Aug 22 10:29:37.233: INFO: Pod "pod-secrets-5f92cc00-27b7-4135-a89c-aaac08ab2594": Phase="Pending", Reason="", readiness=false. Elapsed: 1.564547ms
Aug 22 10:29:39.238: INFO: Pod "pod-secrets-5f92cc00-27b7-4135-a89c-aaac08ab2594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007205464s
STEP: Saw pod success
Aug 22 10:29:39.238: INFO: Pod "pod-secrets-5f92cc00-27b7-4135-a89c-aaac08ab2594" satisfied condition "Succeeded or Failed"
Aug 22 10:29:39.240: INFO: Trying to get logs from node 172.23.79.105 pod pod-secrets-5f92cc00-27b7-4135-a89c-aaac08ab2594 container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 10:29:39.249: INFO: Waiting for pod pod-secrets-5f92cc00-27b7-4135-a89c-aaac08ab2594 to disappear
Aug 22 10:29:39.250: INFO: Pod pod-secrets-5f92cc00-27b7-4135-a89c-aaac08ab2594 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:29:39.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":3943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:29:39.254: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Aug 22 10:29:39.273: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 22 10:30:39.289: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:30:39.290: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Aug 22 10:30:41.326: INFO: found a healthy node: 172.23.79.105
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:30:49.373: INFO: pods created so far: [1 1 1]
Aug 22 10:30:49.374: INFO: length of pods created so far: 3
Aug 22 10:30:51.379: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:30:58.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7514" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:30:58.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5641" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:79.166 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":228,"skipped":3967,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:30:58.420: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Aug 22 10:30:58.437: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-3940 proxy --unix-socket=/tmp/kubectl-proxy-unix865072860/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:30:58.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3940" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":229,"skipped":3980,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:30:58.474: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-59c5cea1-4c6b-49cf-af26-9ce37298d290 in namespace container-probe-3853
Aug 22 10:31:00.497: INFO: Started pod liveness-59c5cea1-4c6b-49cf-af26-9ce37298d290 in namespace container-probe-3853
STEP: checking the pod's current state and verifying that restartCount is present
Aug 22 10:31:00.498: INFO: Initial restart count of pod liveness-59c5cea1-4c6b-49cf-af26-9ce37298d290 is 0
Aug 22 10:31:20.570: INFO: Restart count of pod container-probe-3853/liveness-59c5cea1-4c6b-49cf-af26-9ce37298d290 is now 1 (20.071843844s elapsed)
Aug 22 10:31:40.649: INFO: Restart count of pod container-probe-3853/liveness-59c5cea1-4c6b-49cf-af26-9ce37298d290 is now 2 (40.151100504s elapsed)
Aug 22 10:32:00.742: INFO: Restart count of pod container-probe-3853/liveness-59c5cea1-4c6b-49cf-af26-9ce37298d290 is now 3 (1m0.243779041s elapsed)
Aug 22 10:32:20.832: INFO: Restart count of pod container-probe-3853/liveness-59c5cea1-4c6b-49cf-af26-9ce37298d290 is now 4 (1m20.334299677s elapsed)
Aug 22 10:33:19.050: INFO: Restart count of pod container-probe-3853/liveness-59c5cea1-4c6b-49cf-af26-9ce37298d290 is now 5 (2m18.552338608s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:33:19.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3853" for this suite.

• [SLOW TEST:140.584 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":230,"skipped":4000,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:33:19.058: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Aug 22 10:33:19.081: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 22 10:34:19.097: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Aug 22 10:34:19.107: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 22 10:34:19.113: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 22 10:34:19.123: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 22 10:34:19.125: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:34:31.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1996" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:72.128 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":231,"skipped":4002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:34:31.187: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Aug 22 10:34:31.212: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Aug 22 10:34:31.220: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:34:31.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2959" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":232,"skipped":4025,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:34:31.232: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Aug 22 10:34:31.246: INFO: Major version: 1
STEP: Confirm minor version
Aug 22 10:34:31.246: INFO: cleanMinorVersion: 22
Aug 22 10:34:31.246: INFO: Minor version: 22
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:34:31.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7621" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":233,"skipped":4045,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:34:31.251: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 10:34:31.266: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9422956e-515f-4291-8bb6-889e262e4103" in namespace "downward-api-2545" to be "Succeeded or Failed"
Aug 22 10:34:31.268: INFO: Pod "downwardapi-volume-9422956e-515f-4291-8bb6-889e262e4103": Phase="Pending", Reason="", readiness=false. Elapsed: 1.240067ms
Aug 22 10:34:33.273: INFO: Pod "downwardapi-volume-9422956e-515f-4291-8bb6-889e262e4103": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006916655s
STEP: Saw pod success
Aug 22 10:34:33.273: INFO: Pod "downwardapi-volume-9422956e-515f-4291-8bb6-889e262e4103" satisfied condition "Succeeded or Failed"
Aug 22 10:34:33.275: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-9422956e-515f-4291-8bb6-889e262e4103 container client-container: <nil>
STEP: delete the pod
Aug 22 10:34:33.289: INFO: Waiting for pod downwardapi-volume-9422956e-515f-4291-8bb6-889e262e4103 to disappear
Aug 22 10:34:33.290: INFO: Pod downwardapi-volume-9422956e-515f-4291-8bb6-889e262e4103 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:34:33.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2545" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":234,"skipped":4147,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:34:33.294: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3156
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3156
STEP: creating replication controller externalsvc in namespace services-3156
I0822 10:34:33.355987      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3156, replica count: 2
I0822 10:34:36.406721      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Aug 22 10:34:36.422: INFO: Creating new exec pod
Aug 22 10:34:38.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-3156 exec execpodc4pk6 -- /bin/sh -x -c nslookup nodeport-service.services-3156.svc.cluster.local'
Aug 22 10:34:38.542: INFO: stderr: "+ nslookup nodeport-service.services-3156.svc.cluster.local\n"
Aug 22 10:34:38.542: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3156.svc.cluster.local\tcanonical name = externalsvc.services-3156.svc.cluster.local.\nName:\texternalsvc.services-3156.svc.cluster.local\nAddress: 10.105.75.225\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3156, will wait for the garbage collector to delete the pods
Aug 22 10:34:38.597: INFO: Deleting ReplicationController externalsvc took: 2.801124ms
Aug 22 10:34:38.698: INFO: Terminating ReplicationController externalsvc pods took: 100.86389ms
Aug 22 10:34:40.907: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:34:40.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3156" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.624 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":235,"skipped":4187,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:34:40.918: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Aug 22 10:34:50.967: INFO: The status of Pod kube-controller-manager-172.23.79.104 is Running (Ready = true)
Aug 22 10:34:51.084: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:34:51.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7803" for this suite.

• [SLOW TEST:10.171 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":236,"skipped":4195,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:34:51.089: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Aug 22 10:34:51.117: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 22 10:34:56.122: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Aug 22 10:34:56.124: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Aug 22 10:34:56.128: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Aug 22 10:34:56.129: INFO: Observed &ReplicaSet event: ADDED
Aug 22 10:34:56.129: INFO: Observed &ReplicaSet event: MODIFIED
Aug 22 10:34:56.129: INFO: Observed &ReplicaSet event: MODIFIED
Aug 22 10:34:56.129: INFO: Observed &ReplicaSet event: MODIFIED
Aug 22 10:34:56.129: INFO: Found replicaset test-rs in namespace replicaset-9668 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 22 10:34:56.129: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Aug 22 10:34:56.129: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 22 10:34:56.137: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Aug 22 10:34:56.138: INFO: Observed &ReplicaSet event: ADDED
Aug 22 10:34:56.138: INFO: Observed &ReplicaSet event: MODIFIED
Aug 22 10:34:56.138: INFO: Observed &ReplicaSet event: MODIFIED
Aug 22 10:34:56.138: INFO: Observed &ReplicaSet event: MODIFIED
Aug 22 10:34:56.138: INFO: Observed replicaset test-rs in namespace replicaset-9668 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 22 10:34:56.138: INFO: Observed &ReplicaSet event: MODIFIED
Aug 22 10:34:56.138: INFO: Found replicaset test-rs in namespace replicaset-9668 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 22 10:34:56.138: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:34:56.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9668" for this suite.

• [SLOW TEST:5.053 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":237,"skipped":4208,"failed":0}
SSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:34:56.142: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:36:00.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-804" for this suite.

• [SLOW TEST:64.040 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":238,"skipped":4215,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:36:00.182: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Aug 22 10:36:00.199: INFO: Waiting up to 5m0s for pod "var-expansion-22a07e2e-f257-4ae4-8a7f-16aa7eaadfd5" in namespace "var-expansion-2755" to be "Succeeded or Failed"
Aug 22 10:36:00.208: INFO: Pod "var-expansion-22a07e2e-f257-4ae4-8a7f-16aa7eaadfd5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.900319ms
Aug 22 10:36:02.214: INFO: Pod "var-expansion-22a07e2e-f257-4ae4-8a7f-16aa7eaadfd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014449225s
STEP: Saw pod success
Aug 22 10:36:02.214: INFO: Pod "var-expansion-22a07e2e-f257-4ae4-8a7f-16aa7eaadfd5" satisfied condition "Succeeded or Failed"
Aug 22 10:36:02.215: INFO: Trying to get logs from node 172.23.79.105 pod var-expansion-22a07e2e-f257-4ae4-8a7f-16aa7eaadfd5 container dapi-container: <nil>
STEP: delete the pod
Aug 22 10:36:02.223: INFO: Waiting for pod var-expansion-22a07e2e-f257-4ae4-8a7f-16aa7eaadfd5 to disappear
Aug 22 10:36:02.225: INFO: Pod var-expansion-22a07e2e-f257-4ae4-8a7f-16aa7eaadfd5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:36:02.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2755" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":239,"skipped":4216,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:36:02.228: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:42:00.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2801" for this suite.

• [SLOW TEST:358.043 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":240,"skipped":4224,"failed":0}
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:42:00.271: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:42:00.295: INFO: Creating pod...
Aug 22 10:42:00.299: INFO: Pod Quantity: 1 Status: Pending
Aug 22 10:42:01.304: INFO: Pod Quantity: 1 Status: Pending
Aug 22 10:42:02.303: INFO: Pod Status: Running
Aug 22 10:42:02.303: INFO: Creating service...
Aug 22 10:42:02.312: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/pods/agnhost/proxy/some/path/with/DELETE
Aug 22 10:42:02.315: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 22 10:42:02.315: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/pods/agnhost/proxy/some/path/with/GET
Aug 22 10:42:02.319: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 22 10:42:02.319: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/pods/agnhost/proxy/some/path/with/HEAD
Aug 22 10:42:02.326: INFO: http.Client request:HEAD | StatusCode:200
Aug 22 10:42:02.326: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 22 10:42:02.327: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 22 10:42:02.327: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/pods/agnhost/proxy/some/path/with/PATCH
Aug 22 10:42:02.329: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 22 10:42:02.329: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/pods/agnhost/proxy/some/path/with/POST
Aug 22 10:42:02.330: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 22 10:42:02.330: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/pods/agnhost/proxy/some/path/with/PUT
Aug 22 10:42:02.332: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 22 10:42:02.332: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/services/test-service/proxy/some/path/with/DELETE
Aug 22 10:42:02.333: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 22 10:42:02.333: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/services/test-service/proxy/some/path/with/GET
Aug 22 10:42:02.335: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 22 10:42:02.335: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/services/test-service/proxy/some/path/with/HEAD
Aug 22 10:42:02.337: INFO: http.Client request:HEAD | StatusCode:200
Aug 22 10:42:02.337: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/services/test-service/proxy/some/path/with/OPTIONS
Aug 22 10:42:02.339: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 22 10:42:02.339: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/services/test-service/proxy/some/path/with/PATCH
Aug 22 10:42:02.341: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 22 10:42:02.341: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/services/test-service/proxy/some/path/with/POST
Aug 22 10:42:02.343: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 22 10:42:02.343: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6308/services/test-service/proxy/some/path/with/PUT
Aug 22 10:42:02.350: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:42:02.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6308" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":241,"skipped":4231,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:42:02.354: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7773
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7773
STEP: creating replication controller externalsvc in namespace services-7773
I0822 10:42:02.387046      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7773, replica count: 2
I0822 10:42:05.438556      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Aug 22 10:42:05.448: INFO: Creating new exec pod
Aug 22 10:42:07.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-7773 exec execpodkczx5 -- /bin/sh -x -c nslookup clusterip-service.services-7773.svc.cluster.local'
Aug 22 10:42:07.656: INFO: stderr: "+ nslookup clusterip-service.services-7773.svc.cluster.local\n"
Aug 22 10:42:07.656: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-7773.svc.cluster.local\tcanonical name = externalsvc.services-7773.svc.cluster.local.\nName:\texternalsvc.services-7773.svc.cluster.local\nAddress: 10.96.14.42\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7773, will wait for the garbage collector to delete the pods
Aug 22 10:42:07.712: INFO: Deleting ReplicationController externalsvc took: 2.957516ms
Aug 22 10:42:07.812: INFO: Terminating ReplicationController externalsvc pods took: 100.312744ms
Aug 22 10:42:09.525: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:42:09.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7773" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.181 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":242,"skipped":4248,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:42:09.536: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Aug 22 10:42:11.564: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:42:17.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-947" for this suite.

• [SLOW TEST:8.101 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":243,"skipped":4269,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:42:17.637: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:42:17.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1525" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":244,"skipped":4286,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:42:17.667: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 22 10:42:17.680: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 22 10:42:17.684: INFO: Waiting for terminating namespaces to be deleted...
Aug 22 10:42:17.685: INFO: 
Logging pods the apiserver thinks is on node 172.23.79.103 before test
Aug 22 10:42:17.688: INFO: kube-flannel-ds-s57gz from kube-system started at 2021-08-22 09:15:14 +0000 UTC (1 container statuses recorded)
Aug 22 10:42:17.688: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 10:42:17.688: INFO: kube-proxy-r9sql from kube-system started at 2021-08-22 09:14:45 +0000 UTC (1 container statuses recorded)
Aug 22 10:42:17.688: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 22 10:42:17.688: INFO: sonobuoy from sonobuoy started at 2021-08-22 09:27:55 +0000 UTC (1 container statuses recorded)
Aug 22 10:42:17.688: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 22 10:42:17.688: INFO: sonobuoy-e2e-job-75357183cd8142cd from sonobuoy started at 2021-08-22 09:27:56 +0000 UTC (2 container statuses recorded)
Aug 22 10:42:17.688: INFO: 	Container e2e ready: true, restart count 0
Aug 22 10:42:17.688: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 10:42:17.688: INFO: sonobuoy-systemd-logs-daemon-set-d3d945f99a484f42-ggzhp from sonobuoy started at 2021-08-22 09:27:56 +0000 UTC (2 container statuses recorded)
Aug 22 10:42:17.688: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 10:42:17.688: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 22 10:42:17.688: INFO: 
Logging pods the apiserver thinks is on node 172.23.79.105 before test
Aug 22 10:42:17.690: INFO: rs-fw98r from disruption-947 started at 2021-08-22 10:42:11 +0000 UTC (1 container statuses recorded)
Aug 22 10:42:17.690: INFO: 	Container donothing ready: true, restart count 0
Aug 22 10:42:17.690: INFO: rs-jj5fb from disruption-947 started at 2021-08-22 10:42:11 +0000 UTC (1 container statuses recorded)
Aug 22 10:42:17.690: INFO: 	Container donothing ready: true, restart count 0
Aug 22 10:42:17.690: INFO: rs-lzc5r from disruption-947 started at 2021-08-22 10:42:17 +0000 UTC (1 container statuses recorded)
Aug 22 10:42:17.690: INFO: 	Container donothing ready: false, restart count 0
Aug 22 10:42:17.690: INFO: rs-z7zrx from disruption-947 started at 2021-08-22 10:42:15 +0000 UTC (1 container statuses recorded)
Aug 22 10:42:17.690: INFO: 	Container donothing ready: true, restart count 0
Aug 22 10:42:17.690: INFO: kube-flannel-ds-q9ltg from kube-system started at 2021-08-22 10:15:14 +0000 UTC (1 container statuses recorded)
Aug 22 10:42:17.690: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 10:42:17.690: INFO: kube-proxy-zhmln from kube-system started at 2021-08-22 09:39:54 +0000 UTC (1 container statuses recorded)
Aug 22 10:42:17.690: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 22 10:42:17.690: INFO: sonobuoy-systemd-logs-daemon-set-d3d945f99a484f42-8k4l9 from sonobuoy started at 2021-08-22 09:39:54 +0000 UTC (2 container statuses recorded)
Aug 22 10:42:17.690: INFO: 	Container sonobuoy-worker ready: false, restart count 16
Aug 22 10:42:17.690: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.169d9ae27b4ba21b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:42:18.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-390" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":245,"skipped":4289,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:42:18.707: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-7060
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7060 to expose endpoints map[]
Aug 22 10:42:18.729: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Aug 22 10:42:19.735: INFO: successfully validated that service endpoint-test2 in namespace services-7060 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7060
Aug 22 10:42:19.740: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:42:21.746: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7060 to expose endpoints map[pod1:[80]]
Aug 22 10:42:21.751: INFO: successfully validated that service endpoint-test2 in namespace services-7060 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Aug 22 10:42:21.751: INFO: Creating new exec pod
Aug 22 10:42:24.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-7060 exec execpodl8dqm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 22 10:42:24.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 22 10:42:24.862: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:42:24.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-7060 exec execpodl8dqm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.140.105 80'
Aug 22 10:42:24.963: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.140.105 80\nConnection to 10.102.140.105 80 port [tcp/http] succeeded!\n"
Aug 22 10:42:24.963: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-7060
Aug 22 10:42:24.968: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:42:26.973: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7060 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 22 10:42:26.980: INFO: successfully validated that service endpoint-test2 in namespace services-7060 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Aug 22 10:42:27.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-7060 exec execpodl8dqm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 22 10:42:28.083: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 22 10:42:28.083: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:42:28.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-7060 exec execpodl8dqm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.140.105 80'
Aug 22 10:42:28.179: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.140.105 80\nConnection to 10.102.140.105 80 port [tcp/http] succeeded!\n"
Aug 22 10:42:28.179: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7060
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7060 to expose endpoints map[pod2:[80]]
Aug 22 10:42:28.195: INFO: successfully validated that service endpoint-test2 in namespace services-7060 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Aug 22 10:42:29.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-7060 exec execpodl8dqm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 22 10:42:29.300: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 22 10:42:29.300: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 22 10:42:29.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=services-7060 exec execpodl8dqm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.140.105 80'
Aug 22 10:42:29.401: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.140.105 80\nConnection to 10.102.140.105 80 port [tcp/http] succeeded!\n"
Aug 22 10:42:29.401: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-7060
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7060 to expose endpoints map[]
Aug 22 10:42:30.426: INFO: successfully validated that service endpoint-test2 in namespace services-7060 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:42:30.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7060" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.730 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":246,"skipped":4330,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:42:30.437: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 22 10:42:32.968: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-412 pod-service-account-ceb78b8b-10f1-40ca-810e-517874550d1e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 22 10:42:33.067: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-412 pod-service-account-ceb78b8b-10f1-40ca-810e-517874550d1e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 22 10:42:33.161: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-412 pod-service-account-ceb78b8b-10f1-40ca-810e-517874550d1e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:42:33.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-412" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":247,"skipped":4348,"failed":0}
S
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:42:33.268: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:43:33.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9021" for this suite.

• [SLOW TEST:60.038 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":248,"skipped":4349,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:43:33.306: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Aug 22 10:43:33.321: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:43:36.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8899" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":249,"skipped":4373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:43:36.450: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:43:36.467: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:43:37.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6723" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":250,"skipped":4395,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:43:37.485: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-8008
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 22 10:43:37.500: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 22 10:43:37.509: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:43:39.512: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:43:41.516: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:43:43.514: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:43:45.514: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:43:47.515: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:43:49.516: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:43:51.517: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:43:53.516: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:43:55.514: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:43:57.516: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 22 10:43:59.516: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 22 10:43:59.519: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Aug 22 10:44:01.533: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 22 10:44:01.533: INFO: Breadth first check of 10.244.1.94 on host 172.23.79.103...
Aug 22 10:44:01.534: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.21:9080/dial?request=hostname&protocol=http&host=10.244.1.94&port=8083&tries=1'] Namespace:pod-network-test-8008 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:44:01.534: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:44:01.589: INFO: Waiting for responses: map[]
Aug 22 10:44:01.589: INFO: reached 10.244.1.94 after 0/1 tries
Aug 22 10:44:01.589: INFO: Breadth first check of 10.244.2.20 on host 172.23.79.105...
Aug 22 10:44:01.591: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.21:9080/dial?request=hostname&protocol=http&host=10.244.2.20&port=8083&tries=1'] Namespace:pod-network-test-8008 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:44:01.591: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:44:01.644: INFO: Waiting for responses: map[]
Aug 22 10:44:01.644: INFO: reached 10.244.2.20 after 0/1 tries
Aug 22 10:44:01.644: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:44:01.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8008" for this suite.

• [SLOW TEST:24.165 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":251,"skipped":4406,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:44:01.650: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Aug 22 10:44:01.674: INFO: observed Pod pod-test in namespace pods-7290 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 22 10:44:01.676: INFO: observed Pod pod-test in namespace pods-7290 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:44:01 +0000 UTC  }]
Aug 22 10:44:01.682: INFO: observed Pod pod-test in namespace pods-7290 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:44:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:44:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:44:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:44:01 +0000 UTC  }]
Aug 22 10:44:02.711: INFO: Found Pod pod-test in namespace pods-7290 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:44:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:44:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:44:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:44:01 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Aug 22 10:44:02.716: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Aug 22 10:44:02.725: INFO: observed event type ADDED
Aug 22 10:44:02.725: INFO: observed event type MODIFIED
Aug 22 10:44:02.725: INFO: observed event type MODIFIED
Aug 22 10:44:02.725: INFO: observed event type MODIFIED
Aug 22 10:44:02.725: INFO: observed event type MODIFIED
Aug 22 10:44:02.725: INFO: observed event type MODIFIED
Aug 22 10:44:02.725: INFO: observed event type MODIFIED
Aug 22 10:44:03.810: INFO: observed event type MODIFIED
Aug 22 10:44:05.741: INFO: observed event type MODIFIED
Aug 22 10:44:05.743: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:44:05.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7290" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":252,"skipped":4416,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:44:05.755: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:44:06.237: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:44:09.250: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:44:09.254: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:44:12.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1977" for this suite.
STEP: Destroying namespace "webhook-1977-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.601 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":253,"skipped":4436,"failed":0}
SSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:44:12.356: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Aug 22 10:44:12.389: INFO: Waiting up to 5m0s for pod "client-containers-fff77834-22be-4740-9235-b7a859506d0f" in namespace "containers-4870" to be "Succeeded or Failed"
Aug 22 10:44:12.391: INFO: Pod "client-containers-fff77834-22be-4740-9235-b7a859506d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.230828ms
Aug 22 10:44:14.396: INFO: Pod "client-containers-fff77834-22be-4740-9235-b7a859506d0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007093637s
STEP: Saw pod success
Aug 22 10:44:14.396: INFO: Pod "client-containers-fff77834-22be-4740-9235-b7a859506d0f" satisfied condition "Succeeded or Failed"
Aug 22 10:44:14.397: INFO: Trying to get logs from node 172.23.79.105 pod client-containers-fff77834-22be-4740-9235-b7a859506d0f container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:44:14.410: INFO: Waiting for pod client-containers-fff77834-22be-4740-9235-b7a859506d0f to disappear
Aug 22 10:44:14.411: INFO: Pod client-containers-fff77834-22be-4740-9235-b7a859506d0f no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:44:14.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4870" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":254,"skipped":4443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:44:14.415: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-a753a3f0-a17a-4c0a-ac7a-dd6f115e03b1
STEP: Creating a pod to test consume secrets
Aug 22 10:44:14.436: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5d6e7ee2-1044-4e6a-8d2e-ea68bac779d7" in namespace "projected-7094" to be "Succeeded or Failed"
Aug 22 10:44:14.437: INFO: Pod "pod-projected-secrets-5d6e7ee2-1044-4e6a-8d2e-ea68bac779d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.720037ms
Aug 22 10:44:16.441: INFO: Pod "pod-projected-secrets-5d6e7ee2-1044-4e6a-8d2e-ea68bac779d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005546001s
STEP: Saw pod success
Aug 22 10:44:16.441: INFO: Pod "pod-projected-secrets-5d6e7ee2-1044-4e6a-8d2e-ea68bac779d7" satisfied condition "Succeeded or Failed"
Aug 22 10:44:16.442: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-secrets-5d6e7ee2-1044-4e6a-8d2e-ea68bac779d7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 22 10:44:16.450: INFO: Waiting for pod pod-projected-secrets-5d6e7ee2-1044-4e6a-8d2e-ea68bac779d7 to disappear
Aug 22 10:44:16.451: INFO: Pod pod-projected-secrets-5d6e7ee2-1044-4e6a-8d2e-ea68bac779d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:44:16.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7094" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":255,"skipped":4477,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:44:16.455: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 10:44:16.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f3c010f-89f0-43d4-b072-9cecda8c6644" in namespace "projected-98" to be "Succeeded or Failed"
Aug 22 10:44:16.474: INFO: Pod "downwardapi-volume-5f3c010f-89f0-43d4-b072-9cecda8c6644": Phase="Pending", Reason="", readiness=false. Elapsed: 1.856622ms
Aug 22 10:44:18.479: INFO: Pod "downwardapi-volume-5f3c010f-89f0-43d4-b072-9cecda8c6644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006481579s
STEP: Saw pod success
Aug 22 10:44:18.479: INFO: Pod "downwardapi-volume-5f3c010f-89f0-43d4-b072-9cecda8c6644" satisfied condition "Succeeded or Failed"
Aug 22 10:44:18.480: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-5f3c010f-89f0-43d4-b072-9cecda8c6644 container client-container: <nil>
STEP: delete the pod
Aug 22 10:44:18.488: INFO: Waiting for pod downwardapi-volume-5f3c010f-89f0-43d4-b072-9cecda8c6644 to disappear
Aug 22 10:44:18.489: INFO: Pod downwardapi-volume-5f3c010f-89f0-43d4-b072-9cecda8c6644 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:44:18.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-98" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":256,"skipped":4484,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:44:18.493: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:44:18.508: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 22 10:44:21.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-8837 --namespace=crd-publish-openapi-8837 create -f -'
Aug 22 10:44:21.330: INFO: stderr: ""
Aug 22 10:44:21.330: INFO: stdout: "e2e-test-crd-publish-openapi-1861-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 22 10:44:21.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-8837 --namespace=crd-publish-openapi-8837 delete e2e-test-crd-publish-openapi-1861-crds test-cr'
Aug 22 10:44:21.375: INFO: stderr: ""
Aug 22 10:44:21.375: INFO: stdout: "e2e-test-crd-publish-openapi-1861-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 22 10:44:21.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-8837 --namespace=crd-publish-openapi-8837 apply -f -'
Aug 22 10:44:21.499: INFO: stderr: ""
Aug 22 10:44:21.499: INFO: stdout: "e2e-test-crd-publish-openapi-1861-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 22 10:44:21.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-8837 --namespace=crd-publish-openapi-8837 delete e2e-test-crd-publish-openapi-1861-crds test-cr'
Aug 22 10:44:21.542: INFO: stderr: ""
Aug 22 10:44:21.542: INFO: stdout: "e2e-test-crd-publish-openapi-1861-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Aug 22 10:44:21.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-8837 explain e2e-test-crd-publish-openapi-1861-crds'
Aug 22 10:44:21.649: INFO: stderr: ""
Aug 22 10:44:21.649: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1861-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:44:24.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8837" for this suite.

• [SLOW TEST:5.750 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":257,"skipped":4512,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:44:24.243: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Aug 22 10:44:24.262: INFO: created test-podtemplate-1
Aug 22 10:44:24.264: INFO: created test-podtemplate-2
Aug 22 10:44:24.266: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Aug 22 10:44:24.267: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Aug 22 10:44:24.271: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:44:24.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1012" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":258,"skipped":4547,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:44:24.277: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Aug 22 10:44:24.294: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 22 10:45:24.307: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Aug 22 10:45:24.316: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 22 10:45:24.318: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 22 10:45:24.340: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 22 10:45:24.344: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:45:32.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-943" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:68.111 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":259,"skipped":4549,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:45:32.388: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 22 10:45:42.458: INFO: The status of Pod kube-controller-manager-172.23.79.104 is Running (Ready = true)
Aug 22 10:45:42.595: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 22 10:45:42.595: INFO: Deleting pod "simpletest-rc-to-be-deleted-446zx" in namespace "gc-5419"
Aug 22 10:45:42.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qw9c" in namespace "gc-5419"
Aug 22 10:45:42.605: INFO: Deleting pod "simpletest-rc-to-be-deleted-6z5t6" in namespace "gc-5419"
Aug 22 10:45:42.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-76mzw" in namespace "gc-5419"
Aug 22 10:45:42.612: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxhq9" in namespace "gc-5419"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:45:42.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5419" for this suite.

• [SLOW TEST:10.231 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":260,"skipped":4578,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:45:42.619: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 22 10:45:42.643: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6413  47d453e1-d5b3-42e4-a2f4-00d132d4851b 22785 0 2021-08-22 10:45:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-08-22 10:45:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 10:45:42.643: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6413  47d453e1-d5b3-42e4-a2f4-00d132d4851b 22786 0 2021-08-22 10:45:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-08-22 10:45:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:45:42.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6413" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":261,"skipped":4579,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:45:42.647: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:45:42.660: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5558
I0822 10:45:42.666098      21 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5558, replica count: 1
I0822 10:45:43.718052      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 10:45:44.718701      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 10:45:45.719408      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 10:45:45.828: INFO: Created: latency-svc-28pr9
Aug 22 10:45:45.835: INFO: Got endpoints: latency-svc-28pr9 [15.438158ms]
Aug 22 10:45:45.845: INFO: Created: latency-svc-h62st
Aug 22 10:45:45.855: INFO: Got endpoints: latency-svc-h62st [19.887102ms]
Aug 22 10:45:45.855: INFO: Created: latency-svc-6ndj4
Aug 22 10:45:45.859: INFO: Got endpoints: latency-svc-6ndj4 [24.100843ms]
Aug 22 10:45:45.862: INFO: Created: latency-svc-fsnzx
Aug 22 10:45:45.864: INFO: Got endpoints: latency-svc-fsnzx [29.408953ms]
Aug 22 10:45:45.867: INFO: Created: latency-svc-2nw8r
Aug 22 10:45:45.870: INFO: Got endpoints: latency-svc-2nw8r [34.721112ms]
Aug 22 10:45:45.872: INFO: Created: latency-svc-5sfnc
Aug 22 10:45:45.877: INFO: Created: latency-svc-rjv4n
Aug 22 10:45:45.877: INFO: Got endpoints: latency-svc-5sfnc [42.390402ms]
Aug 22 10:45:45.880: INFO: Got endpoints: latency-svc-rjv4n [44.977173ms]
Aug 22 10:45:45.885: INFO: Created: latency-svc-x9pmm
Aug 22 10:45:45.889: INFO: Created: latency-svc-9wp9k
Aug 22 10:45:45.889: INFO: Got endpoints: latency-svc-x9pmm [54.258635ms]
Aug 22 10:45:45.895: INFO: Created: latency-svc-rnsqs
Aug 22 10:45:45.896: INFO: Got endpoints: latency-svc-9wp9k [61.285847ms]
Aug 22 10:45:45.902: INFO: Got endpoints: latency-svc-rnsqs [67.288827ms]
Aug 22 10:45:45.904: INFO: Created: latency-svc-5966n
Aug 22 10:45:45.908: INFO: Got endpoints: latency-svc-5966n [73.483386ms]
Aug 22 10:45:45.911: INFO: Created: latency-svc-fzcln
Aug 22 10:45:45.914: INFO: Got endpoints: latency-svc-fzcln [78.779799ms]
Aug 22 10:45:45.917: INFO: Created: latency-svc-nj8pm
Aug 22 10:45:45.926: INFO: Got endpoints: latency-svc-nj8pm [90.586893ms]
Aug 22 10:45:45.929: INFO: Created: latency-svc-pvlg9
Aug 22 10:45:45.936: INFO: Got endpoints: latency-svc-pvlg9 [101.021292ms]
Aug 22 10:45:45.936: INFO: Created: latency-svc-m4txt
Aug 22 10:45:45.940: INFO: Got endpoints: latency-svc-m4txt [104.996379ms]
Aug 22 10:45:45.941: INFO: Created: latency-svc-bhhc4
Aug 22 10:45:45.944: INFO: Got endpoints: latency-svc-bhhc4 [109.141766ms]
Aug 22 10:45:45.947: INFO: Created: latency-svc-62dxk
Aug 22 10:45:45.950: INFO: Got endpoints: latency-svc-62dxk [95.707212ms]
Aug 22 10:45:45.957: INFO: Created: latency-svc-g49gh
Aug 22 10:45:45.960: INFO: Got endpoints: latency-svc-g49gh [100.882579ms]
Aug 22 10:45:45.964: INFO: Created: latency-svc-942ld
Aug 22 10:45:45.967: INFO: Got endpoints: latency-svc-942ld [102.337505ms]
Aug 22 10:45:45.969: INFO: Created: latency-svc-mc4rq
Aug 22 10:45:45.972: INFO: Got endpoints: latency-svc-mc4rq [101.96297ms]
Aug 22 10:45:45.975: INFO: Created: latency-svc-z7992
Aug 22 10:45:45.979: INFO: Created: latency-svc-gkwzk
Aug 22 10:45:45.979: INFO: Got endpoints: latency-svc-z7992 [101.87929ms]
Aug 22 10:45:45.982: INFO: Got endpoints: latency-svc-gkwzk [101.641698ms]
Aug 22 10:45:45.984: INFO: Created: latency-svc-vqtft
Aug 22 10:45:45.988: INFO: Got endpoints: latency-svc-vqtft [98.383506ms]
Aug 22 10:45:45.990: INFO: Created: latency-svc-8rzf6
Aug 22 10:45:45.992: INFO: Got endpoints: latency-svc-8rzf6 [95.794584ms]
Aug 22 10:45:45.995: INFO: Created: latency-svc-7fghg
Aug 22 10:45:45.998: INFO: Got endpoints: latency-svc-7fghg [95.29949ms]
Aug 22 10:45:46.000: INFO: Created: latency-svc-p5sgl
Aug 22 10:45:46.004: INFO: Got endpoints: latency-svc-p5sgl [95.189892ms]
Aug 22 10:45:46.004: INFO: Created: latency-svc-bldpq
Aug 22 10:45:46.007: INFO: Got endpoints: latency-svc-bldpq [93.562098ms]
Aug 22 10:45:46.010: INFO: Created: latency-svc-5snhk
Aug 22 10:45:46.012: INFO: Got endpoints: latency-svc-5snhk [85.858521ms]
Aug 22 10:45:46.015: INFO: Created: latency-svc-5276s
Aug 22 10:45:46.019: INFO: Created: latency-svc-rpj5j
Aug 22 10:45:46.019: INFO: Got endpoints: latency-svc-5276s [82.906453ms]
Aug 22 10:45:46.021: INFO: Got endpoints: latency-svc-rpj5j [9.792347ms]
Aug 22 10:45:46.038: INFO: Created: latency-svc-vhbr7
Aug 22 10:45:46.042: INFO: Got endpoints: latency-svc-vhbr7 [101.723336ms]
Aug 22 10:45:46.043: INFO: Created: latency-svc-gb4p7
Aug 22 10:45:46.046: INFO: Got endpoints: latency-svc-gb4p7 [102.327059ms]
Aug 22 10:45:46.050: INFO: Created: latency-svc-2qp64
Aug 22 10:45:46.052: INFO: Got endpoints: latency-svc-2qp64 [102.118ms]
Aug 22 10:45:46.054: INFO: Created: latency-svc-wk8v2
Aug 22 10:45:46.058: INFO: Got endpoints: latency-svc-wk8v2 [98.091076ms]
Aug 22 10:45:46.059: INFO: Created: latency-svc-2jbkl
Aug 22 10:45:46.070: INFO: Created: latency-svc-hs29t
Aug 22 10:45:46.074: INFO: Created: latency-svc-sr4f5
Aug 22 10:45:46.078: INFO: Created: latency-svc-7lkxz
Aug 22 10:45:46.083: INFO: Created: latency-svc-gq5ws
Aug 22 10:45:46.083: INFO: Got endpoints: latency-svc-2jbkl [116.555859ms]
Aug 22 10:45:46.087: INFO: Created: latency-svc-p7zcs
Aug 22 10:45:46.091: INFO: Created: latency-svc-zdvvv
Aug 22 10:45:46.099: INFO: Created: latency-svc-h2ksj
Aug 22 10:45:46.103: INFO: Created: latency-svc-bjwqh
Aug 22 10:45:46.107: INFO: Created: latency-svc-9lxvh
Aug 22 10:45:46.111: INFO: Created: latency-svc-rzwmb
Aug 22 10:45:46.116: INFO: Created: latency-svc-jm9kr
Aug 22 10:45:46.120: INFO: Created: latency-svc-82cr8
Aug 22 10:45:46.126: INFO: Created: latency-svc-hflft
Aug 22 10:45:46.131: INFO: Created: latency-svc-cx7jk
Aug 22 10:45:46.133: INFO: Got endpoints: latency-svc-hs29t [160.797089ms]
Aug 22 10:45:46.136: INFO: Created: latency-svc-fqrkh
Aug 22 10:45:46.155: INFO: Created: latency-svc-g47zn
Aug 22 10:45:46.182: INFO: Got endpoints: latency-svc-sr4f5 [202.750047ms]
Aug 22 10:45:46.189: INFO: Created: latency-svc-cs496
Aug 22 10:45:46.232: INFO: Got endpoints: latency-svc-7lkxz [250.017295ms]
Aug 22 10:45:46.245: INFO: Created: latency-svc-p25lj
Aug 22 10:45:46.281: INFO: Got endpoints: latency-svc-gq5ws [293.656776ms]
Aug 22 10:45:46.288: INFO: Created: latency-svc-8msmg
Aug 22 10:45:46.332: INFO: Got endpoints: latency-svc-p7zcs [339.453822ms]
Aug 22 10:45:46.338: INFO: Created: latency-svc-kddq2
Aug 22 10:45:46.383: INFO: Got endpoints: latency-svc-zdvvv [385.410136ms]
Aug 22 10:45:46.393: INFO: Created: latency-svc-7xdnq
Aug 22 10:45:46.432: INFO: Got endpoints: latency-svc-h2ksj [428.393942ms]
Aug 22 10:45:46.438: INFO: Created: latency-svc-jc28c
Aug 22 10:45:46.482: INFO: Got endpoints: latency-svc-bjwqh [474.876361ms]
Aug 22 10:45:46.488: INFO: Created: latency-svc-b8bzp
Aug 22 10:45:46.532: INFO: Got endpoints: latency-svc-9lxvh [512.829044ms]
Aug 22 10:45:46.538: INFO: Created: latency-svc-km9jk
Aug 22 10:45:46.583: INFO: Got endpoints: latency-svc-rzwmb [562.069143ms]
Aug 22 10:45:46.590: INFO: Created: latency-svc-gbrwl
Aug 22 10:45:46.631: INFO: Got endpoints: latency-svc-jm9kr [589.832315ms]
Aug 22 10:45:46.672: INFO: Created: latency-svc-pbs46
Aug 22 10:45:46.681: INFO: Got endpoints: latency-svc-82cr8 [635.013691ms]
Aug 22 10:45:46.688: INFO: Created: latency-svc-8xq4c
Aug 22 10:45:46.732: INFO: Got endpoints: latency-svc-hflft [679.828134ms]
Aug 22 10:45:46.738: INFO: Created: latency-svc-hqd5v
Aug 22 10:45:46.782: INFO: Got endpoints: latency-svc-cx7jk [723.995561ms]
Aug 22 10:45:46.788: INFO: Created: latency-svc-mll49
Aug 22 10:45:46.832: INFO: Got endpoints: latency-svc-fqrkh [748.35501ms]
Aug 22 10:45:46.839: INFO: Created: latency-svc-s55k4
Aug 22 10:45:46.886: INFO: Got endpoints: latency-svc-g47zn [753.073084ms]
Aug 22 10:45:46.891: INFO: Created: latency-svc-mkj59
Aug 22 10:45:46.932: INFO: Got endpoints: latency-svc-cs496 [750.321098ms]
Aug 22 10:45:46.938: INFO: Created: latency-svc-58jg5
Aug 22 10:45:46.981: INFO: Got endpoints: latency-svc-p25lj [749.630548ms]
Aug 22 10:45:46.993: INFO: Created: latency-svc-l29nh
Aug 22 10:45:47.032: INFO: Got endpoints: latency-svc-8msmg [750.144744ms]
Aug 22 10:45:47.038: INFO: Created: latency-svc-ch8s2
Aug 22 10:45:47.084: INFO: Got endpoints: latency-svc-kddq2 [752.392176ms]
Aug 22 10:45:47.090: INFO: Created: latency-svc-vzf9j
Aug 22 10:45:47.132: INFO: Got endpoints: latency-svc-7xdnq [749.27761ms]
Aug 22 10:45:47.138: INFO: Created: latency-svc-kwjs5
Aug 22 10:45:47.182: INFO: Got endpoints: latency-svc-jc28c [750.10851ms]
Aug 22 10:45:47.188: INFO: Created: latency-svc-5t9xs
Aug 22 10:45:47.232: INFO: Got endpoints: latency-svc-b8bzp [749.821576ms]
Aug 22 10:45:47.239: INFO: Created: latency-svc-6wczh
Aug 22 10:45:47.282: INFO: Got endpoints: latency-svc-km9jk [749.731162ms]
Aug 22 10:45:47.287: INFO: Created: latency-svc-2ggx6
Aug 22 10:45:47.332: INFO: Got endpoints: latency-svc-gbrwl [748.246278ms]
Aug 22 10:45:47.337: INFO: Created: latency-svc-v4f77
Aug 22 10:45:47.382: INFO: Got endpoints: latency-svc-pbs46 [750.459052ms]
Aug 22 10:45:47.388: INFO: Created: latency-svc-5h5jf
Aug 22 10:45:47.432: INFO: Got endpoints: latency-svc-8xq4c [750.859189ms]
Aug 22 10:45:47.438: INFO: Created: latency-svc-kl4qt
Aug 22 10:45:47.483: INFO: Got endpoints: latency-svc-hqd5v [750.47716ms]
Aug 22 10:45:47.488: INFO: Created: latency-svc-ggh6w
Aug 22 10:45:47.532: INFO: Got endpoints: latency-svc-mll49 [750.276221ms]
Aug 22 10:45:47.538: INFO: Created: latency-svc-96rnm
Aug 22 10:45:47.582: INFO: Got endpoints: latency-svc-s55k4 [750.191043ms]
Aug 22 10:45:47.587: INFO: Created: latency-svc-tkdfg
Aug 22 10:45:47.633: INFO: Got endpoints: latency-svc-mkj59 [747.304932ms]
Aug 22 10:45:47.640: INFO: Created: latency-svc-548q8
Aug 22 10:45:47.684: INFO: Got endpoints: latency-svc-58jg5 [751.52211ms]
Aug 22 10:45:47.691: INFO: Created: latency-svc-dk755
Aug 22 10:45:47.731: INFO: Got endpoints: latency-svc-l29nh [749.986349ms]
Aug 22 10:45:47.739: INFO: Created: latency-svc-9q5s5
Aug 22 10:45:47.781: INFO: Got endpoints: latency-svc-ch8s2 [749.467997ms]
Aug 22 10:45:47.787: INFO: Created: latency-svc-9xpvb
Aug 22 10:45:47.831: INFO: Got endpoints: latency-svc-vzf9j [747.240651ms]
Aug 22 10:45:47.838: INFO: Created: latency-svc-rbtbq
Aug 22 10:45:47.882: INFO: Got endpoints: latency-svc-kwjs5 [749.894733ms]
Aug 22 10:45:47.888: INFO: Created: latency-svc-c44zb
Aug 22 10:45:47.934: INFO: Got endpoints: latency-svc-5t9xs [752.21806ms]
Aug 22 10:45:47.940: INFO: Created: latency-svc-rv5qb
Aug 22 10:45:47.983: INFO: Got endpoints: latency-svc-6wczh [750.56385ms]
Aug 22 10:45:47.988: INFO: Created: latency-svc-dgjgj
Aug 22 10:45:48.032: INFO: Got endpoints: latency-svc-2ggx6 [750.284151ms]
Aug 22 10:45:48.039: INFO: Created: latency-svc-g5k9m
Aug 22 10:45:48.082: INFO: Got endpoints: latency-svc-v4f77 [750.013585ms]
Aug 22 10:45:48.087: INFO: Created: latency-svc-jbvnh
Aug 22 10:45:48.131: INFO: Got endpoints: latency-svc-5h5jf [749.409536ms]
Aug 22 10:45:48.137: INFO: Created: latency-svc-6k59c
Aug 22 10:45:48.182: INFO: Got endpoints: latency-svc-kl4qt [749.123439ms]
Aug 22 10:45:48.187: INFO: Created: latency-svc-shhhh
Aug 22 10:45:48.234: INFO: Got endpoints: latency-svc-ggh6w [751.041715ms]
Aug 22 10:45:48.240: INFO: Created: latency-svc-hf586
Aug 22 10:45:48.281: INFO: Got endpoints: latency-svc-96rnm [749.146681ms]
Aug 22 10:45:48.287: INFO: Created: latency-svc-xc65r
Aug 22 10:45:48.333: INFO: Got endpoints: latency-svc-tkdfg [751.259812ms]
Aug 22 10:45:48.339: INFO: Created: latency-svc-sr88r
Aug 22 10:45:48.382: INFO: Got endpoints: latency-svc-548q8 [749.316723ms]
Aug 22 10:45:48.388: INFO: Created: latency-svc-kvv7z
Aug 22 10:45:48.432: INFO: Got endpoints: latency-svc-dk755 [748.361457ms]
Aug 22 10:45:48.438: INFO: Created: latency-svc-zdwgr
Aug 22 10:45:48.483: INFO: Got endpoints: latency-svc-9q5s5 [751.40325ms]
Aug 22 10:45:48.489: INFO: Created: latency-svc-p6hbs
Aug 22 10:45:48.532: INFO: Got endpoints: latency-svc-9xpvb [750.603847ms]
Aug 22 10:45:48.542: INFO: Created: latency-svc-q27z6
Aug 22 10:45:48.582: INFO: Got endpoints: latency-svc-rbtbq [751.001065ms]
Aug 22 10:45:48.588: INFO: Created: latency-svc-lwlfl
Aug 22 10:45:48.631: INFO: Got endpoints: latency-svc-c44zb [749.006626ms]
Aug 22 10:45:48.638: INFO: Created: latency-svc-crgmp
Aug 22 10:45:48.683: INFO: Got endpoints: latency-svc-rv5qb [748.971321ms]
Aug 22 10:45:48.689: INFO: Created: latency-svc-gntzm
Aug 22 10:45:48.732: INFO: Got endpoints: latency-svc-dgjgj [749.683074ms]
Aug 22 10:45:48.738: INFO: Created: latency-svc-r8dcg
Aug 22 10:45:48.782: INFO: Got endpoints: latency-svc-g5k9m [749.664321ms]
Aug 22 10:45:48.787: INFO: Created: latency-svc-4vmx6
Aug 22 10:45:48.832: INFO: Got endpoints: latency-svc-jbvnh [750.453915ms]
Aug 22 10:45:48.842: INFO: Created: latency-svc-29855
Aug 22 10:45:48.882: INFO: Got endpoints: latency-svc-6k59c [750.367031ms]
Aug 22 10:45:48.888: INFO: Created: latency-svc-6nmvc
Aug 22 10:45:48.932: INFO: Got endpoints: latency-svc-shhhh [750.266351ms]
Aug 22 10:45:48.938: INFO: Created: latency-svc-zqh52
Aug 22 10:45:48.982: INFO: Got endpoints: latency-svc-hf586 [748.186439ms]
Aug 22 10:45:48.989: INFO: Created: latency-svc-s6zgs
Aug 22 10:45:49.032: INFO: Got endpoints: latency-svc-xc65r [750.93064ms]
Aug 22 10:45:49.040: INFO: Created: latency-svc-tr4zs
Aug 22 10:45:49.084: INFO: Got endpoints: latency-svc-sr88r [751.287193ms]
Aug 22 10:45:49.091: INFO: Created: latency-svc-5xdk5
Aug 22 10:45:49.132: INFO: Got endpoints: latency-svc-kvv7z [749.356189ms]
Aug 22 10:45:49.138: INFO: Created: latency-svc-sj27v
Aug 22 10:45:49.184: INFO: Got endpoints: latency-svc-zdwgr [751.38721ms]
Aug 22 10:45:49.193: INFO: Created: latency-svc-5jnqj
Aug 22 10:45:49.232: INFO: Got endpoints: latency-svc-p6hbs [749.126859ms]
Aug 22 10:45:49.239: INFO: Created: latency-svc-lc75z
Aug 22 10:45:49.282: INFO: Got endpoints: latency-svc-q27z6 [750.150261ms]
Aug 22 10:45:49.288: INFO: Created: latency-svc-4rqbj
Aug 22 10:45:49.333: INFO: Got endpoints: latency-svc-lwlfl [750.141864ms]
Aug 22 10:45:49.339: INFO: Created: latency-svc-8n2vj
Aug 22 10:45:49.382: INFO: Got endpoints: latency-svc-crgmp [750.711056ms]
Aug 22 10:45:49.388: INFO: Created: latency-svc-n69bw
Aug 22 10:45:49.432: INFO: Got endpoints: latency-svc-gntzm [748.787422ms]
Aug 22 10:45:49.439: INFO: Created: latency-svc-cpsf6
Aug 22 10:45:49.483: INFO: Got endpoints: latency-svc-r8dcg [750.865168ms]
Aug 22 10:45:49.489: INFO: Created: latency-svc-xz5v4
Aug 22 10:45:49.536: INFO: Got endpoints: latency-svc-4vmx6 [754.676645ms]
Aug 22 10:45:49.542: INFO: Created: latency-svc-4sbbj
Aug 22 10:45:49.587: INFO: Got endpoints: latency-svc-29855 [754.31704ms]
Aug 22 10:45:49.593: INFO: Created: latency-svc-8svzh
Aug 22 10:45:49.632: INFO: Got endpoints: latency-svc-6nmvc [750.477638ms]
Aug 22 10:45:49.638: INFO: Created: latency-svc-26mtk
Aug 22 10:45:49.682: INFO: Got endpoints: latency-svc-zqh52 [750.180435ms]
Aug 22 10:45:49.689: INFO: Created: latency-svc-px559
Aug 22 10:45:49.732: INFO: Got endpoints: latency-svc-s6zgs [749.731159ms]
Aug 22 10:45:49.772: INFO: Created: latency-svc-2pv4s
Aug 22 10:45:49.782: INFO: Got endpoints: latency-svc-tr4zs [749.535269ms]
Aug 22 10:45:49.788: INFO: Created: latency-svc-nq46f
Aug 22 10:45:49.833: INFO: Got endpoints: latency-svc-5xdk5 [748.095961ms]
Aug 22 10:45:49.839: INFO: Created: latency-svc-jk68t
Aug 22 10:45:49.883: INFO: Got endpoints: latency-svc-sj27v [750.971686ms]
Aug 22 10:45:49.888: INFO: Created: latency-svc-wvx8b
Aug 22 10:45:49.932: INFO: Got endpoints: latency-svc-5jnqj [748.046729ms]
Aug 22 10:45:49.938: INFO: Created: latency-svc-7c4nr
Aug 22 10:45:49.981: INFO: Got endpoints: latency-svc-lc75z [749.207767ms]
Aug 22 10:45:49.988: INFO: Created: latency-svc-s6qf6
Aug 22 10:45:50.032: INFO: Got endpoints: latency-svc-4rqbj [750.240033ms]
Aug 22 10:45:50.038: INFO: Created: latency-svc-97ggc
Aug 22 10:45:50.082: INFO: Got endpoints: latency-svc-8n2vj [749.091304ms]
Aug 22 10:45:50.088: INFO: Created: latency-svc-kll5n
Aug 22 10:45:50.132: INFO: Got endpoints: latency-svc-n69bw [749.343834ms]
Aug 22 10:45:50.138: INFO: Created: latency-svc-mqkfj
Aug 22 10:45:50.183: INFO: Got endpoints: latency-svc-cpsf6 [750.52445ms]
Aug 22 10:45:50.188: INFO: Created: latency-svc-2gdnp
Aug 22 10:45:50.236: INFO: Got endpoints: latency-svc-xz5v4 [752.784773ms]
Aug 22 10:45:50.242: INFO: Created: latency-svc-5fkzv
Aug 22 10:45:50.282: INFO: Got endpoints: latency-svc-4sbbj [746.237918ms]
Aug 22 10:45:50.288: INFO: Created: latency-svc-l9lr9
Aug 22 10:45:50.332: INFO: Got endpoints: latency-svc-8svzh [745.716665ms]
Aug 22 10:45:50.341: INFO: Created: latency-svc-thksx
Aug 22 10:45:50.382: INFO: Got endpoints: latency-svc-26mtk [749.427731ms]
Aug 22 10:45:50.388: INFO: Created: latency-svc-dbd8q
Aug 22 10:45:50.432: INFO: Got endpoints: latency-svc-px559 [750.178381ms]
Aug 22 10:45:50.439: INFO: Created: latency-svc-5qwzm
Aug 22 10:45:50.481: INFO: Got endpoints: latency-svc-2pv4s [749.443585ms]
Aug 22 10:45:50.489: INFO: Created: latency-svc-k8xgr
Aug 22 10:45:50.532: INFO: Got endpoints: latency-svc-nq46f [749.888987ms]
Aug 22 10:45:50.543: INFO: Created: latency-svc-77fvx
Aug 22 10:45:50.582: INFO: Got endpoints: latency-svc-jk68t [749.702666ms]
Aug 22 10:45:50.588: INFO: Created: latency-svc-v4tp4
Aug 22 10:45:50.632: INFO: Got endpoints: latency-svc-wvx8b [749.042107ms]
Aug 22 10:45:50.637: INFO: Created: latency-svc-8484t
Aug 22 10:45:50.683: INFO: Got endpoints: latency-svc-7c4nr [751.305377ms]
Aug 22 10:45:50.689: INFO: Created: latency-svc-799bs
Aug 22 10:45:50.733: INFO: Got endpoints: latency-svc-s6qf6 [751.703086ms]
Aug 22 10:45:50.740: INFO: Created: latency-svc-jddnj
Aug 22 10:45:50.782: INFO: Got endpoints: latency-svc-97ggc [749.510534ms]
Aug 22 10:45:50.787: INFO: Created: latency-svc-xzzht
Aug 22 10:45:50.833: INFO: Got endpoints: latency-svc-kll5n [751.165091ms]
Aug 22 10:45:50.839: INFO: Created: latency-svc-8q5js
Aug 22 10:45:50.883: INFO: Got endpoints: latency-svc-mqkfj [751.079273ms]
Aug 22 10:45:50.888: INFO: Created: latency-svc-6j6rb
Aug 22 10:45:50.932: INFO: Got endpoints: latency-svc-2gdnp [748.799543ms]
Aug 22 10:45:50.937: INFO: Created: latency-svc-rx7ls
Aug 22 10:45:50.982: INFO: Got endpoints: latency-svc-5fkzv [746.231164ms]
Aug 22 10:45:50.989: INFO: Created: latency-svc-49rmv
Aug 22 10:45:51.032: INFO: Got endpoints: latency-svc-l9lr9 [749.867888ms]
Aug 22 10:45:51.038: INFO: Created: latency-svc-hxmww
Aug 22 10:45:51.082: INFO: Got endpoints: latency-svc-thksx [750.06961ms]
Aug 22 10:45:51.088: INFO: Created: latency-svc-4zjj7
Aug 22 10:45:51.133: INFO: Got endpoints: latency-svc-dbd8q [751.282059ms]
Aug 22 10:45:51.138: INFO: Created: latency-svc-pf4xw
Aug 22 10:45:51.183: INFO: Got endpoints: latency-svc-5qwzm [750.74139ms]
Aug 22 10:45:51.189: INFO: Created: latency-svc-xgktv
Aug 22 10:45:51.232: INFO: Got endpoints: latency-svc-k8xgr [750.964753ms]
Aug 22 10:45:51.239: INFO: Created: latency-svc-bbn8x
Aug 22 10:45:51.284: INFO: Got endpoints: latency-svc-77fvx [751.798802ms]
Aug 22 10:45:51.290: INFO: Created: latency-svc-cl284
Aug 22 10:45:51.332: INFO: Got endpoints: latency-svc-v4tp4 [749.430494ms]
Aug 22 10:45:51.338: INFO: Created: latency-svc-cqtqq
Aug 22 10:45:51.382: INFO: Got endpoints: latency-svc-8484t [750.460022ms]
Aug 22 10:45:51.392: INFO: Created: latency-svc-5jggx
Aug 22 10:45:51.432: INFO: Got endpoints: latency-svc-799bs [748.985608ms]
Aug 22 10:45:51.438: INFO: Created: latency-svc-t8kwp
Aug 22 10:45:51.481: INFO: Got endpoints: latency-svc-jddnj [748.230668ms]
Aug 22 10:45:51.488: INFO: Created: latency-svc-8d8vj
Aug 22 10:45:51.533: INFO: Got endpoints: latency-svc-xzzht [751.547718ms]
Aug 22 10:45:51.539: INFO: Created: latency-svc-mhf78
Aug 22 10:45:51.582: INFO: Got endpoints: latency-svc-8q5js [748.651607ms]
Aug 22 10:45:51.588: INFO: Created: latency-svc-mmrvz
Aug 22 10:45:51.632: INFO: Got endpoints: latency-svc-6j6rb [749.46258ms]
Aug 22 10:45:51.643: INFO: Created: latency-svc-sdmzf
Aug 22 10:45:51.682: INFO: Got endpoints: latency-svc-rx7ls [750.563809ms]
Aug 22 10:45:51.688: INFO: Created: latency-svc-qr284
Aug 22 10:45:51.732: INFO: Got endpoints: latency-svc-49rmv [750.192116ms]
Aug 22 10:45:51.738: INFO: Created: latency-svc-88ghd
Aug 22 10:45:51.781: INFO: Got endpoints: latency-svc-hxmww [748.663777ms]
Aug 22 10:45:51.788: INFO: Created: latency-svc-j8w8v
Aug 22 10:45:51.832: INFO: Got endpoints: latency-svc-4zjj7 [749.866548ms]
Aug 22 10:45:51.840: INFO: Created: latency-svc-6k5xn
Aug 22 10:45:51.883: INFO: Got endpoints: latency-svc-pf4xw [749.865764ms]
Aug 22 10:45:51.888: INFO: Created: latency-svc-zcjz7
Aug 22 10:45:51.933: INFO: Got endpoints: latency-svc-xgktv [749.792008ms]
Aug 22 10:45:51.938: INFO: Created: latency-svc-lb7ps
Aug 22 10:45:51.984: INFO: Got endpoints: latency-svc-bbn8x [752.104796ms]
Aug 22 10:45:51.991: INFO: Created: latency-svc-9vgzh
Aug 22 10:45:52.031: INFO: Got endpoints: latency-svc-cl284 [747.769822ms]
Aug 22 10:45:52.038: INFO: Created: latency-svc-58m6m
Aug 22 10:45:52.082: INFO: Got endpoints: latency-svc-cqtqq [749.770981ms]
Aug 22 10:45:52.088: INFO: Created: latency-svc-fqpm8
Aug 22 10:45:52.132: INFO: Got endpoints: latency-svc-5jggx [749.415339ms]
Aug 22 10:45:52.138: INFO: Created: latency-svc-bnrzs
Aug 22 10:45:52.182: INFO: Got endpoints: latency-svc-t8kwp [749.551816ms]
Aug 22 10:45:52.188: INFO: Created: latency-svc-tbtl7
Aug 22 10:45:52.234: INFO: Got endpoints: latency-svc-8d8vj [753.1703ms]
Aug 22 10:45:52.240: INFO: Created: latency-svc-r9wg8
Aug 22 10:45:52.282: INFO: Got endpoints: latency-svc-mhf78 [748.550477ms]
Aug 22 10:45:52.288: INFO: Created: latency-svc-5wv22
Aug 22 10:45:52.332: INFO: Got endpoints: latency-svc-mmrvz [750.252319ms]
Aug 22 10:45:52.338: INFO: Created: latency-svc-cb28d
Aug 22 10:45:52.382: INFO: Got endpoints: latency-svc-sdmzf [749.78242ms]
Aug 22 10:45:52.388: INFO: Created: latency-svc-txd7c
Aug 22 10:45:52.435: INFO: Got endpoints: latency-svc-qr284 [753.30391ms]
Aug 22 10:45:52.443: INFO: Created: latency-svc-l2pfx
Aug 22 10:45:52.482: INFO: Got endpoints: latency-svc-88ghd [749.103216ms]
Aug 22 10:45:52.494: INFO: Created: latency-svc-5j7tj
Aug 22 10:45:52.531: INFO: Got endpoints: latency-svc-j8w8v [750.258878ms]
Aug 22 10:45:52.542: INFO: Created: latency-svc-2hxvm
Aug 22 10:45:52.581: INFO: Got endpoints: latency-svc-6k5xn [749.064531ms]
Aug 22 10:45:52.588: INFO: Created: latency-svc-lhnhf
Aug 22 10:45:52.633: INFO: Got endpoints: latency-svc-zcjz7 [749.823259ms]
Aug 22 10:45:52.639: INFO: Created: latency-svc-rg79w
Aug 22 10:45:52.682: INFO: Got endpoints: latency-svc-lb7ps [749.281184ms]
Aug 22 10:45:52.689: INFO: Created: latency-svc-l5mng
Aug 22 10:45:52.732: INFO: Got endpoints: latency-svc-9vgzh [747.87244ms]
Aug 22 10:45:52.738: INFO: Created: latency-svc-9nbkl
Aug 22 10:45:52.781: INFO: Got endpoints: latency-svc-58m6m [749.945819ms]
Aug 22 10:45:52.788: INFO: Created: latency-svc-rkjq5
Aug 22 10:45:52.833: INFO: Got endpoints: latency-svc-fqpm8 [750.982642ms]
Aug 22 10:45:52.846: INFO: Created: latency-svc-bbn8q
Aug 22 10:45:52.882: INFO: Got endpoints: latency-svc-bnrzs [750.807865ms]
Aug 22 10:45:52.889: INFO: Created: latency-svc-ltmch
Aug 22 10:45:52.932: INFO: Got endpoints: latency-svc-tbtl7 [750.21368ms]
Aug 22 10:45:52.939: INFO: Created: latency-svc-57vz8
Aug 22 10:45:52.982: INFO: Got endpoints: latency-svc-r9wg8 [747.71364ms]
Aug 22 10:45:52.989: INFO: Created: latency-svc-mrfh6
Aug 22 10:45:53.032: INFO: Got endpoints: latency-svc-5wv22 [749.986923ms]
Aug 22 10:45:53.039: INFO: Created: latency-svc-4m5n9
Aug 22 10:45:53.082: INFO: Got endpoints: latency-svc-cb28d [750.345787ms]
Aug 22 10:45:53.122: INFO: Created: latency-svc-ph2hl
Aug 22 10:45:53.132: INFO: Got endpoints: latency-svc-txd7c [750.382592ms]
Aug 22 10:45:53.138: INFO: Created: latency-svc-dmcbf
Aug 22 10:45:53.182: INFO: Got endpoints: latency-svc-l2pfx [746.379779ms]
Aug 22 10:45:53.188: INFO: Created: latency-svc-nwppq
Aug 22 10:45:53.232: INFO: Got endpoints: latency-svc-5j7tj [750.733178ms]
Aug 22 10:45:53.238: INFO: Created: latency-svc-qzklk
Aug 22 10:45:53.282: INFO: Got endpoints: latency-svc-2hxvm [751.176036ms]
Aug 22 10:45:53.289: INFO: Created: latency-svc-stbzh
Aug 22 10:45:53.332: INFO: Got endpoints: latency-svc-lhnhf [750.980379ms]
Aug 22 10:45:53.338: INFO: Created: latency-svc-wpqxw
Aug 22 10:45:53.382: INFO: Got endpoints: latency-svc-rg79w [749.095576ms]
Aug 22 10:45:53.388: INFO: Created: latency-svc-w8rj9
Aug 22 10:45:53.432: INFO: Got endpoints: latency-svc-l5mng [750.329044ms]
Aug 22 10:45:53.438: INFO: Created: latency-svc-4rq8w
Aug 22 10:45:53.485: INFO: Got endpoints: latency-svc-9nbkl [753.023278ms]
Aug 22 10:45:53.491: INFO: Created: latency-svc-ll4r7
Aug 22 10:45:53.532: INFO: Got endpoints: latency-svc-rkjq5 [750.994789ms]
Aug 22 10:45:53.539: INFO: Created: latency-svc-jkfrt
Aug 22 10:45:53.582: INFO: Got endpoints: latency-svc-bbn8q [749.075502ms]
Aug 22 10:45:53.591: INFO: Created: latency-svc-ss4fd
Aug 22 10:45:53.632: INFO: Got endpoints: latency-svc-ltmch [749.162789ms]
Aug 22 10:45:53.638: INFO: Created: latency-svc-dt9gv
Aug 22 10:45:53.681: INFO: Got endpoints: latency-svc-57vz8 [749.374273ms]
Aug 22 10:45:53.731: INFO: Got endpoints: latency-svc-mrfh6 [749.09313ms]
Aug 22 10:45:53.782: INFO: Got endpoints: latency-svc-4m5n9 [750.103378ms]
Aug 22 10:45:53.832: INFO: Got endpoints: latency-svc-ph2hl [749.365015ms]
Aug 22 10:45:53.883: INFO: Got endpoints: latency-svc-dmcbf [750.905764ms]
Aug 22 10:45:53.932: INFO: Got endpoints: latency-svc-nwppq [750.362753ms]
Aug 22 10:45:53.982: INFO: Got endpoints: latency-svc-qzklk [749.484159ms]
Aug 22 10:45:54.032: INFO: Got endpoints: latency-svc-stbzh [749.782309ms]
Aug 22 10:45:54.083: INFO: Got endpoints: latency-svc-wpqxw [750.198062ms]
Aug 22 10:45:54.133: INFO: Got endpoints: latency-svc-w8rj9 [751.138222ms]
Aug 22 10:45:54.182: INFO: Got endpoints: latency-svc-4rq8w [749.238779ms]
Aug 22 10:45:54.233: INFO: Got endpoints: latency-svc-ll4r7 [747.311341ms]
Aug 22 10:45:54.282: INFO: Got endpoints: latency-svc-jkfrt [749.837994ms]
Aug 22 10:45:54.332: INFO: Got endpoints: latency-svc-ss4fd [750.636787ms]
Aug 22 10:45:54.382: INFO: Got endpoints: latency-svc-dt9gv [750.194313ms]
Aug 22 10:45:54.382: INFO: Latencies: [9.792347ms 19.887102ms 24.100843ms 29.408953ms 34.721112ms 42.390402ms 44.977173ms 54.258635ms 61.285847ms 67.288827ms 73.483386ms 78.779799ms 82.906453ms 85.858521ms 90.586893ms 93.562098ms 95.189892ms 95.29949ms 95.707212ms 95.794584ms 98.091076ms 98.383506ms 100.882579ms 101.021292ms 101.641698ms 101.723336ms 101.87929ms 101.96297ms 102.118ms 102.327059ms 102.337505ms 104.996379ms 109.141766ms 116.555859ms 160.797089ms 202.750047ms 250.017295ms 293.656776ms 339.453822ms 385.410136ms 428.393942ms 474.876361ms 512.829044ms 562.069143ms 589.832315ms 635.013691ms 679.828134ms 723.995561ms 745.716665ms 746.231164ms 746.237918ms 746.379779ms 747.240651ms 747.304932ms 747.311341ms 747.71364ms 747.769822ms 747.87244ms 748.046729ms 748.095961ms 748.186439ms 748.230668ms 748.246278ms 748.35501ms 748.361457ms 748.550477ms 748.651607ms 748.663777ms 748.787422ms 748.799543ms 748.971321ms 748.985608ms 749.006626ms 749.042107ms 749.064531ms 749.075502ms 749.091304ms 749.09313ms 749.095576ms 749.103216ms 749.123439ms 749.126859ms 749.146681ms 749.162789ms 749.207767ms 749.238779ms 749.27761ms 749.281184ms 749.316723ms 749.343834ms 749.356189ms 749.365015ms 749.374273ms 749.409536ms 749.415339ms 749.427731ms 749.430494ms 749.443585ms 749.46258ms 749.467997ms 749.484159ms 749.510534ms 749.535269ms 749.551816ms 749.630548ms 749.664321ms 749.683074ms 749.702666ms 749.731159ms 749.731162ms 749.770981ms 749.782309ms 749.78242ms 749.792008ms 749.821576ms 749.823259ms 749.837994ms 749.865764ms 749.866548ms 749.867888ms 749.888987ms 749.894733ms 749.945819ms 749.986349ms 749.986923ms 750.013585ms 750.06961ms 750.103378ms 750.10851ms 750.141864ms 750.144744ms 750.150261ms 750.178381ms 750.180435ms 750.191043ms 750.192116ms 750.194313ms 750.198062ms 750.21368ms 750.240033ms 750.252319ms 750.258878ms 750.266351ms 750.276221ms 750.284151ms 750.321098ms 750.329044ms 750.345787ms 750.362753ms 750.367031ms 750.382592ms 750.453915ms 750.459052ms 750.460022ms 750.47716ms 750.477638ms 750.52445ms 750.563809ms 750.56385ms 750.603847ms 750.636787ms 750.711056ms 750.733178ms 750.74139ms 750.807865ms 750.859189ms 750.865168ms 750.905764ms 750.93064ms 750.964753ms 750.971686ms 750.980379ms 750.982642ms 750.994789ms 751.001065ms 751.041715ms 751.079273ms 751.138222ms 751.165091ms 751.176036ms 751.259812ms 751.282059ms 751.287193ms 751.305377ms 751.38721ms 751.40325ms 751.52211ms 751.547718ms 751.703086ms 751.798802ms 752.104796ms 752.21806ms 752.392176ms 752.784773ms 753.023278ms 753.073084ms 753.1703ms 753.30391ms 754.31704ms 754.676645ms]
Aug 22 10:45:54.382: INFO: 50 %ile: 749.484159ms
Aug 22 10:45:54.382: INFO: 90 %ile: 751.259812ms
Aug 22 10:45:54.382: INFO: 99 %ile: 754.31704ms
Aug 22 10:45:54.382: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:45:54.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5558" for this suite.

• [SLOW TEST:11.741 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":262,"skipped":4593,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:45:54.388: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:46:05.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1188" for this suite.

• [SLOW TEST:11.073 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":263,"skipped":4609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:46:05.462: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:46:05.486: INFO: The status of Pod busybox-scheduling-1891465e-3075-4fac-be44-f56b5311c6bf is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:46:07.490: INFO: The status of Pod busybox-scheduling-1891465e-3075-4fac-be44-f56b5311c6bf is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:46:07.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7956" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":4684,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:46:07.503: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 22 10:46:07.521: INFO: Waiting up to 5m0s for pod "pod-279f9bac-97fd-49dd-9c74-5634b20a81ab" in namespace "emptydir-9500" to be "Succeeded or Failed"
Aug 22 10:46:07.522: INFO: Pod "pod-279f9bac-97fd-49dd-9c74-5634b20a81ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1.300552ms
Aug 22 10:46:09.528: INFO: Pod "pod-279f9bac-97fd-49dd-9c74-5634b20a81ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006436733s
STEP: Saw pod success
Aug 22 10:46:09.528: INFO: Pod "pod-279f9bac-97fd-49dd-9c74-5634b20a81ab" satisfied condition "Succeeded or Failed"
Aug 22 10:46:09.529: INFO: Trying to get logs from node 172.23.79.105 pod pod-279f9bac-97fd-49dd-9c74-5634b20a81ab container test-container: <nil>
STEP: delete the pod
Aug 22 10:46:09.537: INFO: Waiting for pod pod-279f9bac-97fd-49dd-9c74-5634b20a81ab to disappear
Aug 22 10:46:09.538: INFO: Pod pod-279f9bac-97fd-49dd-9c74-5634b20a81ab no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:46:09.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9500" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":265,"skipped":4713,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:46:09.542: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:46:26.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4584" for this suite.

• [SLOW TEST:17.052 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":266,"skipped":4724,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:46:26.594: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:46:26.618: INFO: The status of Pod pod-secrets-48395873-5a06-46c3-b7a0-9586138eb0c4 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:46:28.621: INFO: The status of Pod pod-secrets-48395873-5a06-46c3-b7a0-9586138eb0c4 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:46:28.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2154" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":267,"skipped":4726,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:46:28.635: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-87jc
STEP: Creating a pod to test atomic-volume-subpath
Aug 22 10:46:28.658: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-87jc" in namespace "subpath-1049" to be "Succeeded or Failed"
Aug 22 10:46:28.660: INFO: Pod "pod-subpath-test-configmap-87jc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.331752ms
Aug 22 10:46:30.665: INFO: Pod "pod-subpath-test-configmap-87jc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007048369s
Aug 22 10:46:32.671: INFO: Pod "pod-subpath-test-configmap-87jc": Phase="Running", Reason="", readiness=true. Elapsed: 4.012332769s
Aug 22 10:46:34.674: INFO: Pod "pod-subpath-test-configmap-87jc": Phase="Running", Reason="", readiness=true. Elapsed: 6.015772266s
Aug 22 10:46:36.678: INFO: Pod "pod-subpath-test-configmap-87jc": Phase="Running", Reason="", readiness=true. Elapsed: 8.019921769s
Aug 22 10:46:38.683: INFO: Pod "pod-subpath-test-configmap-87jc": Phase="Running", Reason="", readiness=true. Elapsed: 10.025209323s
Aug 22 10:46:40.689: INFO: Pod "pod-subpath-test-configmap-87jc": Phase="Running", Reason="", readiness=true. Elapsed: 12.030996713s
Aug 22 10:46:42.693: INFO: Pod "pod-subpath-test-configmap-87jc": Phase="Running", Reason="", readiness=true. Elapsed: 14.035124256s
Aug 22 10:46:44.699: INFO: Pod "pod-subpath-test-configmap-87jc": Phase="Running", Reason="", readiness=true. Elapsed: 16.04043758s
Aug 22 10:46:46.703: INFO: Pod "pod-subpath-test-configmap-87jc": Phase="Running", Reason="", readiness=true. Elapsed: 18.044615786s
Aug 22 10:46:48.708: INFO: Pod "pod-subpath-test-configmap-87jc": Phase="Running", Reason="", readiness=true. Elapsed: 20.049742146s
Aug 22 10:46:50.713: INFO: Pod "pod-subpath-test-configmap-87jc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.054714935s
STEP: Saw pod success
Aug 22 10:46:50.713: INFO: Pod "pod-subpath-test-configmap-87jc" satisfied condition "Succeeded or Failed"
Aug 22 10:46:50.715: INFO: Trying to get logs from node 172.23.79.105 pod pod-subpath-test-configmap-87jc container test-container-subpath-configmap-87jc: <nil>
STEP: delete the pod
Aug 22 10:46:50.722: INFO: Waiting for pod pod-subpath-test-configmap-87jc to disappear
Aug 22 10:46:50.724: INFO: Pod pod-subpath-test-configmap-87jc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-87jc
Aug 22 10:46:50.724: INFO: Deleting pod "pod-subpath-test-configmap-87jc" in namespace "subpath-1049"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:46:50.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1049" for this suite.

• [SLOW TEST:22.093 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":268,"skipped":4750,"failed":0}
SSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:46:50.728: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:46:50.744: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-6876
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:46:56.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-5638" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:46:56.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6876" for this suite.

• [SLOW TEST:6.071 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":269,"skipped":4757,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:46:56.799: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:47:03.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3611" for this suite.

• [SLOW TEST:7.033 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":270,"skipped":4762,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:47:03.832: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:47:04.247: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:47:07.259: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Aug 22 10:47:07.272: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:47:07.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7538" for this suite.
STEP: Destroying namespace "webhook-7538-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":271,"skipped":4795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:47:07.316: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Aug 22 10:47:27.397: INFO: EndpointSlice for Service endpointslice-7263/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:47:37.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7263" for this suite.

• [SLOW TEST:30.095 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":272,"skipped":4818,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:47:37.411: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 22 10:47:37.438: INFO: Waiting up to 5m0s for pod "pod-ceba7366-7602-4376-b4cf-a300d713b66c" in namespace "emptydir-2965" to be "Succeeded or Failed"
Aug 22 10:47:37.439: INFO: Pod "pod-ceba7366-7602-4376-b4cf-a300d713b66c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.360965ms
Aug 22 10:47:39.445: INFO: Pod "pod-ceba7366-7602-4376-b4cf-a300d713b66c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006834998s
STEP: Saw pod success
Aug 22 10:47:39.445: INFO: Pod "pod-ceba7366-7602-4376-b4cf-a300d713b66c" satisfied condition "Succeeded or Failed"
Aug 22 10:47:39.446: INFO: Trying to get logs from node 172.23.79.105 pod pod-ceba7366-7602-4376-b4cf-a300d713b66c container test-container: <nil>
STEP: delete the pod
Aug 22 10:47:39.454: INFO: Waiting for pod pod-ceba7366-7602-4376-b4cf-a300d713b66c to disappear
Aug 22 10:47:39.455: INFO: Pod pod-ceba7366-7602-4376-b4cf-a300d713b66c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:47:39.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2965" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":273,"skipped":4825,"failed":0}
S
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:47:39.459: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:47:39.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6608" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":274,"skipped":4826,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:47:39.500: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:47:39.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5310" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":275,"skipped":4843,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:47:39.535: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:48:00.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3585" for this suite.

• [SLOW TEST:21.133 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":4855,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:48:00.669: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:48:01.084: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:48:04.097: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:48:16.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1560" for this suite.
STEP: Destroying namespace "webhook-1560-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.552 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":277,"skipped":4864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:48:16.221: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-f058f728-7d56-4a62-bcba-814134eab8aa
STEP: Creating a pod to test consume secrets
Aug 22 10:48:16.255: INFO: Waiting up to 5m0s for pod "pod-secrets-6ce9e126-292d-4183-95d8-3b60f38c47c7" in namespace "secrets-1422" to be "Succeeded or Failed"
Aug 22 10:48:16.260: INFO: Pod "pod-secrets-6ce9e126-292d-4183-95d8-3b60f38c47c7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.728521ms
Aug 22 10:48:18.265: INFO: Pod "pod-secrets-6ce9e126-292d-4183-95d8-3b60f38c47c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010651305s
STEP: Saw pod success
Aug 22 10:48:18.265: INFO: Pod "pod-secrets-6ce9e126-292d-4183-95d8-3b60f38c47c7" satisfied condition "Succeeded or Failed"
Aug 22 10:48:18.267: INFO: Trying to get logs from node 172.23.79.105 pod pod-secrets-6ce9e126-292d-4183-95d8-3b60f38c47c7 container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 10:48:18.275: INFO: Waiting for pod pod-secrets-6ce9e126-292d-4183-95d8-3b60f38c47c7 to disappear
Aug 22 10:48:18.276: INFO: Pod pod-secrets-6ce9e126-292d-4183-95d8-3b60f38c47c7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:48:18.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1422" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":278,"skipped":4958,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:48:18.279: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Aug 22 10:48:18.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 create -f -'
Aug 22 10:48:18.438: INFO: stderr: ""
Aug 22 10:48:18.438: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 22 10:48:18.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 22 10:48:18.483: INFO: stderr: ""
Aug 22 10:48:18.483: INFO: stdout: "update-demo-nautilus-4fxkp update-demo-nautilus-m76h8 "
Aug 22 10:48:18.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods update-demo-nautilus-4fxkp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 22 10:48:18.524: INFO: stderr: ""
Aug 22 10:48:18.525: INFO: stdout: ""
Aug 22 10:48:18.525: INFO: update-demo-nautilus-4fxkp is created but not running
Aug 22 10:48:23.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 22 10:48:23.574: INFO: stderr: ""
Aug 22 10:48:23.574: INFO: stdout: "update-demo-nautilus-4fxkp update-demo-nautilus-m76h8 "
Aug 22 10:48:23.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods update-demo-nautilus-4fxkp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 22 10:48:23.617: INFO: stderr: ""
Aug 22 10:48:23.617: INFO: stdout: "true"
Aug 22 10:48:23.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods update-demo-nautilus-4fxkp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 22 10:48:23.657: INFO: stderr: ""
Aug 22 10:48:23.657: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Aug 22 10:48:23.657: INFO: validating pod update-demo-nautilus-4fxkp
Aug 22 10:48:23.660: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 10:48:23.660: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 10:48:23.660: INFO: update-demo-nautilus-4fxkp is verified up and running
Aug 22 10:48:23.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods update-demo-nautilus-m76h8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 22 10:48:23.702: INFO: stderr: ""
Aug 22 10:48:23.702: INFO: stdout: "true"
Aug 22 10:48:23.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods update-demo-nautilus-m76h8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 22 10:48:23.744: INFO: stderr: ""
Aug 22 10:48:23.744: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Aug 22 10:48:23.744: INFO: validating pod update-demo-nautilus-m76h8
Aug 22 10:48:23.747: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 10:48:23.747: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 10:48:23.747: INFO: update-demo-nautilus-m76h8 is verified up and running
STEP: scaling down the replication controller
Aug 22 10:48:23.748: INFO: scanned /root for discovery docs: <nil>
Aug 22 10:48:23.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 22 10:48:24.809: INFO: stderr: ""
Aug 22 10:48:24.809: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 22 10:48:24.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 22 10:48:24.852: INFO: stderr: ""
Aug 22 10:48:24.852: INFO: stdout: "update-demo-nautilus-4fxkp "
Aug 22 10:48:24.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods update-demo-nautilus-4fxkp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 22 10:48:24.893: INFO: stderr: ""
Aug 22 10:48:24.893: INFO: stdout: "true"
Aug 22 10:48:24.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods update-demo-nautilus-4fxkp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 22 10:48:24.933: INFO: stderr: ""
Aug 22 10:48:24.933: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Aug 22 10:48:24.933: INFO: validating pod update-demo-nautilus-4fxkp
Aug 22 10:48:24.935: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 10:48:24.935: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 10:48:24.935: INFO: update-demo-nautilus-4fxkp is verified up and running
STEP: scaling up the replication controller
Aug 22 10:48:24.936: INFO: scanned /root for discovery docs: <nil>
Aug 22 10:48:24.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 22 10:48:25.990: INFO: stderr: ""
Aug 22 10:48:25.990: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 22 10:48:25.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 22 10:48:26.034: INFO: stderr: ""
Aug 22 10:48:26.034: INFO: stdout: "update-demo-nautilus-4fxkp update-demo-nautilus-rg44x "
Aug 22 10:48:26.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods update-demo-nautilus-4fxkp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 22 10:48:26.077: INFO: stderr: ""
Aug 22 10:48:26.077: INFO: stdout: "true"
Aug 22 10:48:26.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods update-demo-nautilus-4fxkp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 22 10:48:26.120: INFO: stderr: ""
Aug 22 10:48:26.120: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Aug 22 10:48:26.120: INFO: validating pod update-demo-nautilus-4fxkp
Aug 22 10:48:26.122: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 10:48:26.122: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 10:48:26.122: INFO: update-demo-nautilus-4fxkp is verified up and running
Aug 22 10:48:26.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods update-demo-nautilus-rg44x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 22 10:48:26.162: INFO: stderr: ""
Aug 22 10:48:26.162: INFO: stdout: "true"
Aug 22 10:48:26.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods update-demo-nautilus-rg44x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 22 10:48:26.202: INFO: stderr: ""
Aug 22 10:48:26.202: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Aug 22 10:48:26.202: INFO: validating pod update-demo-nautilus-rg44x
Aug 22 10:48:26.205: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 10:48:26.205: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 10:48:26.205: INFO: update-demo-nautilus-rg44x is verified up and running
STEP: using delete to clean up resources
Aug 22 10:48:26.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 delete --grace-period=0 --force -f -'
Aug 22 10:48:26.247: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 10:48:26.247: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 22 10:48:26.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get rc,svc -l name=update-demo --no-headers'
Aug 22 10:48:26.292: INFO: stderr: "No resources found in kubectl-1388 namespace.\n"
Aug 22 10:48:26.292: INFO: stdout: ""
Aug 22 10:48:26.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-1388 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 22 10:48:26.335: INFO: stderr: ""
Aug 22 10:48:26.335: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:48:26.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1388" for this suite.

• [SLOW TEST:8.060 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":279,"skipped":4987,"failed":0}
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:48:26.340: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Aug 22 10:48:26.364: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:48:28.369: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Aug 22 10:48:28.376: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:48:30.381: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 22 10:48:30.390: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 10:48:30.391: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 10:48:32.392: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 10:48:32.397: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:48:32.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5274" for this suite.

• [SLOW TEST:6.061 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":280,"skipped":4991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:48:32.401: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Aug 22 10:48:32.420: INFO: Waiting up to 5m0s for pod "client-containers-bdb7e2e0-1056-4d2f-b8a7-41c01fa96d2f" in namespace "containers-3011" to be "Succeeded or Failed"
Aug 22 10:48:32.422: INFO: Pod "client-containers-bdb7e2e0-1056-4d2f-b8a7-41c01fa96d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.372125ms
Aug 22 10:48:34.426: INFO: Pod "client-containers-bdb7e2e0-1056-4d2f-b8a7-41c01fa96d2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00628189s
STEP: Saw pod success
Aug 22 10:48:34.426: INFO: Pod "client-containers-bdb7e2e0-1056-4d2f-b8a7-41c01fa96d2f" satisfied condition "Succeeded or Failed"
Aug 22 10:48:34.428: INFO: Trying to get logs from node 172.23.79.105 pod client-containers-bdb7e2e0-1056-4d2f-b8a7-41c01fa96d2f container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:48:34.436: INFO: Waiting for pod client-containers-bdb7e2e0-1056-4d2f-b8a7-41c01fa96d2f to disappear
Aug 22 10:48:34.437: INFO: Pod client-containers-bdb7e2e0-1056-4d2f-b8a7-41c01fa96d2f no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:48:34.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3011" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":281,"skipped":5028,"failed":0}

------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:48:34.440: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:48:34.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9593" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":282,"skipped":5028,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:48:34.467: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Aug 22 10:48:34.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-5655 create -f -'
Aug 22 10:48:34.593: INFO: stderr: ""
Aug 22 10:48:34.593: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Aug 22 10:48:34.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-5655 diff -f -'
Aug 22 10:48:34.714: INFO: rc: 1
Aug 22 10:48:34.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-5655 delete -f -'
Aug 22 10:48:34.756: INFO: stderr: ""
Aug 22 10:48:34.756: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:48:34.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5655" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":283,"skipped":5038,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:48:34.761: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:48:35.272: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:48:38.287: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:48:38.290: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:48:41.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4471" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.617 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":284,"skipped":5118,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:48:41.378: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-ef1a2ca0-250e-479c-a9c6-707411baa96b in namespace container-probe-7019
Aug 22 10:48:43.411: INFO: Started pod liveness-ef1a2ca0-250e-479c-a9c6-707411baa96b in namespace container-probe-7019
STEP: checking the pod's current state and verifying that restartCount is present
Aug 22 10:48:43.413: INFO: Initial restart count of pod liveness-ef1a2ca0-250e-479c-a9c6-707411baa96b is 0
Aug 22 10:49:03.465: INFO: Restart count of pod container-probe-7019/liveness-ef1a2ca0-250e-479c-a9c6-707411baa96b is now 1 (20.051909005s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:49:03.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7019" for this suite.

• [SLOW TEST:22.095 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":5134,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:49:03.473: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-2581
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-2581
Aug 22 10:49:03.495: INFO: Found 0 stateful pods, waiting for 1
Aug 22 10:49:13.500: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Aug 22 10:49:13.507: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Aug 22 10:49:13.510: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Aug 22 10:49:13.512: INFO: Observed &StatefulSet event: ADDED
Aug 22 10:49:13.512: INFO: Found Statefulset ss in namespace statefulset-2581 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 22 10:49:13.512: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Aug 22 10:49:13.512: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 22 10:49:13.518: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Aug 22 10:49:13.519: INFO: Observed &StatefulSet event: ADDED
Aug 22 10:49:13.519: INFO: Observed Statefulset ss in namespace statefulset-2581 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 22 10:49:13.519: INFO: Observed &StatefulSet event: MODIFIED
Aug 22 10:49:13.519: INFO: Found Statefulset ss in namespace statefulset-2581 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Aug 22 10:49:13.519: INFO: Deleting all statefulset in ns statefulset-2581
Aug 22 10:49:13.520: INFO: Scaling statefulset ss to 0
Aug 22 10:49:23.530: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 10:49:23.532: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:49:23.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2581" for this suite.

• [SLOW TEST:20.076 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":286,"skipped":5146,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:49:23.549: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-1220
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1220
STEP: Deleting pre-stop pod
Aug 22 10:49:32.587: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:49:32.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1220" for this suite.

• [SLOW TEST:9.060 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":287,"skipped":5160,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:49:32.609: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Aug 22 10:49:32.633: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c078972-6e8f-4737-8f73-f902044507d9" in namespace "projected-4167" to be "Succeeded or Failed"
Aug 22 10:49:32.634: INFO: Pod "downwardapi-volume-9c078972-6e8f-4737-8f73-f902044507d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.499255ms
Aug 22 10:49:34.640: INFO: Pod "downwardapi-volume-9c078972-6e8f-4737-8f73-f902044507d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007167373s
STEP: Saw pod success
Aug 22 10:49:34.640: INFO: Pod "downwardapi-volume-9c078972-6e8f-4737-8f73-f902044507d9" satisfied condition "Succeeded or Failed"
Aug 22 10:49:34.641: INFO: Trying to get logs from node 172.23.79.105 pod downwardapi-volume-9c078972-6e8f-4737-8f73-f902044507d9 container client-container: <nil>
STEP: delete the pod
Aug 22 10:49:34.650: INFO: Waiting for pod downwardapi-volume-9c078972-6e8f-4737-8f73-f902044507d9 to disappear
Aug 22 10:49:34.651: INFO: Pod downwardapi-volume-9c078972-6e8f-4737-8f73-f902044507d9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:49:34.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4167" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":288,"skipped":5172,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:49:34.655: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:49:40.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2507" for this suite.

• [SLOW TEST:5.804 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":289,"skipped":5172,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:49:40.459: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 22 10:49:40.988: INFO: starting watch
STEP: patching
STEP: updating
Aug 22 10:49:40.993: INFO: waiting for watch events with expected annotations
Aug 22 10:49:40.993: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:49:41.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5187" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":290,"skipped":5185,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:49:41.018: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:49:41.038: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 22 10:49:41.042: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:49:41.051: INFO: Number of nodes with available pods: 0
Aug 22 10:49:41.051: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 10:49:42.055: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:49:42.057: INFO: Number of nodes with available pods: 1
Aug 22 10:49:42.057: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 10:49:43.055: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:49:43.057: INFO: Number of nodes with available pods: 2
Aug 22 10:49:43.057: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 22 10:49:43.069: INFO: Wrong image for pod: daemon-set-2vmps. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Aug 22 10:49:43.069: INFO: Wrong image for pod: daemon-set-7fwng. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Aug 22 10:49:43.072: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:49:44.077: INFO: Wrong image for pod: daemon-set-2vmps. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Aug 22 10:49:44.079: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:49:45.077: INFO: Wrong image for pod: daemon-set-2vmps. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Aug 22 10:49:45.079: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:49:46.075: INFO: Wrong image for pod: daemon-set-2vmps. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Aug 22 10:49:46.075: INFO: Pod daemon-set-qlwtn is not available
Aug 22 10:49:46.076: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:49:47.077: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:49:48.076: INFO: Pod daemon-set-lflw9 is not available
Aug 22 10:49:48.077: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 22 10:49:48.079: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:49:48.080: INFO: Number of nodes with available pods: 1
Aug 22 10:49:48.080: INFO: Node 172.23.79.105 is running more than one daemon pod
Aug 22 10:49:49.084: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:49:49.086: INFO: Number of nodes with available pods: 2
Aug 22 10:49:49.086: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4143, will wait for the garbage collector to delete the pods
Aug 22 10:49:49.147: INFO: Deleting DaemonSet.extensions daemon-set took: 2.789803ms
Aug 22 10:49:49.248: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.469473ms
Aug 22 10:49:51.253: INFO: Number of nodes with available pods: 0
Aug 22 10:49:51.253: INFO: Number of running nodes: 0, number of available pods: 0
Aug 22 10:49:51.254: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26264"},"items":null}

Aug 22 10:49:51.256: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26264"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:49:51.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4143" for this suite.

• [SLOW TEST:10.246 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":291,"skipped":5196,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:49:51.264: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 22 10:49:51.291: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 22 10:49:51.293: INFO: starting watch
STEP: patching
STEP: updating
Aug 22 10:49:51.299: INFO: waiting for watch events with expected annotations
Aug 22 10:49:51.299: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:49:51.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-7552" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":292,"skipped":5211,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:49:51.318: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-5686
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-5686
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5686
Aug 22 10:49:51.338: INFO: Found 0 stateful pods, waiting for 1
Aug 22 10:50:01.342: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 22 10:50:01.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-5686 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 22 10:50:01.449: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 22 10:50:01.449: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 22 10:50:01.449: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 22 10:50:01.451: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 22 10:50:11.456: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 10:50:11.456: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 10:50:11.463: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 22 10:50:11.463: INFO: ss-0  172.23.79.105  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:49:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:50:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:50:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:49:51 +0000 UTC  }]
Aug 22 10:50:11.463: INFO: 
Aug 22 10:50:11.463: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 22 10:50:12.467: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997477153s
Aug 22 10:50:13.473: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992811618s
Aug 22 10:50:14.477: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98770574s
Aug 22 10:50:15.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984245574s
Aug 22 10:50:16.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980749782s
Aug 22 10:50:17.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975847855s
Aug 22 10:50:18.494: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971048827s
Aug 22 10:50:19.497: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967187372s
Aug 22 10:50:20.501: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.82458ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5686
Aug 22 10:50:21.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-5686 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 22 10:50:21.606: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 22 10:50:21.606: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 22 10:50:21.606: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 22 10:50:21.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-5686 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 22 10:50:21.705: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 22 10:50:21.705: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 22 10:50:21.705: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 22 10:50:21.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-5686 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 22 10:50:21.800: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 22 10:50:21.800: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 22 10:50:21.800: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 22 10:50:21.802: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug 22 10:50:31.812: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 10:50:31.812: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 10:50:31.812: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 22 10:50:31.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-5686 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 22 10:50:31.911: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 22 10:50:31.911: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 22 10:50:31.911: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 22 10:50:31.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-5686 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 22 10:50:32.011: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 22 10:50:32.011: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 22 10:50:32.011: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 22 10:50:32.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-5686 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 22 10:50:32.113: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 22 10:50:32.113: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 22 10:50:32.113: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 22 10:50:32.113: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 10:50:32.115: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 22 10:50:42.125: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 10:50:42.125: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 10:50:42.125: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 10:50:42.136: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 22 10:50:42.136: INFO: ss-0  172.23.79.105  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:49:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:50:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:50:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:49:51 +0000 UTC  }]
Aug 22 10:50:42.136: INFO: ss-1  172.23.79.105  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:50:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:50:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:50:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:50:11 +0000 UTC  }]
Aug 22 10:50:42.136: INFO: ss-2  172.23.79.105  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:50:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:50:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:50:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-08-22 10:50:11 +0000 UTC  }]
Aug 22 10:50:42.136: INFO: 
Aug 22 10:50:42.136: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 22 10:50:43.140: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.991221713s
Aug 22 10:50:44.143: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.987964409s
Aug 22 10:50:45.147: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.984815947s
Aug 22 10:50:46.151: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.981258016s
Aug 22 10:50:47.154: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.97700694s
Aug 22 10:50:48.158: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.973743456s
Aug 22 10:50:49.161: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.970585398s
Aug 22 10:50:50.165: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.967290295s
Aug 22 10:50:51.169: INFO: Verifying statefulset ss doesn't scale past 0 for another 962.759413ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5686
Aug 22 10:50:52.173: INFO: Scaling statefulset ss to 0
Aug 22 10:50:52.179: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Aug 22 10:50:52.180: INFO: Deleting all statefulset in ns statefulset-5686
Aug 22 10:50:52.181: INFO: Scaling statefulset ss to 0
Aug 22 10:50:52.185: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 10:50:52.187: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:50:52.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5686" for this suite.

• [SLOW TEST:60.882 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":293,"skipped":5236,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:50:52.200: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-ca4b2697-a1ee-415b-83b1-39674d60e737
STEP: Creating a pod to test consume configMaps
Aug 22 10:50:52.225: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-67d49ab7-1e75-4bda-ba8d-8a6f23eeb2b7" in namespace "projected-4568" to be "Succeeded or Failed"
Aug 22 10:50:52.226: INFO: Pod "pod-projected-configmaps-67d49ab7-1e75-4bda-ba8d-8a6f23eeb2b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.203522ms
Aug 22 10:50:54.231: INFO: Pod "pod-projected-configmaps-67d49ab7-1e75-4bda-ba8d-8a6f23eeb2b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005907192s
STEP: Saw pod success
Aug 22 10:50:54.231: INFO: Pod "pod-projected-configmaps-67d49ab7-1e75-4bda-ba8d-8a6f23eeb2b7" satisfied condition "Succeeded or Failed"
Aug 22 10:50:54.233: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-configmaps-67d49ab7-1e75-4bda-ba8d-8a6f23eeb2b7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 10:50:54.241: INFO: Waiting for pod pod-projected-configmaps-67d49ab7-1e75-4bda-ba8d-8a6f23eeb2b7 to disappear
Aug 22 10:50:54.243: INFO: Pod pod-projected-configmaps-67d49ab7-1e75-4bda-ba8d-8a6f23eeb2b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:50:54.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4568" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5254,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:50:54.246: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-0a0a4b0e-79ce-4556-bf5f-e06c0d7b23db
STEP: Creating a pod to test consume configMaps
Aug 22 10:50:54.269: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf5eb285-c7c4-4617-a97e-0736235f605d" in namespace "projected-2611" to be "Succeeded or Failed"
Aug 22 10:50:54.271: INFO: Pod "pod-projected-configmaps-bf5eb285-c7c4-4617-a97e-0736235f605d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.683489ms
Aug 22 10:50:56.276: INFO: Pod "pod-projected-configmaps-bf5eb285-c7c4-4617-a97e-0736235f605d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006708854s
STEP: Saw pod success
Aug 22 10:50:56.276: INFO: Pod "pod-projected-configmaps-bf5eb285-c7c4-4617-a97e-0736235f605d" satisfied condition "Succeeded or Failed"
Aug 22 10:50:56.278: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-configmaps-bf5eb285-c7c4-4617-a97e-0736235f605d container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:50:56.288: INFO: Waiting for pod pod-projected-configmaps-bf5eb285-c7c4-4617-a97e-0736235f605d to disappear
Aug 22 10:50:56.289: INFO: Pod pod-projected-configmaps-bf5eb285-c7c4-4617-a97e-0736235f605d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:50:56.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2611" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":295,"skipped":5292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:50:56.293: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 22 10:50:56.315: INFO: The status of Pod pod-update-activedeadlineseconds-ce5fc3d4-4339-4438-a4f6-b0bab8c075ee is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:50:58.319: INFO: The status of Pod pod-update-activedeadlineseconds-ce5fc3d4-4339-4438-a4f6-b0bab8c075ee is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 22 10:50:58.831: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ce5fc3d4-4339-4438-a4f6-b0bab8c075ee"
Aug 22 10:50:58.831: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ce5fc3d4-4339-4438-a4f6-b0bab8c075ee" in namespace "pods-4262" to be "terminated due to deadline exceeded"
Aug 22 10:50:58.832: INFO: Pod "pod-update-activedeadlineseconds-ce5fc3d4-4339-4438-a4f6-b0bab8c075ee": Phase="Running", Reason="", readiness=true. Elapsed: 1.268755ms
Aug 22 10:51:00.838: INFO: Pod "pod-update-activedeadlineseconds-ce5fc3d4-4339-4438-a4f6-b0bab8c075ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.007361263s
Aug 22 10:51:02.844: INFO: Pod "pod-update-activedeadlineseconds-ce5fc3d4-4339-4438-a4f6-b0bab8c075ee": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 4.013155918s
Aug 22 10:51:02.844: INFO: Pod "pod-update-activedeadlineseconds-ce5fc3d4-4339-4438-a4f6-b0bab8c075ee" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:02.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4262" for this suite.

• [SLOW TEST:6.556 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5319,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:02.849: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Aug 22 10:51:02.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-9039 cluster-info'
Aug 22 10:51:02.906: INFO: stderr: ""
Aug 22 10:51:02.906: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:02.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9039" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":297,"skipped":5323,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:02.910: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:15.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5202" for this suite.
STEP: Destroying namespace "nsdeletetest-3480" for this suite.
Aug 22 10:51:15.974: INFO: Namespace nsdeletetest-3480 was already deleted
STEP: Destroying namespace "nsdeletetest-298" for this suite.

• [SLOW TEST:13.066 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":298,"skipped":5327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:15.976: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-2346/configmap-test-e07d566c-1e5b-4610-9424-98ab85f2f86e
STEP: Creating a pod to test consume configMaps
Aug 22 10:51:15.993: INFO: Waiting up to 5m0s for pod "pod-configmaps-6b44bcbf-c832-4349-8d9b-379a08e20360" in namespace "configmap-2346" to be "Succeeded or Failed"
Aug 22 10:51:15.995: INFO: Pod "pod-configmaps-6b44bcbf-c832-4349-8d9b-379a08e20360": Phase="Pending", Reason="", readiness=false. Elapsed: 1.292603ms
Aug 22 10:51:17.998: INFO: Pod "pod-configmaps-6b44bcbf-c832-4349-8d9b-379a08e20360": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004761652s
STEP: Saw pod success
Aug 22 10:51:17.998: INFO: Pod "pod-configmaps-6b44bcbf-c832-4349-8d9b-379a08e20360" satisfied condition "Succeeded or Failed"
Aug 22 10:51:18.000: INFO: Trying to get logs from node 172.23.79.105 pod pod-configmaps-6b44bcbf-c832-4349-8d9b-379a08e20360 container env-test: <nil>
STEP: delete the pod
Aug 22 10:51:18.008: INFO: Waiting for pod pod-configmaps-6b44bcbf-c832-4349-8d9b-379a08e20360 to disappear
Aug 22 10:51:18.009: INFO: Pod pod-configmaps-6b44bcbf-c832-4349-8d9b-379a08e20360 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:18.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2346" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5356,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:18.012: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 22 10:51:18.034: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1540  c22eec5a-5753-419f-8161-b00e5011f1f8 26840 0 2021-08-22 10:51:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-08-22 10:51:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 10:51:18.035: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1540  c22eec5a-5753-419f-8161-b00e5011f1f8 26841 0 2021-08-22 10:51:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-08-22 10:51:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 10:51:18.035: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1540  c22eec5a-5753-419f-8161-b00e5011f1f8 26842 0 2021-08-22 10:51:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-08-22 10:51:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 22 10:51:28.049: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1540  c22eec5a-5753-419f-8161-b00e5011f1f8 26884 0 2021-08-22 10:51:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-08-22 10:51:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 10:51:28.049: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1540  c22eec5a-5753-419f-8161-b00e5011f1f8 26885 0 2021-08-22 10:51:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-08-22 10:51:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 10:51:28.049: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1540  c22eec5a-5753-419f-8161-b00e5011f1f8 26886 0 2021-08-22 10:51:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-08-22 10:51:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:28.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1540" for this suite.

• [SLOW TEST:10.041 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":300,"skipped":5357,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:28.053: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:51:28.068: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 22 10:51:30.089: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:31.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6419" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":301,"skipped":5359,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:31.098: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Aug 22 10:51:31.118: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:51:33.122: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 22 10:51:34.132: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:35.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6046" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":302,"skipped":5374,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:35.147: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 22 10:51:35.167: INFO: Waiting up to 5m0s for pod "pod-8b2f962c-bd29-4e90-ae3a-87bbebae8285" in namespace "emptydir-1324" to be "Succeeded or Failed"
Aug 22 10:51:35.168: INFO: Pod "pod-8b2f962c-bd29-4e90-ae3a-87bbebae8285": Phase="Pending", Reason="", readiness=false. Elapsed: 1.364525ms
Aug 22 10:51:37.171: INFO: Pod "pod-8b2f962c-bd29-4e90-ae3a-87bbebae8285": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004207149s
STEP: Saw pod success
Aug 22 10:51:37.171: INFO: Pod "pod-8b2f962c-bd29-4e90-ae3a-87bbebae8285" satisfied condition "Succeeded or Failed"
Aug 22 10:51:37.173: INFO: Trying to get logs from node 172.23.79.105 pod pod-8b2f962c-bd29-4e90-ae3a-87bbebae8285 container test-container: <nil>
STEP: delete the pod
Aug 22 10:51:37.180: INFO: Waiting for pod pod-8b2f962c-bd29-4e90-ae3a-87bbebae8285 to disappear
Aug 22 10:51:37.181: INFO: Pod pod-8b2f962c-bd29-4e90-ae3a-87bbebae8285 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:37.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1324" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":303,"skipped":5388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:37.185: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:37.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8625" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":304,"skipped":5422,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:37.214: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 22 10:51:43.260: INFO: The status of Pod kube-controller-manager-172.23.79.104 is Running (Ready = true)
Aug 22 10:51:43.385: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:43.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7577" for this suite.

• [SLOW TEST:6.175 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":305,"skipped":5459,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:43.389: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-qp9bp in namespace proxy-6168
I0822 10:51:43.421467      21 runners.go:190] Created replication controller with name: proxy-service-qp9bp, namespace: proxy-6168, replica count: 1
I0822 10:51:44.472146      21 runners.go:190] proxy-service-qp9bp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 10:51:45.472228      21 runners.go:190] proxy-service-qp9bp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 10:51:46.472393      21 runners.go:190] proxy-service-qp9bp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 10:51:46.477: INFO: setup took 3.074285549s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 22 10:51:46.482: INFO: (0) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.331338ms)
Aug 22 10:51:46.482: INFO: (0) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 4.146866ms)
Aug 22 10:51:46.484: INFO: (0) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 6.20082ms)
Aug 22 10:51:46.484: INFO: (0) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 6.389576ms)
Aug 22 10:51:46.484: INFO: (0) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 6.452328ms)
Aug 22 10:51:46.484: INFO: (0) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 6.417152ms)
Aug 22 10:51:46.484: INFO: (0) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 6.496416ms)
Aug 22 10:51:46.484: INFO: (0) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 6.503327ms)
Aug 22 10:51:46.484: INFO: (0) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 6.497839ms)
Aug 22 10:51:46.484: INFO: (0) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 6.470641ms)
Aug 22 10:51:46.484: INFO: (0) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 6.59007ms)
Aug 22 10:51:46.485: INFO: (0) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 7.768286ms)
Aug 22 10:51:46.485: INFO: (0) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 7.780899ms)
Aug 22 10:51:46.485: INFO: (0) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 8.208433ms)
Aug 22 10:51:46.487: INFO: (0) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 10.197495ms)
Aug 22 10:51:46.487: INFO: (0) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 10.072242ms)
Aug 22 10:51:46.490: INFO: (1) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 2.936261ms)
Aug 22 10:51:46.491: INFO: (1) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 3.00109ms)
Aug 22 10:51:46.491: INFO: (1) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 3.027463ms)
Aug 22 10:51:46.491: INFO: (1) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 2.999791ms)
Aug 22 10:51:46.491: INFO: (1) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 3.099283ms)
Aug 22 10:51:46.491: INFO: (1) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 3.064854ms)
Aug 22 10:51:46.491: INFO: (1) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 3.279685ms)
Aug 22 10:51:46.491: INFO: (1) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.061901ms)
Aug 22 10:51:46.491: INFO: (1) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.217615ms)
Aug 22 10:51:46.491: INFO: (1) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.340272ms)
Aug 22 10:51:46.492: INFO: (1) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.69823ms)
Aug 22 10:51:46.492: INFO: (1) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 4.754882ms)
Aug 22 10:51:46.492: INFO: (1) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 4.982993ms)
Aug 22 10:51:46.492: INFO: (1) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.922529ms)
Aug 22 10:51:46.492: INFO: (1) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 4.824225ms)
Aug 22 10:51:46.492: INFO: (1) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 4.86375ms)
Aug 22 10:51:46.494: INFO: (2) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 1.522833ms)
Aug 22 10:51:46.495: INFO: (2) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 2.160826ms)
Aug 22 10:51:46.495: INFO: (2) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 2.35832ms)
Aug 22 10:51:46.495: INFO: (2) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 2.290371ms)
Aug 22 10:51:46.495: INFO: (2) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 2.46655ms)
Aug 22 10:51:46.496: INFO: (2) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 3.850598ms)
Aug 22 10:51:46.496: INFO: (2) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.673961ms)
Aug 22 10:51:46.497: INFO: (2) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 3.964929ms)
Aug 22 10:51:46.497: INFO: (2) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 4.160212ms)
Aug 22 10:51:46.497: INFO: (2) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.99412ms)
Aug 22 10:51:46.497: INFO: (2) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 4.050388ms)
Aug 22 10:51:46.497: INFO: (2) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 4.089047ms)
Aug 22 10:51:46.497: INFO: (2) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 4.338157ms)
Aug 22 10:51:46.497: INFO: (2) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 4.234708ms)
Aug 22 10:51:46.497: INFO: (2) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.605576ms)
Aug 22 10:51:46.497: INFO: (2) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.66131ms)
Aug 22 10:51:46.507: INFO: (3) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 8.815748ms)
Aug 22 10:51:46.507: INFO: (3) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 8.812577ms)
Aug 22 10:51:46.507: INFO: (3) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 8.888929ms)
Aug 22 10:51:46.507: INFO: (3) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 9.206675ms)
Aug 22 10:51:46.507: INFO: (3) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 9.427831ms)
Aug 22 10:51:46.507: INFO: (3) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 9.936305ms)
Aug 22 10:51:46.507: INFO: (3) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 9.884356ms)
Aug 22 10:51:46.507: INFO: (3) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 10.031227ms)
Aug 22 10:51:46.508: INFO: (3) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 9.866246ms)
Aug 22 10:51:46.508: INFO: (3) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 9.83401ms)
Aug 22 10:51:46.508: INFO: (3) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 10.170955ms)
Aug 22 10:51:46.508: INFO: (3) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 9.788214ms)
Aug 22 10:51:46.508: INFO: (3) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 9.930676ms)
Aug 22 10:51:46.508: INFO: (3) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 10.015141ms)
Aug 22 10:51:46.508: INFO: (3) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 10.058586ms)
Aug 22 10:51:46.508: INFO: (3) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 10.491984ms)
Aug 22 10:51:46.511: INFO: (4) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 3.220501ms)
Aug 22 10:51:46.512: INFO: (4) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 3.570881ms)
Aug 22 10:51:46.512: INFO: (4) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 3.607763ms)
Aug 22 10:51:46.512: INFO: (4) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.122042ms)
Aug 22 10:51:46.512: INFO: (4) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 4.157865ms)
Aug 22 10:51:46.512: INFO: (4) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.294672ms)
Aug 22 10:51:46.512: INFO: (4) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 4.643964ms)
Aug 22 10:51:46.512: INFO: (4) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.608286ms)
Aug 22 10:51:46.513: INFO: (4) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 4.569726ms)
Aug 22 10:51:46.513: INFO: (4) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.647409ms)
Aug 22 10:51:46.513: INFO: (4) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 4.651332ms)
Aug 22 10:51:46.513: INFO: (4) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 4.604375ms)
Aug 22 10:51:46.513: INFO: (4) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 4.793946ms)
Aug 22 10:51:46.513: INFO: (4) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 4.842002ms)
Aug 22 10:51:46.513: INFO: (4) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 4.908837ms)
Aug 22 10:51:46.513: INFO: (4) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.86866ms)
Aug 22 10:51:46.515: INFO: (5) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 1.844764ms)
Aug 22 10:51:46.515: INFO: (5) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 1.854425ms)
Aug 22 10:51:46.516: INFO: (5) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 2.874572ms)
Aug 22 10:51:46.516: INFO: (5) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 3.366701ms)
Aug 22 10:51:46.516: INFO: (5) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 3.394363ms)
Aug 22 10:51:46.516: INFO: (5) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.465685ms)
Aug 22 10:51:46.517: INFO: (5) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 3.367908ms)
Aug 22 10:51:46.517: INFO: (5) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 3.57483ms)
Aug 22 10:51:46.517: INFO: (5) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 3.608851ms)
Aug 22 10:51:46.517: INFO: (5) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.68829ms)
Aug 22 10:51:46.518: INFO: (5) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 5.032832ms)
Aug 22 10:51:46.518: INFO: (5) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.865938ms)
Aug 22 10:51:46.518: INFO: (5) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 4.903812ms)
Aug 22 10:51:46.518: INFO: (5) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 5.124571ms)
Aug 22 10:51:46.518: INFO: (5) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 5.193081ms)
Aug 22 10:51:46.518: INFO: (5) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 5.070064ms)
Aug 22 10:51:46.521: INFO: (6) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 2.809716ms)
Aug 22 10:51:46.521: INFO: (6) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.11699ms)
Aug 22 10:51:46.521: INFO: (6) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 3.113701ms)
Aug 22 10:51:46.521: INFO: (6) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.058186ms)
Aug 22 10:51:46.521: INFO: (6) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 3.096674ms)
Aug 22 10:51:46.521: INFO: (6) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 3.129373ms)
Aug 22 10:51:46.521: INFO: (6) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 3.047692ms)
Aug 22 10:51:46.521: INFO: (6) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 3.244003ms)
Aug 22 10:51:46.522: INFO: (6) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.07867ms)
Aug 22 10:51:46.522: INFO: (6) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 3.019356ms)
Aug 22 10:51:46.522: INFO: (6) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 3.867183ms)
Aug 22 10:51:46.522: INFO: (6) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.169489ms)
Aug 22 10:51:46.523: INFO: (6) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 3.996602ms)
Aug 22 10:51:46.523: INFO: (6) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.084915ms)
Aug 22 10:51:46.523: INFO: (6) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 4.315037ms)
Aug 22 10:51:46.523: INFO: (6) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 4.249708ms)
Aug 22 10:51:46.525: INFO: (7) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 2.180826ms)
Aug 22 10:51:46.525: INFO: (7) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 2.306383ms)
Aug 22 10:51:46.525: INFO: (7) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 2.368514ms)
Aug 22 10:51:46.527: INFO: (7) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.193895ms)
Aug 22 10:51:46.527: INFO: (7) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 4.327748ms)
Aug 22 10:51:46.527: INFO: (7) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 4.335436ms)
Aug 22 10:51:46.527: INFO: (7) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.465213ms)
Aug 22 10:51:46.527: INFO: (7) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 4.431838ms)
Aug 22 10:51:46.527: INFO: (7) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 4.48043ms)
Aug 22 10:51:46.528: INFO: (7) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.593448ms)
Aug 22 10:51:46.528: INFO: (7) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.721872ms)
Aug 22 10:51:46.528: INFO: (7) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.916051ms)
Aug 22 10:51:46.528: INFO: (7) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 5.227838ms)
Aug 22 10:51:46.529: INFO: (7) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 5.532527ms)
Aug 22 10:51:46.529: INFO: (7) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 5.580965ms)
Aug 22 10:51:46.529: INFO: (7) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 5.634262ms)
Aug 22 10:51:46.531: INFO: (8) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 2.239871ms)
Aug 22 10:51:46.532: INFO: (8) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 2.775945ms)
Aug 22 10:51:46.532: INFO: (8) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 2.912051ms)
Aug 22 10:51:46.532: INFO: (8) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 2.596941ms)
Aug 22 10:51:46.532: INFO: (8) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 3.146311ms)
Aug 22 10:51:46.532: INFO: (8) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 2.925985ms)
Aug 22 10:51:46.532: INFO: (8) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.087425ms)
Aug 22 10:51:46.532: INFO: (8) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 2.769989ms)
Aug 22 10:51:46.532: INFO: (8) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 3.258371ms)
Aug 22 10:51:46.532: INFO: (8) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 3.309643ms)
Aug 22 10:51:46.533: INFO: (8) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 3.970506ms)
Aug 22 10:51:46.533: INFO: (8) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 4.47711ms)
Aug 22 10:51:46.533: INFO: (8) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.411111ms)
Aug 22 10:51:46.533: INFO: (8) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 4.457794ms)
Aug 22 10:51:46.533: INFO: (8) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 4.395465ms)
Aug 22 10:51:46.533: INFO: (8) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 4.294944ms)
Aug 22 10:51:46.536: INFO: (9) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 2.44602ms)
Aug 22 10:51:46.536: INFO: (9) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 2.618782ms)
Aug 22 10:51:46.537: INFO: (9) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 2.850414ms)
Aug 22 10:51:46.537: INFO: (9) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 2.973341ms)
Aug 22 10:51:46.537: INFO: (9) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 2.723203ms)
Aug 22 10:51:46.537: INFO: (9) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 2.983152ms)
Aug 22 10:51:46.537: INFO: (9) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 3.069828ms)
Aug 22 10:51:46.537: INFO: (9) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 2.925893ms)
Aug 22 10:51:46.537: INFO: (9) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 3.079335ms)
Aug 22 10:51:46.537: INFO: (9) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 2.922316ms)
Aug 22 10:51:46.538: INFO: (9) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 3.758216ms)
Aug 22 10:51:46.538: INFO: (9) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.476706ms)
Aug 22 10:51:46.538: INFO: (9) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 4.582656ms)
Aug 22 10:51:46.538: INFO: (9) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 4.680035ms)
Aug 22 10:51:46.538: INFO: (9) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 4.285539ms)
Aug 22 10:51:46.538: INFO: (9) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.332968ms)
Aug 22 10:51:46.542: INFO: (10) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.012832ms)
Aug 22 10:51:46.543: INFO: (10) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.765178ms)
Aug 22 10:51:46.543: INFO: (10) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 4.714476ms)
Aug 22 10:51:46.543: INFO: (10) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 4.658755ms)
Aug 22 10:51:46.543: INFO: (10) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 4.76267ms)
Aug 22 10:51:46.543: INFO: (10) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 4.577748ms)
Aug 22 10:51:46.543: INFO: (10) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 4.705527ms)
Aug 22 10:51:46.543: INFO: (10) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 4.687688ms)
Aug 22 10:51:46.543: INFO: (10) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.697857ms)
Aug 22 10:51:46.543: INFO: (10) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 4.883238ms)
Aug 22 10:51:46.543: INFO: (10) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.812932ms)
Aug 22 10:51:46.543: INFO: (10) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.714234ms)
Aug 22 10:51:46.543: INFO: (10) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 5.021556ms)
Aug 22 10:51:46.544: INFO: (10) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 5.703758ms)
Aug 22 10:51:46.544: INFO: (10) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 5.69346ms)
Aug 22 10:51:46.544: INFO: (10) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 5.614886ms)
Aug 22 10:51:46.547: INFO: (11) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.036861ms)
Aug 22 10:51:46.547: INFO: (11) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.018297ms)
Aug 22 10:51:46.548: INFO: (11) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.577172ms)
Aug 22 10:51:46.548: INFO: (11) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 3.817548ms)
Aug 22 10:51:46.548: INFO: (11) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.521097ms)
Aug 22 10:51:46.548: INFO: (11) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 3.761269ms)
Aug 22 10:51:46.548: INFO: (11) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 3.942702ms)
Aug 22 10:51:46.548: INFO: (11) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 3.995558ms)
Aug 22 10:51:46.548: INFO: (11) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 3.924694ms)
Aug 22 10:51:46.548: INFO: (11) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 3.898766ms)
Aug 22 10:51:46.549: INFO: (11) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.629744ms)
Aug 22 10:51:46.549: INFO: (11) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.678054ms)
Aug 22 10:51:46.549: INFO: (11) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 4.485118ms)
Aug 22 10:51:46.549: INFO: (11) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 4.528365ms)
Aug 22 10:51:46.549: INFO: (11) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 4.506171ms)
Aug 22 10:51:46.549: INFO: (11) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 4.712049ms)
Aug 22 10:51:46.553: INFO: (12) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.297211ms)
Aug 22 10:51:46.553: INFO: (12) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.626055ms)
Aug 22 10:51:46.553: INFO: (12) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 3.589082ms)
Aug 22 10:51:46.553: INFO: (12) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.631133ms)
Aug 22 10:51:46.553: INFO: (12) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 3.732607ms)
Aug 22 10:51:46.553: INFO: (12) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.734292ms)
Aug 22 10:51:46.553: INFO: (12) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 4.004622ms)
Aug 22 10:51:46.553: INFO: (12) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 3.671128ms)
Aug 22 10:51:46.553: INFO: (12) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 3.793443ms)
Aug 22 10:51:46.553: INFO: (12) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 3.94918ms)
Aug 22 10:51:46.554: INFO: (12) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 4.669411ms)
Aug 22 10:51:46.554: INFO: (12) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.435113ms)
Aug 22 10:51:46.554: INFO: (12) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 4.753593ms)
Aug 22 10:51:46.554: INFO: (12) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.9367ms)
Aug 22 10:51:46.554: INFO: (12) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 4.861607ms)
Aug 22 10:51:46.554: INFO: (12) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 4.771104ms)
Aug 22 10:51:46.556: INFO: (13) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 1.608829ms)
Aug 22 10:51:46.556: INFO: (13) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 1.801831ms)
Aug 22 10:51:46.557: INFO: (13) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 2.490348ms)
Aug 22 10:51:46.557: INFO: (13) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 2.431601ms)
Aug 22 10:51:46.557: INFO: (13) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 2.413591ms)
Aug 22 10:51:46.557: INFO: (13) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 2.569292ms)
Aug 22 10:51:46.558: INFO: (13) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 2.931952ms)
Aug 22 10:51:46.558: INFO: (13) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 3.087018ms)
Aug 22 10:51:46.558: INFO: (13) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.368383ms)
Aug 22 10:51:46.558: INFO: (13) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.818112ms)
Aug 22 10:51:46.558: INFO: (13) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 3.867731ms)
Aug 22 10:51:46.559: INFO: (13) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 4.161257ms)
Aug 22 10:51:46.559: INFO: (13) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 4.146962ms)
Aug 22 10:51:46.559: INFO: (13) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 4.379874ms)
Aug 22 10:51:46.559: INFO: (13) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.406415ms)
Aug 22 10:51:46.559: INFO: (13) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.375454ms)
Aug 22 10:51:46.561: INFO: (14) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 1.80244ms)
Aug 22 10:51:46.563: INFO: (14) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 3.947916ms)
Aug 22 10:51:46.563: INFO: (14) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 4.085639ms)
Aug 22 10:51:46.563: INFO: (14) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.031948ms)
Aug 22 10:51:46.563: INFO: (14) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.162432ms)
Aug 22 10:51:46.563: INFO: (14) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 4.326289ms)
Aug 22 10:51:46.563: INFO: (14) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 4.266477ms)
Aug 22 10:51:46.563: INFO: (14) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.144608ms)
Aug 22 10:51:46.563: INFO: (14) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 4.288918ms)
Aug 22 10:51:46.563: INFO: (14) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 4.321793ms)
Aug 22 10:51:46.563: INFO: (14) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.374834ms)
Aug 22 10:51:46.564: INFO: (14) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 5.060583ms)
Aug 22 10:51:46.564: INFO: (14) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 5.004465ms)
Aug 22 10:51:46.564: INFO: (14) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 5.270777ms)
Aug 22 10:51:46.564: INFO: (14) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 5.196471ms)
Aug 22 10:51:46.564: INFO: (14) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 5.19386ms)
Aug 22 10:51:46.568: INFO: (15) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 3.537244ms)
Aug 22 10:51:46.569: INFO: (15) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 4.682288ms)
Aug 22 10:51:46.569: INFO: (15) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 4.897229ms)
Aug 22 10:51:46.569: INFO: (15) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 4.610315ms)
Aug 22 10:51:46.569: INFO: (15) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 4.59789ms)
Aug 22 10:51:46.569: INFO: (15) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 4.676303ms)
Aug 22 10:51:46.569: INFO: (15) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.840649ms)
Aug 22 10:51:46.569: INFO: (15) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 4.598586ms)
Aug 22 10:51:46.569: INFO: (15) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.718542ms)
Aug 22 10:51:46.569: INFO: (15) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 4.75373ms)
Aug 22 10:51:46.569: INFO: (15) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.881271ms)
Aug 22 10:51:46.569: INFO: (15) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 5.002239ms)
Aug 22 10:51:46.570: INFO: (15) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 5.061012ms)
Aug 22 10:51:46.570: INFO: (15) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 5.024856ms)
Aug 22 10:51:46.570: INFO: (15) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 5.013313ms)
Aug 22 10:51:46.570: INFO: (15) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 4.962868ms)
Aug 22 10:51:46.572: INFO: (16) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 2.155505ms)
Aug 22 10:51:46.574: INFO: (16) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 3.736958ms)
Aug 22 10:51:46.574: INFO: (16) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.30184ms)
Aug 22 10:51:46.574: INFO: (16) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 4.476777ms)
Aug 22 10:51:46.574: INFO: (16) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.437709ms)
Aug 22 10:51:46.574: INFO: (16) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 4.428814ms)
Aug 22 10:51:46.574: INFO: (16) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.442894ms)
Aug 22 10:51:46.574: INFO: (16) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 4.734881ms)
Aug 22 10:51:46.575: INFO: (16) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 4.726592ms)
Aug 22 10:51:46.575: INFO: (16) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 4.817882ms)
Aug 22 10:51:46.575: INFO: (16) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 4.764667ms)
Aug 22 10:51:46.575: INFO: (16) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.835002ms)
Aug 22 10:51:46.575: INFO: (16) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 4.817172ms)
Aug 22 10:51:46.575: INFO: (16) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.894232ms)
Aug 22 10:51:46.575: INFO: (16) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.971197ms)
Aug 22 10:51:46.575: INFO: (16) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 5.001906ms)
Aug 22 10:51:46.577: INFO: (17) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 2.01604ms)
Aug 22 10:51:46.579: INFO: (17) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 4.230811ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 4.713319ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.765032ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 4.861541ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 4.888566ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 4.907707ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 4.860295ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 4.850467ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 5.016537ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 5.032868ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.997185ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 5.033952ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 5.069854ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 5.05021ms)
Aug 22 10:51:46.580: INFO: (17) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 5.106225ms)
Aug 22 10:51:46.583: INFO: (18) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 2.97301ms)
Aug 22 10:51:46.583: INFO: (18) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 2.789616ms)
Aug 22 10:51:46.583: INFO: (18) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 3.092043ms)
Aug 22 10:51:46.583: INFO: (18) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 3.086102ms)
Aug 22 10:51:46.583: INFO: (18) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 3.108358ms)
Aug 22 10:51:46.584: INFO: (18) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.257515ms)
Aug 22 10:51:46.584: INFO: (18) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.106996ms)
Aug 22 10:51:46.584: INFO: (18) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 3.206765ms)
Aug 22 10:51:46.584: INFO: (18) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.077237ms)
Aug 22 10:51:46.584: INFO: (18) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 3.392917ms)
Aug 22 10:51:46.585: INFO: (18) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 4.639516ms)
Aug 22 10:51:46.585: INFO: (18) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 4.411305ms)
Aug 22 10:51:46.585: INFO: (18) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 4.498529ms)
Aug 22 10:51:46.585: INFO: (18) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 4.705491ms)
Aug 22 10:51:46.585: INFO: (18) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.767285ms)
Aug 22 10:51:46.585: INFO: (18) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 5.156876ms)
Aug 22 10:51:46.587: INFO: (19) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 1.880751ms)
Aug 22 10:51:46.589: INFO: (19) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:443/proxy/tlsrewritem... (200; 2.721403ms)
Aug 22 10:51:46.589: INFO: (19) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:462/proxy/: tls qux (200; 3.082792ms)
Aug 22 10:51:46.589: INFO: (19) /api/v1/namespaces/proxy-6168/pods/https:proxy-service-qp9bp-mvdmt:460/proxy/: tls baz (200; 3.126014ms)
Aug 22 10:51:46.589: INFO: (19) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">test<... (200; 2.955774ms)
Aug 22 10:51:46.589: INFO: (19) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:1080/proxy/rewriteme">... (200; 3.016412ms)
Aug 22 10:51:46.589: INFO: (19) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:162/proxy/: bar (200; 3.120885ms)
Aug 22 10:51:46.589: INFO: (19) /api/v1/namespaces/proxy-6168/pods/http:proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.539172ms)
Aug 22 10:51:46.589: INFO: (19) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt:160/proxy/: foo (200; 3.647568ms)
Aug 22 10:51:46.589: INFO: (19) /api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/: <a href="/api/v1/namespaces/proxy-6168/pods/proxy-service-qp9bp-mvdmt/proxy/rewriteme">test</a> (200; 3.831199ms)
Aug 22 10:51:46.590: INFO: (19) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname2/proxy/: bar (200; 4.138132ms)
Aug 22 10:51:46.590: INFO: (19) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname1/proxy/: tls baz (200; 4.557339ms)
Aug 22 10:51:46.590: INFO: (19) /api/v1/namespaces/proxy-6168/services/https:proxy-service-qp9bp:tlsportname2/proxy/: tls qux (200; 4.529031ms)
Aug 22 10:51:46.590: INFO: (19) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname2/proxy/: bar (200; 4.907798ms)
Aug 22 10:51:46.591: INFO: (19) /api/v1/namespaces/proxy-6168/services/proxy-service-qp9bp:portname1/proxy/: foo (200; 5.078653ms)
Aug 22 10:51:46.591: INFO: (19) /api/v1/namespaces/proxy-6168/services/http:proxy-service-qp9bp:portname1/proxy/: foo (200; 4.833928ms)
STEP: deleting ReplicationController proxy-service-qp9bp in namespace proxy-6168, will wait for the garbage collector to delete the pods
Aug 22 10:51:46.645: INFO: Deleting ReplicationController proxy-service-qp9bp took: 2.335814ms
Aug 22 10:51:46.746: INFO: Terminating ReplicationController proxy-service-qp9bp pods took: 100.502364ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:48.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6168" for this suite.

• [SLOW TEST:5.565 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":306,"skipped":5480,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:48.955: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:48.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-638" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":307,"skipped":5492,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:48.981: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-294ecc5d-d6e6-437f-ba53-916ff582d0a6
STEP: Creating a pod to test consume configMaps
Aug 22 10:51:48.999: INFO: Waiting up to 5m0s for pod "pod-configmaps-5df8099c-ce9c-457e-99d9-de42e53d0120" in namespace "configmap-5847" to be "Succeeded or Failed"
Aug 22 10:51:49.000: INFO: Pod "pod-configmaps-5df8099c-ce9c-457e-99d9-de42e53d0120": Phase="Pending", Reason="", readiness=false. Elapsed: 1.38587ms
Aug 22 10:51:51.006: INFO: Pod "pod-configmaps-5df8099c-ce9c-457e-99d9-de42e53d0120": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007240885s
STEP: Saw pod success
Aug 22 10:51:51.006: INFO: Pod "pod-configmaps-5df8099c-ce9c-457e-99d9-de42e53d0120" satisfied condition "Succeeded or Failed"
Aug 22 10:51:51.008: INFO: Trying to get logs from node 172.23.79.105 pod pod-configmaps-5df8099c-ce9c-457e-99d9-de42e53d0120 container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:51:51.017: INFO: Waiting for pod pod-configmaps-5df8099c-ce9c-457e-99d9-de42e53d0120 to disappear
Aug 22 10:51:51.018: INFO: Pod pod-configmaps-5df8099c-ce9c-457e-99d9-de42e53d0120 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:51.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5847" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":5520,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:51.021: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:51:51.304: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 22 10:51:53.312: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765226311, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765226311, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765226311, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765226311, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:51:56.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:51:56.325: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2212-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:51:59.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1389" for this suite.
STEP: Destroying namespace "webhook-1389-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.445 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":309,"skipped":5521,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:51:59.467: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Aug 22 10:51:59.510: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:52:02.106: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:52:11.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2011" for this suite.

• [SLOW TEST:11.574 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":310,"skipped":5538,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:52:11.041: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Aug 22 10:52:11.067: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 22 10:52:16.075: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:52:16.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3742" for this suite.

• [SLOW TEST:5.053 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":311,"skipped":5579,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:52:16.095: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Aug 22 10:52:16.114: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Aug 22 10:52:24.608: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
Aug 22 10:52:27.186: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:52:35.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-596" for this suite.

• [SLOW TEST:19.011 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":312,"skipped":5581,"failed":0}
SS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:52:35.106: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Aug 22 10:52:35.122: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:52:38.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9061" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":313,"skipped":5583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:52:38.513: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:52:38.700: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 22 10:52:38.701: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 22 10:52:38.701: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 22 10:52:38.701: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 22 10:52:38.701: INFO: Checking APIGroup: apps
Aug 22 10:52:38.702: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 22 10:52:38.702: INFO: Versions found [{apps/v1 v1}]
Aug 22 10:52:38.702: INFO: apps/v1 matches apps/v1
Aug 22 10:52:38.702: INFO: Checking APIGroup: events.k8s.io
Aug 22 10:52:38.702: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 22 10:52:38.702: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Aug 22 10:52:38.702: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 22 10:52:38.702: INFO: Checking APIGroup: authentication.k8s.io
Aug 22 10:52:38.703: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 22 10:52:38.703: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 22 10:52:38.703: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 22 10:52:38.703: INFO: Checking APIGroup: authorization.k8s.io
Aug 22 10:52:38.703: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 22 10:52:38.703: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 22 10:52:38.703: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 22 10:52:38.703: INFO: Checking APIGroup: autoscaling
Aug 22 10:52:38.704: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Aug 22 10:52:38.704: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Aug 22 10:52:38.704: INFO: autoscaling/v1 matches autoscaling/v1
Aug 22 10:52:38.704: INFO: Checking APIGroup: batch
Aug 22 10:52:38.705: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 22 10:52:38.705: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Aug 22 10:52:38.705: INFO: batch/v1 matches batch/v1
Aug 22 10:52:38.705: INFO: Checking APIGroup: certificates.k8s.io
Aug 22 10:52:38.705: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 22 10:52:38.705: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 22 10:52:38.705: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 22 10:52:38.705: INFO: Checking APIGroup: networking.k8s.io
Aug 22 10:52:38.706: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 22 10:52:38.706: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 22 10:52:38.706: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 22 10:52:38.706: INFO: Checking APIGroup: policy
Aug 22 10:52:38.706: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 22 10:52:38.706: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Aug 22 10:52:38.706: INFO: policy/v1 matches policy/v1
Aug 22 10:52:38.706: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 22 10:52:38.707: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 22 10:52:38.707: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 22 10:52:38.707: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 22 10:52:38.707: INFO: Checking APIGroup: storage.k8s.io
Aug 22 10:52:38.707: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 22 10:52:38.707: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 22 10:52:38.707: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 22 10:52:38.707: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 22 10:52:38.708: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 22 10:52:38.708: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 22 10:52:38.708: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 22 10:52:38.708: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 22 10:52:38.708: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 22 10:52:38.708: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 22 10:52:38.708: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 22 10:52:38.708: INFO: Checking APIGroup: scheduling.k8s.io
Aug 22 10:52:38.709: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 22 10:52:38.709: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 22 10:52:38.709: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 22 10:52:38.709: INFO: Checking APIGroup: coordination.k8s.io
Aug 22 10:52:38.709: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 22 10:52:38.709: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 22 10:52:38.709: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 22 10:52:38.709: INFO: Checking APIGroup: node.k8s.io
Aug 22 10:52:38.710: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 22 10:52:38.710: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Aug 22 10:52:38.710: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 22 10:52:38.710: INFO: Checking APIGroup: discovery.k8s.io
Aug 22 10:52:38.710: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 22 10:52:38.710: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Aug 22 10:52:38.710: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 22 10:52:38.710: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 22 10:52:38.711: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Aug 22 10:52:38.711: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Aug 22 10:52:38.711: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:52:38.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2684" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":314,"skipped":5608,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:52:38.715: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:52:38.735: INFO: The status of Pod busybox-readonly-fs29e8e5b2-c08b-42c9-9f4c-035735ff3d8b is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:52:40.741: INFO: The status of Pod busybox-readonly-fs29e8e5b2-c08b-42c9-9f4c-035735ff3d8b is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:52:40.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-98" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":315,"skipped":5645,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:52:40.750: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:52:56.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7185" for this suite.

• [SLOW TEST:16.089 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":316,"skipped":5650,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:52:56.839: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:52:56.857: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 22 10:52:58.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-7026 --namespace=crd-publish-openapi-7026 create -f -'
Aug 22 10:52:58.692: INFO: stderr: ""
Aug 22 10:52:58.692: INFO: stdout: "e2e-test-crd-publish-openapi-3421-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 22 10:52:58.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-7026 --namespace=crd-publish-openapi-7026 delete e2e-test-crd-publish-openapi-3421-crds test-cr'
Aug 22 10:52:58.737: INFO: stderr: ""
Aug 22 10:52:58.737: INFO: stdout: "e2e-test-crd-publish-openapi-3421-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 22 10:52:58.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-7026 --namespace=crd-publish-openapi-7026 apply -f -'
Aug 22 10:52:58.853: INFO: stderr: ""
Aug 22 10:52:58.853: INFO: stdout: "e2e-test-crd-publish-openapi-3421-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 22 10:52:58.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-7026 --namespace=crd-publish-openapi-7026 delete e2e-test-crd-publish-openapi-3421-crds test-cr'
Aug 22 10:52:58.895: INFO: stderr: ""
Aug 22 10:52:58.895: INFO: stdout: "e2e-test-crd-publish-openapi-3421-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 22 10:52:58.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-7026 explain e2e-test-crd-publish-openapi-3421-crds'
Aug 22 10:52:59.004: INFO: stderr: ""
Aug 22 10:52:59.004: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3421-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:53:01.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7026" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":317,"skipped":5653,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:53:01.582: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:53:01.599: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:53:07.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1723" for this suite.

• [SLOW TEST:6.184 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":318,"skipped":5688,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:53:07.766: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 22 10:53:07.784: INFO: Waiting up to 5m0s for pod "pod-5a03977d-b232-4692-89c3-62677b9b83a5" in namespace "emptydir-3989" to be "Succeeded or Failed"
Aug 22 10:53:07.786: INFO: Pod "pod-5a03977d-b232-4692-89c3-62677b9b83a5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.660688ms
Aug 22 10:53:09.791: INFO: Pod "pod-5a03977d-b232-4692-89c3-62677b9b83a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006882896s
STEP: Saw pod success
Aug 22 10:53:09.791: INFO: Pod "pod-5a03977d-b232-4692-89c3-62677b9b83a5" satisfied condition "Succeeded or Failed"
Aug 22 10:53:09.792: INFO: Trying to get logs from node 172.23.79.105 pod pod-5a03977d-b232-4692-89c3-62677b9b83a5 container test-container: <nil>
STEP: delete the pod
Aug 22 10:53:09.800: INFO: Waiting for pod pod-5a03977d-b232-4692-89c3-62677b9b83a5 to disappear
Aug 22 10:53:09.801: INFO: Pod pod-5a03977d-b232-4692-89c3-62677b9b83a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:53:09.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3989" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":319,"skipped":5698,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:53:09.805: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Aug 22 10:53:09.824: INFO: The status of Pod annotationupdatea793fa6d-b849-46fb-b6e2-3c6f4678eb38 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:53:11.829: INFO: The status of Pod annotationupdatea793fa6d-b849-46fb-b6e2-3c6f4678eb38 is Running (Ready = true)
Aug 22 10:53:12.346: INFO: Successfully updated pod "annotationupdatea793fa6d-b849-46fb-b6e2-3c6f4678eb38"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:53:16.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9161" for this suite.

• [SLOW TEST:6.565 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":320,"skipped":5704,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:53:16.370: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Aug 22 10:53:16.389: INFO: created test-pod-1
Aug 22 10:53:16.391: INFO: created test-pod-2
Aug 22 10:53:16.396: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Aug 22 10:53:16.409: INFO: Pod quantity 3 is different from expected quantity 0
Aug 22 10:53:17.412: INFO: Pod quantity 3 is different from expected quantity 0
Aug 22 10:53:18.413: INFO: Pod quantity 3 is different from expected quantity 0
Aug 22 10:53:19.412: INFO: Pod quantity 2 is different from expected quantity 0
Aug 22 10:53:20.413: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:53:21.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-828" for this suite.

• [SLOW TEST:5.046 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":321,"skipped":5734,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:53:21.417: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Aug 22 10:53:21.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-4814 api-versions'
Aug 22 10:53:21.475: INFO: stderr: ""
Aug 22 10:53:21.475: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:53:21.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4814" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":322,"skipped":5747,"failed":0}

------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:53:21.479: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-6248
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Aug 22 10:53:21.501: INFO: Found 0 stateful pods, waiting for 3
Aug 22 10:53:31.507: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 10:53:31.507: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 10:53:31.507: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 10:53:31.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-6248 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 22 10:53:31.609: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 22 10:53:31.610: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 22 10:53:31.610: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Aug 22 10:53:41.631: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 22 10:53:51.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-6248 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 22 10:53:51.763: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 22 10:53:51.763: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 22 10:53:51.763: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Aug 22 10:54:01.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-6248 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 22 10:54:01.878: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 22 10:54:01.878: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 22 10:54:01.878: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 22 10:54:11.909: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 22 10:54:21.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=statefulset-6248 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 22 10:54:22.020: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 22 10:54:22.020: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 22 10:54:22.020: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Aug 22 10:54:32.038: INFO: Deleting all statefulset in ns statefulset-6248
Aug 22 10:54:32.039: INFO: Scaling statefulset ss2 to 0
Aug 22 10:54:42.047: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 10:54:42.049: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:54:42.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6248" for this suite.

• [SLOW TEST:80.583 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":323,"skipped":5747,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:54:42.063: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:54:42.359: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:54:45.373: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Aug 22 10:54:47.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=webhook-1164 attach --namespace=webhook-1164 to-be-attached-pod -i -c=container1'
Aug 22 10:54:47.450: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:54:47.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1164" for this suite.
STEP: Destroying namespace "webhook-1164-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.411 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":324,"skipped":5761,"failed":0}
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:54:47.474: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:54:47.494: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Creating first CR 
Aug 22 10:54:50.186: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-08-22T10:54:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-08-22T10:54:50Z]] name:name1 resourceVersion:28681 uid:0edd2a1f-3f00-4b72-8e57-0d6b2207b1b3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Aug 22 10:55:00.193: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-08-22T10:55:00Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-08-22T10:55:00Z]] name:name2 resourceVersion:28722 uid:22cb344d-0913-4ccb-84c6-4c251ad85c77] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Aug 22 10:55:10.199: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-08-22T10:54:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-08-22T10:55:10Z]] name:name1 resourceVersion:28737 uid:0edd2a1f-3f00-4b72-8e57-0d6b2207b1b3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Aug 22 10:55:20.207: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-08-22T10:55:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-08-22T10:55:20Z]] name:name2 resourceVersion:28753 uid:22cb344d-0913-4ccb-84c6-4c251ad85c77] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Aug 22 10:55:30.219: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-08-22T10:54:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-08-22T10:55:10Z]] name:name1 resourceVersion:28766 uid:0edd2a1f-3f00-4b72-8e57-0d6b2207b1b3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Aug 22 10:55:40.229: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-08-22T10:55:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-08-22T10:55:20Z]] name:name2 resourceVersion:28783 uid:22cb344d-0913-4ccb-84c6-4c251ad85c77] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:55:50.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6718" for this suite.

• [SLOW TEST:63.286 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":325,"skipped":5761,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:55:50.760: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-11ea53bf-5ed6-4acf-b927-060b7fbb73df
STEP: Creating a pod to test consume secrets
Aug 22 10:55:50.799: INFO: Waiting up to 5m0s for pod "pod-secrets-9766a66f-9761-4a60-9dab-bc9e63375bb2" in namespace "secrets-2626" to be "Succeeded or Failed"
Aug 22 10:55:50.801: INFO: Pod "pod-secrets-9766a66f-9761-4a60-9dab-bc9e63375bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.627106ms
Aug 22 10:55:52.806: INFO: Pod "pod-secrets-9766a66f-9761-4a60-9dab-bc9e63375bb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006849434s
STEP: Saw pod success
Aug 22 10:55:52.806: INFO: Pod "pod-secrets-9766a66f-9761-4a60-9dab-bc9e63375bb2" satisfied condition "Succeeded or Failed"
Aug 22 10:55:52.807: INFO: Trying to get logs from node 172.23.79.105 pod pod-secrets-9766a66f-9761-4a60-9dab-bc9e63375bb2 container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 10:55:52.822: INFO: Waiting for pod pod-secrets-9766a66f-9761-4a60-9dab-bc9e63375bb2 to disappear
Aug 22 10:55:52.823: INFO: Pod pod-secrets-9766a66f-9761-4a60-9dab-bc9e63375bb2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:55:52.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2626" for this suite.
STEP: Destroying namespace "secret-namespace-888" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":5771,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:55:52.829: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-eb10051e-2d31-49c5-a733-7d07b6b3587d
STEP: Creating a pod to test consume configMaps
Aug 22 10:55:52.849: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-40073074-e45a-47db-8802-b641c317e48d" in namespace "projected-2901" to be "Succeeded or Failed"
Aug 22 10:55:52.850: INFO: Pod "pod-projected-configmaps-40073074-e45a-47db-8802-b641c317e48d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.773159ms
Aug 22 10:55:54.856: INFO: Pod "pod-projected-configmaps-40073074-e45a-47db-8802-b641c317e48d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006894704s
STEP: Saw pod success
Aug 22 10:55:54.856: INFO: Pod "pod-projected-configmaps-40073074-e45a-47db-8802-b641c317e48d" satisfied condition "Succeeded or Failed"
Aug 22 10:55:54.857: INFO: Trying to get logs from node 172.23.79.105 pod pod-projected-configmaps-40073074-e45a-47db-8802-b641c317e48d container agnhost-container: <nil>
STEP: delete the pod
Aug 22 10:55:54.865: INFO: Waiting for pod pod-projected-configmaps-40073074-e45a-47db-8802-b641c317e48d to disappear
Aug 22 10:55:54.866: INFO: Pod pod-projected-configmaps-40073074-e45a-47db-8802-b641c317e48d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:55:54.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2901" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":5784,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:55:54.870: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:55:54.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9669" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":328,"skipped":5790,"failed":0}
SSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:55:54.905: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:55:54.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9694" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":329,"skipped":5796,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:55:54.925: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:55:54.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6400" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":330,"skipped":5797,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:55:54.945: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Aug 22 10:55:54.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-4373 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 22 10:55:55.005: INFO: stderr: ""
Aug 22 10:55:55.005: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Aug 22 10:55:55.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-4373 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Aug 22 10:55:55.149: INFO: stderr: ""
Aug 22 10:55:55.149: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Aug 22 10:55:55.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=kubectl-4373 delete pods e2e-test-httpd-pod'
Aug 22 10:55:56.975: INFO: stderr: ""
Aug 22 10:55:56.975: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:55:56.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4373" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":331,"skipped":5805,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:55:56.985: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:55:57.009: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Aug 22 10:55:59.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 create -f -'
Aug 22 10:55:59.854: INFO: stderr: ""
Aug 22 10:55:59.854: INFO: stdout: "e2e-test-crd-publish-openapi-1425-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 22 10:55:59.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 delete e2e-test-crd-publish-openapi-1425-crds test-foo'
Aug 22 10:55:59.898: INFO: stderr: ""
Aug 22 10:55:59.898: INFO: stdout: "e2e-test-crd-publish-openapi-1425-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 22 10:55:59.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 apply -f -'
Aug 22 10:56:00.015: INFO: stderr: ""
Aug 22 10:56:00.015: INFO: stdout: "e2e-test-crd-publish-openapi-1425-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 22 10:56:00.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 delete e2e-test-crd-publish-openapi-1425-crds test-foo'
Aug 22 10:56:00.065: INFO: stderr: ""
Aug 22 10:56:00.065: INFO: stdout: "e2e-test-crd-publish-openapi-1425-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Aug 22 10:56:00.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 create -f -'
Aug 22 10:56:00.170: INFO: rc: 1
Aug 22 10:56:00.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 apply -f -'
Aug 22 10:56:00.280: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Aug 22 10:56:00.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 create -f -'
Aug 22 10:56:00.388: INFO: rc: 1
Aug 22 10:56:00.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 apply -f -'
Aug 22 10:56:00.498: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Aug 22 10:56:00.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 explain e2e-test-crd-publish-openapi-1425-crds'
Aug 22 10:56:00.614: INFO: stderr: ""
Aug 22 10:56:00.614: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1425-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Aug 22 10:56:00.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 explain e2e-test-crd-publish-openapi-1425-crds.metadata'
Aug 22 10:56:00.726: INFO: stderr: ""
Aug 22 10:56:00.726: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1425-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 22 10:56:00.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 explain e2e-test-crd-publish-openapi-1425-crds.spec'
Aug 22 10:56:00.843: INFO: stderr: ""
Aug 22 10:56:00.843: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1425-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 22 10:56:00.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 explain e2e-test-crd-publish-openapi-1425-crds.spec.bars'
Aug 22 10:56:00.953: INFO: stderr: ""
Aug 22 10:56:00.953: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1425-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Aug 22 10:56:00.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-218892673 --namespace=crd-publish-openapi-6740 explain e2e-test-crd-publish-openapi-1425-crds.spec.bars2'
Aug 22 10:56:01.062: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:56:02.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6740" for this suite.

• [SLOW TEST:5.649 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":332,"skipped":5817,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:56:02.635: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:56:13.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4050" for this suite.

• [SLOW TEST:11.089 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":333,"skipped":5827,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:56:13.724: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Aug 22 10:56:13.743: INFO: Waiting up to 5m0s for pod "var-expansion-2ed1001e-8553-4906-a063-39f374d64c3b" in namespace "var-expansion-5729" to be "Succeeded or Failed"
Aug 22 10:56:13.744: INFO: Pod "var-expansion-2ed1001e-8553-4906-a063-39f374d64c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.230765ms
Aug 22 10:56:15.750: INFO: Pod "var-expansion-2ed1001e-8553-4906-a063-39f374d64c3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006758396s
STEP: Saw pod success
Aug 22 10:56:15.750: INFO: Pod "var-expansion-2ed1001e-8553-4906-a063-39f374d64c3b" satisfied condition "Succeeded or Failed"
Aug 22 10:56:15.751: INFO: Trying to get logs from node 172.23.79.105 pod var-expansion-2ed1001e-8553-4906-a063-39f374d64c3b container dapi-container: <nil>
STEP: delete the pod
Aug 22 10:56:15.759: INFO: Waiting for pod var-expansion-2ed1001e-8553-4906-a063-39f374d64c3b to disappear
Aug 22 10:56:15.761: INFO: Pod var-expansion-2ed1001e-8553-4906-a063-39f374d64c3b no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:56:15.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5729" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":334,"skipped":5830,"failed":0}

------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:56:15.764: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Aug 22 10:56:15.787: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:56:17.793: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Aug 22 10:56:17.801: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:56:19.805: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Aug 22 10:56:19.809: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 10:56:19.811: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 10:56:21.812: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 10:56:21.817: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 10:56:23.811: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 10:56:23.816: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:56:23.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6806" for this suite.

• [SLOW TEST:8.060 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":335,"skipped":5830,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:56:23.824: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Aug 22 10:56:23.844: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:56:26.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-348" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":336,"skipped":5843,"failed":0}
SSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:56:26.540: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Aug 22 10:56:26.561: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:56:28.565: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.23.79.105 on the node which pod1 resides and expect scheduled
Aug 22 10:56:28.570: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:56:30.573: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.23.79.105 but use UDP protocol on the node which pod2 resides
Aug 22 10:56:30.577: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:56:32.582: INFO: The status of Pod pod3 is Running (Ready = false)
Aug 22 10:56:34.583: INFO: The status of Pod pod3 is Running (Ready = true)
Aug 22 10:56:34.589: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 22 10:56:36.594: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Aug 22 10:56:36.596: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.23.79.105 http://127.0.0.1:54323/hostname] Namespace:hostport-5604 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:56:36.596: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.23.79.105, port: 54323
Aug 22 10:56:36.653: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.23.79.105:54323/hostname] Namespace:hostport-5604 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:56:36.653: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.23.79.105, port: 54323 UDP
Aug 22 10:56:36.710: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.23.79.105 54323] Namespace:hostport-5604 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 22 10:56:36.710: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:56:41.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-5604" for this suite.

• [SLOW TEST:15.232 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":337,"skipped":5846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:56:41.772: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 22 10:56:41.794: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 22 10:56:46.800: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:56:47.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9988" for this suite.

• [SLOW TEST:6.042 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":338,"skipped":5877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:56:47.815: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:56:48.021: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 22 10:56:50.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765226608, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765226608, loc:(*time.Location)(0xa09cc60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63765226608, loc:(*time.Location)(0xa09cc60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63765226608, loc:(*time.Location)(0xa09cc60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:56:53.042: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:56:53.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1379" for this suite.
STEP: Destroying namespace "webhook-1379-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.280 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":339,"skipped":5922,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:56:53.095: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 22 10:56:53.119: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9052  471793e9-e816-4a8d-8613-de004d47b4dd 29407 0 2021-08-22 10:56:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-22 10:56:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 10:56:53.119: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9052  471793e9-e816-4a8d-8613-de004d47b4dd 29407 0 2021-08-22 10:56:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-22 10:56:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 22 10:57:03.130: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9052  471793e9-e816-4a8d-8613-de004d47b4dd 29440 0 2021-08-22 10:56:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-22 10:57:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 10:57:03.130: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9052  471793e9-e816-4a8d-8613-de004d47b4dd 29440 0 2021-08-22 10:56:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-22 10:57:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 22 10:57:13.141: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9052  471793e9-e816-4a8d-8613-de004d47b4dd 29455 0 2021-08-22 10:56:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-22 10:57:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 10:57:13.141: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9052  471793e9-e816-4a8d-8613-de004d47b4dd 29455 0 2021-08-22 10:56:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-22 10:57:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 22 10:57:23.151: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9052  471793e9-e816-4a8d-8613-de004d47b4dd 29470 0 2021-08-22 10:56:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-22 10:57:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 10:57:23.151: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9052  471793e9-e816-4a8d-8613-de004d47b4dd 29470 0 2021-08-22 10:56:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-08-22 10:57:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 22 10:57:33.161: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9052  8a9fc213-8728-4298-ad1b-3b4a0e9abe53 29485 0 2021-08-22 10:57:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-08-22 10:57:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 10:57:33.161: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9052  8a9fc213-8728-4298-ad1b-3b4a0e9abe53 29485 0 2021-08-22 10:57:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-08-22 10:57:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 22 10:57:43.168: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9052  8a9fc213-8728-4298-ad1b-3b4a0e9abe53 29500 0 2021-08-22 10:57:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-08-22 10:57:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 22 10:57:43.169: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9052  8a9fc213-8728-4298-ad1b-3b4a0e9abe53 29500 0 2021-08-22 10:57:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-08-22 10:57:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:57:53.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9052" for this suite.

• [SLOW TEST:60.084 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":340,"skipped":5929,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:57:53.179: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 22 10:57:53.208: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:57:53.212: INFO: Number of nodes with available pods: 0
Aug 22 10:57:53.212: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 10:57:54.216: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:57:54.217: INFO: Number of nodes with available pods: 0
Aug 22 10:57:54.217: INFO: Node 172.23.79.103 is running more than one daemon pod
Aug 22 10:57:55.216: INFO: DaemonSet pods can't tolerate node 172.23.79.104 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 22 10:57:55.218: INFO: Number of nodes with available pods: 2
Aug 22 10:57:55.218: INFO: Number of running nodes: 2, number of available pods: 2
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Aug 22 10:57:55.229: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29550"},"items":null}

Aug 22 10:57:55.231: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29550"},"items":[{"metadata":{"name":"daemon-set-v4ssk","generateName":"daemon-set-","namespace":"daemonsets-3642","uid":"f649da82-b7f7-44b1-99eb-bd3d974f11c8","resourceVersion":"29542","creationTimestamp":"2021-08-22T10:57:53Z","labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4f12b0ac-68a6-44a1-ab7a-25ed8cfdb3dd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-08-22T10:57:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f12b0ac-68a6-44a1-ab7a-25ed8cfdb3dd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-08-22T10:57:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fkltl","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fkltl","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"172.23.79.103","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["172.23.79.103"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-08-22T10:57:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-08-22T10:57:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-08-22T10:57:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-08-22T10:57:53Z"}],"hostIP":"172.23.79.103","podIP":"10.244.1.101","podIPs":[{"ip":"10.244.1.101"}],"startTime":"2021-08-22T10:57:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-08-22T10:57:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"docker://b9f3a3ab7908270f6bc946ee810349da340f5b3bd1d374ad9cf5f8e054581ba8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-x2gg6","generateName":"daemon-set-","namespace":"daemonsets-3642","uid":"9d66938b-7aea-43af-b741-7b0686c00bfb","resourceVersion":"29544","creationTimestamp":"2021-08-22T10:57:53Z","labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4f12b0ac-68a6-44a1-ab7a-25ed8cfdb3dd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-08-22T10:57:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f12b0ac-68a6-44a1-ab7a-25ed8cfdb3dd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-08-22T10:57:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b5pq6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b5pq6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"172.23.79.105","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["172.23.79.105"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-08-22T10:57:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-08-22T10:57:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-08-22T10:57:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-08-22T10:57:53Z"}],"hostIP":"172.23.79.105","podIP":"10.244.2.125","podIPs":[{"ip":"10.244.2.125"}],"startTime":"2021-08-22T10:57:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-08-22T10:57:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"docker://18ee5ae15ba60f6f938b5482a279490490bc087a8e887b4cbd3aba2cfcd4fecd","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:57:55.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3642" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":341,"skipped":5938,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:57:55.248: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 22 10:57:55.267: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 22 10:57:55.269: INFO: starting watch
STEP: patching
STEP: updating
Aug 22 10:57:55.279: INFO: waiting for watch events with expected annotations
Aug 22 10:57:55.279: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:57:55.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4861" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":342,"skipped":5946,"failed":0}
SSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:57:55.300: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 22 10:57:55.324: INFO: starting watch
STEP: patching
STEP: updating
Aug 22 10:57:55.337: INFO: waiting for watch events with expected annotations
Aug 22 10:57:55.337: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:57:55.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-813" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":343,"skipped":5953,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:57:55.387: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-6hsf
STEP: Creating a pod to test atomic-volume-subpath
Aug 22 10:57:55.413: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6hsf" in namespace "subpath-5905" to be "Succeeded or Failed"
Aug 22 10:57:55.414: INFO: Pod "pod-subpath-test-projected-6hsf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.224703ms
Aug 22 10:57:57.419: INFO: Pod "pod-subpath-test-projected-6hsf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006130394s
Aug 22 10:57:59.423: INFO: Pod "pod-subpath-test-projected-6hsf": Phase="Running", Reason="", readiness=true. Elapsed: 4.010009992s
Aug 22 10:58:01.428: INFO: Pod "pod-subpath-test-projected-6hsf": Phase="Running", Reason="", readiness=true. Elapsed: 6.015511957s
Aug 22 10:58:03.434: INFO: Pod "pod-subpath-test-projected-6hsf": Phase="Running", Reason="", readiness=true. Elapsed: 8.02139543s
Aug 22 10:58:05.439: INFO: Pod "pod-subpath-test-projected-6hsf": Phase="Running", Reason="", readiness=true. Elapsed: 10.026026115s
Aug 22 10:58:07.441: INFO: Pod "pod-subpath-test-projected-6hsf": Phase="Running", Reason="", readiness=true. Elapsed: 12.028554099s
Aug 22 10:58:09.446: INFO: Pod "pod-subpath-test-projected-6hsf": Phase="Running", Reason="", readiness=true. Elapsed: 14.03345004s
Aug 22 10:58:11.452: INFO: Pod "pod-subpath-test-projected-6hsf": Phase="Running", Reason="", readiness=true. Elapsed: 16.039197838s
Aug 22 10:58:13.457: INFO: Pod "pod-subpath-test-projected-6hsf": Phase="Running", Reason="", readiness=true. Elapsed: 18.044592273s
Aug 22 10:58:15.463: INFO: Pod "pod-subpath-test-projected-6hsf": Phase="Running", Reason="", readiness=true. Elapsed: 20.050264693s
Aug 22 10:58:17.465: INFO: Pod "pod-subpath-test-projected-6hsf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.052560351s
STEP: Saw pod success
Aug 22 10:58:17.465: INFO: Pod "pod-subpath-test-projected-6hsf" satisfied condition "Succeeded or Failed"
Aug 22 10:58:17.467: INFO: Trying to get logs from node 172.23.79.105 pod pod-subpath-test-projected-6hsf container test-container-subpath-projected-6hsf: <nil>
STEP: delete the pod
Aug 22 10:58:17.480: INFO: Waiting for pod pod-subpath-test-projected-6hsf to disappear
Aug 22 10:58:17.481: INFO: Pod pod-subpath-test-projected-6hsf no longer exists
STEP: Deleting pod pod-subpath-test-projected-6hsf
Aug 22 10:58:17.481: INFO: Deleting pod "pod-subpath-test-projected-6hsf" in namespace "subpath-5905"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:58:17.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5905" for this suite.

• [SLOW TEST:22.098 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":344,"skipped":6023,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:58:17.486: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 22 10:58:18.004: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 22 10:58:21.018: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:58:21.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1807" for this suite.
STEP: Destroying namespace "webhook-1807-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":345,"skipped":6041,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Aug 22 10:58:21.122: INFO: >>> kubeConfig: /tmp/kubeconfig-218892673
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Aug 22 10:58:23.149: INFO: Deleting pod "var-expansion-8d9b9ef9-4402-4fd3-bb91-24e33983b416" in namespace "var-expansion-2234"
Aug 22 10:58:23.151: INFO: Wait up to 5m0s for pod "var-expansion-8d9b9ef9-4402-4fd3-bb91-24e33983b416" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Aug 22 10:58:25.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2234" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":346,"skipped":6050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSAug 22 10:58:25.164: INFO: Running AfterSuite actions on all nodes
Aug 22 10:58:25.164: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Aug 22 10:58:25.164: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Aug 22 10:58:25.164: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Aug 22 10:58:25.164: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Aug 22 10:58:25.164: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Aug 22 10:58:25.164: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Aug 22 10:58:25.164: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Aug 22 10:58:25.164: INFO: Running AfterSuite actions on node 1
Aug 22 10:58:25.164: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6086,"failed":0}

Ran 346 of 6432 Specs in 5426.474 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6086 Skipped
PASS

Ginkgo ran 1 suite in 1h30m28.056374151s
Test Suite Passed
