I0429 06:40:08.052129      22 e2e.go:132] Starting e2e run "c17bdccc-a052-4b70-888a-fe8957adfc3a" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1651214407 - Will randomize all specs
Will run 346 of 7042 specs

Apr 29 06:40:10.381: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:40:10.383: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 29 06:40:10.402: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 29 06:40:10.428: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 29 06:40:10.428: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Apr 29 06:40:10.428: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 29 06:40:10.433: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Apr 29 06:40:10.433: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 29 06:40:10.433: INFO: e2e test version: v1.23.5
Apr 29 06:40:10.434: INFO: kube-apiserver version: v1.23.5
Apr 29 06:40:10.434: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:40:10.438: INFO: Cluster IP family: ipv4
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:40:10.439: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename replication-controller
W0429 06:40:10.476624      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Apr 29 06:40:10.476: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Apr 29 06:40:10.483: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:40:13.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2528" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":1,"skipped":11,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:40:13.574: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7306
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7306
STEP: creating replication controller externalsvc in namespace services-7306
I0429 06:40:13.693971      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7306, replica count: 2
I0429 06:40:16.746062      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Apr 29 06:40:16.761: INFO: Creating new exec pod
Apr 29 06:40:18.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-7306 exec execpodqtcpt -- /bin/sh -x -c nslookup clusterip-service.services-7306.svc.cluster.local'
Apr 29 06:40:21.147: INFO: stderr: "+ nslookup clusterip-service.services-7306.svc.cluster.local\n"
Apr 29 06:40:21.147: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-7306.svc.cluster.local\tcanonical name = externalsvc.services-7306.svc.cluster.local.\nName:\texternalsvc.services-7306.svc.cluster.local\nAddress: 10.103.65.230\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7306, will wait for the garbage collector to delete the pods
Apr 29 06:40:21.212: INFO: Deleting ReplicationController externalsvc took: 7.182808ms
Apr 29 06:40:21.313: INFO: Terminating ReplicationController externalsvc pods took: 100.663081ms
Apr 29 06:40:23.629: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:40:23.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7306" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.076 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":2,"skipped":16,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:40:23.650: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 06:40:23.681: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:40:26.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6619" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":3,"skipped":19,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:40:26.830: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 06:40:27.184: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 06:40:30.205: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:40:42.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1499" for this suite.
STEP: Destroying namespace "webhook-1499-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.609 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":4,"skipped":58,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:40:42.440: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Apr 29 06:40:42.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-8720 create -f -'
Apr 29 06:40:43.980: INFO: stderr: ""
Apr 29 06:40:43.980: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 06:40:43.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-8720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 06:40:44.056: INFO: stderr: ""
Apr 29 06:40:44.056: INFO: stdout: "update-demo-nautilus-frztn update-demo-nautilus-pkn2x "
Apr 29 06:40:44.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-8720 get pods update-demo-nautilus-frztn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 06:40:44.122: INFO: stderr: ""
Apr 29 06:40:44.122: INFO: stdout: ""
Apr 29 06:40:44.122: INFO: update-demo-nautilus-frztn is created but not running
Apr 29 06:40:49.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-8720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 06:40:49.209: INFO: stderr: ""
Apr 29 06:40:49.209: INFO: stdout: "update-demo-nautilus-frztn update-demo-nautilus-pkn2x "
Apr 29 06:40:49.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-8720 get pods update-demo-nautilus-frztn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 06:40:49.275: INFO: stderr: ""
Apr 29 06:40:49.276: INFO: stdout: "true"
Apr 29 06:40:49.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-8720 get pods update-demo-nautilus-frztn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 06:40:49.345: INFO: stderr: ""
Apr 29 06:40:49.345: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Apr 29 06:40:49.345: INFO: validating pod update-demo-nautilus-frztn
Apr 29 06:40:49.349: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 06:40:49.349: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 06:40:49.349: INFO: update-demo-nautilus-frztn is verified up and running
Apr 29 06:40:49.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-8720 get pods update-demo-nautilus-pkn2x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 06:40:49.431: INFO: stderr: ""
Apr 29 06:40:49.431: INFO: stdout: "true"
Apr 29 06:40:49.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-8720 get pods update-demo-nautilus-pkn2x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 06:40:49.507: INFO: stderr: ""
Apr 29 06:40:49.507: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Apr 29 06:40:49.507: INFO: validating pod update-demo-nautilus-pkn2x
Apr 29 06:40:49.512: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 06:40:49.512: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 06:40:49.512: INFO: update-demo-nautilus-pkn2x is verified up and running
STEP: using delete to clean up resources
Apr 29 06:40:49.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-8720 delete --grace-period=0 --force -f -'
Apr 29 06:40:49.592: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 06:40:49.592: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 29 06:40:49.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-8720 get rc,svc -l name=update-demo --no-headers'
Apr 29 06:40:49.681: INFO: stderr: "No resources found in kubectl-8720 namespace.\n"
Apr 29 06:40:49.681: INFO: stdout: ""
Apr 29 06:40:49.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-8720 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 06:40:49.769: INFO: stderr: ""
Apr 29 06:40:49.769: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:40:49.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8720" for this suite.

• [SLOW TEST:7.341 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":5,"skipped":66,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:40:49.781: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Apr 29 06:40:49.840: INFO: Waiting up to 5m0s for pod "client-containers-6bc971d5-b785-449f-a12d-9fc8e023aba6" in namespace "containers-3756" to be "Succeeded or Failed"
Apr 29 06:40:49.844: INFO: Pod "client-containers-6bc971d5-b785-449f-a12d-9fc8e023aba6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.4104ms
Apr 29 06:40:51.849: INFO: Pod "client-containers-6bc971d5-b785-449f-a12d-9fc8e023aba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00826832s
STEP: Saw pod success
Apr 29 06:40:51.849: INFO: Pod "client-containers-6bc971d5-b785-449f-a12d-9fc8e023aba6" satisfied condition "Succeeded or Failed"
Apr 29 06:40:51.851: INFO: Trying to get logs from node localhost.localdomain151 pod client-containers-6bc971d5-b785-449f-a12d-9fc8e023aba6 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 06:40:51.878: INFO: Waiting for pod client-containers-6bc971d5-b785-449f-a12d-9fc8e023aba6 to disappear
Apr 29 06:40:51.881: INFO: Pod client-containers-6bc971d5-b785-449f-a12d-9fc8e023aba6 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:40:51.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3756" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":6,"skipped":76,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:40:51.890: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-39526cbe-45b3-4c89-9feb-cf540dabc602
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:40:51.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5852" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":7,"skipped":87,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:40:52.002: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 06:40:52.104: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1a1974c-729a-4fe1-96c5-410584ab54a5" in namespace "projected-1414" to be "Succeeded or Failed"
Apr 29 06:40:52.106: INFO: Pod "downwardapi-volume-d1a1974c-729a-4fe1-96c5-410584ab54a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.661166ms
Apr 29 06:40:54.110: INFO: Pod "downwardapi-volume-d1a1974c-729a-4fe1-96c5-410584ab54a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006061501s
STEP: Saw pod success
Apr 29 06:40:54.110: INFO: Pod "downwardapi-volume-d1a1974c-729a-4fe1-96c5-410584ab54a5" satisfied condition "Succeeded or Failed"
Apr 29 06:40:54.113: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-d1a1974c-729a-4fe1-96c5-410584ab54a5 container client-container: <nil>
STEP: delete the pod
Apr 29 06:40:54.133: INFO: Waiting for pod downwardapi-volume-d1a1974c-729a-4fe1-96c5-410584ab54a5 to disappear
Apr 29 06:40:54.135: INFO: Pod downwardapi-volume-d1a1974c-729a-4fe1-96c5-410584ab54a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:40:54.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1414" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":8,"skipped":178,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:40:54.144: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 06:40:54.202: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"462ebe3e-e3a7-45d3-8fe3-9024e2ac6551", Controller:(*bool)(0xc002fe9656), BlockOwnerDeletion:(*bool)(0xc002fe9657)}}
Apr 29 06:40:54.268: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"39dc3589-668f-4dd4-b1b1-5f78e3889b34", Controller:(*bool)(0xc0031e4236), BlockOwnerDeletion:(*bool)(0xc0031e4237)}}
Apr 29 06:40:54.276: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"12735582-954b-4ff9-a925-b4be10ecb108", Controller:(*bool)(0xc0031e44fe), BlockOwnerDeletion:(*bool)(0xc0031e44ff)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:40:59.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7361" for this suite.

• [SLOW TEST:5.150 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":9,"skipped":186,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:40:59.295: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-6539425d-7a86-4231-80e2-9985356d8777
STEP: Creating a pod to test consume configMaps
Apr 29 06:40:59.375: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ec90ec11-8edc-4185-8c88-a35212e2c55f" in namespace "projected-4640" to be "Succeeded or Failed"
Apr 29 06:40:59.379: INFO: Pod "pod-projected-configmaps-ec90ec11-8edc-4185-8c88-a35212e2c55f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.710704ms
Apr 29 06:41:01.384: INFO: Pod "pod-projected-configmaps-ec90ec11-8edc-4185-8c88-a35212e2c55f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008205972s
STEP: Saw pod success
Apr 29 06:41:01.384: INFO: Pod "pod-projected-configmaps-ec90ec11-8edc-4185-8c88-a35212e2c55f" satisfied condition "Succeeded or Failed"
Apr 29 06:41:01.386: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-configmaps-ec90ec11-8edc-4185-8c88-a35212e2c55f container agnhost-container: <nil>
STEP: delete the pod
Apr 29 06:41:01.401: INFO: Waiting for pod pod-projected-configmaps-ec90ec11-8edc-4185-8c88-a35212e2c55f to disappear
Apr 29 06:41:01.403: INFO: Pod pod-projected-configmaps-ec90ec11-8edc-4185-8c88-a35212e2c55f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:41:01.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4640" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":237,"failed":0}
SSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:41:01.412: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Apr 29 06:41:21.738: INFO: EndpointSlice for Service endpointslice-9157/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:41:31.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9157" for this suite.

• [SLOW TEST:30.351 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":11,"skipped":243,"failed":0}
S
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:41:31.763: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Apr 29 06:41:34.343: INFO: Successfully updated pod "adopt-release-nznq4"
STEP: Checking that the Job readopts the Pod
Apr 29 06:41:34.343: INFO: Waiting up to 15m0s for pod "adopt-release-nznq4" in namespace "job-6441" to be "adopted"
Apr 29 06:41:34.347: INFO: Pod "adopt-release-nznq4": Phase="Running", Reason="", readiness=true. Elapsed: 3.870387ms
Apr 29 06:41:36.351: INFO: Pod "adopt-release-nznq4": Phase="Running", Reason="", readiness=true. Elapsed: 2.0079381s
Apr 29 06:41:36.351: INFO: Pod "adopt-release-nznq4" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Apr 29 06:41:36.864: INFO: Successfully updated pod "adopt-release-nznq4"
STEP: Checking that the Job releases the Pod
Apr 29 06:41:36.864: INFO: Waiting up to 15m0s for pod "adopt-release-nznq4" in namespace "job-6441" to be "released"
Apr 29 06:41:36.867: INFO: Pod "adopt-release-nznq4": Phase="Running", Reason="", readiness=true. Elapsed: 2.656372ms
Apr 29 06:41:38.870: INFO: Pod "adopt-release-nznq4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006085757s
Apr 29 06:41:38.870: INFO: Pod "adopt-release-nznq4" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:41:38.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6441" for this suite.

• [SLOW TEST:7.116 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":12,"skipped":244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:41:38.881: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 06:41:38.990: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 06:41:39.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 06:41:39.002: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 06:41:40.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 06:41:40.013: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 06:41:41.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 29 06:41:41.012: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 29 06:41:41.039: INFO: Wrong image for pod: daemon-set-hldqf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Apr 29 06:41:41.039: INFO: Wrong image for pod: daemon-set-lkp9p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Apr 29 06:41:42.047: INFO: Wrong image for pod: daemon-set-hldqf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Apr 29 06:41:43.049: INFO: Wrong image for pod: daemon-set-hldqf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Apr 29 06:41:44.097: INFO: Pod daemon-set-fq626 is not available
Apr 29 06:41:44.097: INFO: Wrong image for pod: daemon-set-hldqf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Apr 29 06:41:46.048: INFO: Pod daemon-set-qzrkq is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 29 06:41:46.060: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 29 06:41:46.060: INFO: Node localhost.localdomain151 is running 0 daemon pod, expected 1
Apr 29 06:41:47.073: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 29 06:41:47.073: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4183, will wait for the garbage collector to delete the pods
Apr 29 06:41:47.146: INFO: Deleting DaemonSet.extensions daemon-set took: 7.618311ms
Apr 29 06:41:47.247: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.103026ms
Apr 29 06:41:49.557: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 06:41:49.557: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 29 06:41:49.560: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"87038"},"items":null}

Apr 29 06:41:49.564: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"87038"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:41:49.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4183" for this suite.

• [SLOW TEST:10.705 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":13,"skipped":285,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:41:49.586: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Apr 29 06:41:49.667: INFO: Waiting up to 5m0s for pod "var-expansion-12465868-417d-4dca-bf16-e0577fabceee" in namespace "var-expansion-3324" to be "Succeeded or Failed"
Apr 29 06:41:49.670: INFO: Pod "var-expansion-12465868-417d-4dca-bf16-e0577fabceee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281407ms
Apr 29 06:41:51.673: INFO: Pod "var-expansion-12465868-417d-4dca-bf16-e0577fabceee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005313958s
STEP: Saw pod success
Apr 29 06:41:51.673: INFO: Pod "var-expansion-12465868-417d-4dca-bf16-e0577fabceee" satisfied condition "Succeeded or Failed"
Apr 29 06:41:51.675: INFO: Trying to get logs from node localhost.localdomain151 pod var-expansion-12465868-417d-4dca-bf16-e0577fabceee container dapi-container: <nil>
STEP: delete the pod
Apr 29 06:41:51.691: INFO: Waiting for pod var-expansion-12465868-417d-4dca-bf16-e0577fabceee to disappear
Apr 29 06:41:51.699: INFO: Pod var-expansion-12465868-417d-4dca-bf16-e0577fabceee no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:41:51.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3324" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":14,"skipped":288,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:41:51.709: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-7406/configmap-test-57747b73-8561-4150-89ed-a2e576110918
STEP: Creating a pod to test consume configMaps
Apr 29 06:41:51.784: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee83ed4c-a786-4937-b215-7f87ec4cbf26" in namespace "configmap-7406" to be "Succeeded or Failed"
Apr 29 06:41:51.787: INFO: Pod "pod-configmaps-ee83ed4c-a786-4937-b215-7f87ec4cbf26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.784228ms
Apr 29 06:41:53.792: INFO: Pod "pod-configmaps-ee83ed4c-a786-4937-b215-7f87ec4cbf26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008190945s
STEP: Saw pod success
Apr 29 06:41:53.793: INFO: Pod "pod-configmaps-ee83ed4c-a786-4937-b215-7f87ec4cbf26" satisfied condition "Succeeded or Failed"
Apr 29 06:41:53.795: INFO: Trying to get logs from node localhost.localdomain151 pod pod-configmaps-ee83ed4c-a786-4937-b215-7f87ec4cbf26 container env-test: <nil>
STEP: delete the pod
Apr 29 06:41:53.824: INFO: Waiting for pod pod-configmaps-ee83ed4c-a786-4937-b215-7f87ec4cbf26 to disappear
Apr 29 06:41:53.827: INFO: Pod pod-configmaps-ee83ed4c-a786-4937-b215-7f87ec4cbf26 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:41:53.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7406" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":15,"skipped":300,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:41:53.837: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 06:41:53.874: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58643b8a-da8e-4c15-9190-2ae699d75579" in namespace "downward-api-6417" to be "Succeeded or Failed"
Apr 29 06:41:53.877: INFO: Pod "downwardapi-volume-58643b8a-da8e-4c15-9190-2ae699d75579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.241098ms
Apr 29 06:41:55.882: INFO: Pod "downwardapi-volume-58643b8a-da8e-4c15-9190-2ae699d75579": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007055362s
STEP: Saw pod success
Apr 29 06:41:55.882: INFO: Pod "downwardapi-volume-58643b8a-da8e-4c15-9190-2ae699d75579" satisfied condition "Succeeded or Failed"
Apr 29 06:41:55.884: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-58643b8a-da8e-4c15-9190-2ae699d75579 container client-container: <nil>
STEP: delete the pod
Apr 29 06:41:55.900: INFO: Waiting for pod downwardapi-volume-58643b8a-da8e-4c15-9190-2ae699d75579 to disappear
Apr 29 06:41:55.902: INFO: Pod downwardapi-volume-58643b8a-da8e-4c15-9190-2ae699d75579 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:41:55.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6417" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":16,"skipped":320,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:41:55.920: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr 29 06:41:58.496: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3757 pod-service-account-e1f59425-93c4-41ba-bdfb-4f92db3f2121 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr 29 06:41:58.639: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3757 pod-service-account-e1f59425-93c4-41ba-bdfb-4f92db3f2121 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr 29 06:41:58.765: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3757 pod-service-account-e1f59425-93c4-41ba-bdfb-4f92db3f2121 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:41:58.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3757" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":17,"skipped":340,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:41:58.913: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Apr 29 06:41:58.971: INFO: The status of Pod pod-update-f4e2d5fa-d442-4e27-a1dd-53806e0566af is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:42:00.976: INFO: The status of Pod pod-update-f4e2d5fa-d442-4e27-a1dd-53806e0566af is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 29 06:42:01.494: INFO: Successfully updated pod "pod-update-f4e2d5fa-d442-4e27-a1dd-53806e0566af"
STEP: verifying the updated pod is in kubernetes
Apr 29 06:42:01.498: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:42:01.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4999" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":18,"skipped":347,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:42:01.508: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Apr 29 06:42:01.570: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-7131 proxy --unix-socket=/tmp/kubectl-proxy-unix860243356/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:42:01.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7131" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":19,"skipped":374,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:42:01.633: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 06:42:03.677: INFO: Deleting pod "var-expansion-079acdf7-e232-4e7b-94ef-323b612236b4" in namespace "var-expansion-7131"
Apr 29 06:42:03.687: INFO: Wait up to 5m0s for pod "var-expansion-079acdf7-e232-4e7b-94ef-323b612236b4" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:42:05.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7131" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":20,"skipped":379,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:42:05.705: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Apr 29 06:42:05.739: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Apr 29 06:42:05.743: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 29 06:42:05.743: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Apr 29 06:42:05.750: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 29 06:42:05.750: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Apr 29 06:42:05.770: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Apr 29 06:42:05.770: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Apr 29 06:42:12.813: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:42:12.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9989" for this suite.

• [SLOW TEST:7.127 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":21,"skipped":413,"failed":0}
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:42:12.833: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Apr 29 06:42:12.869: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 06:43:12.908: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 06:43:12.912: INFO: Starting informer...
STEP: Starting pod...
Apr 29 06:43:13.126: INFO: Pod is running on localhost.localdomain151. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Apr 29 06:43:13.143: INFO: Pod wasn't evicted. Proceeding
Apr 29 06:43:13.143: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Apr 29 06:44:28.169: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:44:28.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3099" for this suite.

• [SLOW TEST:135.354 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":22,"skipped":413,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:44:28.186: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:44:41.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3002" for this suite.

• [SLOW TEST:13.194 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":23,"skipped":414,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:44:41.380: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:44:41.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3325" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":24,"skipped":415,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:44:41.496: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 06:44:41.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 06:44:41.599: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 06:44:42.609: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 06:44:42.609: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 06:44:43.617: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 29 06:44:43.617: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status
Apr 29 06:44:43.626: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Apr 29 06:44:43.633: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Apr 29 06:44:43.635: INFO: Observed &DaemonSet event: ADDED
Apr 29 06:44:43.635: INFO: Observed &DaemonSet event: MODIFIED
Apr 29 06:44:43.636: INFO: Observed &DaemonSet event: MODIFIED
Apr 29 06:44:43.636: INFO: Observed &DaemonSet event: MODIFIED
Apr 29 06:44:43.636: INFO: Found daemon set daemon-set in namespace daemonsets-1343 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 29 06:44:43.636: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Apr 29 06:44:43.641: INFO: Observed &DaemonSet event: ADDED
Apr 29 06:44:43.642: INFO: Observed &DaemonSet event: MODIFIED
Apr 29 06:44:43.642: INFO: Observed &DaemonSet event: MODIFIED
Apr 29 06:44:43.642: INFO: Observed &DaemonSet event: MODIFIED
Apr 29 06:44:43.642: INFO: Observed daemon set daemon-set in namespace daemonsets-1343 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 29 06:44:43.642: INFO: Observed &DaemonSet event: MODIFIED
Apr 29 06:44:43.642: INFO: Found daemon set daemon-set in namespace daemonsets-1343 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Apr 29 06:44:43.642: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1343, will wait for the garbage collector to delete the pods
Apr 29 06:44:43.703: INFO: Deleting DaemonSet.extensions daemon-set took: 6.892603ms
Apr 29 06:44:43.804: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.469666ms
Apr 29 06:44:46.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 06:44:46.307: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 29 06:44:46.309: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"88331"},"items":null}

Apr 29 06:44:46.311: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"88331"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:44:46.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1343" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":25,"skipped":415,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:44:46.331: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:46:00.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5742" for this suite.

• [SLOW TEST:74.064 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":26,"skipped":430,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:46:00.395: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1490
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1490
I0429 06:46:00.612940      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1490, replica count: 2
Apr 29 06:46:03.663: INFO: Creating new exec pod
I0429 06:46:03.663724      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 06:46:06.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-1490 exec execpodkd2pd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Apr 29 06:46:06.827: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 29 06:46:06.827: INFO: stdout: "externalname-service-w29s7"
Apr 29 06:46:06.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-1490 exec execpodkd2pd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.126.153 80'
Apr 29 06:46:06.957: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.126.153 80\nConnection to 10.102.126.153 80 port [tcp/http] succeeded!\n"
Apr 29 06:46:06.957: INFO: stdout: ""
Apr 29 06:46:07.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-1490 exec execpodkd2pd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.126.153 80'
Apr 29 06:46:08.097: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.126.153 80\nConnection to 10.102.126.153 80 port [tcp/http] succeeded!\n"
Apr 29 06:46:08.097: INFO: stdout: ""
Apr 29 06:46:08.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-1490 exec execpodkd2pd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.126.153 80'
Apr 29 06:46:09.125: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.126.153 80\nConnection to 10.102.126.153 80 port [tcp/http] succeeded!\n"
Apr 29 06:46:09.125: INFO: stdout: ""
Apr 29 06:46:09.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-1490 exec execpodkd2pd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.126.153 80'
Apr 29 06:46:10.117: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.126.153 80\nConnection to 10.102.126.153 80 port [tcp/http] succeeded!\n"
Apr 29 06:46:10.117: INFO: stdout: ""
Apr 29 06:46:10.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-1490 exec execpodkd2pd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.126.153 80'
Apr 29 06:46:11.099: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.126.153 80\nConnection to 10.102.126.153 80 port [tcp/http] succeeded!\n"
Apr 29 06:46:11.099: INFO: stdout: ""
Apr 29 06:46:11.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-1490 exec execpodkd2pd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.126.153 80'
Apr 29 06:46:12.109: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.126.153 80\nConnection to 10.102.126.153 80 port [tcp/http] succeeded!\n"
Apr 29 06:46:12.109: INFO: stdout: "externalname-service-8dmgk"
Apr 29 06:46:12.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-1490 exec execpodkd2pd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.150 31582'
Apr 29 06:46:12.268: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.150 31582\nConnection to 123.58.176.150 31582 port [tcp/*] succeeded!\n"
Apr 29 06:46:12.268: INFO: stdout: ""
Apr 29 06:46:13.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-1490 exec execpodkd2pd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.150 31582'
Apr 29 06:46:13.434: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.150 31582\nConnection to 123.58.176.150 31582 port [tcp/*] succeeded!\n"
Apr 29 06:46:13.434: INFO: stdout: "externalname-service-w29s7"
Apr 29 06:46:13.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-1490 exec execpodkd2pd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.151 31582'
Apr 29 06:46:13.568: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.151 31582\nConnection to 123.58.176.151 31582 port [tcp/*] succeeded!\n"
Apr 29 06:46:13.568: INFO: stdout: ""
Apr 29 06:46:14.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-1490 exec execpodkd2pd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.151 31582'
Apr 29 06:46:14.714: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.151 31582\nConnection to 123.58.176.151 31582 port [tcp/*] succeeded!\n"
Apr 29 06:46:14.714: INFO: stdout: "externalname-service-w29s7"
Apr 29 06:46:14.714: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:46:14.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1490" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:14.356 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":27,"skipped":440,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:46:14.752: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 29 06:46:14.790: INFO: Waiting up to 5m0s for pod "pod-31dd193a-f51b-4d38-854a-0bf7f8b1f9f8" in namespace "emptydir-6230" to be "Succeeded or Failed"
Apr 29 06:46:14.793: INFO: Pod "pod-31dd193a-f51b-4d38-854a-0bf7f8b1f9f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.6267ms
Apr 29 06:46:16.798: INFO: Pod "pod-31dd193a-f51b-4d38-854a-0bf7f8b1f9f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007838909s
STEP: Saw pod success
Apr 29 06:46:16.798: INFO: Pod "pod-31dd193a-f51b-4d38-854a-0bf7f8b1f9f8" satisfied condition "Succeeded or Failed"
Apr 29 06:46:16.801: INFO: Trying to get logs from node localhost.localdomain151 pod pod-31dd193a-f51b-4d38-854a-0bf7f8b1f9f8 container test-container: <nil>
STEP: delete the pod
Apr 29 06:46:16.825: INFO: Waiting for pod pod-31dd193a-f51b-4d38-854a-0bf7f8b1f9f8 to disappear
Apr 29 06:46:16.828: INFO: Pod pod-31dd193a-f51b-4d38-854a-0bf7f8b1f9f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:46:16.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6230" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":28,"skipped":468,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:46:16.836: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2698
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 06:46:16.870: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 29 06:46:16.893: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:46:18.897: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:46:20.898: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:46:22.898: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:46:24.897: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:46:26.897: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:46:28.897: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:46:30.897: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:46:32.898: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:46:34.898: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:46:36.897: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:46:38.897: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 29 06:46:38.902: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Apr 29 06:46:40.924: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Apr 29 06:46:40.924: INFO: Breadth first check of 172.16.102.138 on host 123.58.176.150...
Apr 29 06:46:40.927: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.115.149:9080/dial?request=hostname&protocol=udp&host=172.16.102.138&port=8081&tries=1'] Namespace:pod-network-test-2698 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:46:40.927: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:46:40.928: INFO: ExecWithOptions: Clientset creation
Apr 29 06:46:40.928: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2698/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.115.149%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.102.138%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:46:41.010: INFO: Waiting for responses: map[]
Apr 29 06:46:41.010: INFO: reached 172.16.102.138 after 0/1 tries
Apr 29 06:46:41.010: INFO: Breadth first check of 172.16.115.158 on host 123.58.176.151...
Apr 29 06:46:41.013: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.115.149:9080/dial?request=hostname&protocol=udp&host=172.16.115.158&port=8081&tries=1'] Namespace:pod-network-test-2698 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:46:41.013: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:46:41.014: INFO: ExecWithOptions: Clientset creation
Apr 29 06:46:41.014: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2698/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.115.149%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.115.158%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:46:41.089: INFO: Waiting for responses: map[]
Apr 29 06:46:41.089: INFO: reached 172.16.115.158 after 0/1 tries
Apr 29 06:46:41.089: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:46:41.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2698" for this suite.

• [SLOW TEST:24.265 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":29,"skipped":468,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:46:41.102: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 06:46:41.793: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 06:46:44.819: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 06:46:44.824: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:46:47.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8902" for this suite.
STEP: Destroying namespace "webhook-8902-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.888 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":30,"skipped":492,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:46:47.990: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 06:46:48.846: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 06:46:51.867: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:46:51.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1474" for this suite.
STEP: Destroying namespace "webhook-1474-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":31,"skipped":494,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:46:51.970: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-9f5h
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 06:46:52.069: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-9f5h" in namespace "subpath-2864" to be "Succeeded or Failed"
Apr 29 06:46:52.076: INFO: Pod "pod-subpath-test-secret-9f5h": Phase="Pending", Reason="", readiness=false. Elapsed: 6.869485ms
Apr 29 06:46:54.080: INFO: Pod "pod-subpath-test-secret-9f5h": Phase="Running", Reason="", readiness=true. Elapsed: 2.010863079s
Apr 29 06:46:56.085: INFO: Pod "pod-subpath-test-secret-9f5h": Phase="Running", Reason="", readiness=true. Elapsed: 4.016039962s
Apr 29 06:46:58.091: INFO: Pod "pod-subpath-test-secret-9f5h": Phase="Running", Reason="", readiness=true. Elapsed: 6.021609496s
Apr 29 06:47:00.096: INFO: Pod "pod-subpath-test-secret-9f5h": Phase="Running", Reason="", readiness=true. Elapsed: 8.02677844s
Apr 29 06:47:02.100: INFO: Pod "pod-subpath-test-secret-9f5h": Phase="Running", Reason="", readiness=true. Elapsed: 10.030802819s
Apr 29 06:47:04.103: INFO: Pod "pod-subpath-test-secret-9f5h": Phase="Running", Reason="", readiness=true. Elapsed: 12.033758518s
Apr 29 06:47:06.107: INFO: Pod "pod-subpath-test-secret-9f5h": Phase="Running", Reason="", readiness=true. Elapsed: 14.037794108s
Apr 29 06:47:08.111: INFO: Pod "pod-subpath-test-secret-9f5h": Phase="Running", Reason="", readiness=true. Elapsed: 16.042040202s
Apr 29 06:47:10.116: INFO: Pod "pod-subpath-test-secret-9f5h": Phase="Running", Reason="", readiness=true. Elapsed: 18.047275129s
Apr 29 06:47:12.124: INFO: Pod "pod-subpath-test-secret-9f5h": Phase="Running", Reason="", readiness=true. Elapsed: 20.054870798s
Apr 29 06:47:14.130: INFO: Pod "pod-subpath-test-secret-9f5h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.061066841s
STEP: Saw pod success
Apr 29 06:47:14.130: INFO: Pod "pod-subpath-test-secret-9f5h" satisfied condition "Succeeded or Failed"
Apr 29 06:47:14.132: INFO: Trying to get logs from node localhost.localdomain151 pod pod-subpath-test-secret-9f5h container test-container-subpath-secret-9f5h: <nil>
STEP: delete the pod
Apr 29 06:47:14.150: INFO: Waiting for pod pod-subpath-test-secret-9f5h to disappear
Apr 29 06:47:14.153: INFO: Pod pod-subpath-test-secret-9f5h no longer exists
STEP: Deleting pod pod-subpath-test-secret-9f5h
Apr 29 06:47:14.153: INFO: Deleting pod "pod-subpath-test-secret-9f5h" in namespace "subpath-2864"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:47:14.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2864" for this suite.

• [SLOW TEST:22.192 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":32,"skipped":507,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:47:14.163: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 29 06:47:14.204: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5232  b9670920-6d09-4cc1-b80d-34d1f7d25114 89434 0 2022-04-29 06:47:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-04-29 06:47:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 06:47:14.204: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5232  b9670920-6d09-4cc1-b80d-34d1f7d25114 89435 0 2022-04-29 06:47:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-04-29 06:47:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 29 06:47:14.217: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5232  b9670920-6d09-4cc1-b80d-34d1f7d25114 89436 0 2022-04-29 06:47:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-04-29 06:47:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 06:47:14.217: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5232  b9670920-6d09-4cc1-b80d-34d1f7d25114 89437 0 2022-04-29 06:47:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-04-29 06:47:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:47:14.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5232" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":33,"skipped":527,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:47:14.226: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-8786
STEP: creating service affinity-nodeport-transition in namespace services-8786
STEP: creating replication controller affinity-nodeport-transition in namespace services-8786
I0429 06:47:14.373651      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8786, replica count: 3
I0429 06:47:17.423958      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 06:47:17.434: INFO: Creating new exec pod
Apr 29 06:47:20.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8786 exec execpod-affinitytlpdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Apr 29 06:47:20.613: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Apr 29 06:47:20.613: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 06:47:20.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8786 exec execpod-affinitytlpdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.120.143 80'
Apr 29 06:47:20.762: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.120.143 80\nConnection to 10.98.120.143 80 port [tcp/http] succeeded!\n"
Apr 29 06:47:20.762: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 06:47:20.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8786 exec execpod-affinitytlpdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.150 30365'
Apr 29 06:47:20.904: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.150 30365\nConnection to 123.58.176.150 30365 port [tcp/*] succeeded!\n"
Apr 29 06:47:20.904: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 06:47:20.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8786 exec execpod-affinitytlpdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.151 30365'
Apr 29 06:47:21.038: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.151 30365\nConnection to 123.58.176.151 30365 port [tcp/*] succeeded!\n"
Apr 29 06:47:21.038: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 06:47:21.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8786 exec execpod-affinitytlpdh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://123.58.176.150:30365/ ; done'
Apr 29 06:47:21.289: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n"
Apr 29 06:47:21.289: INFO: stdout: "\naffinity-nodeport-transition-sbdf4\naffinity-nodeport-transition-sbdf4\naffinity-nodeport-transition-szvdl\naffinity-nodeport-transition-sbdf4\naffinity-nodeport-transition-sbdf4\naffinity-nodeport-transition-szvdl\naffinity-nodeport-transition-sbdf4\naffinity-nodeport-transition-szvdl\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-sbdf4\naffinity-nodeport-transition-sbdf4\naffinity-nodeport-transition-szvdl\naffinity-nodeport-transition-sbdf4\naffinity-nodeport-transition-sbdf4\naffinity-nodeport-transition-szvdl\naffinity-nodeport-transition-sbdf4"
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-sbdf4
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-sbdf4
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-szvdl
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-sbdf4
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-sbdf4
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-szvdl
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-sbdf4
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-szvdl
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-sbdf4
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-sbdf4
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-szvdl
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-sbdf4
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-sbdf4
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-szvdl
Apr 29 06:47:21.289: INFO: Received response from host: affinity-nodeport-transition-sbdf4
Apr 29 06:47:21.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8786 exec execpod-affinitytlpdh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://123.58.176.150:30365/ ; done'
Apr 29 06:47:21.504: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:30365/\n"
Apr 29 06:47:21.504: INFO: stdout: "\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk\naffinity-nodeport-transition-2thgk"
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Received response from host: affinity-nodeport-transition-2thgk
Apr 29 06:47:21.504: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8786, will wait for the garbage collector to delete the pods
Apr 29 06:47:21.589: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.300292ms
Apr 29 06:47:21.690: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.927103ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:47:23.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8786" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.594 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":34,"skipped":537,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:47:23.821: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Apr 29 06:47:23.870: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Apr 29 06:47:25.888: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Apr 29 06:47:27.902: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:47:29.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9259" for this suite.

• [SLOW TEST:6.097 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":35,"skipped":564,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:47:29.919: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Apr 29 06:47:29.975: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 06:48:30.014: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 06:48:30.018: INFO: Starting informer...
STEP: Starting pods...
Apr 29 06:48:30.238: INFO: Pod1 is running on localhost.localdomain151. Tainting Node
Apr 29 06:48:32.460: INFO: Pod2 is running on localhost.localdomain151. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Apr 29 06:48:40.415: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Apr 29 06:48:58.452: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:48:58.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7843" for this suite.

• [SLOW TEST:88.565 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":36,"skipped":607,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:48:58.484: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-a956b50d-5ec2-47b4-8462-63892c6ae65f
STEP: Creating a pod to test consume configMaps
Apr 29 06:48:58.728: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-210d9b46-b213-462a-8b94-d69e3944f859" in namespace "projected-4609" to be "Succeeded or Failed"
Apr 29 06:48:58.735: INFO: Pod "pod-projected-configmaps-210d9b46-b213-462a-8b94-d69e3944f859": Phase="Pending", Reason="", readiness=false. Elapsed: 7.27415ms
Apr 29 06:49:00.740: INFO: Pod "pod-projected-configmaps-210d9b46-b213-462a-8b94-d69e3944f859": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011911324s
STEP: Saw pod success
Apr 29 06:49:00.740: INFO: Pod "pod-projected-configmaps-210d9b46-b213-462a-8b94-d69e3944f859" satisfied condition "Succeeded or Failed"
Apr 29 06:49:00.743: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-configmaps-210d9b46-b213-462a-8b94-d69e3944f859 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 06:49:00.767: INFO: Waiting for pod pod-projected-configmaps-210d9b46-b213-462a-8b94-d69e3944f859 to disappear
Apr 29 06:49:00.770: INFO: Pod pod-projected-configmaps-210d9b46-b213-462a-8b94-d69e3944f859 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:49:00.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4609" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":37,"skipped":642,"failed":0}
SSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:49:00.779: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Apr 29 06:49:00.840: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:49:02.845: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Apr 29 06:49:02.855: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:49:04.862: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 29 06:49:04.865: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:49:04.865: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:49:04.865: INFO: ExecWithOptions: Clientset creation
Apr 29 06:49:04.865: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:49:04.940: INFO: Exec stderr: ""
Apr 29 06:49:04.940: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:49:04.940: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:49:04.941: INFO: ExecWithOptions: Clientset creation
Apr 29 06:49:04.941: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:49:05.029: INFO: Exec stderr: ""
Apr 29 06:49:05.029: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:49:05.029: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:49:05.030: INFO: ExecWithOptions: Clientset creation
Apr 29 06:49:05.030: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:49:05.097: INFO: Exec stderr: ""
Apr 29 06:49:05.097: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:49:05.097: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:49:05.097: INFO: ExecWithOptions: Clientset creation
Apr 29 06:49:05.097: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:49:05.173: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 29 06:49:05.173: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:49:05.173: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:49:05.174: INFO: ExecWithOptions: Clientset creation
Apr 29 06:49:05.174: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:49:05.278: INFO: Exec stderr: ""
Apr 29 06:49:05.278: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:49:05.278: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:49:05.279: INFO: ExecWithOptions: Clientset creation
Apr 29 06:49:05.279: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:49:05.347: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 29 06:49:05.347: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:49:05.347: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:49:05.347: INFO: ExecWithOptions: Clientset creation
Apr 29 06:49:05.347: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:49:05.417: INFO: Exec stderr: ""
Apr 29 06:49:05.417: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:49:05.417: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:49:05.418: INFO: ExecWithOptions: Clientset creation
Apr 29 06:49:05.418: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:49:05.492: INFO: Exec stderr: ""
Apr 29 06:49:05.492: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:49:05.492: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:49:05.492: INFO: ExecWithOptions: Clientset creation
Apr 29 06:49:05.492: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:49:05.567: INFO: Exec stderr: ""
Apr 29 06:49:05.567: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:49:05.567: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:49:05.568: INFO: ExecWithOptions: Clientset creation
Apr 29 06:49:05.568: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:49:05.639: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:49:05.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4691" for this suite.
•{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":648,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:49:05.651: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-d94e54f1-0602-421d-b9d4-87de639f9a7f
STEP: Creating a pod to test consume configMaps
Apr 29 06:49:05.738: INFO: Waiting up to 5m0s for pod "pod-configmaps-345df9d3-81c0-46c9-9a33-b03f8e973371" in namespace "configmap-5315" to be "Succeeded or Failed"
Apr 29 06:49:05.744: INFO: Pod "pod-configmaps-345df9d3-81c0-46c9-9a33-b03f8e973371": Phase="Pending", Reason="", readiness=false. Elapsed: 5.289535ms
Apr 29 06:49:07.748: INFO: Pod "pod-configmaps-345df9d3-81c0-46c9-9a33-b03f8e973371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009858284s
STEP: Saw pod success
Apr 29 06:49:07.748: INFO: Pod "pod-configmaps-345df9d3-81c0-46c9-9a33-b03f8e973371" satisfied condition "Succeeded or Failed"
Apr 29 06:49:07.751: INFO: Trying to get logs from node localhost.localdomain151 pod pod-configmaps-345df9d3-81c0-46c9-9a33-b03f8e973371 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 06:49:07.763: INFO: Waiting for pod pod-configmaps-345df9d3-81c0-46c9-9a33-b03f8e973371 to disappear
Apr 29 06:49:07.765: INFO: Pod pod-configmaps-345df9d3-81c0-46c9-9a33-b03f8e973371 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:49:07.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5315" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":39,"skipped":658,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:49:07.774: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Apr 29 06:49:07.822: INFO: Waiting up to 5m0s for pod "var-expansion-990ecfe6-095c-4715-8cf2-612e3e0184e9" in namespace "var-expansion-2747" to be "Succeeded or Failed"
Apr 29 06:49:07.825: INFO: Pod "var-expansion-990ecfe6-095c-4715-8cf2-612e3e0184e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.286171ms
Apr 29 06:49:09.830: INFO: Pod "var-expansion-990ecfe6-095c-4715-8cf2-612e3e0184e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007720188s
STEP: Saw pod success
Apr 29 06:49:09.830: INFO: Pod "var-expansion-990ecfe6-095c-4715-8cf2-612e3e0184e9" satisfied condition "Succeeded or Failed"
Apr 29 06:49:09.832: INFO: Trying to get logs from node localhost.localdomain151 pod var-expansion-990ecfe6-095c-4715-8cf2-612e3e0184e9 container dapi-container: <nil>
STEP: delete the pod
Apr 29 06:49:09.847: INFO: Waiting for pod var-expansion-990ecfe6-095c-4715-8cf2-612e3e0184e9 to disappear
Apr 29 06:49:09.849: INFO: Pod var-expansion-990ecfe6-095c-4715-8cf2-612e3e0184e9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:49:09.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2747" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":40,"skipped":686,"failed":0}
SSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:49:09.857: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-cacf09a7-31f4-42c5-b92f-aa6b5d76a430 in namespace container-probe-5877
Apr 29 06:49:11.897: INFO: Started pod liveness-cacf09a7-31f4-42c5-b92f-aa6b5d76a430 in namespace container-probe-5877
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 06:49:11.899: INFO: Initial restart count of pod liveness-cacf09a7-31f4-42c5-b92f-aa6b5d76a430 is 0
Apr 29 06:49:31.961: INFO: Restart count of pod container-probe-5877/liveness-cacf09a7-31f4-42c5-b92f-aa6b5d76a430 is now 1 (20.062029424s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:49:31.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5877" for this suite.

• [SLOW TEST:22.122 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":41,"skipped":693,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:49:31.979: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 06:49:32.012: INFO: Endpoints addresses: [123.58.176.150] , ports: [6443]
Apr 29 06:49:32.012: INFO: EndpointSlices addresses: [123.58.176.150] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:49:32.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1702" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":42,"skipped":710,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:49:32.021: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 29 06:49:32.122: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8345  2e439751-772a-4540-a012-e51258a811ab 90598 0 2022-04-29 06:49:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-04-29 06:49:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 06:49:32.122: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8345  2e439751-772a-4540-a012-e51258a811ab 90599 0 2022-04-29 06:49:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-04-29 06:49:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 06:49:32.122: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8345  2e439751-772a-4540-a012-e51258a811ab 90600 0 2022-04-29 06:49:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-04-29 06:49:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 29 06:49:42.151: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8345  2e439751-772a-4540-a012-e51258a811ab 90662 0 2022-04-29 06:49:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-04-29 06:49:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 06:49:42.151: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8345  2e439751-772a-4540-a012-e51258a811ab 90663 0 2022-04-29 06:49:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-04-29 06:49:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 06:49:42.151: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8345  2e439751-772a-4540-a012-e51258a811ab 90664 0 2022-04-29 06:49:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-04-29 06:49:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:49:42.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8345" for this suite.

• [SLOW TEST:10.139 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":43,"skipped":724,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:49:42.161: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-d5cc2cc3-3831-4742-9bd1-097b9da5d20c
STEP: Creating a pod to test consume secrets
Apr 29 06:49:42.204: INFO: Waiting up to 5m0s for pod "pod-secrets-75fb2893-5ace-4df7-88b3-2294b2cbbea6" in namespace "secrets-6415" to be "Succeeded or Failed"
Apr 29 06:49:42.206: INFO: Pod "pod-secrets-75fb2893-5ace-4df7-88b3-2294b2cbbea6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003039ms
Apr 29 06:49:44.215: INFO: Pod "pod-secrets-75fb2893-5ace-4df7-88b3-2294b2cbbea6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011083928s
STEP: Saw pod success
Apr 29 06:49:44.215: INFO: Pod "pod-secrets-75fb2893-5ace-4df7-88b3-2294b2cbbea6" satisfied condition "Succeeded or Failed"
Apr 29 06:49:44.218: INFO: Trying to get logs from node localhost.localdomain151 pod pod-secrets-75fb2893-5ace-4df7-88b3-2294b2cbbea6 container secret-env-test: <nil>
STEP: delete the pod
Apr 29 06:49:44.236: INFO: Waiting for pod pod-secrets-75fb2893-5ace-4df7-88b3-2294b2cbbea6 to disappear
Apr 29 06:49:44.238: INFO: Pod pod-secrets-75fb2893-5ace-4df7-88b3-2294b2cbbea6 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:49:44.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6415" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":768,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:49:44.247: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-zlphz in namespace proxy-8415
I0429 06:49:44.345500      22 runners.go:193] Created replication controller with name: proxy-service-zlphz, namespace: proxy-8415, replica count: 1
I0429 06:49:45.396986      22 runners.go:193] proxy-service-zlphz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 06:49:46.397139      22 runners.go:193] proxy-service-zlphz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0429 06:49:47.398218      22 runners.go:193] proxy-service-zlphz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 06:49:47.401: INFO: setup took 3.07173045s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 29 06:49:47.407: INFO: (0) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 5.674589ms)
Apr 29 06:49:47.409: INFO: (0) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 7.78243ms)
Apr 29 06:49:47.409: INFO: (0) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 7.930647ms)
Apr 29 06:49:47.409: INFO: (0) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 7.889771ms)
Apr 29 06:49:47.409: INFO: (0) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 7.975821ms)
Apr 29 06:49:47.409: INFO: (0) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 7.957631ms)
Apr 29 06:49:47.409: INFO: (0) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 7.952624ms)
Apr 29 06:49:47.413: INFO: (0) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 11.554722ms)
Apr 29 06:49:47.413: INFO: (0) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 11.635151ms)
Apr 29 06:49:47.414: INFO: (0) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 12.599037ms)
Apr 29 06:49:47.414: INFO: (0) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 12.576677ms)
Apr 29 06:49:47.419: INFO: (0) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 18.020318ms)
Apr 29 06:49:47.419: INFO: (0) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 18.089794ms)
Apr 29 06:49:47.419: INFO: (0) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 17.988948ms)
Apr 29 06:49:47.420: INFO: (0) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 18.069538ms)
Apr 29 06:49:47.420: INFO: (0) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 18.102039ms)
Apr 29 06:49:47.424: INFO: (1) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 3.455827ms)
Apr 29 06:49:47.425: INFO: (1) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 5.582535ms)
Apr 29 06:49:47.426: INFO: (1) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 5.422753ms)
Apr 29 06:49:47.426: INFO: (1) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 4.671998ms)
Apr 29 06:49:47.426: INFO: (1) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 5.157697ms)
Apr 29 06:49:47.426: INFO: (1) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 5.311931ms)
Apr 29 06:49:47.426: INFO: (1) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 5.944347ms)
Apr 29 06:49:47.426: INFO: (1) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 5.061296ms)
Apr 29 06:49:47.426: INFO: (1) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 4.596226ms)
Apr 29 06:49:47.426: INFO: (1) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 4.749028ms)
Apr 29 06:49:47.426: INFO: (1) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 4.823839ms)
Apr 29 06:49:47.427: INFO: (1) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 5.520989ms)
Apr 29 06:49:47.428: INFO: (1) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 5.69075ms)
Apr 29 06:49:47.428: INFO: (1) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 6.190339ms)
Apr 29 06:49:47.428: INFO: (1) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 6.348203ms)
Apr 29 06:49:47.428: INFO: (1) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 6.515795ms)
Apr 29 06:49:47.432: INFO: (2) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 3.805692ms)
Apr 29 06:49:47.432: INFO: (2) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 4.137436ms)
Apr 29 06:49:47.432: INFO: (2) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 4.21281ms)
Apr 29 06:49:47.432: INFO: (2) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 4.030537ms)
Apr 29 06:49:47.433: INFO: (2) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 5.270159ms)
Apr 29 06:49:47.434: INFO: (2) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 6.21007ms)
Apr 29 06:49:47.434: INFO: (2) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 6.198135ms)
Apr 29 06:49:47.434: INFO: (2) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 6.35901ms)
Apr 29 06:49:47.435: INFO: (2) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 6.458962ms)
Apr 29 06:49:47.435: INFO: (2) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 6.419906ms)
Apr 29 06:49:47.435: INFO: (2) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 7.385641ms)
Apr 29 06:49:47.435: INFO: (2) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 7.204754ms)
Apr 29 06:49:47.435: INFO: (2) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 7.389264ms)
Apr 29 06:49:47.435: INFO: (2) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 7.209967ms)
Apr 29 06:49:47.435: INFO: (2) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 7.361869ms)
Apr 29 06:49:47.435: INFO: (2) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 7.307999ms)
Apr 29 06:49:47.441: INFO: (3) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 5.160233ms)
Apr 29 06:49:47.441: INFO: (3) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 5.693564ms)
Apr 29 06:49:47.443: INFO: (3) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 6.862973ms)
Apr 29 06:49:47.443: INFO: (3) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 6.80924ms)
Apr 29 06:49:47.443: INFO: (3) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 6.879873ms)
Apr 29 06:49:47.443: INFO: (3) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 7.012126ms)
Apr 29 06:49:47.443: INFO: (3) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 7.111605ms)
Apr 29 06:49:47.443: INFO: (3) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 6.896808ms)
Apr 29 06:49:47.443: INFO: (3) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 6.955549ms)
Apr 29 06:49:47.444: INFO: (3) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 8.031624ms)
Apr 29 06:49:47.444: INFO: (3) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 8.111121ms)
Apr 29 06:49:47.444: INFO: (3) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 8.450796ms)
Apr 29 06:49:47.444: INFO: (3) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 8.310615ms)
Apr 29 06:49:47.444: INFO: (3) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 8.538995ms)
Apr 29 06:49:47.444: INFO: (3) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 8.748498ms)
Apr 29 06:49:47.444: INFO: (3) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 8.841457ms)
Apr 29 06:49:47.448: INFO: (4) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 3.172282ms)
Apr 29 06:49:47.449: INFO: (4) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 3.727274ms)
Apr 29 06:49:47.449: INFO: (4) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 3.995118ms)
Apr 29 06:49:47.450: INFO: (4) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 4.183781ms)
Apr 29 06:49:47.451: INFO: (4) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 4.893427ms)
Apr 29 06:49:47.453: INFO: (4) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 7.441026ms)
Apr 29 06:49:47.453: INFO: (4) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 7.395454ms)
Apr 29 06:49:47.453: INFO: (4) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 7.44882ms)
Apr 29 06:49:47.453: INFO: (4) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 7.196553ms)
Apr 29 06:49:47.453: INFO: (4) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 7.781714ms)
Apr 29 06:49:47.454: INFO: (4) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 8.595407ms)
Apr 29 06:49:47.454: INFO: (4) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 8.21581ms)
Apr 29 06:49:47.454: INFO: (4) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 8.324933ms)
Apr 29 06:49:47.455: INFO: (4) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 8.994136ms)
Apr 29 06:49:47.456: INFO: (4) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 10.311933ms)
Apr 29 06:49:47.456: INFO: (4) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 9.350243ms)
Apr 29 06:49:47.460: INFO: (5) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 4.629821ms)
Apr 29 06:49:47.460: INFO: (5) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 4.551209ms)
Apr 29 06:49:47.460: INFO: (5) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 4.603521ms)
Apr 29 06:49:47.461: INFO: (5) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 4.998668ms)
Apr 29 06:49:47.461: INFO: (5) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 5.518023ms)
Apr 29 06:49:47.462: INFO: (5) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 5.851045ms)
Apr 29 06:49:47.462: INFO: (5) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 5.956575ms)
Apr 29 06:49:47.462: INFO: (5) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 5.876405ms)
Apr 29 06:49:47.462: INFO: (5) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 5.963304ms)
Apr 29 06:49:47.464: INFO: (5) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 7.78346ms)
Apr 29 06:49:47.464: INFO: (5) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 7.866833ms)
Apr 29 06:49:47.464: INFO: (5) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 8.42974ms)
Apr 29 06:49:47.464: INFO: (5) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 8.505577ms)
Apr 29 06:49:47.464: INFO: (5) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 8.626755ms)
Apr 29 06:49:47.465: INFO: (5) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 8.663761ms)
Apr 29 06:49:47.465: INFO: (5) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 8.688161ms)
Apr 29 06:49:47.468: INFO: (6) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 3.111353ms)
Apr 29 06:49:47.469: INFO: (6) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 3.670018ms)
Apr 29 06:49:47.471: INFO: (6) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 5.133849ms)
Apr 29 06:49:47.471: INFO: (6) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 4.810876ms)
Apr 29 06:49:47.471: INFO: (6) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 5.317332ms)
Apr 29 06:49:47.472: INFO: (6) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 5.62121ms)
Apr 29 06:49:47.472: INFO: (6) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 5.555239ms)
Apr 29 06:49:47.472: INFO: (6) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 6.174459ms)
Apr 29 06:49:47.472: INFO: (6) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 6.965644ms)
Apr 29 06:49:47.472: INFO: (6) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 6.320338ms)
Apr 29 06:49:47.472: INFO: (6) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 6.101079ms)
Apr 29 06:49:47.473: INFO: (6) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 7.729403ms)
Apr 29 06:49:47.473: INFO: (6) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 6.180793ms)
Apr 29 06:49:47.473: INFO: (6) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 6.322843ms)
Apr 29 06:49:47.473: INFO: (6) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 7.701302ms)
Apr 29 06:49:47.473: INFO: (6) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 7.573682ms)
Apr 29 06:49:47.478: INFO: (7) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 4.838738ms)
Apr 29 06:49:47.478: INFO: (7) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 5.505118ms)
Apr 29 06:49:47.480: INFO: (7) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 6.619081ms)
Apr 29 06:49:47.480: INFO: (7) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 6.321292ms)
Apr 29 06:49:47.480: INFO: (7) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 6.395222ms)
Apr 29 06:49:47.480: INFO: (7) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 6.370959ms)
Apr 29 06:49:47.480: INFO: (7) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 6.335343ms)
Apr 29 06:49:47.480: INFO: (7) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 6.481507ms)
Apr 29 06:49:47.480: INFO: (7) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 6.427024ms)
Apr 29 06:49:47.480: INFO: (7) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 6.454223ms)
Apr 29 06:49:47.480: INFO: (7) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 6.84258ms)
Apr 29 06:49:47.481: INFO: (7) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 7.687157ms)
Apr 29 06:49:47.481: INFO: (7) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 7.807555ms)
Apr 29 06:49:47.482: INFO: (7) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 8.011934ms)
Apr 29 06:49:47.482: INFO: (7) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 7.973465ms)
Apr 29 06:49:47.482: INFO: (7) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 8.407199ms)
Apr 29 06:49:47.487: INFO: (8) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 4.640983ms)
Apr 29 06:49:47.488: INFO: (8) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 4.554249ms)
Apr 29 06:49:47.488: INFO: (8) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 4.865099ms)
Apr 29 06:49:47.488: INFO: (8) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 5.563246ms)
Apr 29 06:49:47.488: INFO: (8) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 4.852707ms)
Apr 29 06:49:47.488: INFO: (8) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 4.721823ms)
Apr 29 06:49:47.488: INFO: (8) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 4.793265ms)
Apr 29 06:49:47.488: INFO: (8) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 4.945873ms)
Apr 29 06:49:47.488: INFO: (8) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 4.841417ms)
Apr 29 06:49:47.488: INFO: (8) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 5.160436ms)
Apr 29 06:49:47.488: INFO: (8) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 4.896327ms)
Apr 29 06:49:47.488: INFO: (8) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 6.17474ms)
Apr 29 06:49:47.488: INFO: (8) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 6.093315ms)
Apr 29 06:49:47.489: INFO: (8) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 6.013845ms)
Apr 29 06:49:47.489: INFO: (8) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 5.933924ms)
Apr 29 06:49:47.489: INFO: (8) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 5.30567ms)
Apr 29 06:49:47.492: INFO: (9) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 3.174553ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 7.337655ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 7.367869ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 7.375826ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 7.399617ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 7.49712ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 8.05072ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 7.704532ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 7.593141ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 7.456378ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 7.69899ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 7.923177ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 8.511869ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 7.584795ms)
Apr 29 06:49:47.497: INFO: (9) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 7.719862ms)
Apr 29 06:49:47.498: INFO: (9) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 8.518323ms)
Apr 29 06:49:47.505: INFO: (10) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 6.73791ms)
Apr 29 06:49:47.505: INFO: (10) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 6.70309ms)
Apr 29 06:49:47.505: INFO: (10) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 6.778662ms)
Apr 29 06:49:47.505: INFO: (10) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 7.073917ms)
Apr 29 06:49:47.505: INFO: (10) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 7.30021ms)
Apr 29 06:49:47.505: INFO: (10) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 6.998568ms)
Apr 29 06:49:47.505: INFO: (10) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 7.096945ms)
Apr 29 06:49:47.505: INFO: (10) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 7.553462ms)
Apr 29 06:49:47.505: INFO: (10) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 7.209731ms)
Apr 29 06:49:47.505: INFO: (10) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 7.47263ms)
Apr 29 06:49:47.506: INFO: (10) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 7.582944ms)
Apr 29 06:49:47.506: INFO: (10) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 7.352787ms)
Apr 29 06:49:47.506: INFO: (10) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 7.444415ms)
Apr 29 06:49:47.506: INFO: (10) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 7.857636ms)
Apr 29 06:49:47.506: INFO: (10) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 7.59816ms)
Apr 29 06:49:47.506: INFO: (10) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 7.756614ms)
Apr 29 06:49:47.510: INFO: (11) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 3.94624ms)
Apr 29 06:49:47.510: INFO: (11) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 4.274031ms)
Apr 29 06:49:47.511: INFO: (11) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 4.095278ms)
Apr 29 06:49:47.511: INFO: (11) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 4.26874ms)
Apr 29 06:49:47.511: INFO: (11) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 4.797067ms)
Apr 29 06:49:47.511: INFO: (11) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 4.85253ms)
Apr 29 06:49:47.512: INFO: (11) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 5.05302ms)
Apr 29 06:49:47.513: INFO: (11) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 6.355935ms)
Apr 29 06:49:47.513: INFO: (11) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 6.497011ms)
Apr 29 06:49:47.513: INFO: (11) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 6.507532ms)
Apr 29 06:49:47.513: INFO: (11) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 6.438685ms)
Apr 29 06:49:47.513: INFO: (11) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 6.407368ms)
Apr 29 06:49:47.513: INFO: (11) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 6.700768ms)
Apr 29 06:49:47.513: INFO: (11) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 6.41006ms)
Apr 29 06:49:47.513: INFO: (11) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 6.847812ms)
Apr 29 06:49:47.513: INFO: (11) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 6.462335ms)
Apr 29 06:49:47.516: INFO: (12) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 3.071622ms)
Apr 29 06:49:47.518: INFO: (12) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 4.09594ms)
Apr 29 06:49:47.518: INFO: (12) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 4.832795ms)
Apr 29 06:49:47.520: INFO: (12) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 5.943926ms)
Apr 29 06:49:47.520: INFO: (12) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 5.786113ms)
Apr 29 06:49:47.520: INFO: (12) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 5.947176ms)
Apr 29 06:49:47.520: INFO: (12) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 5.885443ms)
Apr 29 06:49:47.520: INFO: (12) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 6.026514ms)
Apr 29 06:49:47.520: INFO: (12) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 6.580338ms)
Apr 29 06:49:47.520: INFO: (12) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 6.164294ms)
Apr 29 06:49:47.520: INFO: (12) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 6.358216ms)
Apr 29 06:49:47.521: INFO: (12) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 7.10778ms)
Apr 29 06:49:47.521: INFO: (12) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 7.735568ms)
Apr 29 06:49:47.522: INFO: (12) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 7.996954ms)
Apr 29 06:49:47.522: INFO: (12) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 7.828492ms)
Apr 29 06:49:47.522: INFO: (12) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 8.342214ms)
Apr 29 06:49:47.527: INFO: (13) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 4.426187ms)
Apr 29 06:49:47.530: INFO: (13) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 6.7428ms)
Apr 29 06:49:47.530: INFO: (13) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 6.666314ms)
Apr 29 06:49:47.530: INFO: (13) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 6.653331ms)
Apr 29 06:49:47.530: INFO: (13) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 6.655257ms)
Apr 29 06:49:47.530: INFO: (13) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 6.738004ms)
Apr 29 06:49:47.530: INFO: (13) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 7.732359ms)
Apr 29 06:49:47.530: INFO: (13) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 6.777183ms)
Apr 29 06:49:47.530: INFO: (13) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 7.248834ms)
Apr 29 06:49:47.530: INFO: (13) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 8.111655ms)
Apr 29 06:49:47.530: INFO: (13) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 8.08392ms)
Apr 29 06:49:47.530: INFO: (13) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 7.299193ms)
Apr 29 06:49:47.530: INFO: (13) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 7.809319ms)
Apr 29 06:49:47.531: INFO: (13) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 7.683385ms)
Apr 29 06:49:47.531: INFO: (13) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 7.613435ms)
Apr 29 06:49:47.531: INFO: (13) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 8.401521ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 7.681257ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 7.820411ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 7.799627ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 7.826423ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 8.21154ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 7.674257ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 7.777953ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 7.917099ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 8.227785ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 8.119088ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 8.037675ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 7.411263ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 8.067804ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 8.120024ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 8.071109ms)
Apr 29 06:49:47.540: INFO: (14) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 8.140282ms)
Apr 29 06:49:47.547: INFO: (15) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 5.606083ms)
Apr 29 06:49:47.547: INFO: (15) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 5.786972ms)
Apr 29 06:49:47.548: INFO: (15) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 6.9561ms)
Apr 29 06:49:47.548: INFO: (15) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 6.964428ms)
Apr 29 06:49:47.548: INFO: (15) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 7.792463ms)
Apr 29 06:49:47.548: INFO: (15) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 7.351481ms)
Apr 29 06:49:47.548: INFO: (15) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 7.916836ms)
Apr 29 06:49:47.548: INFO: (15) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 8.052151ms)
Apr 29 06:49:47.549: INFO: (15) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 7.339275ms)
Apr 29 06:49:47.549: INFO: (15) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 8.099764ms)
Apr 29 06:49:47.549: INFO: (15) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 8.24481ms)
Apr 29 06:49:47.550: INFO: (15) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 8.552949ms)
Apr 29 06:49:47.550: INFO: (15) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 9.406101ms)
Apr 29 06:49:47.550: INFO: (15) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 8.356134ms)
Apr 29 06:49:47.550: INFO: (15) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 9.487409ms)
Apr 29 06:49:47.550: INFO: (15) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 9.079649ms)
Apr 29 06:49:47.556: INFO: (16) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 5.467073ms)
Apr 29 06:49:47.556: INFO: (16) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 5.67478ms)
Apr 29 06:49:47.557: INFO: (16) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 5.524117ms)
Apr 29 06:49:47.557: INFO: (16) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 6.005651ms)
Apr 29 06:49:47.560: INFO: (16) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 9.180184ms)
Apr 29 06:49:47.560: INFO: (16) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 8.893276ms)
Apr 29 06:49:47.560: INFO: (16) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 8.985547ms)
Apr 29 06:49:47.560: INFO: (16) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 9.411763ms)
Apr 29 06:49:47.560: INFO: (16) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 9.52861ms)
Apr 29 06:49:47.560: INFO: (16) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 9.192195ms)
Apr 29 06:49:47.560: INFO: (16) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 9.395399ms)
Apr 29 06:49:47.560: INFO: (16) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 9.631883ms)
Apr 29 06:49:47.560: INFO: (16) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 9.372478ms)
Apr 29 06:49:47.560: INFO: (16) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 9.497384ms)
Apr 29 06:49:47.560: INFO: (16) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 8.871621ms)
Apr 29 06:49:47.560: INFO: (16) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 9.92218ms)
Apr 29 06:49:47.565: INFO: (17) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 4.479506ms)
Apr 29 06:49:47.565: INFO: (17) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 4.639773ms)
Apr 29 06:49:47.565: INFO: (17) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 4.420669ms)
Apr 29 06:49:47.568: INFO: (17) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 7.486375ms)
Apr 29 06:49:47.568: INFO: (17) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 7.3579ms)
Apr 29 06:49:47.568: INFO: (17) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 7.670775ms)
Apr 29 06:49:47.568: INFO: (17) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 7.429306ms)
Apr 29 06:49:47.568: INFO: (17) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 7.376407ms)
Apr 29 06:49:47.568: INFO: (17) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 7.652108ms)
Apr 29 06:49:47.568: INFO: (17) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 7.745108ms)
Apr 29 06:49:47.568: INFO: (17) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 8.043541ms)
Apr 29 06:49:47.568: INFO: (17) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 8.219808ms)
Apr 29 06:49:47.569: INFO: (17) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 8.41991ms)
Apr 29 06:49:47.569: INFO: (17) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 9.083599ms)
Apr 29 06:49:47.569: INFO: (17) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 9.032542ms)
Apr 29 06:49:47.569: INFO: (17) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 9.20536ms)
Apr 29 06:49:47.573: INFO: (18) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 3.056724ms)
Apr 29 06:49:47.573: INFO: (18) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 3.530589ms)
Apr 29 06:49:47.575: INFO: (18) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 3.865657ms)
Apr 29 06:49:47.576: INFO: (18) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 5.952229ms)
Apr 29 06:49:47.576: INFO: (18) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 5.481366ms)
Apr 29 06:49:47.577: INFO: (18) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 5.652977ms)
Apr 29 06:49:47.577: INFO: (18) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 6.299836ms)
Apr 29 06:49:47.577: INFO: (18) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 6.185382ms)
Apr 29 06:49:47.577: INFO: (18) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 6.281256ms)
Apr 29 06:49:47.578: INFO: (18) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 7.218514ms)
Apr 29 06:49:47.578: INFO: (18) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 6.856272ms)
Apr 29 06:49:47.578: INFO: (18) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 7.012105ms)
Apr 29 06:49:47.578: INFO: (18) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 7.376943ms)
Apr 29 06:49:47.578: INFO: (18) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 7.971056ms)
Apr 29 06:49:47.578: INFO: (18) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 8.701928ms)
Apr 29 06:49:47.579: INFO: (18) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 9.066251ms)
Apr 29 06:49:47.585: INFO: (19) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:462/proxy/: tls qux (200; 4.762827ms)
Apr 29 06:49:47.585: INFO: (19) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 4.520299ms)
Apr 29 06:49:47.585: INFO: (19) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 4.348845ms)
Apr 29 06:49:47.585: INFO: (19) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">... (200; 4.170917ms)
Apr 29 06:49:47.585: INFO: (19) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:160/proxy/: foo (200; 5.079204ms)
Apr 29 06:49:47.585: INFO: (19) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:162/proxy/: bar (200; 4.482769ms)
Apr 29 06:49:47.585: INFO: (19) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:460/proxy/: tls baz (200; 5.004115ms)
Apr 29 06:49:47.585: INFO: (19) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4/proxy/rewriteme">test</a> (200; 4.339035ms)
Apr 29 06:49:47.585: INFO: (19) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-zlphz-mmxn4:443/proxy/tlsrewritem... (200; 4.241109ms)
Apr 29 06:49:47.586: INFO: (19) /api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-zlphz-mmxn4:1080/proxy/rewriteme">test<... (200; 4.869527ms)
Apr 29 06:49:47.586: INFO: (19) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname1/proxy/: foo (200; 5.195395ms)
Apr 29 06:49:47.586: INFO: (19) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname2/proxy/: bar (200; 5.946274ms)
Apr 29 06:49:47.588: INFO: (19) /api/v1/namespaces/proxy-8415/services/http:proxy-service-zlphz:portname2/proxy/: bar (200; 8.097301ms)
Apr 29 06:49:47.588: INFO: (19) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname2/proxy/: tls qux (200; 8.687991ms)
Apr 29 06:49:47.588: INFO: (19) /api/v1/namespaces/proxy-8415/services/proxy-service-zlphz:portname1/proxy/: foo (200; 8.340316ms)
Apr 29 06:49:47.588: INFO: (19) /api/v1/namespaces/proxy-8415/services/https:proxy-service-zlphz:tlsportname1/proxy/: tls baz (200; 8.870857ms)
STEP: deleting ReplicationController proxy-service-zlphz in namespace proxy-8415, will wait for the garbage collector to delete the pods
Apr 29 06:49:47.649: INFO: Deleting ReplicationController proxy-service-zlphz took: 6.113144ms
Apr 29 06:49:47.749: INFO: Terminating ReplicationController proxy-service-zlphz pods took: 100.309935ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:49:50.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8415" for this suite.

• [SLOW TEST:6.414 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":45,"skipped":821,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:49:50.662: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 06:49:50.696: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3f45245f-114a-4326-b432-f070f3e4f06e" in namespace "security-context-test-1907" to be "Succeeded or Failed"
Apr 29 06:49:50.699: INFO: Pod "busybox-privileged-false-3f45245f-114a-4326-b432-f070f3e4f06e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.681359ms
Apr 29 06:49:52.703: INFO: Pod "busybox-privileged-false-3f45245f-114a-4326-b432-f070f3e4f06e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007398741s
Apr 29 06:49:52.703: INFO: Pod "busybox-privileged-false-3f45245f-114a-4326-b432-f070f3e4f06e" satisfied condition "Succeeded or Failed"
Apr 29 06:49:52.708: INFO: Got logs for pod "busybox-privileged-false-3f45245f-114a-4326-b432-f070f3e4f06e": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:49:52.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1907" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":46,"skipped":844,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:49:52.718: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:50:00.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1317" for this suite.

• [SLOW TEST:8.046 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":47,"skipped":888,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:50:00.766: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 06:50:00.806: INFO: The status of Pod test-webserver-c0cf4c65-e1f8-4816-aad5-afb23ef40666 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:50:02.810: INFO: The status of Pod test-webserver-c0cf4c65-e1f8-4816-aad5-afb23ef40666 is Running (Ready = false)
Apr 29 06:50:04.812: INFO: The status of Pod test-webserver-c0cf4c65-e1f8-4816-aad5-afb23ef40666 is Running (Ready = false)
Apr 29 06:50:06.812: INFO: The status of Pod test-webserver-c0cf4c65-e1f8-4816-aad5-afb23ef40666 is Running (Ready = false)
Apr 29 06:50:08.811: INFO: The status of Pod test-webserver-c0cf4c65-e1f8-4816-aad5-afb23ef40666 is Running (Ready = false)
Apr 29 06:50:10.811: INFO: The status of Pod test-webserver-c0cf4c65-e1f8-4816-aad5-afb23ef40666 is Running (Ready = false)
Apr 29 06:50:12.811: INFO: The status of Pod test-webserver-c0cf4c65-e1f8-4816-aad5-afb23ef40666 is Running (Ready = false)
Apr 29 06:50:14.811: INFO: The status of Pod test-webserver-c0cf4c65-e1f8-4816-aad5-afb23ef40666 is Running (Ready = false)
Apr 29 06:50:16.811: INFO: The status of Pod test-webserver-c0cf4c65-e1f8-4816-aad5-afb23ef40666 is Running (Ready = false)
Apr 29 06:50:18.812: INFO: The status of Pod test-webserver-c0cf4c65-e1f8-4816-aad5-afb23ef40666 is Running (Ready = false)
Apr 29 06:50:20.811: INFO: The status of Pod test-webserver-c0cf4c65-e1f8-4816-aad5-afb23ef40666 is Running (Ready = false)
Apr 29 06:50:22.810: INFO: The status of Pod test-webserver-c0cf4c65-e1f8-4816-aad5-afb23ef40666 is Running (Ready = true)
Apr 29 06:50:22.812: INFO: Container started at 2022-04-29 06:50:01 +0000 UTC, pod became ready at 2022-04-29 06:50:20 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:50:22.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8904" for this suite.

• [SLOW TEST:22.055 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":48,"skipped":923,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:50:22.822: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-8be28865-646b-4cdb-b9b9-541d66540b0a
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:50:24.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5126" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":49,"skipped":929,"failed":0}
SS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:50:24.902: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 29 06:50:24.957: INFO: starting watch
STEP: patching
STEP: updating
Apr 29 06:50:24.964: INFO: waiting for watch events with expected annotations
Apr 29 06:50:24.964: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:50:24.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7374" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":50,"skipped":931,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:50:24.988: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 29 06:50:25.030: INFO: Waiting up to 5m0s for pod "pod-7260bed0-046d-402e-b148-1d6f45407687" in namespace "emptydir-9583" to be "Succeeded or Failed"
Apr 29 06:50:25.032: INFO: Pod "pod-7260bed0-046d-402e-b148-1d6f45407687": Phase="Pending", Reason="", readiness=false. Elapsed: 2.314349ms
Apr 29 06:50:27.036: INFO: Pod "pod-7260bed0-046d-402e-b148-1d6f45407687": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006026101s
STEP: Saw pod success
Apr 29 06:50:27.036: INFO: Pod "pod-7260bed0-046d-402e-b148-1d6f45407687" satisfied condition "Succeeded or Failed"
Apr 29 06:50:27.038: INFO: Trying to get logs from node localhost.localdomain151 pod pod-7260bed0-046d-402e-b148-1d6f45407687 container test-container: <nil>
STEP: delete the pod
Apr 29 06:50:27.056: INFO: Waiting for pod pod-7260bed0-046d-402e-b148-1d6f45407687 to disappear
Apr 29 06:50:27.059: INFO: Pod pod-7260bed0-046d-402e-b148-1d6f45407687 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:50:27.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9583" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":51,"skipped":937,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:50:27.068: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-12ea363a-387c-43c4-ac4f-2a42438d0aea in namespace container-probe-4439
Apr 29 06:50:29.111: INFO: Started pod liveness-12ea363a-387c-43c4-ac4f-2a42438d0aea in namespace container-probe-4439
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 06:50:29.113: INFO: Initial restart count of pod liveness-12ea363a-387c-43c4-ac4f-2a42438d0aea is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:54:29.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4439" for this suite.

• [SLOW TEST:242.716 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":52,"skipped":943,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:54:29.784: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5066
Apr 29 06:54:29.940: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:54:31.946: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Apr 29 06:54:31.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-5066 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr 29 06:54:32.164: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr 29 06:54:32.164: INFO: stdout: "iptables"
Apr 29 06:54:32.164: INFO: proxyMode: iptables
Apr 29 06:54:32.174: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 29 06:54:32.177: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5066
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5066
I0429 06:54:32.196988      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5066, replica count: 3
I0429 06:54:35.248836      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 06:54:35.257: INFO: Creating new exec pod
Apr 29 06:54:38.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-5066 exec execpod-affinityksdv2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Apr 29 06:54:38.423: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Apr 29 06:54:38.423: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 06:54:38.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-5066 exec execpod-affinityksdv2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.90.130 80'
Apr 29 06:54:38.560: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.90.130 80\nConnection to 10.99.90.130 80 port [tcp/http] succeeded!\n"
Apr 29 06:54:38.560: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 06:54:38.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-5066 exec execpod-affinityksdv2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.150 31265'
Apr 29 06:54:38.706: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.150 31265\nConnection to 123.58.176.150 31265 port [tcp/*] succeeded!\n"
Apr 29 06:54:38.706: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 06:54:38.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-5066 exec execpod-affinityksdv2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.151 31265'
Apr 29 06:54:38.869: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.151 31265\nConnection to 123.58.176.151 31265 port [tcp/*] succeeded!\n"
Apr 29 06:54:38.869: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 06:54:38.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-5066 exec execpod-affinityksdv2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://123.58.176.150:31265/ ; done'
Apr 29 06:54:39.098: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n"
Apr 29 06:54:39.098: INFO: stdout: "\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt\naffinity-nodeport-timeout-xcxpt"
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Received response from host: affinity-nodeport-timeout-xcxpt
Apr 29 06:54:39.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-5066 exec execpod-affinityksdv2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://123.58.176.150:31265/'
Apr 29 06:54:39.244: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n"
Apr 29 06:54:39.244: INFO: stdout: "affinity-nodeport-timeout-xcxpt"
Apr 29 06:54:59.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-5066 exec execpod-affinityksdv2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://123.58.176.150:31265/'
Apr 29 06:54:59.391: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://123.58.176.150:31265/\n"
Apr 29 06:54:59.391: INFO: stdout: "affinity-nodeport-timeout-dhzsl"
Apr 29 06:54:59.391: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5066, will wait for the garbage collector to delete the pods
Apr 29 06:54:59.461: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.062314ms
Apr 29 06:54:59.562: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.017971ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:55:03.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5066" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:33.604 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":53,"skipped":952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:55:03.388: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Apr 29 06:55:03.417: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Apr 29 06:55:03.915: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Apr 29 06:55:11.175: INFO: Waited 5.21676425s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Apr 29 06:55:11.467: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:55:12.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2924" for this suite.

• [SLOW TEST:8.976 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":54,"skipped":987,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:55:12.364: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 06:55:12.392: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: creating the pod
STEP: submitting the pod to kubernetes
Apr 29 06:55:12.401: INFO: The status of Pod pod-logs-websocket-9794042f-da1a-44da-87a7-eaaeeb426433 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:55:14.406: INFO: The status of Pod pod-logs-websocket-9794042f-da1a-44da-87a7-eaaeeb426433 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:55:14.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8491" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":55,"skipped":990,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:55:14.439: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 29 06:55:14.477: INFO: Waiting up to 5m0s for pod "pod-01cb9260-73b0-4e1d-b252-c40796ea47fa" in namespace "emptydir-3499" to be "Succeeded or Failed"
Apr 29 06:55:14.479: INFO: Pod "pod-01cb9260-73b0-4e1d-b252-c40796ea47fa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.937395ms
Apr 29 06:55:16.484: INFO: Pod "pod-01cb9260-73b0-4e1d-b252-c40796ea47fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00703212s
STEP: Saw pod success
Apr 29 06:55:16.484: INFO: Pod "pod-01cb9260-73b0-4e1d-b252-c40796ea47fa" satisfied condition "Succeeded or Failed"
Apr 29 06:55:16.486: INFO: Trying to get logs from node localhost.localdomain151 pod pod-01cb9260-73b0-4e1d-b252-c40796ea47fa container test-container: <nil>
STEP: delete the pod
Apr 29 06:55:16.501: INFO: Waiting for pod pod-01cb9260-73b0-4e1d-b252-c40796ea47fa to disappear
Apr 29 06:55:16.503: INFO: Pod pod-01cb9260-73b0-4e1d-b252-c40796ea47fa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:55:16.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3499" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":56,"skipped":1000,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:55:16.513: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 29 06:55:18.557: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:55:18.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-931" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":57,"skipped":1019,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:55:18.582: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 06:55:18.625: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edcb882a-b142-44fd-b1da-ef485e8efb13" in namespace "downward-api-9091" to be "Succeeded or Failed"
Apr 29 06:55:18.629: INFO: Pod "downwardapi-volume-edcb882a-b142-44fd-b1da-ef485e8efb13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.285385ms
Apr 29 06:55:20.634: INFO: Pod "downwardapi-volume-edcb882a-b142-44fd-b1da-ef485e8efb13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008785232s
STEP: Saw pod success
Apr 29 06:55:20.634: INFO: Pod "downwardapi-volume-edcb882a-b142-44fd-b1da-ef485e8efb13" satisfied condition "Succeeded or Failed"
Apr 29 06:55:20.636: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-edcb882a-b142-44fd-b1da-ef485e8efb13 container client-container: <nil>
STEP: delete the pod
Apr 29 06:55:20.649: INFO: Waiting for pod downwardapi-volume-edcb882a-b142-44fd-b1da-ef485e8efb13 to disappear
Apr 29 06:55:20.656: INFO: Pod downwardapi-volume-edcb882a-b142-44fd-b1da-ef485e8efb13 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:55:20.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9091" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":58,"skipped":1020,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:55:20.664: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:55:20.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9707" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":59,"skipped":1030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:55:20.723: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 29 06:55:20.762: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 29 06:55:25.765: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:55:26.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1866" for this suite.

• [SLOW TEST:6.070 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":60,"skipped":1062,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:55:26.793: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 06:55:26.854: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 06:55:26.854: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 06:55:27.865: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 06:55:27.865: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 06:55:28.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 29 06:55:28.864: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 29 06:55:28.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 29 06:55:28.883: INFO: Node localhost.localdomain151 is running 0 daemon pod, expected 1
Apr 29 06:55:29.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 29 06:55:29.892: INFO: Node localhost.localdomain151 is running 0 daemon pod, expected 1
Apr 29 06:55:30.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 29 06:55:30.892: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5977, will wait for the garbage collector to delete the pods
Apr 29 06:55:30.955: INFO: Deleting DaemonSet.extensions daemon-set took: 5.519627ms
Apr 29 06:55:31.056: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.66271ms
Apr 29 06:55:33.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 06:55:33.461: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 29 06:55:33.463: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"93042"},"items":null}

Apr 29 06:55:33.466: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"93042"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:55:33.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5977" for this suite.

• [SLOW TEST:6.692 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":61,"skipped":1066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:55:33.485: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-8317
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 06:55:33.516: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 29 06:55:33.537: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:55:35.542: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:55:37.544: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:55:39.543: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:55:41.544: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:55:43.542: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:55:45.544: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:55:47.542: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:55:49.541: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:55:51.542: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:55:53.546: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 06:55:55.544: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 29 06:55:55.549: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Apr 29 06:55:57.580: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Apr 29 06:55:57.580: INFO: Going to poll 172.16.102.174 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Apr 29 06:55:57.581: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.102.174 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8317 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:55:57.581: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:55:57.582: INFO: ExecWithOptions: Clientset creation
Apr 29 06:55:57.582: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8317/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.102.174+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:55:58.657: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 29 06:55:58.657: INFO: Going to poll 172.16.115.155 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Apr 29 06:55:58.660: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.115.155 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8317 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:55:58.660: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:55:58.661: INFO: ExecWithOptions: Clientset creation
Apr 29 06:55:58.661: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8317/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.115.155+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Apr 29 06:55:59.732: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:55:59.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8317" for this suite.

• [SLOW TEST:26.258 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":62,"skipped":1092,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:55:59.744: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr 29 06:55:59.776: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 06:55:59.784: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 06:55:59.788: INFO: 
Logging pods the apiserver thinks is on node localhost.localdomain before test
Apr 29 06:55:59.801: INFO: sample-crd-conversion-webhook-deployment-bb9577b7b-b2dfv from crd-webhook-2934 started at 2022-04-29 06:48:32 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container sample-crd-conversion-webhook ready: true, restart count 0
Apr 29 06:55:59.801: INFO: elasticsearch-master-0 from elasticsearch started at 2022-04-29 02:41:34 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container elasticsearch ready: true, restart count 0
Apr 29 06:55:59.801: INFO: hnc-controller-manager-6d47799b76-dcfk2 from hnc-system started at 2022-04-29 06:48:32 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container manager ready: true, restart count 0
Apr 29 06:55:59.801: INFO: ingress-nginx-admission-create-d6gjd from ingress-nginx started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container create ready: false, restart count 0
Apr 29 06:55:59.801: INFO: ingress-nginx-admission-patch-wlvbg from ingress-nginx started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container patch ready: false, restart count 1
Apr 29 06:55:59.801: INFO: ingress-nginx-controller-776d88d5c9-wsdhf from ingress-nginx started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container controller ready: true, restart count 0
Apr 29 06:55:59.801: INFO: calico-kube-controllers-d84f7b88b-rqbrz from kube-system started at 2022-04-29 02:39:15 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr 29 06:55:59.801: INFO: calico-node-kb4fj from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container calico-node ready: true, restart count 0
Apr 29 06:55:59.801: INFO: coredns-65c54cc984-q685t from kube-system started at 2022-04-29 02:39:15 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container coredns ready: true, restart count 0
Apr 29 06:55:59.801: INFO: coredns-65c54cc984-vcx96 from kube-system started at 2022-04-29 02:39:15 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container coredns ready: true, restart count 0
Apr 29 06:55:59.801: INFO: etcd-localhost.localdomain from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container etcd ready: true, restart count 5
Apr 29 06:55:59.801: INFO: kube-apiserver-localhost.localdomain from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container kube-apiserver ready: true, restart count 5
Apr 29 06:55:59.801: INFO: kube-controller-manager-localhost.localdomain from kube-system started at 2022-04-29 02:39:05 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container kube-controller-manager ready: true, restart count 5
Apr 29 06:55:59.801: INFO: kube-proxy-tmzt7 from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.801: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 06:55:59.802: INFO: kube-scheduler-localhost.localdomain from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container kube-scheduler ready: true, restart count 5
Apr 29 06:55:59.802: INFO: metrics-server-6d69ff96c6-vth6c from kube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container metrics-server ready: true, restart count 0
Apr 29 06:55:59.802: INFO: kubecube-monitoring-kube-state-metrics-6cf5c8c6d4-7knxg from kubecube-monitoring started at 2022-04-29 02:42:04 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 29 06:55:59.802: INFO: kubecube-monitoring-operator-5f84949849-f69dc from kubecube-monitoring started at 2022-04-29 02:42:04 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container kubecube-monitoring ready: true, restart count 0
Apr 29 06:55:59.802: INFO: kubecube-monitoring-prometheus-node-exporter-wmrth from kubecube-monitoring started at 2022-04-29 02:42:04 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container node-exporter ready: true, restart count 0
Apr 29 06:55:59.802: INFO: kubecube-thanos-query-86f669566f-vzpq5 from kubecube-monitoring started at 2022-04-29 05:02:36 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container query ready: true, restart count 0
Apr 29 06:55:59.802: INFO: prometheus-kubecube-monitoring-prometheus-0 from kubecube-monitoring started at 2022-04-29 02:42:11 +0000 UTC (2 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container config-reloader ready: true, restart count 0
Apr 29 06:55:59.802: INFO: 	Container prometheus ready: true, restart count 1
Apr 29 06:55:59.802: INFO: audit-78b7d4cf7d-fs2pm from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container kubecube-audit ready: true, restart count 0
Apr 29 06:55:59.802: INFO: cloud-shell-5c87948759-ksk75 from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container cloud-shell ready: true, restart count 0
Apr 29 06:55:59.802: INFO: frontend-7cbccf84fb-wnxk7 from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container nginx ready: true, restart count 0
Apr 29 06:55:59.802: INFO: kubecube-756775dbb-6wvmx from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container kubecube ready: true, restart count 2
Apr 29 06:55:59.802: INFO: warden-d876d9fd8-rbkr2 from kubecube-system started at 2022-04-29 02:41:18 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container warden ready: true, restart count 0
Apr 29 06:55:59.802: INFO: webconsole-7d7f989996-h84gm from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container kubecube-webconsole ready: true, restart count 0
Apr 29 06:55:59.802: INFO: local-path-provisioner-86b7fd94b5-wdtg4 from local-path-storage started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container local-path-provisioner ready: true, restart count 2
Apr 29 06:55:59.802: INFO: netserver-0 from pod-network-test-8317 started at 2022-04-29 06:55:33 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container webserver ready: true, restart count 0
Apr 29 06:55:59.802: INFO: sonobuoy-systemd-logs-daemon-set-82ef1d0cc98c43bb-5fv6g from sonobuoy started at 2022-04-29 06:40:07 +0000 UTC (2 container statuses recorded)
Apr 29 06:55:59.802: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 06:55:59.802: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 06:55:59.802: INFO: 
Logging pods the apiserver thinks is on node localhost.localdomain151 before test
Apr 29 06:55:59.812: INFO: calico-node-tszc4 from kube-system started at 2022-04-29 02:46:05 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.812: INFO: 	Container calico-node ready: true, restart count 0
Apr 29 06:55:59.812: INFO: kube-proxy-2xqmv from kube-system started at 2022-04-29 02:46:05 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.812: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 06:55:59.812: INFO: kubecube-monitoring-prometheus-node-exporter-lrdt2 from kubecube-monitoring started at 2022-04-29 06:48:58 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.812: INFO: 	Container node-exporter ready: true, restart count 0
Apr 29 06:55:59.812: INFO: kubecube-thanos-receive-0 from kubecube-monitoring started at 2022-04-29 06:49:03 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.812: INFO: 	Container receive ready: true, restart count 0
Apr 29 06:55:59.812: INFO: host-test-container-pod from pod-network-test-8317 started at 2022-04-29 06:55:55 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.812: INFO: 	Container agnhost-container ready: true, restart count 0
Apr 29 06:55:59.812: INFO: netserver-1 from pod-network-test-8317 started at 2022-04-29 06:55:33 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.812: INFO: 	Container webserver ready: true, restart count 0
Apr 29 06:55:59.812: INFO: test-container-pod from pod-network-test-8317 started at 2022-04-29 06:55:55 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.812: INFO: 	Container webserver ready: true, restart count 0
Apr 29 06:55:59.812: INFO: pod-qos-class-211cb7eb-13b7-4604-ac67-183ffdc0e172 from pods-9707 started at 2022-04-29 06:55:20 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.812: INFO: 	Container agnhost ready: false, restart count 0
Apr 29 06:55:59.812: INFO: sonobuoy from sonobuoy started at 2022-04-29 06:40:06 +0000 UTC (1 container statuses recorded)
Apr 29 06:55:59.812: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 06:55:59.812: INFO: sonobuoy-e2e-job-829345ef71224423 from sonobuoy started at 2022-04-29 06:40:07 +0000 UTC (2 container statuses recorded)
Apr 29 06:55:59.812: INFO: 	Container e2e ready: true, restart count 0
Apr 29 06:55:59.812: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 06:55:59.812: INFO: sonobuoy-systemd-logs-daemon-set-82ef1d0cc98c43bb-9qkq7 from sonobuoy started at 2022-04-29 06:40:07 +0000 UTC (2 container statuses recorded)
Apr 29 06:55:59.812: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 06:55:59.812: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16ea4b9f0af7653f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:56:00.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4789" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":63,"skipped":1133,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:56:00.853: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Apr 29 06:56:06.943: INFO: 80 pods remaining
Apr 29 06:56:06.943: INFO: 80 pods has nil DeletionTimestamp
Apr 29 06:56:06.943: INFO: 
Apr 29 06:56:07.942: INFO: 73 pods remaining
Apr 29 06:56:07.942: INFO: 73 pods has nil DeletionTimestamp
Apr 29 06:56:07.942: INFO: 
Apr 29 06:56:08.944: INFO: 59 pods remaining
Apr 29 06:56:08.944: INFO: 59 pods has nil DeletionTimestamp
Apr 29 06:56:08.944: INFO: 
Apr 29 06:56:09.946: INFO: 40 pods remaining
Apr 29 06:56:09.946: INFO: 40 pods has nil DeletionTimestamp
Apr 29 06:56:09.946: INFO: 
Apr 29 06:56:10.949: INFO: 33 pods remaining
Apr 29 06:56:10.949: INFO: 33 pods has nil DeletionTimestamp
Apr 29 06:56:10.949: INFO: 
Apr 29 06:56:11.936: INFO: 19 pods remaining
Apr 29 06:56:11.936: INFO: 19 pods has nil DeletionTimestamp
Apr 29 06:56:11.936: INFO: 
STEP: Gathering metrics
Apr 29 06:56:14.323: INFO: The status of Pod kube-controller-manager-localhost.localdomain is Running (Ready = true)
E0429 06:56:17.985674      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:21.066464      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:23.134421      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:24.159935      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:25.189702      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:26.215429      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:27.241510      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:29.291055      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:32.378233      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:34.440579      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:37.518402      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:39.262731      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:41.314973      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:45.424368      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:46.450525      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:47.476489      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:49.244317      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:53.377462      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:54.400414      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:57.483229      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:58.509847      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:56:59.542818      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:00.244297      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:01.276273      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:03.332814      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:05.386427      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:07.440977      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:12.272879      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:13.302342      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:15.363011      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:16.389333      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:19.469067      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:20.497216      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:22.245385      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:25.325736      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:26.360167      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:27.384037      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:29.441740      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:32.525573      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:36.668023      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:37.707920      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:39.763604      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:40.792281      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:41.828299      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 06:57:42.860315      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
Apr 29 06:57:42.860: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:57:42.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-486" for this suite.

• [SLOW TEST:102.026 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":64,"skipped":1143,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:57:42.879: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-2abe6c2b-77ad-4e7f-8281-6d991bf9c7de
STEP: Creating a pod to test consume secrets
Apr 29 06:57:42.954: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5c7e67d2-aea4-4996-bc8d-4bcb38befa25" in namespace "projected-9174" to be "Succeeded or Failed"
Apr 29 06:57:42.957: INFO: Pod "pod-projected-secrets-5c7e67d2-aea4-4996-bc8d-4bcb38befa25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.461676ms
Apr 29 06:57:44.962: INFO: Pod "pod-projected-secrets-5c7e67d2-aea4-4996-bc8d-4bcb38befa25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007225102s
STEP: Saw pod success
Apr 29 06:57:44.962: INFO: Pod "pod-projected-secrets-5c7e67d2-aea4-4996-bc8d-4bcb38befa25" satisfied condition "Succeeded or Failed"
Apr 29 06:57:44.964: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-secrets-5c7e67d2-aea4-4996-bc8d-4bcb38befa25 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 06:57:44.985: INFO: Waiting for pod pod-projected-secrets-5c7e67d2-aea4-4996-bc8d-4bcb38befa25 to disappear
Apr 29 06:57:44.987: INFO: Pod pod-projected-secrets-5c7e67d2-aea4-4996-bc8d-4bcb38befa25 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:57:44.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9174" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":65,"skipped":1154,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:57:44.999: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Apr 29 06:57:45.037: INFO: created test-pod-1
Apr 29 06:57:47.044: INFO: running and ready test-pod-1
Apr 29 06:57:47.049: INFO: created test-pod-2
Apr 29 06:57:49.057: INFO: running and ready test-pod-2
Apr 29 06:57:49.063: INFO: created test-pod-3
Apr 29 06:57:51.072: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Apr 29 06:57:51.095: INFO: Pod quantity 3 is different from expected quantity 0
Apr 29 06:57:52.100: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:57:53.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8524" for this suite.

• [SLOW TEST:8.109 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":66,"skipped":1157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:57:53.108: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Apr 29 06:57:53.248: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:57:55.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7565" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":67,"skipped":1216,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:57:55.997: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Apr 29 06:57:56.034: INFO: Waiting up to 5m0s for pod "downward-api-06e46b4e-070d-4676-b3a5-4f969c5dfe5a" in namespace "downward-api-205" to be "Succeeded or Failed"
Apr 29 06:57:56.042: INFO: Pod "downward-api-06e46b4e-070d-4676-b3a5-4f969c5dfe5a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.451584ms
Apr 29 06:57:58.046: INFO: Pod "downward-api-06e46b4e-070d-4676-b3a5-4f969c5dfe5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012951972s
STEP: Saw pod success
Apr 29 06:57:58.047: INFO: Pod "downward-api-06e46b4e-070d-4676-b3a5-4f969c5dfe5a" satisfied condition "Succeeded or Failed"
Apr 29 06:57:58.049: INFO: Trying to get logs from node localhost.localdomain151 pod downward-api-06e46b4e-070d-4676-b3a5-4f969c5dfe5a container dapi-container: <nil>
STEP: delete the pod
Apr 29 06:57:58.064: INFO: Waiting for pod downward-api-06e46b4e-070d-4676-b3a5-4f969c5dfe5a to disappear
Apr 29 06:57:58.066: INFO: Pod downward-api-06e46b4e-070d-4676-b3a5-4f969c5dfe5a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:57:58.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-205" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":68,"skipped":1245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:57:58.077: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:57:58.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9442" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":69,"skipped":1285,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:57:58.152: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-c4cce1fb-62ce-4a1c-9f2a-1d5fc5581132
STEP: Creating a pod to test consume configMaps
Apr 29 06:57:58.241: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-382f728b-a906-4845-8216-591043bb3339" in namespace "projected-4136" to be "Succeeded or Failed"
Apr 29 06:57:58.246: INFO: Pod "pod-projected-configmaps-382f728b-a906-4845-8216-591043bb3339": Phase="Pending", Reason="", readiness=false. Elapsed: 5.382448ms
Apr 29 06:58:00.251: INFO: Pod "pod-projected-configmaps-382f728b-a906-4845-8216-591043bb3339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010362877s
STEP: Saw pod success
Apr 29 06:58:00.251: INFO: Pod "pod-projected-configmaps-382f728b-a906-4845-8216-591043bb3339" satisfied condition "Succeeded or Failed"
Apr 29 06:58:00.253: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-configmaps-382f728b-a906-4845-8216-591043bb3339 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 06:58:00.299: INFO: Waiting for pod pod-projected-configmaps-382f728b-a906-4845-8216-591043bb3339 to disappear
Apr 29 06:58:00.302: INFO: Pod pod-projected-configmaps-382f728b-a906-4845-8216-591043bb3339 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:58:00.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4136" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":70,"skipped":1292,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:58:00.329: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Apr 29 06:58:00.426: INFO: The status of Pod annotationupdate97ad5fa2-18c7-4543-90a4-c9fbf67e4fa8 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:58:02.431: INFO: The status of Pod annotationupdate97ad5fa2-18c7-4543-90a4-c9fbf67e4fa8 is Running (Ready = true)
Apr 29 06:58:02.952: INFO: Successfully updated pod "annotationupdate97ad5fa2-18c7-4543-90a4-c9fbf67e4fa8"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:58:04.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1447" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":71,"skipped":1300,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:58:04.974: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:58:22.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1589" for this suite.

• [SLOW TEST:17.212 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":72,"skipped":1301,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:58:22.186: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-60ff0d1b-e36b-4e41-b51c-f52d5d06580a
STEP: Creating a pod to test consume secrets
Apr 29 06:58:22.234: INFO: Waiting up to 5m0s for pod "pod-secrets-dbd8af7f-5c14-4de4-a507-b1cb94f90f92" in namespace "secrets-231" to be "Succeeded or Failed"
Apr 29 06:58:22.236: INFO: Pod "pod-secrets-dbd8af7f-5c14-4de4-a507-b1cb94f90f92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.29564ms
Apr 29 06:58:24.239: INFO: Pod "pod-secrets-dbd8af7f-5c14-4de4-a507-b1cb94f90f92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005176227s
STEP: Saw pod success
Apr 29 06:58:24.239: INFO: Pod "pod-secrets-dbd8af7f-5c14-4de4-a507-b1cb94f90f92" satisfied condition "Succeeded or Failed"
Apr 29 06:58:24.241: INFO: Trying to get logs from node localhost.localdomain151 pod pod-secrets-dbd8af7f-5c14-4de4-a507-b1cb94f90f92 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 06:58:24.259: INFO: Waiting for pod pod-secrets-dbd8af7f-5c14-4de4-a507-b1cb94f90f92 to disappear
Apr 29 06:58:24.261: INFO: Pod pod-secrets-dbd8af7f-5c14-4de4-a507-b1cb94f90f92 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:58:24.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-231" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":73,"skipped":1309,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:58:24.270: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 29 06:58:26.316: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:58:26.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4952" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1320,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:58:26.337: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 29 06:58:26.432: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 06:59:26.488: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Apr 29 06:59:26.509: INFO: Created pod: pod0-0-sched-preemption-low-priority
Apr 29 06:59:26.515: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Apr 29 06:59:26.530: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Apr 29 06:59:26.535: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:59:42.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2103" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:76.270 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":75,"skipped":1337,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:59:42.608: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 06:59:42.745: INFO: The status of Pod server-envvars-9160894b-dde0-45b6-9018-c6cb04b205c5 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:59:44.750: INFO: The status of Pod server-envvars-9160894b-dde0-45b6-9018-c6cb04b205c5 is Running (Ready = true)
Apr 29 06:59:44.770: INFO: Waiting up to 5m0s for pod "client-envvars-70b0f654-baff-4f0c-a305-7b3c4d987590" in namespace "pods-5003" to be "Succeeded or Failed"
Apr 29 06:59:44.774: INFO: Pod "client-envvars-70b0f654-baff-4f0c-a305-7b3c4d987590": Phase="Pending", Reason="", readiness=false. Elapsed: 4.475811ms
Apr 29 06:59:46.779: INFO: Pod "client-envvars-70b0f654-baff-4f0c-a305-7b3c4d987590": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009425612s
STEP: Saw pod success
Apr 29 06:59:46.779: INFO: Pod "client-envvars-70b0f654-baff-4f0c-a305-7b3c4d987590" satisfied condition "Succeeded or Failed"
Apr 29 06:59:46.781: INFO: Trying to get logs from node localhost.localdomain151 pod client-envvars-70b0f654-baff-4f0c-a305-7b3c4d987590 container env3cont: <nil>
STEP: delete the pod
Apr 29 06:59:46.797: INFO: Waiting for pod client-envvars-70b0f654-baff-4f0c-a305-7b3c4d987590 to disappear
Apr 29 06:59:46.799: INFO: Pod client-envvars-70b0f654-baff-4f0c-a305-7b3c4d987590 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 06:59:46.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5003" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":76,"skipped":1466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 06:59:46.807: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Apr 29 06:59:46.851: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:59:48.854: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 123.58.176.151 on the node which pod1 resides and expect scheduled
Apr 29 06:59:48.870: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:59:50.876: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 123.58.176.151 but use UDP protocol on the node which pod2 resides
Apr 29 06:59:50.887: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:59:52.892: INFO: The status of Pod pod3 is Running (Ready = true)
Apr 29 06:59:52.901: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Apr 29 06:59:54.905: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Apr 29 06:59:54.908: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 123.58.176.151 http://127.0.0.1:54323/hostname] Namespace:hostport-2616 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:59:54.908: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:59:54.909: INFO: ExecWithOptions: Clientset creation
Apr 29 06:59:54.909: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2616/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+123.58.176.151+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 123.58.176.151, port: 54323
Apr 29 06:59:54.988: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://123.58.176.151:54323/hostname] Namespace:hostport-2616 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:59:54.988: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:59:54.989: INFO: ExecWithOptions: Clientset creation
Apr 29 06:59:54.989: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2616/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F123.58.176.151%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 123.58.176.151, port: 54323 UDP
Apr 29 06:59:55.062: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 123.58.176.151 54323] Namespace:hostport-2616 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 06:59:55.062: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 06:59:55.062: INFO: ExecWithOptions: Clientset creation
Apr 29 06:59:55.062: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2616/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+123.58.176.151+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:00:00.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-2616" for this suite.

• [SLOW TEST:13.361 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":77,"skipped":1502,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:00:00.168: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6003
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6003
I0429 07:00:00.222646      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6003, replica count: 2
Apr 29 07:00:03.273: INFO: Creating new exec pod
I0429 07:00:03.273854      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 07:00:06.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-6003 exec execpodbdxqz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Apr 29 07:00:06.444: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 29 07:00:06.444: INFO: stdout: "externalname-service-gjl8g"
Apr 29 07:00:06.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-6003 exec execpodbdxqz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.119.233 80'
Apr 29 07:00:06.587: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.119.233 80\nConnection to 10.109.119.233 80 port [tcp/http] succeeded!\n"
Apr 29 07:00:06.587: INFO: stdout: ""
Apr 29 07:00:07.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-6003 exec execpodbdxqz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.119.233 80'
Apr 29 07:00:07.796: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.119.233 80\nConnection to 10.109.119.233 80 port [tcp/http] succeeded!\n"
Apr 29 07:00:07.796: INFO: stdout: "externalname-service-gjl8g"
Apr 29 07:00:07.796: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:00:07.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6003" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.762 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":78,"skipped":1502,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:00:07.931: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1571
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Apr 29 07:00:07.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-2982 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Apr 29 07:00:08.054: INFO: stderr: ""
Apr 29 07:00:08.054: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Apr 29 07:00:13.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-2982 get pod e2e-test-httpd-pod -o json'
Apr 29 07:00:13.188: INFO: stderr: ""
Apr 29 07:00:13.188: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.16.115.150/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.16.115.150/32\"\n        },\n        \"creationTimestamp\": \"2022-04-29T07:00:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2982\",\n        \"resourceVersion\": \"97043\",\n        \"uid\": \"ba26c9eb-a66b-42ce-8583-f3f148fa4720\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-wncpt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"localhost.localdomain151\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-wncpt\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-04-29T07:00:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-04-29T07:00:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-04-29T07:00:09Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-04-29T07:00:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ba3ac263517cb1e8798105df1ee6fbc8f534cd0b6bf865fbb63fdb17da6fabe0\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-04-29T07:00:08Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"123.58.176.151\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.115.150\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.115.150\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-04-29T07:00:08Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 29 07:00:13.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-2982 replace -f -'
Apr 29 07:00:14.463: INFO: stderr: ""
Apr 29 07:00:14.463: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1575
Apr 29 07:00:14.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-2982 delete pods e2e-test-httpd-pod'
Apr 29 07:00:16.330: INFO: stderr: ""
Apr 29 07:00:16.330: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:00:16.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2982" for this suite.

• [SLOW TEST:8.408 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":79,"skipped":1529,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:00:16.338: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Apr 29 07:00:16.383: INFO: Waiting up to 5m0s for pod "downward-api-a8e2ccc4-c54e-4235-92e3-b05dfeae7898" in namespace "downward-api-4825" to be "Succeeded or Failed"
Apr 29 07:00:16.389: INFO: Pod "downward-api-a8e2ccc4-c54e-4235-92e3-b05dfeae7898": Phase="Pending", Reason="", readiness=false. Elapsed: 5.365024ms
Apr 29 07:00:18.393: INFO: Pod "downward-api-a8e2ccc4-c54e-4235-92e3-b05dfeae7898": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010083987s
STEP: Saw pod success
Apr 29 07:00:18.393: INFO: Pod "downward-api-a8e2ccc4-c54e-4235-92e3-b05dfeae7898" satisfied condition "Succeeded or Failed"
Apr 29 07:00:18.397: INFO: Trying to get logs from node localhost.localdomain151 pod downward-api-a8e2ccc4-c54e-4235-92e3-b05dfeae7898 container dapi-container: <nil>
STEP: delete the pod
Apr 29 07:00:18.416: INFO: Waiting for pod downward-api-a8e2ccc4-c54e-4235-92e3-b05dfeae7898 to disappear
Apr 29 07:00:18.419: INFO: Pod downward-api-a8e2ccc4-c54e-4235-92e3-b05dfeae7898 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:00:18.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4825" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":80,"skipped":1536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:00:18.430: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 07:00:18.469: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fdc93022-1ffc-4b64-ac31-41316f881c04" in namespace "downward-api-6951" to be "Succeeded or Failed"
Apr 29 07:00:18.472: INFO: Pod "downwardapi-volume-fdc93022-1ffc-4b64-ac31-41316f881c04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.124848ms
Apr 29 07:00:20.476: INFO: Pod "downwardapi-volume-fdc93022-1ffc-4b64-ac31-41316f881c04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006818163s
STEP: Saw pod success
Apr 29 07:00:20.476: INFO: Pod "downwardapi-volume-fdc93022-1ffc-4b64-ac31-41316f881c04" satisfied condition "Succeeded or Failed"
Apr 29 07:00:20.478: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-fdc93022-1ffc-4b64-ac31-41316f881c04 container client-container: <nil>
STEP: delete the pod
Apr 29 07:00:20.494: INFO: Waiting for pod downwardapi-volume-fdc93022-1ffc-4b64-ac31-41316f881c04 to disappear
Apr 29 07:00:20.496: INFO: Pod downwardapi-volume-fdc93022-1ffc-4b64-ac31-41316f881c04 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:00:20.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6951" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":81,"skipped":1626,"failed":0}
SSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:00:20.505: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:00:20.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8987" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":82,"skipped":1633,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:00:20.691: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:00:20.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7965" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":83,"skipped":1642,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:00:20.737: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:00:20.770: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 29 07:00:25.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-4054 --namespace=crd-publish-openapi-4054 create -f -'
Apr 29 07:00:26.233: INFO: stderr: ""
Apr 29 07:00:26.233: INFO: stdout: "e2e-test-crd-publish-openapi-6826-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 29 07:00:26.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-4054 --namespace=crd-publish-openapi-4054 delete e2e-test-crd-publish-openapi-6826-crds test-cr'
Apr 29 07:00:26.309: INFO: stderr: ""
Apr 29 07:00:26.309: INFO: stdout: "e2e-test-crd-publish-openapi-6826-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr 29 07:00:26.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-4054 --namespace=crd-publish-openapi-4054 apply -f -'
Apr 29 07:00:26.518: INFO: stderr: ""
Apr 29 07:00:26.518: INFO: stdout: "e2e-test-crd-publish-openapi-6826-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 29 07:00:26.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-4054 --namespace=crd-publish-openapi-4054 delete e2e-test-crd-publish-openapi-6826-crds test-cr'
Apr 29 07:00:26.598: INFO: stderr: ""
Apr 29 07:00:26.598: INFO: stdout: "e2e-test-crd-publish-openapi-6826-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 29 07:00:26.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-4054 explain e2e-test-crd-publish-openapi-6826-crds'
Apr 29 07:00:26.852: INFO: stderr: ""
Apr 29 07:00:26.852: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6826-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:00:31.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4054" for this suite.

• [SLOW TEST:10.397 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":84,"skipped":1660,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:00:31.134: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 29 07:00:31.773: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 07:00:34.790: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:00:34.794: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:00:37.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4468" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.908 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":85,"skipped":1664,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:00:38.042: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Apr 29 07:00:38.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 create -f -'
Apr 29 07:00:39.411: INFO: stderr: ""
Apr 29 07:00:39.411: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 07:00:39.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 07:00:39.483: INFO: stderr: ""
Apr 29 07:00:39.483: INFO: stdout: "update-demo-nautilus-2ddl8 update-demo-nautilus-72cjz "
Apr 29 07:00:39.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods update-demo-nautilus-2ddl8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 07:00:39.550: INFO: stderr: ""
Apr 29 07:00:39.550: INFO: stdout: ""
Apr 29 07:00:39.550: INFO: update-demo-nautilus-2ddl8 is created but not running
Apr 29 07:00:44.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 07:00:44.623: INFO: stderr: ""
Apr 29 07:00:44.623: INFO: stdout: "update-demo-nautilus-2ddl8 update-demo-nautilus-72cjz "
Apr 29 07:00:44.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods update-demo-nautilus-2ddl8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 07:00:44.686: INFO: stderr: ""
Apr 29 07:00:44.686: INFO: stdout: "true"
Apr 29 07:00:44.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods update-demo-nautilus-2ddl8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 07:00:44.749: INFO: stderr: ""
Apr 29 07:00:44.749: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Apr 29 07:00:44.749: INFO: validating pod update-demo-nautilus-2ddl8
Apr 29 07:00:44.754: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 07:00:44.754: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 07:00:44.754: INFO: update-demo-nautilus-2ddl8 is verified up and running
Apr 29 07:00:44.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods update-demo-nautilus-72cjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 07:00:44.818: INFO: stderr: ""
Apr 29 07:00:44.818: INFO: stdout: "true"
Apr 29 07:00:44.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods update-demo-nautilus-72cjz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 07:00:44.890: INFO: stderr: ""
Apr 29 07:00:44.891: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Apr 29 07:00:44.891: INFO: validating pod update-demo-nautilus-72cjz
Apr 29 07:00:44.894: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 07:00:44.894: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 07:00:44.894: INFO: update-demo-nautilus-72cjz is verified up and running
STEP: scaling down the replication controller
Apr 29 07:00:44.896: INFO: scanned /root for discovery docs: <nil>
Apr 29 07:00:44.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Apr 29 07:00:45.991: INFO: stderr: ""
Apr 29 07:00:45.991: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 07:00:45.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 07:00:46.074: INFO: stderr: ""
Apr 29 07:00:46.074: INFO: stdout: "update-demo-nautilus-2ddl8 update-demo-nautilus-72cjz "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 29 07:00:51.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 07:00:51.156: INFO: stderr: ""
Apr 29 07:00:51.156: INFO: stdout: "update-demo-nautilus-72cjz "
Apr 29 07:00:51.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods update-demo-nautilus-72cjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 07:00:51.223: INFO: stderr: ""
Apr 29 07:00:51.223: INFO: stdout: "true"
Apr 29 07:00:51.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods update-demo-nautilus-72cjz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 07:00:51.291: INFO: stderr: ""
Apr 29 07:00:51.291: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Apr 29 07:00:51.291: INFO: validating pod update-demo-nautilus-72cjz
Apr 29 07:00:51.294: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 07:00:51.294: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 07:00:51.294: INFO: update-demo-nautilus-72cjz is verified up and running
STEP: scaling up the replication controller
Apr 29 07:00:51.296: INFO: scanned /root for discovery docs: <nil>
Apr 29 07:00:51.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Apr 29 07:00:52.382: INFO: stderr: ""
Apr 29 07:00:52.382: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 07:00:52.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 07:00:52.460: INFO: stderr: ""
Apr 29 07:00:52.460: INFO: stdout: "update-demo-nautilus-72cjz update-demo-nautilus-xxfjb "
Apr 29 07:00:52.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods update-demo-nautilus-72cjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 07:00:52.537: INFO: stderr: ""
Apr 29 07:00:52.537: INFO: stdout: "true"
Apr 29 07:00:52.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods update-demo-nautilus-72cjz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 07:00:52.602: INFO: stderr: ""
Apr 29 07:00:52.603: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Apr 29 07:00:52.603: INFO: validating pod update-demo-nautilus-72cjz
Apr 29 07:00:52.606: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 07:00:52.606: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 07:00:52.606: INFO: update-demo-nautilus-72cjz is verified up and running
Apr 29 07:00:52.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods update-demo-nautilus-xxfjb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 07:00:52.682: INFO: stderr: ""
Apr 29 07:00:52.682: INFO: stdout: "true"
Apr 29 07:00:52.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods update-demo-nautilus-xxfjb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 07:00:52.747: INFO: stderr: ""
Apr 29 07:00:52.747: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Apr 29 07:00:52.747: INFO: validating pod update-demo-nautilus-xxfjb
Apr 29 07:00:52.752: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 07:00:52.752: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 07:00:52.752: INFO: update-demo-nautilus-xxfjb is verified up and running
STEP: using delete to clean up resources
Apr 29 07:00:52.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 delete --grace-period=0 --force -f -'
Apr 29 07:00:52.818: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 07:00:52.818: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 29 07:00:52.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get rc,svc -l name=update-demo --no-headers'
Apr 29 07:00:52.892: INFO: stderr: "No resources found in kubectl-1305 namespace.\n"
Apr 29 07:00:52.892: INFO: stdout: ""
Apr 29 07:00:52.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-1305 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 07:00:52.966: INFO: stderr: ""
Apr 29 07:00:52.966: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:00:52.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1305" for this suite.

• [SLOW TEST:14.935 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":86,"skipped":1664,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:00:52.977: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 07:00:53.606: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 07:00:56.640: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:01:06.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8786" for this suite.
STEP: Destroying namespace "webhook-8786-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.056 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":87,"skipped":1673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:01:07.034: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:01:07.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1995" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":88,"skipped":1738,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:01:07.247: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Apr 29 07:01:07.325: INFO: The status of Pod pod-update-activedeadlineseconds-b0bfc2de-d618-40ff-a8b0-0ccefd138235 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:01:09.329: INFO: The status of Pod pod-update-activedeadlineseconds-b0bfc2de-d618-40ff-a8b0-0ccefd138235 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 29 07:01:09.850: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b0bfc2de-d618-40ff-a8b0-0ccefd138235"
Apr 29 07:01:09.850: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b0bfc2de-d618-40ff-a8b0-0ccefd138235" in namespace "pods-5855" to be "terminated due to deadline exceeded"
Apr 29 07:01:09.854: INFO: Pod "pod-update-activedeadlineseconds-b0bfc2de-d618-40ff-a8b0-0ccefd138235": Phase="Running", Reason="", readiness=true. Elapsed: 3.85494ms
Apr 29 07:01:11.859: INFO: Pod "pod-update-activedeadlineseconds-b0bfc2de-d618-40ff-a8b0-0ccefd138235": Phase="Running", Reason="", readiness=true. Elapsed: 2.008397204s
Apr 29 07:01:13.864: INFO: Pod "pod-update-activedeadlineseconds-b0bfc2de-d618-40ff-a8b0-0ccefd138235": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 4.013970478s
Apr 29 07:01:13.864: INFO: Pod "pod-update-activedeadlineseconds-b0bfc2de-d618-40ff-a8b0-0ccefd138235" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:01:13.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5855" for this suite.

• [SLOW TEST:6.627 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":89,"skipped":1757,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:01:13.875: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:01:13.905: INFO: Creating ReplicaSet my-hostname-basic-9bc343ef-d20a-485a-9b60-379f58d8274a
Apr 29 07:01:13.912: INFO: Pod name my-hostname-basic-9bc343ef-d20a-485a-9b60-379f58d8274a: Found 0 pods out of 1
Apr 29 07:01:18.924: INFO: Pod name my-hostname-basic-9bc343ef-d20a-485a-9b60-379f58d8274a: Found 1 pods out of 1
Apr 29 07:01:18.924: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9bc343ef-d20a-485a-9b60-379f58d8274a" is running
Apr 29 07:01:18.926: INFO: Pod "my-hostname-basic-9bc343ef-d20a-485a-9b60-379f58d8274a-hhz5l" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-29 07:01:13 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-29 07:01:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-29 07:01:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-29 07:01:13 +0000 UTC Reason: Message:}])
Apr 29 07:01:18.926: INFO: Trying to dial the pod
Apr 29 07:01:23.938: INFO: Controller my-hostname-basic-9bc343ef-d20a-485a-9b60-379f58d8274a: Got expected result from replica 1 [my-hostname-basic-9bc343ef-d20a-485a-9b60-379f58d8274a-hhz5l]: "my-hostname-basic-9bc343ef-d20a-485a-9b60-379f58d8274a-hhz5l", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:01:23.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1285" for this suite.

• [SLOW TEST:10.074 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":90,"skipped":1772,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:01:23.949: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Apr 29 07:01:26.028: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:01:28.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1838" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":91,"skipped":1790,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:01:28.046: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:01:28.084: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 29 07:01:33.088: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 29 07:01:33.088: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr 29 07:01:33.105: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9033  4d13caa2-d3e7-47a5-a0e8-7816151d9f2c 98016 1 2022-04-29 07:01:33 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-04-29 07:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006514748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Apr 29 07:01:33.107: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Apr 29 07:01:33.107: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Apr 29 07:01:33.108: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9033  31fb3ae9-1af7-4d84-b636-7852061f80e6 98019 1 2022-04-29 07:01:28 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4d13caa2-d3e7-47a5-a0e8-7816151d9f2c 0xc006514e67 0xc006514e68}] []  [{e2e.test Update apps/v1 2022-04-29 07:01:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 07:01:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-04-29 07:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4d13caa2-d3e7-47a5-a0e8-7816151d9f2c\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006514f68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 29 07:01:33.110: INFO: Pod "test-cleanup-controller-hdzsw" is available:
&Pod{ObjectMeta:{test-cleanup-controller-hdzsw test-cleanup-controller- deployment-9033  87c6b919-da2b-4d09-adfa-2321ff265c7a 97992 0 2022-04-29 07:01:28 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.16.115.130/32 cni.projectcalico.org/podIPs:172.16.115.130/32] [{apps/v1 ReplicaSet test-cleanup-controller 31fb3ae9-1af7-4d84-b636-7852061f80e6 0xc0065400b7 0xc0065400b8}] []  [{calico Update v1 2022-04-29 07:01:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-04-29 07:01:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31fb3ae9-1af7-4d84-b636-7852061f80e6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-04-29 07:01:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.115.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zb5x5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zb5x5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:01:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:01:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:01:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:01:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:172.16.115.130,StartTime:2022-04-29 07:01:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 07:01:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://fb9d1cfda9557fb6394d4bfc63864656b24d47eded7369e33dd96dffc9ca21dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.115.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:01:33.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9033" for this suite.

• [SLOW TEST:5.076 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":92,"skipped":1804,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:01:33.123: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 29 07:01:33.191: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 07:02:33.236: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:02:33.238: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:02:33.280: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Apr 29 07:02:33.288: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:02:33.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9958" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:02:33.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1225" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.246 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":93,"skipped":1831,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:02:33.370: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:02:33.409: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Apr 29 07:02:34.541: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:02:35.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1101" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":94,"skipped":1847,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:02:35.559: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 07:02:36.507: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 07:02:39.550: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:02:39.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8305" for this suite.
STEP: Destroying namespace "webhook-8305-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":95,"skipped":1879,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:02:39.788: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr 29 07:02:39.856: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 07:02:39.867: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 07:02:39.877: INFO: 
Logging pods the apiserver thinks is on node localhost.localdomain before test
Apr 29 07:02:39.890: INFO: sample-crd-conversion-webhook-deployment-bb9577b7b-b2dfv from crd-webhook-2934 started at 2022-04-29 06:48:32 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container sample-crd-conversion-webhook ready: true, restart count 0
Apr 29 07:02:39.890: INFO: elasticsearch-master-0 from elasticsearch started at 2022-04-29 02:41:34 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container elasticsearch ready: true, restart count 0
Apr 29 07:02:39.890: INFO: hnc-controller-manager-6d47799b76-dcfk2 from hnc-system started at 2022-04-29 06:48:32 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container manager ready: true, restart count 0
Apr 29 07:02:39.890: INFO: ingress-nginx-admission-create-d6gjd from ingress-nginx started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container create ready: false, restart count 0
Apr 29 07:02:39.890: INFO: ingress-nginx-admission-patch-wlvbg from ingress-nginx started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container patch ready: false, restart count 1
Apr 29 07:02:39.890: INFO: ingress-nginx-controller-776d88d5c9-wsdhf from ingress-nginx started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container controller ready: true, restart count 0
Apr 29 07:02:39.890: INFO: calico-kube-controllers-d84f7b88b-rqbrz from kube-system started at 2022-04-29 02:39:15 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr 29 07:02:39.890: INFO: calico-node-kb4fj from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container calico-node ready: true, restart count 0
Apr 29 07:02:39.890: INFO: coredns-65c54cc984-q685t from kube-system started at 2022-04-29 02:39:15 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container coredns ready: true, restart count 0
Apr 29 07:02:39.890: INFO: coredns-65c54cc984-vcx96 from kube-system started at 2022-04-29 02:39:15 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container coredns ready: true, restart count 0
Apr 29 07:02:39.890: INFO: etcd-localhost.localdomain from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container etcd ready: true, restart count 5
Apr 29 07:02:39.890: INFO: kube-apiserver-localhost.localdomain from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container kube-apiserver ready: true, restart count 5
Apr 29 07:02:39.890: INFO: kube-controller-manager-localhost.localdomain from kube-system started at 2022-04-29 02:39:05 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container kube-controller-manager ready: true, restart count 5
Apr 29 07:02:39.890: INFO: kube-proxy-tmzt7 from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 07:02:39.890: INFO: kube-scheduler-localhost.localdomain from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container kube-scheduler ready: true, restart count 5
Apr 29 07:02:39.890: INFO: metrics-server-6d69ff96c6-vth6c from kube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container metrics-server ready: true, restart count 0
Apr 29 07:02:39.890: INFO: kubecube-monitoring-kube-state-metrics-6cf5c8c6d4-7knxg from kubecube-monitoring started at 2022-04-29 02:42:04 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 29 07:02:39.890: INFO: kubecube-monitoring-operator-5f84949849-f69dc from kubecube-monitoring started at 2022-04-29 02:42:04 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container kubecube-monitoring ready: true, restart count 0
Apr 29 07:02:39.890: INFO: kubecube-monitoring-prometheus-node-exporter-wmrth from kubecube-monitoring started at 2022-04-29 02:42:04 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container node-exporter ready: true, restart count 0
Apr 29 07:02:39.890: INFO: kubecube-thanos-query-86f669566f-vzpq5 from kubecube-monitoring started at 2022-04-29 05:02:36 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container query ready: true, restart count 0
Apr 29 07:02:39.890: INFO: prometheus-kubecube-monitoring-prometheus-0 from kubecube-monitoring started at 2022-04-29 02:42:11 +0000 UTC (2 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container config-reloader ready: true, restart count 0
Apr 29 07:02:39.890: INFO: 	Container prometheus ready: true, restart count 1
Apr 29 07:02:39.890: INFO: audit-78b7d4cf7d-fs2pm from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container kubecube-audit ready: true, restart count 0
Apr 29 07:02:39.890: INFO: cloud-shell-5c87948759-ksk75 from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container cloud-shell ready: true, restart count 0
Apr 29 07:02:39.890: INFO: frontend-7cbccf84fb-wnxk7 from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container nginx ready: true, restart count 0
Apr 29 07:02:39.890: INFO: kubecube-756775dbb-6wvmx from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container kubecube ready: true, restart count 2
Apr 29 07:02:39.890: INFO: warden-d876d9fd8-rbkr2 from kubecube-system started at 2022-04-29 02:41:18 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container warden ready: true, restart count 0
Apr 29 07:02:39.890: INFO: webconsole-7d7f989996-h84gm from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container kubecube-webconsole ready: true, restart count 0
Apr 29 07:02:39.890: INFO: local-path-provisioner-86b7fd94b5-wdtg4 from local-path-storage started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container local-path-provisioner ready: true, restart count 2
Apr 29 07:02:39.890: INFO: condition-test-fxzgt from replication-controller-1101 started at 2022-04-29 07:02:33 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container httpd ready: true, restart count 0
Apr 29 07:02:39.890: INFO: sonobuoy-systemd-logs-daemon-set-82ef1d0cc98c43bb-5fv6g from sonobuoy started at 2022-04-29 06:40:07 +0000 UTC (2 container statuses recorded)
Apr 29 07:02:39.890: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 07:02:39.890: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 07:02:39.890: INFO: 
Logging pods the apiserver thinks is on node localhost.localdomain151 before test
Apr 29 07:02:39.900: INFO: calico-node-tszc4 from kube-system started at 2022-04-29 02:46:05 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.900: INFO: 	Container calico-node ready: true, restart count 0
Apr 29 07:02:39.900: INFO: kube-proxy-2xqmv from kube-system started at 2022-04-29 02:46:05 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.900: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 07:02:39.900: INFO: kubecube-monitoring-prometheus-node-exporter-lrdt2 from kubecube-monitoring started at 2022-04-29 06:48:58 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.901: INFO: 	Container node-exporter ready: true, restart count 0
Apr 29 07:02:39.901: INFO: kubecube-thanos-receive-0 from kubecube-monitoring started at 2022-04-29 06:49:03 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.901: INFO: 	Container receive ready: true, restart count 0
Apr 29 07:02:39.901: INFO: condition-test-pbtcq from replication-controller-1101 started at 2022-04-29 07:02:33 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.901: INFO: 	Container httpd ready: true, restart count 0
Apr 29 07:02:39.901: INFO: sonobuoy from sonobuoy started at 2022-04-29 06:40:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:02:39.901: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 07:02:39.901: INFO: sonobuoy-e2e-job-829345ef71224423 from sonobuoy started at 2022-04-29 06:40:07 +0000 UTC (2 container statuses recorded)
Apr 29 07:02:39.901: INFO: 	Container e2e ready: true, restart count 0
Apr 29 07:02:39.901: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 07:02:39.901: INFO: sonobuoy-systemd-logs-daemon-set-82ef1d0cc98c43bb-9qkq7 from sonobuoy started at 2022-04-29 06:40:07 +0000 UTC (2 container statuses recorded)
Apr 29 07:02:39.901: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 07:02:39.901: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node localhost.localdomain
STEP: verifying the node has the label node localhost.localdomain151
Apr 29 07:02:39.991: INFO: Pod sample-crd-conversion-webhook-deployment-bb9577b7b-b2dfv requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod elasticsearch-master-0 requesting resource cpu=1000m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod hnc-controller-manager-6d47799b76-dcfk2 requesting resource cpu=100m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod ingress-nginx-controller-776d88d5c9-wsdhf requesting resource cpu=100m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod calico-kube-controllers-d84f7b88b-rqbrz requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod calico-node-kb4fj requesting resource cpu=250m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod calico-node-tszc4 requesting resource cpu=250m on Node localhost.localdomain151
Apr 29 07:02:39.991: INFO: Pod coredns-65c54cc984-q685t requesting resource cpu=100m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod coredns-65c54cc984-vcx96 requesting resource cpu=100m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod etcd-localhost.localdomain requesting resource cpu=100m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod kube-apiserver-localhost.localdomain requesting resource cpu=250m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod kube-controller-manager-localhost.localdomain requesting resource cpu=200m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod kube-proxy-2xqmv requesting resource cpu=0m on Node localhost.localdomain151
Apr 29 07:02:39.991: INFO: Pod kube-proxy-tmzt7 requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod kube-scheduler-localhost.localdomain requesting resource cpu=100m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod metrics-server-6d69ff96c6-vth6c requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod kubecube-monitoring-kube-state-metrics-6cf5c8c6d4-7knxg requesting resource cpu=100m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod kubecube-monitoring-operator-5f84949849-f69dc requesting resource cpu=100m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod kubecube-monitoring-prometheus-node-exporter-lrdt2 requesting resource cpu=100m on Node localhost.localdomain151
Apr 29 07:02:39.991: INFO: Pod kubecube-monitoring-prometheus-node-exporter-wmrth requesting resource cpu=100m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod kubecube-thanos-query-86f669566f-vzpq5 requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod kubecube-thanos-receive-0 requesting resource cpu=0m on Node localhost.localdomain151
Apr 29 07:02:39.991: INFO: Pod prometheus-kubecube-monitoring-prometheus-0 requesting resource cpu=400m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod audit-78b7d4cf7d-fs2pm requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod cloud-shell-5c87948759-ksk75 requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod frontend-7cbccf84fb-wnxk7 requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod kubecube-756775dbb-6wvmx requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod warden-d876d9fd8-rbkr2 requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod webconsole-7d7f989996-h84gm requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod local-path-provisioner-86b7fd94b5-wdtg4 requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod condition-test-fxzgt requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod condition-test-pbtcq requesting resource cpu=0m on Node localhost.localdomain151
Apr 29 07:02:39.991: INFO: Pod sonobuoy requesting resource cpu=0m on Node localhost.localdomain151
Apr 29 07:02:39.991: INFO: Pod sonobuoy-e2e-job-829345ef71224423 requesting resource cpu=0m on Node localhost.localdomain151
Apr 29 07:02:39.991: INFO: Pod sonobuoy-systemd-logs-daemon-set-82ef1d0cc98c43bb-5fv6g requesting resource cpu=0m on Node localhost.localdomain
Apr 29 07:02:39.991: INFO: Pod sonobuoy-systemd-logs-daemon-set-82ef1d0cc98c43bb-9qkq7 requesting resource cpu=0m on Node localhost.localdomain151
STEP: Starting Pods to consume most of the cluster CPU.
Apr 29 07:02:39.991: INFO: Creating a pod which consumes cpu=3500m on Node localhost.localdomain
Apr 29 07:02:39.998: INFO: Creating a pod which consumes cpu=5355m on Node localhost.localdomain151
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b58217bc-f97e-4d8a-92c8-191ea3256fc0.16ea4bfc3681a519], Reason = [Scheduled], Message = [Successfully assigned sched-pred-557/filler-pod-b58217bc-f97e-4d8a-92c8-191ea3256fc0 to localhost.localdomain]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b58217bc-f97e-4d8a-92c8-191ea3256fc0.16ea4bfc654ab49e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b58217bc-f97e-4d8a-92c8-191ea3256fc0.16ea4bfc68a672c5], Reason = [Created], Message = [Created container filler-pod-b58217bc-f97e-4d8a-92c8-191ea3256fc0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b58217bc-f97e-4d8a-92c8-191ea3256fc0.16ea4bfc72c254d1], Reason = [Started], Message = [Started container filler-pod-b58217bc-f97e-4d8a-92c8-191ea3256fc0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f558d1bc-637b-4330-8562-8afadfa73f54.16ea4bfc37057277], Reason = [Scheduled], Message = [Successfully assigned sched-pred-557/filler-pod-f558d1bc-637b-4330-8562-8afadfa73f54 to localhost.localdomain151]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f558d1bc-637b-4330-8562-8afadfa73f54.16ea4bfc62a1b52b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f558d1bc-637b-4330-8562-8afadfa73f54.16ea4bfc6462439f], Reason = [Created], Message = [Created container filler-pod-f558d1bc-637b-4330-8562-8afadfa73f54]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f558d1bc-637b-4330-8562-8afadfa73f54.16ea4bfc6ab573e5], Reason = [Started], Message = [Started container filler-pod-f558d1bc-637b-4330-8562-8afadfa73f54]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16ea4bfcafa7505c], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node localhost.localdomain
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node localhost.localdomain151
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:02:43.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-557" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":96,"skipped":1892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:02:43.091: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-3aa1e456-6b7c-4724-89e7-ecfaf4032d76
STEP: Creating a pod to test consume configMaps
Apr 29 07:02:43.149: INFO: Waiting up to 5m0s for pod "pod-configmaps-520d0cc5-f378-4017-99a0-c4699d8ba875" in namespace "configmap-888" to be "Succeeded or Failed"
Apr 29 07:02:43.152: INFO: Pod "pod-configmaps-520d0cc5-f378-4017-99a0-c4699d8ba875": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568552ms
Apr 29 07:02:45.157: INFO: Pod "pod-configmaps-520d0cc5-f378-4017-99a0-c4699d8ba875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008319686s
STEP: Saw pod success
Apr 29 07:02:45.157: INFO: Pod "pod-configmaps-520d0cc5-f378-4017-99a0-c4699d8ba875" satisfied condition "Succeeded or Failed"
Apr 29 07:02:45.160: INFO: Trying to get logs from node localhost.localdomain151 pod pod-configmaps-520d0cc5-f378-4017-99a0-c4699d8ba875 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 07:02:45.197: INFO: Waiting for pod pod-configmaps-520d0cc5-f378-4017-99a0-c4699d8ba875 to disappear
Apr 29 07:02:45.199: INFO: Pod pod-configmaps-520d0cc5-f378-4017-99a0-c4699d8ba875 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:02:45.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-888" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":97,"skipped":1963,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:02:45.208: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Apr 29 07:02:45.253: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Apr 29 07:02:45.267: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:02:45.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2556" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":98,"skipped":1996,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:02:45.299: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Apr 29 07:02:45.335: INFO: created test-event-1
Apr 29 07:02:45.344: INFO: created test-event-2
Apr 29 07:02:45.348: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Apr 29 07:02:45.350: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Apr 29 07:02:45.526: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:02:45.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1669" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":99,"skipped":2005,"failed":0}
SSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:02:45.537: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:02:45.724: INFO: Waiting up to 5m0s for pod "busybox-user-65534-1eae902a-b23a-465f-931d-46a5b7302934" in namespace "security-context-test-8177" to be "Succeeded or Failed"
Apr 29 07:02:45.726: INFO: Pod "busybox-user-65534-1eae902a-b23a-465f-931d-46a5b7302934": Phase="Pending", Reason="", readiness=false. Elapsed: 2.652912ms
Apr 29 07:02:47.730: INFO: Pod "busybox-user-65534-1eae902a-b23a-465f-931d-46a5b7302934": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00658611s
Apr 29 07:02:47.730: INFO: Pod "busybox-user-65534-1eae902a-b23a-465f-931d-46a5b7302934" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:02:47.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8177" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":100,"skipped":2008,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:02:47.739: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Apr 29 07:02:48.293: INFO: created pod pod-service-account-defaultsa
Apr 29 07:02:48.293: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 29 07:02:48.300: INFO: created pod pod-service-account-mountsa
Apr 29 07:02:48.300: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 29 07:02:48.304: INFO: created pod pod-service-account-nomountsa
Apr 29 07:02:48.304: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 29 07:02:48.316: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 29 07:02:48.316: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 29 07:02:48.333: INFO: created pod pod-service-account-mountsa-mountspec
Apr 29 07:02:48.333: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 29 07:02:48.339: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 29 07:02:48.339: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 29 07:02:48.345: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 29 07:02:48.345: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 29 07:02:48.361: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 29 07:02:48.361: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 29 07:02:48.375: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 29 07:02:48.375: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:02:48.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5816" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":101,"skipped":2028,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:02:48.394: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 29 07:02:48.453: INFO: Waiting up to 5m0s for pod "pod-f7862cb8-9181-4ee1-b591-6b284fee0837" in namespace "emptydir-6941" to be "Succeeded or Failed"
Apr 29 07:02:48.458: INFO: Pod "pod-f7862cb8-9181-4ee1-b591-6b284fee0837": Phase="Pending", Reason="", readiness=false. Elapsed: 4.896413ms
Apr 29 07:02:50.462: INFO: Pod "pod-f7862cb8-9181-4ee1-b591-6b284fee0837": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009189429s
Apr 29 07:02:52.466: INFO: Pod "pod-f7862cb8-9181-4ee1-b591-6b284fee0837": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013152281s
Apr 29 07:02:54.470: INFO: Pod "pod-f7862cb8-9181-4ee1-b591-6b284fee0837": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017110609s
Apr 29 07:02:56.475: INFO: Pod "pod-f7862cb8-9181-4ee1-b591-6b284fee0837": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021820774s
Apr 29 07:02:58.479: INFO: Pod "pod-f7862cb8-9181-4ee1-b591-6b284fee0837": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.025404342s
STEP: Saw pod success
Apr 29 07:02:58.479: INFO: Pod "pod-f7862cb8-9181-4ee1-b591-6b284fee0837" satisfied condition "Succeeded or Failed"
Apr 29 07:02:58.481: INFO: Trying to get logs from node localhost.localdomain151 pod pod-f7862cb8-9181-4ee1-b591-6b284fee0837 container test-container: <nil>
STEP: delete the pod
Apr 29 07:02:58.503: INFO: Waiting for pod pod-f7862cb8-9181-4ee1-b591-6b284fee0837 to disappear
Apr 29 07:02:58.505: INFO: Pod pod-f7862cb8-9181-4ee1-b591-6b284fee0837 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:02:58.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6941" for this suite.

• [SLOW TEST:10.121 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":102,"skipped":2048,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:02:58.515: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:03:26.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4855" for this suite.

• [SLOW TEST:28.094 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":103,"skipped":2050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:03:26.609: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Apr 29 07:03:26.638: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:03:29.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6219" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":104,"skipped":2082,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:03:29.851: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Apr 29 07:03:29.890: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:03:31.895: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Apr 29 07:03:31.904: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:03:33.908: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 29 07:03:33.923: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 07:03:33.926: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 07:03:35.927: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 07:03:35.931: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:03:35.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8037" for this suite.

• [SLOW TEST:6.089 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":105,"skipped":2100,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:03:35.941: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:03:36.031: INFO: Creating deployment "webserver-deployment"
Apr 29 07:03:36.036: INFO: Waiting for observed generation 1
Apr 29 07:03:38.045: INFO: Waiting for all required pods to come up
Apr 29 07:03:38.052: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 29 07:03:44.061: INFO: Waiting for deployment "webserver-deployment" to complete
Apr 29 07:03:44.067: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr 29 07:03:44.076: INFO: Updating deployment webserver-deployment
Apr 29 07:03:44.076: INFO: Waiting for observed generation 2
Apr 29 07:03:46.081: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 29 07:03:46.084: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 29 07:03:46.086: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 29 07:03:46.092: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 29 07:03:46.092: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 29 07:03:46.094: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 29 07:03:46.102: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr 29 07:03:46.102: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr 29 07:03:46.110: INFO: Updating deployment webserver-deployment
Apr 29 07:03:46.110: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr 29 07:03:46.126: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 29 07:03:46.130: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr 29 07:03:48.144: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2669  8d2b0d62-73fe-4193-8b98-ba2b54a131fc 99630 3 2022-04-29 07:03:36 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034a0628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-04-29 07:03:46 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-04-29 07:03:46 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Apr 29 07:03:48.147: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-2669  a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 99629 3 2022-04-29 07:03:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8d2b0d62-73fe-4193-8b98-ba2b54a131fc 0xc0037790c7 0xc0037790c8}] []  [{kube-controller-manager Update apps/v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d2b0d62-73fe-4193-8b98-ba2b54a131fc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003779168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 07:03:48.148: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr 29 07:03:48.148: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-2669  1878a199-be08-45ad-9dfb-bb8c5e70c6d6 99594 3 2022-04-29 07:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8d2b0d62-73fe-4193-8b98-ba2b54a131fc 0xc0037791c7 0xc0037791c8}] []  [{kube-controller-manager Update apps/v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d2b0d62-73fe-4193-8b98-ba2b54a131fc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 07:03:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003779258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Apr 29 07:03:48.155: INFO: Pod "webserver-deployment-566f96c878-4drcd" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-4drcd webserver-deployment-566f96c878- deployment-2669  8f0c686e-ccf4-4aa5-99de-8f806f77dfe4 99513 0 2022-04-29 07:03:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:172.16.115.167/32 cni.projectcalico.org/podIPs:172.16.115.167/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a09e7 0xc0034a09e8}] []  [{Go-http-client Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rlrhr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rlrhr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:,StartTime:2022-04-29 07:03:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.155: INFO: Pod "webserver-deployment-566f96c878-4w4fx" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-4w4fx webserver-deployment-566f96c878- deployment-2669  d0ee9306-74e6-4aef-89f1-20ed7e298f98 99686 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:172.16.102.141/32 cni.projectcalico.org/podIPs:172.16.102.141/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a0be7 0xc0034a0be8}] []  [{Go-http-client Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fb847,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fb847,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.150,PodIP:,StartTime:2022-04-29 07:03:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.155: INFO: Pod "webserver-deployment-566f96c878-86fgn" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-86fgn webserver-deployment-566f96c878- deployment-2669  8a46edae-c1ae-4eee-8f83-7efcbb417efb 99565 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a0df7 0xc0034a0df8}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qw77v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qw77v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.155: INFO: Pod "webserver-deployment-566f96c878-8twcg" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-8twcg webserver-deployment-566f96c878- deployment-2669  fc53c12f-7277-4066-9b94-45b095ae8ea5 99517 0 2022-04-29 07:03:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:172.16.115.157/32 cni.projectcalico.org/podIPs:172.16.115.157/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a0f60 0xc0034a0f61}] []  [{Go-http-client Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9qqbp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9qqbp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:,StartTime:2022-04-29 07:03:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.155: INFO: Pod "webserver-deployment-566f96c878-9nqc2" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-9nqc2 webserver-deployment-566f96c878- deployment-2669  6e1979ad-4cd9-447b-aefc-0032341e4966 99650 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:172.16.102.167/32 cni.projectcalico.org/podIPs:172.16.102.167/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a1157 0xc0034a1158}] []  [{Go-http-client Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vvfrz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vvfrz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.150,PodIP:,StartTime:2022-04-29 07:03:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.156: INFO: Pod "webserver-deployment-566f96c878-bkjkl" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-bkjkl webserver-deployment-566f96c878- deployment-2669  660b2232-81cc-409f-ae43-db02067cfdd1 99529 0 2022-04-29 07:03:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:172.16.102.170/32 cni.projectcalico.org/podIPs:172.16.102.170/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a1357 0xc0034a1358}] []  [{Go-http-client Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sv7ks,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sv7ks,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.150,PodIP:,StartTime:2022-04-29 07:03:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.156: INFO: Pod "webserver-deployment-566f96c878-chpjd" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-chpjd webserver-deployment-566f96c878- deployment-2669  4f25c80a-ec0d-4a37-a87f-8b93ecedc0b2 99534 0 2022-04-29 07:03:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:172.16.115.133/32 cni.projectcalico.org/podIPs:172.16.115.133/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a1557 0xc0034a1558}] []  [{Go-http-client Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l52gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l52gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:,StartTime:2022-04-29 07:03:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.156: INFO: Pod "webserver-deployment-566f96c878-fckhv" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-fckhv webserver-deployment-566f96c878- deployment-2669  cee65c84-a972-448f-92be-ce5f80d45685 99589 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a1757 0xc0034a1758}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4r4lz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4r4lz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.156: INFO: Pod "webserver-deployment-566f96c878-jkfww" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-jkfww webserver-deployment-566f96c878- deployment-2669  0aac30ea-060b-4329-b3b9-f70e27960cb9 99709 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:172.16.102.157/32 cni.projectcalico.org/podIPs:172.16.102.157/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a18c0 0xc0034a18c1}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ts5md,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ts5md,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.156: INFO: Pod "webserver-deployment-566f96c878-nchm6" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-nchm6 webserver-deployment-566f96c878- deployment-2669  5abc2dc5-a6a9-4dc2-887a-95d84072f5d5 99507 0 2022-04-29 07:03:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:172.16.102.168/32 cni.projectcalico.org/podIPs:172.16.102.168/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a1a50 0xc0034a1a51}] []  [{Go-http-client Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k9hjw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9hjw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.150,PodIP:,StartTime:2022-04-29 07:03:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.156: INFO: Pod "webserver-deployment-566f96c878-ntqqf" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-ntqqf webserver-deployment-566f96c878- deployment-2669  51ca0150-78b9-49a6-97c1-0c19528a5e4e 99681 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:172.16.115.146/32 cni.projectcalico.org/podIPs:172.16.115.146/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a1c57 0xc0034a1c58}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k964m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k964m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.157: INFO: Pod "webserver-deployment-566f96c878-x9lgw" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-x9lgw webserver-deployment-566f96c878- deployment-2669  27ad95a0-14aa-41ec-a22c-fb6ae641dc96 99656 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:172.16.115.141/32 cni.projectcalico.org/podIPs:172.16.115.141/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a1df0 0xc0034a1df1}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mmzwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mmzwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.157: INFO: Pod "webserver-deployment-566f96c878-xb6t4" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-xb6t4 webserver-deployment-566f96c878- deployment-2669  d8e30152-e18f-4bb9-a045-383e4cb67172 99612 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 a32ef8a3-5c8b-49b7-b1bd-41583360f3a7 0xc0034a1f80 0xc0034a1f81}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32ef8a3-5c8b-49b7-b1bd-41583360f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vbsc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vbsc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.157: INFO: Pod "webserver-deployment-5d9fdcc779-2d9jj" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-2d9jj webserver-deployment-5d9fdcc779- deployment-2669  943b37d5-af39-461f-bf56-d4d66b64b93e 99672 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.102.189/32 cni.projectcalico.org/podIPs:172.16.102.189/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd6110 0xc001cd6111}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45nrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45nrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.157: INFO: Pod "webserver-deployment-5d9fdcc779-4984f" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-4984f webserver-deployment-5d9fdcc779- deployment-2669  a06c6870-fb7d-4165-ad43-942db6fc6db4 99698 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.115.170/32 cni.projectcalico.org/podIPs:172.16.115.170/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd6280 0xc001cd6281}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2q4sk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2q4sk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.157: INFO: Pod "webserver-deployment-5d9fdcc779-4bx6z" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-4bx6z webserver-deployment-5d9fdcc779- deployment-2669  247bef6d-e1d1-4138-bdd1-17d5f3998a08 99703 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.102.154/32 cni.projectcalico.org/podIPs:172.16.102.154/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd63f0 0xc001cd63f1}] []  [{Go-http-client Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rjsvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rjsvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.150,PodIP:,StartTime:2022-04-29 07:03:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.157: INFO: Pod "webserver-deployment-5d9fdcc779-5mz9c" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-5mz9c webserver-deployment-5d9fdcc779- deployment-2669  4c88c3d2-4e69-4625-9707-78f2c177138e 99588 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd65c7 0xc001cd65c8}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hgxt5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hgxt5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.158: INFO: Pod "webserver-deployment-5d9fdcc779-74prt" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-74prt webserver-deployment-5d9fdcc779- deployment-2669  89beb29e-4ae7-47c5-a8a2-df6ed7b94d4a 99431 0 2022-04-29 07:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.115.179/32 cni.projectcalico.org/podIPs:172.16.115.179/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd6720 0xc001cd6721}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-04-29 07:03:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.115.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pmkpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pmkpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:172.16.115.179,StartTime:2022-04-29 07:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 07:03:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d47f5c7b9b0c24495b4b8c537b3838bf861a9e15c4a64de498c9321deac6a960,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.115.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.158: INFO: Pod "webserver-deployment-5d9fdcc779-7wwpn" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-7wwpn webserver-deployment-5d9fdcc779- deployment-2669  f5c64722-ff99-42ba-aec5-fab9e9128d30 99670 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.115.166/32 cni.projectcalico.org/podIPs:172.16.115.166/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd6927 0xc001cd6928}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lbfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lbfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.158: INFO: Pod "webserver-deployment-5d9fdcc779-bf5bq" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-bf5bq webserver-deployment-5d9fdcc779- deployment-2669  eff6a7f7-d7b1-4394-ba89-e1eecd00730a 99692 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.115.163/32 cni.projectcalico.org/podIPs:172.16.115.163/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd6aa0 0xc001cd6aa1}] []  [{Go-http-client Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9s8z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9s8z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:,StartTime:2022-04-29 07:03:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.158: INFO: Pod "webserver-deployment-5d9fdcc779-ctst6" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-ctst6 webserver-deployment-5d9fdcc779- deployment-2669  5fe8b4a8-0b38-4afb-bd51-fe6534a38b29 99335 0 2022-04-29 07:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.102.173/32 cni.projectcalico.org/podIPs:172.16.102.173/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd6c87 0xc001cd6c88}] []  [{calico Update v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-04-29 07:03:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.102.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jx7ng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jx7ng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.150,PodIP:172.16.102.173,StartTime:2022-04-29 07:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 07:03:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7098dcf8a22743b846f271103f565def8226ecc01f3351a874f4a94c488a9a99,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.102.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.158: INFO: Pod "webserver-deployment-5d9fdcc779-fnpwq" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-fnpwq webserver-deployment-5d9fdcc779- deployment-2669  2f36d979-7ae0-4c08-8c4c-f3a35e1a9de2 99596 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd6e87 0xc001cd6e88}] []  [{Go-http-client Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5j5n5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5j5n5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.150,PodIP:,StartTime:2022-04-29 07:03:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.158: INFO: Pod "webserver-deployment-5d9fdcc779-g29fk" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-g29fk webserver-deployment-5d9fdcc779- deployment-2669  dcffd3b9-ed23-4ac8-a61d-703ef1e86ea5 99601 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd7047 0xc001cd7048}] []  [{Go-http-client Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfrzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfrzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.150,PodIP:,StartTime:2022-04-29 07:03:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.159: INFO: Pod "webserver-deployment-5d9fdcc779-gwg65" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-gwg65 webserver-deployment-5d9fdcc779- deployment-2669  8894b18e-6112-4b2b-8ee5-2e6ae6bc3020 99380 0 2022-04-29 07:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.102.182/32 cni.projectcalico.org/podIPs:172.16.102.182/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd7207 0xc001cd7208}] []  [{calico Update v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-04-29 07:03:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.102.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z4k85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z4k85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.150,PodIP:172.16.102.182,StartTime:2022-04-29 07:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 07:03:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9154cb02807f42bd896326fad206eea8c12198f55b6a083414c45a264a78e9b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.102.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.159: INFO: Pod "webserver-deployment-5d9fdcc779-jtmcc" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-jtmcc webserver-deployment-5d9fdcc779- deployment-2669  b3e1233d-19f4-499e-ad4a-c2e0df6d4ff5 99665 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.115.169/32 cni.projectcalico.org/podIPs:172.16.115.169/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd7417 0xc001cd7418}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9pm6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9pm6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.159: INFO: Pod "webserver-deployment-5d9fdcc779-ll2zc" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-ll2zc webserver-deployment-5d9fdcc779- deployment-2669  57e10309-b433-4dcd-a18f-d0eb68af8dab 99438 0 2022-04-29 07:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.115.158/32 cni.projectcalico.org/podIPs:172.16.115.158/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc001cd76b0 0xc001cd76b1}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-04-29 07:03:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.115.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ttbm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ttbm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:172.16.115.158,StartTime:2022-04-29 07:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 07:03:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://93b993315e1bd3e902653551b96af67a730d752d1f8ed102d794e0a1271465f3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.115.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.159: INFO: Pod "webserver-deployment-5d9fdcc779-mnfvp" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-mnfvp webserver-deployment-5d9fdcc779- deployment-2669  291d911e-3a73-4757-b9a7-5511cfa9b6e2 99591 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc000d540f7 0xc000d540f8}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mr6km,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mr6km,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.159: INFO: Pod "webserver-deployment-5d9fdcc779-p5btw" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-p5btw webserver-deployment-5d9fdcc779- deployment-2669  59e46a92-c250-4980-87be-7b4e2d629dc3 99404 0 2022-04-29 07:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.115.182/32 cni.projectcalico.org/podIPs:172.16.115.182/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc000d54250 0xc000d54251}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-04-29 07:03:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.115.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcpvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcpvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:172.16.115.182,StartTime:2022-04-29 07:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 07:03:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0231a5954df501410dc1f388a2d5cb9b26befdefc6b2e214418b8a2ad229fb34,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.115.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.160: INFO: Pod "webserver-deployment-5d9fdcc779-q4cxc" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-q4cxc webserver-deployment-5d9fdcc779- deployment-2669  03e1f752-77b6-4080-b04c-09facac65338 99655 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.102.181/32 cni.projectcalico.org/podIPs:172.16.102.181/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc000d54477 0xc000d54478}] []  [{Go-http-client Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wmjd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wmjd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.150,PodIP:,StartTime:2022-04-29 07:03:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.160: INFO: Pod "webserver-deployment-5d9fdcc779-qtfbx" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-qtfbx webserver-deployment-5d9fdcc779- deployment-2669  bf67f3b3-886c-4fcd-a44f-4059be61e5f7 99376 0 2022-04-29 07:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.102.183/32 cni.projectcalico.org/podIPs:172.16.102.183/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc000d54657 0xc000d54658}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-04-29 07:03:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.102.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6877m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6877m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.150,PodIP:172.16.102.183,StartTime:2022-04-29 07:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 07:03:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a9e5b03dca4a137d3c4f92dd381291925b7587a67bb406c067d244fc44fd8d43,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.102.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.160: INFO: Pod "webserver-deployment-5d9fdcc779-r275n" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-r275n webserver-deployment-5d9fdcc779- deployment-2669  a7088f6e-ffc2-4792-984d-65cb8af2ab9d 99570 0 2022-04-29 07:03:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc000d54857 0xc000d54858}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drzvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drzvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.160: INFO: Pod "webserver-deployment-5d9fdcc779-vkpjr" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-vkpjr webserver-deployment-5d9fdcc779- deployment-2669  8cfc58e1-7cb3-4cc7-a6ca-dcbdf0ccac51 99351 0 2022-04-29 07:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.102.144/32 cni.projectcalico.org/podIPs:172.16.102.144/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc000d549b0 0xc000d549b1}] []  [{calico Update v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-04-29 07:03:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.102.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qngq7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qngq7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.150,PodIP:172.16.102.144,StartTime:2022-04-29 07:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 07:03:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://782eb7bcf0af3215498aba1253d0a2a44a61858f175634dd196490a223ba7458,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.102.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:03:48.160: INFO: Pod "webserver-deployment-5d9fdcc779-vl4qw" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-vl4qw webserver-deployment-5d9fdcc779- deployment-2669  6b4a0294-c854-4d85-9d72-07d1d37557dc 99411 0 2022-04-29 07:03:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.115.148/32 cni.projectcalico.org/podIPs:172.16.115.148/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 1878a199-be08-45ad-9dfb-bb8c5e70c6d6 0xc000d54ba7 0xc000d54ba8}] []  [{kube-controller-manager Update v1 2022-04-29 07:03:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1878a199-be08-45ad-9dfb-bb8c5e70c6d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:03:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-04-29 07:03:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.115.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c6tcr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c6tcr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:03:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:172.16.115.148,StartTime:2022-04-29 07:03:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 07:03:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0595039b36a29dd253eb8d82fba181f5ac26bfcf95e19e538506679be15d6c1a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.115.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:03:48.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2669" for this suite.

• [SLOW TEST:12.229 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":106,"skipped":2115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:03:48.170: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-a69ccdeb-7909-40b7-a693-e7910ff524ea
STEP: Creating a pod to test consume configMaps
Apr 29 07:03:48.222: INFO: Waiting up to 5m0s for pod "pod-configmaps-681348e5-dba5-4a14-b822-7ea6a8223c31" in namespace "configmap-6892" to be "Succeeded or Failed"
Apr 29 07:03:48.228: INFO: Pod "pod-configmaps-681348e5-dba5-4a14-b822-7ea6a8223c31": Phase="Pending", Reason="", readiness=false. Elapsed: 5.577565ms
Apr 29 07:03:50.233: INFO: Pod "pod-configmaps-681348e5-dba5-4a14-b822-7ea6a8223c31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010905366s
Apr 29 07:03:52.239: INFO: Pod "pod-configmaps-681348e5-dba5-4a14-b822-7ea6a8223c31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016256814s
Apr 29 07:03:54.246: INFO: Pod "pod-configmaps-681348e5-dba5-4a14-b822-7ea6a8223c31": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024197392s
Apr 29 07:03:56.250: INFO: Pod "pod-configmaps-681348e5-dba5-4a14-b822-7ea6a8223c31": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027665392s
Apr 29 07:03:58.254: INFO: Pod "pod-configmaps-681348e5-dba5-4a14-b822-7ea6a8223c31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.031727502s
STEP: Saw pod success
Apr 29 07:03:58.254: INFO: Pod "pod-configmaps-681348e5-dba5-4a14-b822-7ea6a8223c31" satisfied condition "Succeeded or Failed"
Apr 29 07:03:58.257: INFO: Trying to get logs from node localhost.localdomain151 pod pod-configmaps-681348e5-dba5-4a14-b822-7ea6a8223c31 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 07:03:58.274: INFO: Waiting for pod pod-configmaps-681348e5-dba5-4a14-b822-7ea6a8223c31 to disappear
Apr 29 07:03:58.277: INFO: Pod pod-configmaps-681348e5-dba5-4a14-b822-7ea6a8223c31 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:03:58.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6892" for this suite.

• [SLOW TEST:10.117 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":107,"skipped":2140,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:03:58.287: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-e9f82a93-212f-447f-8380-d65182d1f68c
STEP: Creating a pod to test consume secrets
Apr 29 07:03:58.331: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-75fb3d56-c108-46d3-9858-fe8cc622579c" in namespace "projected-8704" to be "Succeeded or Failed"
Apr 29 07:03:58.333: INFO: Pod "pod-projected-secrets-75fb3d56-c108-46d3-9858-fe8cc622579c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126953ms
Apr 29 07:04:00.340: INFO: Pod "pod-projected-secrets-75fb3d56-c108-46d3-9858-fe8cc622579c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00893256s
Apr 29 07:04:02.344: INFO: Pod "pod-projected-secrets-75fb3d56-c108-46d3-9858-fe8cc622579c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013142721s
Apr 29 07:04:04.348: INFO: Pod "pod-projected-secrets-75fb3d56-c108-46d3-9858-fe8cc622579c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016969583s
Apr 29 07:04:06.352: INFO: Pod "pod-projected-secrets-75fb3d56-c108-46d3-9858-fe8cc622579c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021333109s
STEP: Saw pod success
Apr 29 07:04:06.353: INFO: Pod "pod-projected-secrets-75fb3d56-c108-46d3-9858-fe8cc622579c" satisfied condition "Succeeded or Failed"
Apr 29 07:04:06.355: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-secrets-75fb3d56-c108-46d3-9858-fe8cc622579c container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 07:04:06.376: INFO: Waiting for pod pod-projected-secrets-75fb3d56-c108-46d3-9858-fe8cc622579c to disappear
Apr 29 07:04:06.379: INFO: Pod pod-projected-secrets-75fb3d56-c108-46d3-9858-fe8cc622579c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:04:06.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8704" for this suite.

• [SLOW TEST:8.100 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":108,"skipped":2142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:04:06.388: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1331
STEP: creating the pod
Apr 29 07:04:06.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-744 create -f -'
Apr 29 07:04:07.722: INFO: stderr: ""
Apr 29 07:04:07.722: INFO: stdout: "pod/pause created\n"
Apr 29 07:04:07.722: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 29 07:04:07.722: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-744" to be "running and ready"
Apr 29 07:04:07.725: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.948402ms
Apr 29 07:04:09.730: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007958352s
Apr 29 07:04:09.730: INFO: Pod "pause" satisfied condition "running and ready"
Apr 29 07:04:09.730: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 29 07:04:09.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-744 label pods pause testing-label=testing-label-value'
Apr 29 07:04:09.813: INFO: stderr: ""
Apr 29 07:04:09.813: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 29 07:04:09.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-744 get pod pause -L testing-label'
Apr 29 07:04:09.889: INFO: stderr: ""
Apr 29 07:04:09.889: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 29 07:04:09.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-744 label pods pause testing-label-'
Apr 29 07:04:09.969: INFO: stderr: ""
Apr 29 07:04:09.969: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 29 07:04:09.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-744 get pod pause -L testing-label'
Apr 29 07:04:10.040: INFO: stderr: ""
Apr 29 07:04:10.040: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1337
STEP: using delete to clean up resources
Apr 29 07:04:10.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-744 delete --grace-period=0 --force -f -'
Apr 29 07:04:10.114: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 07:04:10.114: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 29 07:04:10.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-744 get rc,svc -l name=pause --no-headers'
Apr 29 07:04:10.187: INFO: stderr: "No resources found in kubectl-744 namespace.\n"
Apr 29 07:04:10.187: INFO: stdout: ""
Apr 29 07:04:10.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-744 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 07:04:10.251: INFO: stderr: ""
Apr 29 07:04:10.251: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:04:10.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-744" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":109,"skipped":2170,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:04:10.260: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Apr 29 07:04:10.290: INFO: namespace kubectl-6356
Apr 29 07:04:10.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6356 create -f -'
Apr 29 07:04:10.483: INFO: stderr: ""
Apr 29 07:04:10.483: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 29 07:04:11.487: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 07:04:11.487: INFO: Found 0 / 1
Apr 29 07:04:12.487: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 07:04:12.487: INFO: Found 1 / 1
Apr 29 07:04:12.487: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 29 07:04:12.490: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 07:04:12.490: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 29 07:04:12.490: INFO: wait on agnhost-primary startup in kubectl-6356 
Apr 29 07:04:12.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6356 logs agnhost-primary-2mxlg agnhost-primary'
Apr 29 07:04:12.569: INFO: stderr: ""
Apr 29 07:04:12.569: INFO: stdout: "Paused\n"
STEP: exposing RC
Apr 29 07:04:12.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6356 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Apr 29 07:04:12.663: INFO: stderr: ""
Apr 29 07:04:12.663: INFO: stdout: "service/rm2 exposed\n"
Apr 29 07:04:12.665: INFO: Service rm2 in namespace kubectl-6356 found.
STEP: exposing service
Apr 29 07:04:14.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6356 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Apr 29 07:04:14.780: INFO: stderr: ""
Apr 29 07:04:14.780: INFO: stdout: "service/rm3 exposed\n"
Apr 29 07:04:14.782: INFO: Service rm3 in namespace kubectl-6356 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:04:16.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6356" for this suite.

• [SLOW TEST:6.543 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":110,"skipped":2171,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:04:16.803: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:04:18.844: INFO: Deleting pod "var-expansion-57a48e72-688f-4002-b00f-7aefdf15768b" in namespace "var-expansion-7837"
Apr 29 07:04:18.853: INFO: Wait up to 5m0s for pod "var-expansion-57a48e72-688f-4002-b00f-7aefdf15768b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:04:22.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7837" for this suite.

• [SLOW TEST:6.068 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":111,"skipped":2175,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:04:22.871: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Apr 29 07:04:22.929: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 07:04:22.929: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 07:04:22.934: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 07:04:22.934: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 07:04:22.948: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 07:04:22.948: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 07:04:22.974: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 07:04:22.975: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 07:04:24.036: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 29 07:04:24.037: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 29 07:04:24.469: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Apr 29 07:04:24.478: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 0
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:24.481: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:24.488: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:24.488: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:24.505: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:24.505: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:24.514: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1
Apr 29 07:04:24.514: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1
Apr 29 07:04:24.523: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1
Apr 29 07:04:24.523: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1
Apr 29 07:04:26.047: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:26.047: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:26.059: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1
STEP: listing Deployments
Apr 29 07:04:26.063: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Apr 29 07:04:26.073: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Apr 29 07:04:26.079: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 07:04:26.081: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 07:04:26.117: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 07:04:26.126: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 07:04:27.071: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 07:04:27.086: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 07:04:27.092: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 07:04:27.102: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 07:04:28.494: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Apr 29 07:04:28.530: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1
Apr 29 07:04:28.530: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1
Apr 29 07:04:28.530: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1
Apr 29 07:04:28.531: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 1
Apr 29 07:04:28.531: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:28.531: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:28.531: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:28.531: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 2
Apr 29 07:04:28.531: INFO: observed Deployment test-deployment in namespace deployment-3361 with ReadyReplicas 3
STEP: deleting the Deployment
Apr 29 07:04:28.539: INFO: observed event type MODIFIED
Apr 29 07:04:28.540: INFO: observed event type MODIFIED
Apr 29 07:04:28.540: INFO: observed event type MODIFIED
Apr 29 07:04:28.540: INFO: observed event type MODIFIED
Apr 29 07:04:28.540: INFO: observed event type MODIFIED
Apr 29 07:04:28.540: INFO: observed event type MODIFIED
Apr 29 07:04:28.540: INFO: observed event type MODIFIED
Apr 29 07:04:28.540: INFO: observed event type MODIFIED
Apr 29 07:04:28.540: INFO: observed event type MODIFIED
Apr 29 07:04:28.540: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr 29 07:04:28.543: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:04:28.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3361" for this suite.

• [SLOW TEST:5.688 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":112,"skipped":2196,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:04:28.559: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Apr 29 07:04:28.628: INFO: The status of Pod labelsupdate16d97191-e30b-4cf1-9654-b45b74cf9a8c is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:04:30.633: INFO: The status of Pod labelsupdate16d97191-e30b-4cf1-9654-b45b74cf9a8c is Running (Ready = true)
Apr 29 07:04:31.150: INFO: Successfully updated pod "labelsupdate16d97191-e30b-4cf1-9654-b45b74cf9a8c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:04:35.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1918" for this suite.

• [SLOW TEST:6.620 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":2200,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:04:35.179: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 29 07:04:37.226: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:04:37.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5194" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":114,"skipped":2209,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:04:37.256: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-3262247a-a194-4174-9990-8e903524abfd in namespace container-probe-1022
Apr 29 07:04:39.347: INFO: Started pod test-webserver-3262247a-a194-4174-9990-8e903524abfd in namespace container-probe-1022
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 07:04:39.350: INFO: Initial restart count of pod test-webserver-3262247a-a194-4174-9990-8e903524abfd is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:08:39.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1022" for this suite.

• [SLOW TEST:242.730 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":115,"skipped":2215,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:08:39.987: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Apr 29 07:08:40.136: INFO: Waiting up to 5m0s for pod "security-context-1aac46cb-bc3e-4c6b-bebb-74c0cde92855" in namespace "security-context-7992" to be "Succeeded or Failed"
Apr 29 07:08:40.155: INFO: Pod "security-context-1aac46cb-bc3e-4c6b-bebb-74c0cde92855": Phase="Pending", Reason="", readiness=false. Elapsed: 18.771966ms
Apr 29 07:08:42.160: INFO: Pod "security-context-1aac46cb-bc3e-4c6b-bebb-74c0cde92855": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023726145s
STEP: Saw pod success
Apr 29 07:08:42.160: INFO: Pod "security-context-1aac46cb-bc3e-4c6b-bebb-74c0cde92855" satisfied condition "Succeeded or Failed"
Apr 29 07:08:42.162: INFO: Trying to get logs from node localhost.localdomain151 pod security-context-1aac46cb-bc3e-4c6b-bebb-74c0cde92855 container test-container: <nil>
STEP: delete the pod
Apr 29 07:08:42.189: INFO: Waiting for pod security-context-1aac46cb-bc3e-4c6b-bebb-74c0cde92855 to disappear
Apr 29 07:08:42.192: INFO: Pod security-context-1aac46cb-bc3e-4c6b-bebb-74c0cde92855 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:08:42.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7992" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":116,"skipped":2251,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:08:42.200: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Apr 29 07:08:42.240: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 07:08:46.683: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:09:00.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2584" for this suite.

• [SLOW TEST:18.344 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":117,"skipped":2269,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:09:00.545: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:09:00.635: INFO: The status of Pod busybox-scheduling-180ed898-d25b-4aab-a576-5787c3b9301a is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:09:02.637: INFO: The status of Pod busybox-scheduling-180ed898-d25b-4aab-a576-5787c3b9301a is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:09:02.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6682" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":118,"skipped":2281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:09:02.654: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Apr 29 07:09:02.696: INFO: Major version: 1
STEP: Confirm minor version
Apr 29 07:09:02.696: INFO: cleanMinorVersion: 23
Apr 29 07:09:02.696: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:09:02.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2097" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":119,"skipped":2305,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:09:02.709: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Apr 29 07:09:04.767: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8465 PodName:var-expansion-046e6861-c202-4e00-b5ce-801f14c2a0a5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 07:09:04.767: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 07:09:04.767: INFO: ExecWithOptions: Clientset creation
Apr 29 07:09:04.767: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8465/pods/var-expansion-046e6861-c202-4e00-b5ce-801f14c2a0a5/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
Apr 29 07:09:04.899: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8465 PodName:var-expansion-046e6861-c202-4e00-b5ce-801f14c2a0a5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 07:09:04.899: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 07:09:04.900: INFO: ExecWithOptions: Clientset creation
Apr 29 07:09:04.900: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8465/pods/var-expansion-046e6861-c202-4e00-b5ce-801f14c2a0a5/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
Apr 29 07:09:05.481: INFO: Successfully updated pod "var-expansion-046e6861-c202-4e00-b5ce-801f14c2a0a5"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Apr 29 07:09:05.484: INFO: Deleting pod "var-expansion-046e6861-c202-4e00-b5ce-801f14c2a0a5" in namespace "var-expansion-8465"
Apr 29 07:09:05.488: INFO: Wait up to 5m0s for pod "var-expansion-046e6861-c202-4e00-b5ce-801f14c2a0a5" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:09:39.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8465" for this suite.

• [SLOW TEST:36.796 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":120,"skipped":2322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:09:39.506: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Apr 29 07:09:39.545: INFO: Waiting up to 5m0s for pod "client-containers-e4133dae-9661-406b-bcc4-39e94bdb547e" in namespace "containers-6175" to be "Succeeded or Failed"
Apr 29 07:09:39.547: INFO: Pod "client-containers-e4133dae-9661-406b-bcc4-39e94bdb547e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.14035ms
Apr 29 07:09:41.552: INFO: Pod "client-containers-e4133dae-9661-406b-bcc4-39e94bdb547e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007529708s
STEP: Saw pod success
Apr 29 07:09:41.552: INFO: Pod "client-containers-e4133dae-9661-406b-bcc4-39e94bdb547e" satisfied condition "Succeeded or Failed"
Apr 29 07:09:41.555: INFO: Trying to get logs from node localhost.localdomain151 pod client-containers-e4133dae-9661-406b-bcc4-39e94bdb547e container agnhost-container: <nil>
STEP: delete the pod
Apr 29 07:09:41.580: INFO: Waiting for pod client-containers-e4133dae-9661-406b-bcc4-39e94bdb547e to disappear
Apr 29 07:09:41.587: INFO: Pod client-containers-e4133dae-9661-406b-bcc4-39e94bdb547e no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:09:41.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6175" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":121,"skipped":2366,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:09:41.601: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Apr 29 07:09:41.645: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-4028 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:09:41.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4028" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":122,"skipped":2411,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:09:41.709: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-dgd5
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 07:09:41.851: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dgd5" in namespace "subpath-3556" to be "Succeeded or Failed"
Apr 29 07:09:41.856: INFO: Pod "pod-subpath-test-configmap-dgd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076346ms
Apr 29 07:09:43.860: INFO: Pod "pod-subpath-test-configmap-dgd5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008118115s
Apr 29 07:09:45.866: INFO: Pod "pod-subpath-test-configmap-dgd5": Phase="Running", Reason="", readiness=true. Elapsed: 4.014244053s
Apr 29 07:09:47.871: INFO: Pod "pod-subpath-test-configmap-dgd5": Phase="Running", Reason="", readiness=true. Elapsed: 6.01995531s
Apr 29 07:09:49.879: INFO: Pod "pod-subpath-test-configmap-dgd5": Phase="Running", Reason="", readiness=true. Elapsed: 8.027962758s
Apr 29 07:09:51.884: INFO: Pod "pod-subpath-test-configmap-dgd5": Phase="Running", Reason="", readiness=true. Elapsed: 10.032754198s
Apr 29 07:09:53.888: INFO: Pod "pod-subpath-test-configmap-dgd5": Phase="Running", Reason="", readiness=true. Elapsed: 12.036956448s
Apr 29 07:09:55.893: INFO: Pod "pod-subpath-test-configmap-dgd5": Phase="Running", Reason="", readiness=true. Elapsed: 14.041975315s
Apr 29 07:09:57.898: INFO: Pod "pod-subpath-test-configmap-dgd5": Phase="Running", Reason="", readiness=true. Elapsed: 16.046389129s
Apr 29 07:09:59.902: INFO: Pod "pod-subpath-test-configmap-dgd5": Phase="Running", Reason="", readiness=true. Elapsed: 18.050058212s
Apr 29 07:10:01.906: INFO: Pod "pod-subpath-test-configmap-dgd5": Phase="Running", Reason="", readiness=true. Elapsed: 20.05478973s
Apr 29 07:10:03.910: INFO: Pod "pod-subpath-test-configmap-dgd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.05852839s
STEP: Saw pod success
Apr 29 07:10:03.910: INFO: Pod "pod-subpath-test-configmap-dgd5" satisfied condition "Succeeded or Failed"
Apr 29 07:10:03.914: INFO: Trying to get logs from node localhost.localdomain151 pod pod-subpath-test-configmap-dgd5 container test-container-subpath-configmap-dgd5: <nil>
STEP: delete the pod
Apr 29 07:10:03.935: INFO: Waiting for pod pod-subpath-test-configmap-dgd5 to disappear
Apr 29 07:10:03.937: INFO: Pod pod-subpath-test-configmap-dgd5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-dgd5
Apr 29 07:10:03.937: INFO: Deleting pod "pod-subpath-test-configmap-dgd5" in namespace "subpath-3556"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:10:03.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3556" for this suite.

• [SLOW TEST:22.243 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":123,"skipped":2424,"failed":0}
SSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:10:03.952: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-2358
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2358
STEP: Deleting pre-stop pod
Apr 29 07:10:13.078: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:10:13.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2358" for this suite.

• [SLOW TEST:9.333 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":124,"skipped":2430,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:10:13.286: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 07:10:13.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f98b272d-80cf-43f3-957d-70dc560dd189" in namespace "downward-api-4872" to be "Succeeded or Failed"
Apr 29 07:10:13.345: INFO: Pod "downwardapi-volume-f98b272d-80cf-43f3-957d-70dc560dd189": Phase="Pending", Reason="", readiness=false. Elapsed: 9.388377ms
Apr 29 07:10:15.349: INFO: Pod "downwardapi-volume-f98b272d-80cf-43f3-957d-70dc560dd189": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013109332s
STEP: Saw pod success
Apr 29 07:10:15.349: INFO: Pod "downwardapi-volume-f98b272d-80cf-43f3-957d-70dc560dd189" satisfied condition "Succeeded or Failed"
Apr 29 07:10:15.351: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-f98b272d-80cf-43f3-957d-70dc560dd189 container client-container: <nil>
STEP: delete the pod
Apr 29 07:10:15.371: INFO: Waiting for pod downwardapi-volume-f98b272d-80cf-43f3-957d-70dc560dd189 to disappear
Apr 29 07:10:15.373: INFO: Pod downwardapi-volume-f98b272d-80cf-43f3-957d-70dc560dd189 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:10:15.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4872" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2473,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:10:15.381: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Apr 29 07:10:15.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-8760 cluster-info'
Apr 29 07:10:15.518: INFO: stderr: ""
Apr 29 07:10:15.518: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:10:15.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8760" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":126,"skipped":2473,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:10:15.532: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:10:19.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7615" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":127,"skipped":2507,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:10:19.595: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 29 07:10:19.662: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4493  241a4467-1dc7-4eb3-b3ef-5f524aba9a76 102509 0 2022-04-29 07:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-04-29 07:10:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 07:10:19.662: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4493  241a4467-1dc7-4eb3-b3ef-5f524aba9a76 102510 0 2022-04-29 07:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-04-29 07:10:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:10:19.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4493" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":128,"skipped":2514,"failed":0}
S
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:10:19.675: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Apr 29 07:10:21.758: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:10:23.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8427" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":129,"skipped":2515,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:10:23.795: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-d4855fc8-2257-4e9f-80e7-f6006115ed73
STEP: Creating secret with name s-test-opt-upd-f704fa7a-6c4e-4b01-b37b-34838c4a4e0c
STEP: Creating the pod
Apr 29 07:10:23.846: INFO: The status of Pod pod-projected-secrets-a88aff9b-3fe1-4a7c-80b3-7f9fe463e33e is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:10:25.850: INFO: The status of Pod pod-projected-secrets-a88aff9b-3fe1-4a7c-80b3-7f9fe463e33e is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-d4855fc8-2257-4e9f-80e7-f6006115ed73
STEP: Updating secret s-test-opt-upd-f704fa7a-6c4e-4b01-b37b-34838c4a4e0c
STEP: Creating secret with name s-test-opt-create-d93d57cb-ddf4-4949-9ab5-1c41035bdd39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:10:27.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6542" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":130,"skipped":2559,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:10:27.914: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5642
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Apr 29 07:10:27.960: INFO: Found 0 stateful pods, waiting for 3
Apr 29 07:10:37.965: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 07:10:37.965: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 07:10:37.965: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 07:10:37.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-5642 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 07:10:38.204: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 07:10:38.204: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 07:10:38.204: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Apr 29 07:10:48.236: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 29 07:10:58.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-5642 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 07:10:58.395: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 07:10:58.395: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 07:10:58.395: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Apr 29 07:11:08.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-5642 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 07:11:08.585: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 07:11:08.585: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 07:11:08.585: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 07:11:18.620: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 29 07:11:28.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-5642 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 07:11:28.794: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 07:11:28.794: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 07:11:28.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Apr 29 07:11:38.812: INFO: Deleting all statefulset in ns statefulset-5642
Apr 29 07:11:38.814: INFO: Scaling statefulset ss2 to 0
Apr 29 07:11:48.827: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 07:11:48.829: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:11:48.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5642" for this suite.

• [SLOW TEST:80.941 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":131,"skipped":2565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:11:48.856: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:11:49.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4188" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":132,"skipped":2600,"failed":0}

------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:11:49.124: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Apr 29 07:11:49.244: INFO: PodSpec: initContainers in spec.initContainers
Apr 29 07:12:38.070: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3394de13-15b4-492c-b504-5234ae05637f", GenerateName:"", Namespace:"init-container-4347", SelfLink:"", UID:"c18ca061-be06-48af-bf1c-e69b071eba96", ResourceVersion:"103735", Generation:0, CreationTimestamp:time.Date(2022, time.April, 29, 7, 11, 49, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"244450169"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.16.115.189/32", "cni.projectcalico.org/podIPs":"172.16.115.189/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.April, 29, 7, 11, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00053eed0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.April, 29, 7, 11, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00053ef00), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.April, 29, 7, 11, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00053ef30), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-kpscm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0029a7b80), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kpscm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kpscm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kpscm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00630f200), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"localhost.localdomain151", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003fb6d90), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00630f280)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00630f2a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00630f2a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00630f2ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0056dd690), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.April, 29, 7, 11, 49, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.April, 29, 7, 11, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.April, 29, 7, 11, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.April, 29, 7, 11, 49, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"123.58.176.151", PodIP:"172.16.115.189", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.16.115.189"}}, StartTime:time.Date(2022, time.April, 29, 7, 11, 49, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003fb6e70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003fb6ee0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://9991004aeaa7f54bd408c2b24ec868735b3f747e8e291a35adbd639c0ffa8278", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0029a7c00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0029a7be0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc00630f324)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:12:38.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4347" for this suite.

• [SLOW TEST:48.962 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":133,"skipped":2600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:12:38.086: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:12:38.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6581 create -f -'
Apr 29 07:12:38.912: INFO: stderr: ""
Apr 29 07:12:38.912: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Apr 29 07:12:38.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6581 create -f -'
Apr 29 07:12:39.103: INFO: stderr: ""
Apr 29 07:12:39.103: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 29 07:12:40.107: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 07:12:40.107: INFO: Found 1 / 1
Apr 29 07:12:40.107: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 29 07:12:40.110: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 07:12:40.110: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 29 07:12:40.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6581 describe pod agnhost-primary-bdlg7'
Apr 29 07:12:40.192: INFO: stderr: ""
Apr 29 07:12:40.192: INFO: stdout: "Name:         agnhost-primary-bdlg7\nNamespace:    kubectl-6581\nPriority:     0\nNode:         localhost.localdomain151/123.58.176.151\nStart Time:   Fri, 29 Apr 2022 07:12:38 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 172.16.115.130/32\n              cni.projectcalico.org/podIPs: 172.16.115.130/32\nStatus:       Running\nIP:           172.16.115.130\nIPs:\n  IP:           172.16.115.130\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://3ea2e889e4e00bff5dd453e7dc2ca272b9653b71888e3914d05e809824fa5391\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 29 Apr 2022 07:12:39 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7pntm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-7pntm:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6581/agnhost-primary-bdlg7 to localhost.localdomain151\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.33\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Apr 29 07:12:40.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6581 describe rc agnhost-primary'
Apr 29 07:12:40.287: INFO: stderr: ""
Apr 29 07:12:40.287: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6581\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-bdlg7\n"
Apr 29 07:12:40.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6581 describe service agnhost-primary'
Apr 29 07:12:40.369: INFO: stderr: ""
Apr 29 07:12:40.369: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6581\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.104.211.109\nIPs:               10.104.211.109\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.16.115.130:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 29 07:12:40.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6581 describe node localhost.localdomain'
Apr 29 07:12:40.480: INFO: stderr: ""
Apr 29 07:12:40.480: INFO: stdout: "Name:               localhost.localdomain\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=localhost.localdomain\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.230.228.46/25\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.16.102.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 29 Apr 2022 02:38:57 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  localhost.localdomain\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 29 Apr 2022 07:12:35 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 29 Apr 2022 02:39:14 +0000   Fri, 29 Apr 2022 02:39:14 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 29 Apr 2022 07:07:45 +0000   Fri, 29 Apr 2022 02:38:54 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 29 Apr 2022 07:07:45 +0000   Fri, 29 Apr 2022 02:38:54 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 29 Apr 2022 07:07:45 +0000   Fri, 29 Apr 2022 02:38:54 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 29 Apr 2022 07:07:45 +0000   Fri, 29 Apr 2022 02:39:15 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  123.58.176.150\n  Hostname:    localhost.localdomain\nCapacity:\n  cpu:                8\n  ephemeral-storage:  103079864Ki\n  hugepages-2Mi:      0\n  memory:             16265744Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  94998402506\n  hugepages-2Mi:      0\n  memory:             16163344Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 67c017f3411541b9b46ecfff6271cb64\n  System UUID:                604DF406-9C44-4183-8AB1-7210355CF21D\n  Boot ID:                    4e9d061f-4b37-49c1-a039-0d550561b3d9\n  Kernel Version:             3.10.0-957.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.5\n  Kubelet Version:            v1.23.5\n  Kube-Proxy Version:         v1.23.5\nNon-terminated Pods:          (27 in total)\n  Namespace                   Name                                                        CPU Requests  CPU Limits   Memory Requests  Memory Limits  Age\n  ---------                   ----                                                        ------------  ----------   ---------------  -------------  ---\n  crd-webhook-2934            sample-crd-conversion-webhook-deployment-bb9577b7b-b2dfv    0 (0%)        0 (0%)       0 (0%)           0 (0%)         24m\n  elasticsearch               elasticsearch-master-0                                      1 (12%)       2 (25%)      1Gi (6%)         2Gi (12%)      4h31m\n  hnc-system                  hnc-controller-manager-6d47799b76-dcfk2                     100m (1%)     100m (1%)    150Mi (0%)       300Mi (1%)     24m\n  ingress-nginx               ingress-nginx-controller-776d88d5c9-wsdhf                   100m (1%)     0 (0%)       90Mi (0%)        0 (0%)         4h33m\n  kube-system                 calico-kube-controllers-d84f7b88b-rqbrz                     0 (0%)        0 (0%)       0 (0%)           0 (0%)         4h33m\n  kube-system                 calico-node-kb4fj                                           250m (3%)     0 (0%)       0 (0%)           0 (0%)         4h33m\n  kube-system                 coredns-65c54cc984-q685t                                    100m (1%)     0 (0%)       70Mi (0%)        170Mi (1%)     4h33m\n  kube-system                 coredns-65c54cc984-vcx96                                    100m (1%)     0 (0%)       70Mi (0%)        170Mi (1%)     4h33m\n  kube-system                 etcd-localhost.localdomain                                  100m (1%)     0 (0%)       100Mi (0%)       0 (0%)         4h33m\n  kube-system                 kube-apiserver-localhost.localdomain                        250m (3%)     0 (0%)       0 (0%)           0 (0%)         4h33m\n  kube-system                 kube-controller-manager-localhost.localdomain               200m (2%)     0 (0%)       0 (0%)           0 (0%)         4h33m\n  kube-system                 kube-proxy-tmzt7                                            0 (0%)        0 (0%)       0 (0%)           0 (0%)         4h33m\n  kube-system                 kube-scheduler-localhost.localdomain                        100m (1%)     0 (0%)       0 (0%)           0 (0%)         4h33m\n  kube-system                 metrics-server-6d69ff96c6-vth6c                             0 (0%)        0 (0%)       0 (0%)           0 (0%)         4h33m\n  kubecube-monitoring         kubecube-monitoring-kube-state-metrics-6cf5c8c6d4-7knxg     100m (1%)     200m (2%)    100Mi (0%)       200Mi (1%)     4h30m\n  kubecube-monitoring         kubecube-monitoring-operator-5f84949849-f69dc               100m (1%)     300m (3%)    100Mi (0%)       500Mi (3%)     4h30m\n  kubecube-monitoring         kubecube-monitoring-prometheus-node-exporter-wmrth          100m (1%)     200m (2%)    50Mi (0%)        100Mi (0%)     4h30m\n  kubecube-monitoring         kubecube-thanos-query-86f669566f-vzpq5                      0 (0%)        0 (0%)       0 (0%)           0 (0%)         130m\n  kubecube-monitoring         prometheus-kubecube-monitoring-prometheus-0                 400m (5%)     1100m (13%)  1550Mi (9%)      3050Mi (19%)   4h30m\n  kubecube-system             audit-78b7d4cf7d-fs2pm                                      0 (0%)        0 (0%)       0 (0%)           0 (0%)         4h33m\n  kubecube-system             cloud-shell-5c87948759-ksk75                                0 (0%)        0 (0%)       0 (0%)           0 (0%)         4h33m\n  kubecube-system             frontend-7cbccf84fb-wnxk7                                   0 (0%)        0 (0%)       0 (0%)           0 (0%)         4h33m\n  kubecube-system             kubecube-756775dbb-6wvmx                                    0 (0%)        0 (0%)       0 (0%)           0 (0%)         4h33m\n  kubecube-system             warden-d876d9fd8-rbkr2                                      0 (0%)        0 (0%)       0 (0%)           0 (0%)         4h31m\n  kubecube-system             webconsole-7d7f989996-h84gm                                 0 (0%)        0 (0%)       0 (0%)           0 (0%)         4h33m\n  local-path-storage          local-path-provisioner-86b7fd94b5-wdtg4                     0 (0%)        0 (0%)       0 (0%)           0 (0%)         4h33m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-82ef1d0cc98c43bb-5fv6g     0 (0%)        0 (0%)       0 (0%)           0 (0%)         32m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                3 (37%)       3900m (48%)\n  memory             3304Mi (20%)  6538Mi (41%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  hugepages-2Mi      0 (0%)        0 (0%)\nEvents:              <none>\n"
Apr 29 07:12:40.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6581 describe namespace kubectl-6581'
Apr 29 07:12:40.552: INFO: stderr: ""
Apr 29 07:12:40.552: INFO: stdout: "Name:         kubectl-6581\nLabels:       e2e-framework=kubectl\n              e2e-run=c17bdccc-a052-4b70-888a-fe8957adfc3a\n              hnc.x-k8s.io/included-namespace=true\n              kubectl-6581.tree.hnc.x-k8s.io/depth=0\n              kubernetes.io/metadata.name=kubectl-6581\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:12:40.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6581" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":134,"skipped":2624,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:12:40.561: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-4d4a624b-fc5d-4eb4-ab1a-019c106ad25a
STEP: Creating a pod to test consume configMaps
Apr 29 07:12:40.600: INFO: Waiting up to 5m0s for pod "pod-configmaps-376cad9c-8e7d-419e-bc24-8702ae844bd8" in namespace "configmap-2586" to be "Succeeded or Failed"
Apr 29 07:12:40.604: INFO: Pod "pod-configmaps-376cad9c-8e7d-419e-bc24-8702ae844bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.122838ms
Apr 29 07:12:42.610: INFO: Pod "pod-configmaps-376cad9c-8e7d-419e-bc24-8702ae844bd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010167827s
STEP: Saw pod success
Apr 29 07:12:42.610: INFO: Pod "pod-configmaps-376cad9c-8e7d-419e-bc24-8702ae844bd8" satisfied condition "Succeeded or Failed"
Apr 29 07:12:42.613: INFO: Trying to get logs from node localhost.localdomain151 pod pod-configmaps-376cad9c-8e7d-419e-bc24-8702ae844bd8 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 07:12:42.636: INFO: Waiting for pod pod-configmaps-376cad9c-8e7d-419e-bc24-8702ae844bd8 to disappear
Apr 29 07:12:42.638: INFO: Pod pod-configmaps-376cad9c-8e7d-419e-bc24-8702ae844bd8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:12:42.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2586" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":135,"skipped":2627,"failed":0}
SSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:12:42.648: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:17:42.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1447" for this suite.

• [SLOW TEST:300.116 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":136,"skipped":2632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:17:42.764: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Apr 29 07:17:42.869: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:17:42.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1652" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":137,"skipped":2658,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:17:42.950: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 29 07:17:42.986: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 07:18:43.035: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:18:43.039: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Apr 29 07:18:45.158: INFO: found a healthy node: localhost.localdomain151
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:18:51.216: INFO: pods created so far: [1 1 1]
Apr 29 07:18:51.216: INFO: length of pods created so far: 3
Apr 29 07:18:53.227: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:19:00.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2158" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:19:00.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9478" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:77.358 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":138,"skipped":2702,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:19:00.309: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6726.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6726.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 07:19:02.377: INFO: DNS probes using dns-test-3ceb458e-b6b3-4c76-93c6-fb0714ca28ce succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6726.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6726.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 07:19:04.426: INFO: File wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local from pod  dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 29 07:19:04.429: INFO: File jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local from pod  dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 29 07:19:04.429: INFO: Lookups using dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 failed for: [wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local]

Apr 29 07:19:09.433: INFO: File wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local from pod  dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 29 07:19:09.436: INFO: File jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local from pod  dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 29 07:19:09.436: INFO: Lookups using dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 failed for: [wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local]

Apr 29 07:19:14.433: INFO: File wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local from pod  dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 29 07:19:14.435: INFO: File jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local from pod  dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 29 07:19:14.435: INFO: Lookups using dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 failed for: [wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local]

Apr 29 07:19:19.434: INFO: File wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local from pod  dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 29 07:19:19.438: INFO: File jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local from pod  dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 29 07:19:19.438: INFO: Lookups using dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 failed for: [wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local]

Apr 29 07:19:24.434: INFO: File wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local from pod  dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 29 07:19:24.438: INFO: File jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local from pod  dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 29 07:19:24.438: INFO: Lookups using dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 failed for: [wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local]

Apr 29 07:19:29.433: INFO: File wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local from pod  dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 29 07:19:29.436: INFO: File jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local from pod  dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 29 07:19:29.436: INFO: Lookups using dns-6726/dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 failed for: [wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local]

Apr 29 07:19:34.437: INFO: DNS probes using dns-test-6e72c74a-cee4-464d-bcd4-805cbebe43b9 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6726.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6726.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6726.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6726.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 07:19:36.504: INFO: DNS probes using dns-test-695678b7-4e0b-4942-a9ca-4f2281510515 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:19:36.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6726" for this suite.

• [SLOW TEST:36.247 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":139,"skipped":2715,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:19:36.556: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 29 07:19:36.597: INFO: Waiting up to 5m0s for pod "pod-df385235-1355-4064-8628-c0acff18175a" in namespace "emptydir-750" to be "Succeeded or Failed"
Apr 29 07:19:36.600: INFO: Pod "pod-df385235-1355-4064-8628-c0acff18175a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.617003ms
Apr 29 07:19:38.606: INFO: Pod "pod-df385235-1355-4064-8628-c0acff18175a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008550942s
STEP: Saw pod success
Apr 29 07:19:38.606: INFO: Pod "pod-df385235-1355-4064-8628-c0acff18175a" satisfied condition "Succeeded or Failed"
Apr 29 07:19:38.609: INFO: Trying to get logs from node localhost.localdomain151 pod pod-df385235-1355-4064-8628-c0acff18175a container test-container: <nil>
STEP: delete the pod
Apr 29 07:19:38.632: INFO: Waiting for pod pod-df385235-1355-4064-8628-c0acff18175a to disappear
Apr 29 07:19:38.634: INFO: Pod pod-df385235-1355-4064-8628-c0acff18175a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:19:38.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-750" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":140,"skipped":2717,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:19:38.642: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:19:38.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3476" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":141,"skipped":2725,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:19:38.766: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-07d60c76-9810-46b3-849e-f1277252cb7b
Apr 29 07:19:38.808: INFO: Pod name my-hostname-basic-07d60c76-9810-46b3-849e-f1277252cb7b: Found 0 pods out of 1
Apr 29 07:19:43.811: INFO: Pod name my-hostname-basic-07d60c76-9810-46b3-849e-f1277252cb7b: Found 1 pods out of 1
Apr 29 07:19:43.811: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-07d60c76-9810-46b3-849e-f1277252cb7b" are running
Apr 29 07:19:43.813: INFO: Pod "my-hostname-basic-07d60c76-9810-46b3-849e-f1277252cb7b-5mzzd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-29 07:19:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-29 07:19:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-29 07:19:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-04-29 07:19:38 +0000 UTC Reason: Message:}])
Apr 29 07:19:43.813: INFO: Trying to dial the pod
Apr 29 07:19:48.824: INFO: Controller my-hostname-basic-07d60c76-9810-46b3-849e-f1277252cb7b: Got expected result from replica 1 [my-hostname-basic-07d60c76-9810-46b3-849e-f1277252cb7b-5mzzd]: "my-hostname-basic-07d60c76-9810-46b3-849e-f1277252cb7b-5mzzd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:19:48.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6566" for this suite.

• [SLOW TEST:10.069 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":142,"skipped":2747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:19:48.835: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-9039175a-0477-4baa-a80e-8ada048942d9
STEP: Creating secret with name secret-projected-all-test-volume-b7be35f5-e806-4843-8157-37164227ef9a
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 29 07:19:48.932: INFO: Waiting up to 5m0s for pod "projected-volume-9eccc7b0-16dc-46f3-8198-42dca294114d" in namespace "projected-8469" to be "Succeeded or Failed"
Apr 29 07:19:48.935: INFO: Pod "projected-volume-9eccc7b0-16dc-46f3-8198-42dca294114d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.878856ms
Apr 29 07:19:50.938: INFO: Pod "projected-volume-9eccc7b0-16dc-46f3-8198-42dca294114d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006346507s
STEP: Saw pod success
Apr 29 07:19:50.938: INFO: Pod "projected-volume-9eccc7b0-16dc-46f3-8198-42dca294114d" satisfied condition "Succeeded or Failed"
Apr 29 07:19:50.941: INFO: Trying to get logs from node localhost.localdomain151 pod projected-volume-9eccc7b0-16dc-46f3-8198-42dca294114d container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 29 07:19:51.026: INFO: Waiting for pod projected-volume-9eccc7b0-16dc-46f3-8198-42dca294114d to disappear
Apr 29 07:19:51.028: INFO: Pod projected-volume-9eccc7b0-16dc-46f3-8198-42dca294114d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:19:51.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8469" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":143,"skipped":2770,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:19:51.038: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-362984da-aa4d-4c11-b912-1f8bf46c8b91
STEP: Creating a pod to test consume configMaps
Apr 29 07:19:51.339: INFO: Waiting up to 5m0s for pod "pod-configmaps-c159091a-c1e5-4cd5-bf1a-18f3fda9600a" in namespace "configmap-4019" to be "Succeeded or Failed"
Apr 29 07:19:51.341: INFO: Pod "pod-configmaps-c159091a-c1e5-4cd5-bf1a-18f3fda9600a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.582208ms
Apr 29 07:19:53.346: INFO: Pod "pod-configmaps-c159091a-c1e5-4cd5-bf1a-18f3fda9600a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006978466s
STEP: Saw pod success
Apr 29 07:19:53.346: INFO: Pod "pod-configmaps-c159091a-c1e5-4cd5-bf1a-18f3fda9600a" satisfied condition "Succeeded or Failed"
Apr 29 07:19:53.348: INFO: Trying to get logs from node localhost.localdomain151 pod pod-configmaps-c159091a-c1e5-4cd5-bf1a-18f3fda9600a container agnhost-container: <nil>
STEP: delete the pod
Apr 29 07:19:53.359: INFO: Waiting for pod pod-configmaps-c159091a-c1e5-4cd5-bf1a-18f3fda9600a to disappear
Apr 29 07:19:53.361: INFO: Pod pod-configmaps-c159091a-c1e5-4cd5-bf1a-18f3fda9600a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:19:53.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4019" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":144,"skipped":2789,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:19:53.369: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Apr 29 07:19:53.410: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:19:55.414: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Apr 29 07:19:55.424: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:19:57.429: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Apr 29 07:19:57.437: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 07:19:57.440: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 07:19:59.440: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 07:19:59.445: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 07:20:01.440: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 07:20:01.443: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:20:01.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7858" for this suite.

• [SLOW TEST:8.086 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":145,"skipped":2791,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:20:01.455: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3687.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3687.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3687.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3687.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3687.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3687.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3687.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3687.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 07:20:03.511: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local from pod dns-3687/dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494: the server could not find the requested resource (get pods dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494)
Apr 29 07:20:03.514: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local from pod dns-3687/dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494: the server could not find the requested resource (get pods dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494)
Apr 29 07:20:03.517: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3687.svc.cluster.local from pod dns-3687/dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494: the server could not find the requested resource (get pods dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494)
Apr 29 07:20:03.519: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3687.svc.cluster.local from pod dns-3687/dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494: the server could not find the requested resource (get pods dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494)
Apr 29 07:20:03.522: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local from pod dns-3687/dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494: the server could not find the requested resource (get pods dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494)
Apr 29 07:20:03.525: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local from pod dns-3687/dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494: the server could not find the requested resource (get pods dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494)
Apr 29 07:20:03.528: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3687.svc.cluster.local from pod dns-3687/dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494: the server could not find the requested resource (get pods dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494)
Apr 29 07:20:03.531: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3687.svc.cluster.local from pod dns-3687/dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494: the server could not find the requested resource (get pods dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494)
Apr 29 07:20:03.531: INFO: Lookups using dns-3687/dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3687.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3687.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3687.svc.cluster.local jessie_udp@dns-test-service-2.dns-3687.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3687.svc.cluster.local]

Apr 29 07:20:08.549: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3687.svc.cluster.local from pod dns-3687/dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494: the server could not find the requested resource (get pods dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494)
Apr 29 07:20:08.561: INFO: Lookups using dns-3687/dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494 failed for: [wheezy_tcp@dns-test-service-2.dns-3687.svc.cluster.local]

Apr 29 07:20:13.556: INFO: DNS probes using dns-3687/dns-test-cbfad073-fc07-4629-a45b-56e2c74f3494 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:20:13.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3687" for this suite.

• [SLOW TEST:12.191 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":146,"skipped":2810,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:20:13.646: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:20:24.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3625" for this suite.

• [SLOW TEST:11.164 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":147,"skipped":2813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:20:24.810: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:20:25.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7030" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":148,"skipped":2841,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:20:25.061: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:20:25.143: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: creating the pod
STEP: submitting the pod to kubernetes
Apr 29 07:20:25.170: INFO: The status of Pod pod-exec-websocket-e8d5dd1a-997a-47de-97f3-2a85caec4218 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:20:27.176: INFO: The status of Pod pod-exec-websocket-e8d5dd1a-997a-47de-97f3-2a85caec4218 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:20:27.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4009" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":149,"skipped":2846,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:20:27.255: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Apr 29 07:22:27.845: INFO: Successfully updated pod "var-expansion-58576ad6-61b9-4906-993a-d9dd324b2f99"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Apr 29 07:22:29.852: INFO: Deleting pod "var-expansion-58576ad6-61b9-4906-993a-d9dd324b2f99" in namespace "var-expansion-2307"
Apr 29 07:22:29.858: INFO: Wait up to 5m0s for pod "var-expansion-58576ad6-61b9-4906-993a-d9dd324b2f99" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:01.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2307" for this suite.

• [SLOW TEST:154.632 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":150,"skipped":2851,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:01.888: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Apr 29 07:23:01.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-5316 api-versions'
Apr 29 07:23:01.996: INFO: stderr: ""
Apr 29 07:23:01.996: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncluster.kubecube.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextension.kubecube.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhnc.x-k8s.io/v1alpha2\nhotplug.kubecube.io/v1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.kubecube.io/v1\nnetease.com/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nquota.kubecube.io/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntenant.kubecube.io/v1\nuser.kubecube.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:01.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5316" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":151,"skipped":2937,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:02.007: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Apr 29 07:23:02.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-9939 create -f -'
Apr 29 07:23:02.364: INFO: stderr: ""
Apr 29 07:23:02.364: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 29 07:23:03.368: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 07:23:03.368: INFO: Found 1 / 1
Apr 29 07:23:03.368: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 29 07:23:03.370: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 07:23:03.370: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 29 07:23:03.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-9939 patch pod agnhost-primary-bm6cq -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 29 07:23:03.449: INFO: stderr: ""
Apr 29 07:23:03.449: INFO: stdout: "pod/agnhost-primary-bm6cq patched\n"
STEP: checking annotations
Apr 29 07:23:03.451: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 07:23:03.451: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:03.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9939" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":152,"skipped":2938,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:03.459: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 07:23:03.544: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:23:03.544: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:23:04.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:23:04.553: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:23:05.557: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 29 07:23:05.557: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 29 07:23:05.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 29 07:23:05.576: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:23:06.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 29 07:23:06.593: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:23:07.585: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 29 07:23:07.585: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:23:08.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 29 07:23:08.588: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:23:09.586: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 29 07:23:09.586: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5240, will wait for the garbage collector to delete the pods
Apr 29 07:23:09.647: INFO: Deleting DaemonSet.extensions daemon-set took: 6.264751ms
Apr 29 07:23:09.747: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.490855ms
Apr 29 07:23:11.752: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:23:11.752: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 29 07:23:11.754: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"107188"},"items":null}

Apr 29 07:23:11.756: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"107188"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:11.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5240" for this suite.

• [SLOW TEST:8.319 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":153,"skipped":2946,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:11.778: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 07:23:12.347: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 07:23:15.374: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:15.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2673" for this suite.
STEP: Destroying namespace "webhook-2673-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":154,"skipped":2961,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:15.478: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-3b146790-6e6a-4b3c-922e-4b9a8796a601
STEP: Creating a pod to test consume configMaps
Apr 29 07:23:15.541: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2769ea20-66b1-49af-97a9-0d9b5bc876db" in namespace "projected-6658" to be "Succeeded or Failed"
Apr 29 07:23:15.543: INFO: Pod "pod-projected-configmaps-2769ea20-66b1-49af-97a9-0d9b5bc876db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25049ms
Apr 29 07:23:17.547: INFO: Pod "pod-projected-configmaps-2769ea20-66b1-49af-97a9-0d9b5bc876db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006085826s
STEP: Saw pod success
Apr 29 07:23:17.547: INFO: Pod "pod-projected-configmaps-2769ea20-66b1-49af-97a9-0d9b5bc876db" satisfied condition "Succeeded or Failed"
Apr 29 07:23:17.550: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-configmaps-2769ea20-66b1-49af-97a9-0d9b5bc876db container agnhost-container: <nil>
STEP: delete the pod
Apr 29 07:23:17.581: INFO: Waiting for pod pod-projected-configmaps-2769ea20-66b1-49af-97a9-0d9b5bc876db to disappear
Apr 29 07:23:17.583: INFO: Pod pod-projected-configmaps-2769ea20-66b1-49af-97a9-0d9b5bc876db no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:17.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6658" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":155,"skipped":2966,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:17.592: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 07:23:18.339: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 07:23:21.367: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:21.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9378" for this suite.
STEP: Destroying namespace "webhook-9378-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":156,"skipped":2967,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:21.534: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 07:23:21.841: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4dfa6fca-6cc1-4305-b252-86ce19c3647b" in namespace "projected-2884" to be "Succeeded or Failed"
Apr 29 07:23:21.846: INFO: Pod "downwardapi-volume-4dfa6fca-6cc1-4305-b252-86ce19c3647b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.736574ms
Apr 29 07:23:23.850: INFO: Pod "downwardapi-volume-4dfa6fca-6cc1-4305-b252-86ce19c3647b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00898753s
STEP: Saw pod success
Apr 29 07:23:23.850: INFO: Pod "downwardapi-volume-4dfa6fca-6cc1-4305-b252-86ce19c3647b" satisfied condition "Succeeded or Failed"
Apr 29 07:23:23.853: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-4dfa6fca-6cc1-4305-b252-86ce19c3647b container client-container: <nil>
STEP: delete the pod
Apr 29 07:23:23.871: INFO: Waiting for pod downwardapi-volume-4dfa6fca-6cc1-4305-b252-86ce19c3647b to disappear
Apr 29 07:23:23.873: INFO: Pod downwardapi-volume-4dfa6fca-6cc1-4305-b252-86ce19c3647b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:23.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2884" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":157,"skipped":2996,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:23.881: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:29.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9952" for this suite.

• [SLOW TEST:6.113 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":158,"skipped":3013,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:29.994: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4486
STEP: creating service affinity-clusterip-transition in namespace services-4486
STEP: creating replication controller affinity-clusterip-transition in namespace services-4486
I0429 07:23:30.047727      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4486, replica count: 3
I0429 07:23:33.098758      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 07:23:33.105: INFO: Creating new exec pod
Apr 29 07:23:36.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-4486 exec execpod-affinityc59f9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Apr 29 07:23:36.263: INFO: stderr: "+ + echonc -v -t -w hostName 2\n affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Apr 29 07:23:36.263: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:23:36.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-4486 exec execpod-affinityc59f9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.233.158 80'
Apr 29 07:23:36.409: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.233.158 80\nConnection to 10.108.233.158 80 port [tcp/http] succeeded!\n"
Apr 29 07:23:36.409: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:23:36.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-4486 exec execpod-affinityc59f9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.233.158:80/ ; done'
Apr 29 07:23:36.621: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n"
Apr 29 07:23:36.621: INFO: stdout: "\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-wkrfn\naffinity-clusterip-transition-wkrfn\naffinity-clusterip-transition-j2n6d\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-j2n6d\naffinity-clusterip-transition-wkrfn\naffinity-clusterip-transition-j2n6d\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-j2n6d\naffinity-clusterip-transition-wkrfn\naffinity-clusterip-transition-wkrfn\naffinity-clusterip-transition-wkrfn\naffinity-clusterip-transition-j2n6d\naffinity-clusterip-transition-wkrfn\naffinity-clusterip-transition-wkrfn"
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-wkrfn
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-wkrfn
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-j2n6d
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-j2n6d
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-wkrfn
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-j2n6d
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-j2n6d
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-wkrfn
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-wkrfn
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-wkrfn
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-j2n6d
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-wkrfn
Apr 29 07:23:36.621: INFO: Received response from host: affinity-clusterip-transition-wkrfn
Apr 29 07:23:36.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-4486 exec execpod-affinityc59f9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.233.158:80/ ; done'
Apr 29 07:23:36.847: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.233.158:80/\n"
Apr 29 07:23:36.847: INFO: stdout: "\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v\naffinity-clusterip-transition-5tz5v"
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Received response from host: affinity-clusterip-transition-5tz5v
Apr 29 07:23:36.847: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4486, will wait for the garbage collector to delete the pods
Apr 29 07:23:36.917: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.184199ms
Apr 29 07:23:37.018: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.392538ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:39.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4486" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.553 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":159,"skipped":3018,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:39.547: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 07:23:40.436: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 07:23:43.469: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:43.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7381" for this suite.
STEP: Destroying namespace "webhook-7381-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":160,"skipped":3022,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:43.841: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Apr 29 07:23:43.962: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2279  79901630-a591-4d45-be8b-d1b1d9080f47 107814 0 2022-04-29 07:23:43 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-04-29 07:23:43 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zxp2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zxp2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 07:23:43.965: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:23:45.970: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Apr 29 07:23:45.971: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2279 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 07:23:45.971: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 07:23:45.971: INFO: ExecWithOptions: Clientset creation
Apr 29 07:23:45.971: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2279/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
Apr 29 07:23:46.061: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2279 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 07:23:46.061: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 07:23:46.062: INFO: ExecWithOptions: Clientset creation
Apr 29 07:23:46.062: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2279/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Apr 29 07:23:46.147: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:46.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2279" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":161,"skipped":3025,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:46.167: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:23:46.197: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 29 07:23:49.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7729 --namespace=crd-publish-openapi-7729 create -f -'
Apr 29 07:23:50.006: INFO: stderr: ""
Apr 29 07:23:50.006: INFO: stdout: "e2e-test-crd-publish-openapi-5273-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 29 07:23:50.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7729 --namespace=crd-publish-openapi-7729 delete e2e-test-crd-publish-openapi-5273-crds test-cr'
Apr 29 07:23:50.084: INFO: stderr: ""
Apr 29 07:23:50.084: INFO: stdout: "e2e-test-crd-publish-openapi-5273-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr 29 07:23:50.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7729 --namespace=crd-publish-openapi-7729 apply -f -'
Apr 29 07:23:50.273: INFO: stderr: ""
Apr 29 07:23:50.273: INFO: stdout: "e2e-test-crd-publish-openapi-5273-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 29 07:23:50.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7729 --namespace=crd-publish-openapi-7729 delete e2e-test-crd-publish-openapi-5273-crds test-cr'
Apr 29 07:23:50.354: INFO: stderr: ""
Apr 29 07:23:50.354: INFO: stdout: "e2e-test-crd-publish-openapi-5273-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 29 07:23:50.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7729 explain e2e-test-crd-publish-openapi-5273-crds'
Apr 29 07:23:50.536: INFO: stderr: ""
Apr 29 07:23:50.536: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5273-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:54.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7729" for this suite.

• [SLOW TEST:8.700 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":162,"skipped":3035,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:54.867: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-fbe45683-00b6-400d-a301-cbe9058c941d
STEP: Creating a pod to test consume secrets
Apr 29 07:23:54.997: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bb85f7cd-01fc-47b8-adf4-7b6a94bb7125" in namespace "projected-1355" to be "Succeeded or Failed"
Apr 29 07:23:55.011: INFO: Pod "pod-projected-secrets-bb85f7cd-01fc-47b8-adf4-7b6a94bb7125": Phase="Pending", Reason="", readiness=false. Elapsed: 14.033638ms
Apr 29 07:23:57.015: INFO: Pod "pod-projected-secrets-bb85f7cd-01fc-47b8-adf4-7b6a94bb7125": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018081555s
STEP: Saw pod success
Apr 29 07:23:57.015: INFO: Pod "pod-projected-secrets-bb85f7cd-01fc-47b8-adf4-7b6a94bb7125" satisfied condition "Succeeded or Failed"
Apr 29 07:23:57.018: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-secrets-bb85f7cd-01fc-47b8-adf4-7b6a94bb7125 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 07:23:57.033: INFO: Waiting for pod pod-projected-secrets-bb85f7cd-01fc-47b8-adf4-7b6a94bb7125 to disappear
Apr 29 07:23:57.035: INFO: Pod pod-projected-secrets-bb85f7cd-01fc-47b8-adf4-7b6a94bb7125 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:57.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1355" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":163,"skipped":3049,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:57.043: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 29 07:23:57.144: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Apr 29 07:23:57.149: INFO: starting watch
STEP: patching
STEP: updating
Apr 29 07:23:57.163: INFO: waiting for watch events with expected annotations
Apr 29 07:23:57.163: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:23:57.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1971" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":164,"skipped":3099,"failed":0}

------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:23:57.215: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-8695
STEP: creating replication controller nodeport-test in namespace services-8695
I0429 07:23:57.274528      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8695, replica count: 2
Apr 29 07:24:00.325: INFO: Creating new exec pod
I0429 07:24:00.325774      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 07:24:03.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8695 exec execpod4r275 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Apr 29 07:24:03.493: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 29 07:24:03.493: INFO: stdout: ""
Apr 29 07:24:04.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8695 exec execpod4r275 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Apr 29 07:24:04.656: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 29 07:24:04.656: INFO: stdout: ""
Apr 29 07:24:05.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8695 exec execpod4r275 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Apr 29 07:24:05.629: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 29 07:24:05.629: INFO: stdout: "nodeport-test-sw6wt"
Apr 29 07:24:05.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8695 exec execpod4r275 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.16.126 80'
Apr 29 07:24:05.769: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.16.126 80\nConnection to 10.110.16.126 80 port [tcp/http] succeeded!\n"
Apr 29 07:24:05.769: INFO: stdout: ""
Apr 29 07:24:06.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8695 exec execpod4r275 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.16.126 80'
Apr 29 07:24:06.947: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.16.126 80\nConnection to 10.110.16.126 80 port [tcp/http] succeeded!\n"
Apr 29 07:24:06.947: INFO: stdout: ""
Apr 29 07:24:07.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8695 exec execpod4r275 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.16.126 80'
Apr 29 07:24:07.913: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.16.126 80\nConnection to 10.110.16.126 80 port [tcp/http] succeeded!\n"
Apr 29 07:24:07.913: INFO: stdout: ""
Apr 29 07:24:08.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8695 exec execpod4r275 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.16.126 80'
Apr 29 07:24:08.950: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.16.126 80\nConnection to 10.110.16.126 80 port [tcp/http] succeeded!\n"
Apr 29 07:24:08.950: INFO: stdout: ""
Apr 29 07:24:09.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8695 exec execpod4r275 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.16.126 80'
Apr 29 07:24:09.911: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.16.126 80\nConnection to 10.110.16.126 80 port [tcp/http] succeeded!\n"
Apr 29 07:24:09.911: INFO: stdout: "nodeport-test-sw6wt"
Apr 29 07:24:09.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8695 exec execpod4r275 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.150 30133'
Apr 29 07:24:10.049: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.150 30133\nConnection to 123.58.176.150 30133 port [tcp/*] succeeded!\n"
Apr 29 07:24:10.049: INFO: stdout: ""
Apr 29 07:24:11.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8695 exec execpod4r275 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.150 30133'
Apr 29 07:24:11.214: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.150 30133\nConnection to 123.58.176.150 30133 port [tcp/*] succeeded!\n"
Apr 29 07:24:11.214: INFO: stdout: "nodeport-test-dkknr"
Apr 29 07:24:11.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-8695 exec execpod4r275 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.151 30133'
Apr 29 07:24:11.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.151 30133\nConnection to 123.58.176.151 30133 port [tcp/*] succeeded!\n"
Apr 29 07:24:11.366: INFO: stdout: "nodeport-test-dkknr"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:24:11.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8695" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:14.162 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":165,"skipped":3099,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:24:11.377: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 07:24:12.030: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 07:24:15.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:24:15.065: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1852-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:24:18.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8779" for this suite.
STEP: Destroying namespace "webhook-8779-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.931 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":166,"skipped":3182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:24:18.309: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Apr 29 07:24:29.073: INFO: 71 pods remaining
Apr 29 07:24:29.073: INFO: 71 pods has nil DeletionTimestamp
Apr 29 07:24:29.073: INFO: 
STEP: Gathering metrics
Apr 29 07:24:34.401: INFO: The status of Pod kube-controller-manager-localhost.localdomain is Running (Ready = true)
E0429 07:24:34.804383      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:24:40.969606      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:24:43.023385      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:24:44.050753      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:24:45.075937      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:24:46.101183      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:24:47.133069      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:24:48.161185      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:24:49.186807      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:24:51.249154      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:24:54.331694      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:24:55.358370      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:24:59.153934      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:00.181713      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:04.297102      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:05.324394      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:07.381071      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:08.103560      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:09.136690      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:10.163530      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:11.192732      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:14.277070      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:15.301350      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:16.340855      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:17.370545      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:19.106607      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:20.130831      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:21.161379      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:22.221187      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:23.250246      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:24.279953      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:25.308969      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:26.333753      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:28.393040      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:29.420865      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:32.156581      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:34.211985      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:36.266151      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:37.291357      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:39.366694      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:40.392881      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:41.104833      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:42.130884      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:43.155398      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:44.185085      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:45.212527      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:47.272697      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:48.298719      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:49.328219      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:50.356824      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:54.522366      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:55.559880      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:56.584510      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:58.641387      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:25:59.666581      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:26:00.691101      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:26:01.716361      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
Apr 29 07:26:01.717: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Apr 29 07:26:01.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rtmh" in namespace "gc-7059"
Apr 29 07:26:01.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-495zz" in namespace "gc-7059"
Apr 29 07:26:01.739: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rkrm" in namespace "gc-7059"
Apr 29 07:26:01.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lvjk" in namespace "gc-7059"
Apr 29 07:26:01.764: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nr67" in namespace "gc-7059"
Apr 29 07:26:01.774: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rbcd" in namespace "gc-7059"
Apr 29 07:26:01.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rm59" in namespace "gc-7059"
Apr 29 07:26:01.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-5s2h2" in namespace "gc-7059"
Apr 29 07:26:01.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-652kl" in namespace "gc-7059"
Apr 29 07:26:01.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dd2v" in namespace "gc-7059"
Apr 29 07:26:01.901: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p6m5" in namespace "gc-7059"
Apr 29 07:26:01.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qqwv" in namespace "gc-7059"
Apr 29 07:26:01.942: INFO: Deleting pod "simpletest-rc-to-be-deleted-99f45" in namespace "gc-7059"
Apr 29 07:26:01.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pzzt" in namespace "gc-7059"
Apr 29 07:26:01.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-9q8kl" in namespace "gc-7059"
Apr 29 07:26:01.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-b69ch" in namespace "gc-7059"
Apr 29 07:26:01.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-b825m" in namespace "gc-7059"
Apr 29 07:26:02.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-bd9xm" in namespace "gc-7059"
Apr 29 07:26:02.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdfb4" in namespace "gc-7059"
Apr 29 07:26:02.060: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdfxj" in namespace "gc-7059"
Apr 29 07:26:02.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwhg4" in namespace "gc-7059"
Apr 29 07:26:02.106: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzgxv" in namespace "gc-7059"
Apr 29 07:26:02.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2ch6" in namespace "gc-7059"
Apr 29 07:26:02.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5s99" in namespace "gc-7059"
Apr 29 07:26:02.161: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8ptt" in namespace "gc-7059"
Apr 29 07:26:02.174: INFO: Deleting pod "simpletest-rc-to-be-deleted-cc9qv" in namespace "gc-7059"
Apr 29 07:26:02.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-cm9tw" in namespace "gc-7059"
Apr 29 07:26:02.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-cp9k9" in namespace "gc-7059"
Apr 29 07:26:02.231: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctxjd" in namespace "gc-7059"
Apr 29 07:26:02.257: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6z4g" in namespace "gc-7059"
Apr 29 07:26:02.277: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkcl6" in namespace "gc-7059"
Apr 29 07:26:02.295: INFO: Deleting pod "simpletest-rc-to-be-deleted-dp647" in namespace "gc-7059"
Apr 29 07:26:02.310: INFO: Deleting pod "simpletest-rc-to-be-deleted-ds2sl" in namespace "gc-7059"
Apr 29 07:26:02.335: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxjw6" in namespace "gc-7059"
Apr 29 07:26:02.347: INFO: Deleting pod "simpletest-rc-to-be-deleted-fh7cc" in namespace "gc-7059"
Apr 29 07:26:02.381: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpztx" in namespace "gc-7059"
Apr 29 07:26:02.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4qhf" in namespace "gc-7059"
Apr 29 07:26:02.410: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8k49" in namespace "gc-7059"
Apr 29 07:26:02.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8x9t" in namespace "gc-7059"
Apr 29 07:26:02.445: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdpdh" in namespace "gc-7059"
Apr 29 07:26:02.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-gv6z4" in namespace "gc-7059"
Apr 29 07:26:02.474: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxqtz" in namespace "gc-7059"
Apr 29 07:26:02.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8dk4" in namespace "gc-7059"
Apr 29 07:26:02.513: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnzmr" in namespace "gc-7059"
Apr 29 07:26:02.529: INFO: Deleting pod "simpletest-rc-to-be-deleted-hv2hp" in namespace "gc-7059"
Apr 29 07:26:02.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5mt6" in namespace "gc-7059"
Apr 29 07:26:02.643: INFO: Deleting pod "simpletest-rc-to-be-deleted-j98d5" in namespace "gc-7059"
Apr 29 07:26:02.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-jf7nz" in namespace "gc-7059"
Apr 29 07:26:02.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-k6jb4" in namespace "gc-7059"
Apr 29 07:26:02.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-kb2m2" in namespace "gc-7059"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:26:02.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7059" for this suite.

• [SLOW TEST:104.486 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":167,"skipped":3224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:26:02.795: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-40e65f6f-8568-46c6-97bc-732d0422dd14
STEP: Creating a pod to test consume secrets
Apr 29 07:26:02.958: INFO: Waiting up to 5m0s for pod "pod-secrets-ee5851a2-1cb1-4a90-9f7e-f964ebb651a6" in namespace "secrets-7170" to be "Succeeded or Failed"
Apr 29 07:26:02.965: INFO: Pod "pod-secrets-ee5851a2-1cb1-4a90-9f7e-f964ebb651a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.883499ms
Apr 29 07:26:04.969: INFO: Pod "pod-secrets-ee5851a2-1cb1-4a90-9f7e-f964ebb651a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010841545s
Apr 29 07:26:06.973: INFO: Pod "pod-secrets-ee5851a2-1cb1-4a90-9f7e-f964ebb651a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014955237s
STEP: Saw pod success
Apr 29 07:26:06.973: INFO: Pod "pod-secrets-ee5851a2-1cb1-4a90-9f7e-f964ebb651a6" satisfied condition "Succeeded or Failed"
Apr 29 07:26:06.976: INFO: Trying to get logs from node localhost.localdomain151 pod pod-secrets-ee5851a2-1cb1-4a90-9f7e-f964ebb651a6 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 07:26:07.001: INFO: Waiting for pod pod-secrets-ee5851a2-1cb1-4a90-9f7e-f964ebb651a6 to disappear
Apr 29 07:26:07.005: INFO: Pod pod-secrets-ee5851a2-1cb1-4a90-9f7e-f964ebb651a6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:26:07.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7170" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":168,"skipped":3249,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:26:07.013: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-eb6eddeb-cadb-4065-a9cd-e7da09221043
STEP: Creating a pod to test consume configMaps
Apr 29 07:26:07.063: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ef8cc90-29e7-44a4-a447-c5bf993416cc" in namespace "projected-1574" to be "Succeeded or Failed"
Apr 29 07:26:07.068: INFO: Pod "pod-projected-configmaps-0ef8cc90-29e7-44a4-a447-c5bf993416cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.744877ms
Apr 29 07:26:09.071: INFO: Pod "pod-projected-configmaps-0ef8cc90-29e7-44a4-a447-c5bf993416cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008053134s
Apr 29 07:26:11.076: INFO: Pod "pod-projected-configmaps-0ef8cc90-29e7-44a4-a447-c5bf993416cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013073381s
Apr 29 07:26:13.080: INFO: Pod "pod-projected-configmaps-0ef8cc90-29e7-44a4-a447-c5bf993416cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017268423s
STEP: Saw pod success
Apr 29 07:26:13.080: INFO: Pod "pod-projected-configmaps-0ef8cc90-29e7-44a4-a447-c5bf993416cc" satisfied condition "Succeeded or Failed"
Apr 29 07:26:13.083: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-configmaps-0ef8cc90-29e7-44a4-a447-c5bf993416cc container agnhost-container: <nil>
STEP: delete the pod
Apr 29 07:26:13.096: INFO: Waiting for pod pod-projected-configmaps-0ef8cc90-29e7-44a4-a447-c5bf993416cc to disappear
Apr 29 07:26:13.098: INFO: Pod pod-projected-configmaps-0ef8cc90-29e7-44a4-a447-c5bf993416cc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:26:13.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1574" for this suite.

• [SLOW TEST:6.093 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":169,"skipped":3258,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:26:13.106: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 07:26:13.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d0274c5-0af4-481f-8298-b284d37c42cc" in namespace "projected-3950" to be "Succeeded or Failed"
Apr 29 07:26:13.154: INFO: Pod "downwardapi-volume-1d0274c5-0af4-481f-8298-b284d37c42cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.369563ms
Apr 29 07:26:15.160: INFO: Pod "downwardapi-volume-1d0274c5-0af4-481f-8298-b284d37c42cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008099587s
Apr 29 07:26:17.165: INFO: Pod "downwardapi-volume-1d0274c5-0af4-481f-8298-b284d37c42cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012807336s
STEP: Saw pod success
Apr 29 07:26:17.165: INFO: Pod "downwardapi-volume-1d0274c5-0af4-481f-8298-b284d37c42cc" satisfied condition "Succeeded or Failed"
Apr 29 07:26:17.168: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-1d0274c5-0af4-481f-8298-b284d37c42cc container client-container: <nil>
STEP: delete the pod
Apr 29 07:26:17.184: INFO: Waiting for pod downwardapi-volume-1d0274c5-0af4-481f-8298-b284d37c42cc to disappear
Apr 29 07:26:17.187: INFO: Pod downwardapi-volume-1d0274c5-0af4-481f-8298-b284d37c42cc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:26:17.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3950" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":170,"skipped":3263,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:26:17.196: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-3408
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3408 to expose endpoints map[]
Apr 29 07:26:17.258: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Apr 29 07:26:18.265: INFO: successfully validated that service multi-endpoint-test in namespace services-3408 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3408
Apr 29 07:26:18.277: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:26:20.283: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3408 to expose endpoints map[pod1:[100]]
Apr 29 07:26:20.293: INFO: successfully validated that service multi-endpoint-test in namespace services-3408 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3408
Apr 29 07:26:20.310: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:26:22.314: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3408 to expose endpoints map[pod1:[100] pod2:[101]]
Apr 29 07:26:22.326: INFO: successfully validated that service multi-endpoint-test in namespace services-3408 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Apr 29 07:26:22.326: INFO: Creating new exec pod
Apr 29 07:26:25.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-3408 exec execpodn6mpm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Apr 29 07:26:25.480: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Apr 29 07:26:25.480: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:26:25.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-3408 exec execpodn6mpm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.230.226 80'
Apr 29 07:26:25.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.230.226 80\nConnection to 10.110.230.226 80 port [tcp/http] succeeded!\n"
Apr 29 07:26:25.619: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:26:25.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-3408 exec execpodn6mpm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Apr 29 07:26:25.757: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Apr 29 07:26:25.757: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:26:25.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-3408 exec execpodn6mpm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.230.226 81'
Apr 29 07:26:25.887: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.230.226 81\nConnection to 10.110.230.226 81 port [tcp/*] succeeded!\n"
Apr 29 07:26:25.887: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3408
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3408 to expose endpoints map[pod2:[101]]
Apr 29 07:26:25.916: INFO: successfully validated that service multi-endpoint-test in namespace services-3408 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3408
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3408 to expose endpoints map[]
Apr 29 07:26:26.957: INFO: successfully validated that service multi-endpoint-test in namespace services-3408 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:26:26.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3408" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.799 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":171,"skipped":3281,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:26:26.995: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:26:27.035: INFO: Got root ca configmap in namespace "svcaccounts-6965"
Apr 29 07:26:27.041: INFO: Deleted root ca configmap in namespace "svcaccounts-6965"
STEP: waiting for a new root ca configmap created
Apr 29 07:26:27.545: INFO: Recreated root ca configmap in namespace "svcaccounts-6965"
Apr 29 07:26:27.549: INFO: Updated root ca configmap in namespace "svcaccounts-6965"
STEP: waiting for the root ca configmap reconciled
Apr 29 07:26:28.053: INFO: Reconciled root ca configmap in namespace "svcaccounts-6965"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:26:28.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6965" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":172,"skipped":3282,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:26:28.062: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:26:28.097: INFO: The status of Pod busybox-host-aliasesab52f783-915b-4e19-8ba3-5d76a7107314 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:26:30.101: INFO: The status of Pod busybox-host-aliasesab52f783-915b-4e19-8ba3-5d76a7107314 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:26:30.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5275" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":173,"skipped":3290,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:26:30.119: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-f76b2ce5-6b29-4afa-b478-0ada9441adee
STEP: Creating a pod to test consume secrets
Apr 29 07:26:30.256: INFO: Waiting up to 5m0s for pod "pod-secrets-36c23933-fef5-4ab9-9a5a-56bd62afbd5e" in namespace "secrets-5265" to be "Succeeded or Failed"
Apr 29 07:26:30.260: INFO: Pod "pod-secrets-36c23933-fef5-4ab9-9a5a-56bd62afbd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.051145ms
Apr 29 07:26:32.263: INFO: Pod "pod-secrets-36c23933-fef5-4ab9-9a5a-56bd62afbd5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006371783s
STEP: Saw pod success
Apr 29 07:26:32.263: INFO: Pod "pod-secrets-36c23933-fef5-4ab9-9a5a-56bd62afbd5e" satisfied condition "Succeeded or Failed"
Apr 29 07:26:32.265: INFO: Trying to get logs from node localhost.localdomain151 pod pod-secrets-36c23933-fef5-4ab9-9a5a-56bd62afbd5e container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 07:26:32.296: INFO: Waiting for pod pod-secrets-36c23933-fef5-4ab9-9a5a-56bd62afbd5e to disappear
Apr 29 07:26:32.298: INFO: Pod pod-secrets-36c23933-fef5-4ab9-9a5a-56bd62afbd5e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:26:32.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5265" for this suite.
STEP: Destroying namespace "secret-namespace-4242" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":174,"skipped":3370,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:26:32.312: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:26:43.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6497" for this suite.

• [SLOW TEST:11.088 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":175,"skipped":3379,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:26:43.401: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-c14f44dd-c7c5-4479-ab14-74c3503e15e4
STEP: Creating a pod to test consume configMaps
Apr 29 07:26:43.457: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64b4c7ae-3c78-46c1-b4d2-9a4c4ee1faf5" in namespace "projected-645" to be "Succeeded or Failed"
Apr 29 07:26:43.461: INFO: Pod "pod-projected-configmaps-64b4c7ae-3c78-46c1-b4d2-9a4c4ee1faf5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.949226ms
Apr 29 07:26:45.469: INFO: Pod "pod-projected-configmaps-64b4c7ae-3c78-46c1-b4d2-9a4c4ee1faf5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012423581s
STEP: Saw pod success
Apr 29 07:26:45.470: INFO: Pod "pod-projected-configmaps-64b4c7ae-3c78-46c1-b4d2-9a4c4ee1faf5" satisfied condition "Succeeded or Failed"
Apr 29 07:26:45.473: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-configmaps-64b4c7ae-3c78-46c1-b4d2-9a4c4ee1faf5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 07:26:45.494: INFO: Waiting for pod pod-projected-configmaps-64b4c7ae-3c78-46c1-b4d2-9a4c4ee1faf5 to disappear
Apr 29 07:26:45.497: INFO: Pod pod-projected-configmaps-64b4c7ae-3c78-46c1-b4d2-9a4c4ee1faf5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:26:45.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-645" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":176,"skipped":3394,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:26:45.506: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Apr 29 07:26:45.647: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 07:26:50.590: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:27:05.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5397" for this suite.

• [SLOW TEST:20.501 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":177,"skipped":3398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:27:06.007: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:27:19.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8209" for this suite.
STEP: Destroying namespace "nsdeletetest-2258" for this suite.
Apr 29 07:27:19.206: INFO: Namespace nsdeletetest-2258 was already deleted
STEP: Destroying namespace "nsdeletetest-3810" for this suite.

• [SLOW TEST:13.205 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":178,"skipped":3443,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:27:19.212: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Apr 29 07:27:19.251: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:27:43.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7456" for this suite.

• [SLOW TEST:24.179 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":179,"skipped":3452,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:27:43.392: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:27:59.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3475" for this suite.

• [SLOW TEST:16.132 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":180,"skipped":3465,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:27:59.525: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:27:59.559: INFO: Creating deployment "test-recreate-deployment"
Apr 29 07:27:59.566: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 29 07:27:59.571: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr 29 07:28:01.578: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 29 07:28:01.580: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 29 07:28:01.588: INFO: Updating deployment test-recreate-deployment
Apr 29 07:28:01.588: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr 29 07:28:01.641: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4882  00a14f42-72b9-43f9-a43d-eff5455f83f0 111968 2 2022-04-29 07:27:59 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-04-29 07:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 07:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a9878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-04-29 07:28:01 +0000 UTC,LastTransitionTime:2022-04-29 07:28:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-04-29 07:28:01 +0000 UTC,LastTransitionTime:2022-04-29 07:27:59 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr 29 07:28:01.644: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-4882  a190ecaa-8b09-4273-b9b0-cc347b9fcbaf 111965 1 2022-04-29 07:28:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 00a14f42-72b9-43f9-a43d-eff5455f83f0 0xc0051a9c87 0xc0051a9c88}] []  [{kube-controller-manager Update apps/v1 2022-04-29 07:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00a14f42-72b9-43f9-a43d-eff5455f83f0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 07:28:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a9d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 07:28:01.644: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 29 07:28:01.644: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d659f7dc9  deployment-4882  ae3cebcb-8e55-44d9-853b-3becf7114265 111958 2 2022-04-29 07:27:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 00a14f42-72b9-43f9-a43d-eff5455f83f0 0xc0051a9dc7 0xc0051a9dc8}] []  [{kube-controller-manager Update apps/v1 2022-04-29 07:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00a14f42-72b9-43f9-a43d-eff5455f83f0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 07:28:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d659f7dc9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a9e78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 07:28:01.647: INFO: Pod "test-recreate-deployment-5b99bd5487-ftq7d" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-ftq7d test-recreate-deployment-5b99bd5487- deployment-4882  6b95b89a-5ef5-4b02-8071-de4a326a2283 111970 0 2022-04-29 07:28:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 a190ecaa-8b09-4273-b9b0-cc347b9fcbaf 0xc0052700c7 0xc0052700c8}] []  [{Go-http-client Update v1 2022-04-29 07:28:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-04-29 07:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a190ecaa-8b09-4273-b9b0-cc347b9fcbaf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qgsp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qgsp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:28:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:,StartTime:2022-04-29 07:28:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:28:01.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4882" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":181,"skipped":3466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:28:01.656: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:28:03.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5569" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3530,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:28:03.716: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Apr 29 07:28:03.752: INFO: Waiting up to 5m0s for pod "var-expansion-b91c78df-4156-4277-8e28-db71be7f0f3f" in namespace "var-expansion-6864" to be "Succeeded or Failed"
Apr 29 07:28:03.756: INFO: Pod "var-expansion-b91c78df-4156-4277-8e28-db71be7f0f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.53715ms
Apr 29 07:28:05.761: INFO: Pod "var-expansion-b91c78df-4156-4277-8e28-db71be7f0f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008695362s
STEP: Saw pod success
Apr 29 07:28:05.761: INFO: Pod "var-expansion-b91c78df-4156-4277-8e28-db71be7f0f3f" satisfied condition "Succeeded or Failed"
Apr 29 07:28:05.763: INFO: Trying to get logs from node localhost.localdomain151 pod var-expansion-b91c78df-4156-4277-8e28-db71be7f0f3f container dapi-container: <nil>
STEP: delete the pod
Apr 29 07:28:05.781: INFO: Waiting for pod var-expansion-b91c78df-4156-4277-8e28-db71be7f0f3f to disappear
Apr 29 07:28:05.783: INFO: Pod var-expansion-b91c78df-4156-4277-8e28-db71be7f0f3f no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:28:05.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6864" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":183,"skipped":3541,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:28:05.793: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:28:05.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3269" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":184,"skipped":3586,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:28:05.842: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Apr 29 07:28:05.960: INFO: The status of Pod labelsupdate1cae557a-618f-4932-bc19-02f4e1662aca is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:28:07.965: INFO: The status of Pod labelsupdate1cae557a-618f-4932-bc19-02f4e1662aca is Running (Ready = true)
Apr 29 07:28:08.487: INFO: Successfully updated pod "labelsupdate1cae557a-618f-4932-bc19-02f4e1662aca"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:28:10.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3986" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":185,"skipped":3597,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:28:10.507: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 07:28:11.081: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 07:28:14.108: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:28:14.113: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1816-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:28:17.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7185" for this suite.
STEP: Destroying namespace "webhook-7185-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.734 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":186,"skipped":3614,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:28:17.242: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 29 07:28:17.287: INFO: Waiting up to 5m0s for pod "pod-aea6fc9a-5da7-4439-9b9c-946e3cd73b19" in namespace "emptydir-1735" to be "Succeeded or Failed"
Apr 29 07:28:17.290: INFO: Pod "pod-aea6fc9a-5da7-4439-9b9c-946e3cd73b19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.964245ms
Apr 29 07:28:19.295: INFO: Pod "pod-aea6fc9a-5da7-4439-9b9c-946e3cd73b19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008823974s
STEP: Saw pod success
Apr 29 07:28:19.295: INFO: Pod "pod-aea6fc9a-5da7-4439-9b9c-946e3cd73b19" satisfied condition "Succeeded or Failed"
Apr 29 07:28:19.300: INFO: Trying to get logs from node localhost.localdomain151 pod pod-aea6fc9a-5da7-4439-9b9c-946e3cd73b19 container test-container: <nil>
STEP: delete the pod
Apr 29 07:28:19.321: INFO: Waiting for pod pod-aea6fc9a-5da7-4439-9b9c-946e3cd73b19 to disappear
Apr 29 07:28:19.323: INFO: Pod pod-aea6fc9a-5da7-4439-9b9c-946e3cd73b19 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:28:19.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1735" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":187,"skipped":3622,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:28:19.333: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Apr 29 07:28:19.371: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Apr 29 07:28:19.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6975 create -f -'
Apr 29 07:28:20.666: INFO: stderr: ""
Apr 29 07:28:20.666: INFO: stdout: "service/agnhost-replica created\n"
Apr 29 07:28:20.666: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Apr 29 07:28:20.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6975 create -f -'
Apr 29 07:28:20.864: INFO: stderr: ""
Apr 29 07:28:20.864: INFO: stdout: "service/agnhost-primary created\n"
Apr 29 07:28:20.864: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 29 07:28:20.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6975 create -f -'
Apr 29 07:28:21.084: INFO: stderr: ""
Apr 29 07:28:21.084: INFO: stdout: "service/frontend created\n"
Apr 29 07:28:21.084: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Apr 29 07:28:21.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6975 create -f -'
Apr 29 07:28:21.299: INFO: stderr: ""
Apr 29 07:28:21.299: INFO: stdout: "deployment.apps/frontend created\n"
Apr 29 07:28:21.300: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 29 07:28:21.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6975 create -f -'
Apr 29 07:28:21.520: INFO: stderr: ""
Apr 29 07:28:21.520: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Apr 29 07:28:21.520: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 29 07:28:21.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6975 create -f -'
Apr 29 07:28:21.719: INFO: stderr: ""
Apr 29 07:28:21.719: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Apr 29 07:28:21.719: INFO: Waiting for all frontend pods to be Running.
Apr 29 07:28:26.771: INFO: Waiting for frontend to serve content.
Apr 29 07:28:26.781: INFO: Trying to add a new entry to the guestbook.
Apr 29 07:28:26.793: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr 29 07:28:26.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6975 delete --grace-period=0 --force -f -'
Apr 29 07:28:26.884: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 07:28:26.884: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 07:28:26.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6975 delete --grace-period=0 --force -f -'
Apr 29 07:28:26.996: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 07:28:26.996: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 07:28:26.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6975 delete --grace-period=0 --force -f -'
Apr 29 07:28:27.075: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 07:28:27.075: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 07:28:27.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6975 delete --grace-period=0 --force -f -'
Apr 29 07:28:27.142: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 07:28:27.142: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 07:28:27.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6975 delete --grace-period=0 --force -f -'
Apr 29 07:28:27.225: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 07:28:27.225: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 07:28:27.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6975 delete --grace-period=0 --force -f -'
Apr 29 07:28:27.299: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 07:28:27.299: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:28:27.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6975" for this suite.

• [SLOW TEST:7.986 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":188,"skipped":3664,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:28:27.320: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2298
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2298
STEP: Waiting until pod test-pod will start running in namespace statefulset-2298
STEP: Creating statefulset with conflicting port in namespace statefulset-2298
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2298
Apr 29 07:28:29.391: INFO: Observed stateful pod in namespace: statefulset-2298, name: ss-0, uid: cf45a34d-0a62-486f-9ef4-a4b6a543314c, status phase: Pending. Waiting for statefulset controller to delete.
Apr 29 07:28:29.404: INFO: Observed stateful pod in namespace: statefulset-2298, name: ss-0, uid: cf45a34d-0a62-486f-9ef4-a4b6a543314c, status phase: Failed. Waiting for statefulset controller to delete.
Apr 29 07:28:29.409: INFO: Observed stateful pod in namespace: statefulset-2298, name: ss-0, uid: cf45a34d-0a62-486f-9ef4-a4b6a543314c, status phase: Failed. Waiting for statefulset controller to delete.
Apr 29 07:28:29.412: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2298
STEP: Removing pod with conflicting port in namespace statefulset-2298
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2298 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Apr 29 07:28:31.431: INFO: Deleting all statefulset in ns statefulset-2298
Apr 29 07:28:31.434: INFO: Scaling statefulset ss to 0
Apr 29 07:28:41.449: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 07:28:41.452: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:28:41.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2298" for this suite.

• [SLOW TEST:14.157 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":189,"skipped":3673,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:28:41.477: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:34:01.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3960" for this suite.

• [SLOW TEST:320.098 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":190,"skipped":3681,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:34:01.575: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-str6
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 07:34:01.834: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-str6" in namespace "subpath-1644" to be "Succeeded or Failed"
Apr 29 07:34:01.837: INFO: Pod "pod-subpath-test-projected-str6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.270064ms
Apr 29 07:34:03.843: INFO: Pod "pod-subpath-test-projected-str6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008351497s
Apr 29 07:34:05.848: INFO: Pod "pod-subpath-test-projected-str6": Phase="Running", Reason="", readiness=true. Elapsed: 4.013823267s
Apr 29 07:34:07.853: INFO: Pod "pod-subpath-test-projected-str6": Phase="Running", Reason="", readiness=true. Elapsed: 6.018638948s
Apr 29 07:34:09.858: INFO: Pod "pod-subpath-test-projected-str6": Phase="Running", Reason="", readiness=true. Elapsed: 8.023847429s
Apr 29 07:34:11.865: INFO: Pod "pod-subpath-test-projected-str6": Phase="Running", Reason="", readiness=true. Elapsed: 10.030510775s
Apr 29 07:34:13.870: INFO: Pod "pod-subpath-test-projected-str6": Phase="Running", Reason="", readiness=true. Elapsed: 12.035432484s
Apr 29 07:34:15.874: INFO: Pod "pod-subpath-test-projected-str6": Phase="Running", Reason="", readiness=true. Elapsed: 14.039479598s
Apr 29 07:34:17.882: INFO: Pod "pod-subpath-test-projected-str6": Phase="Running", Reason="", readiness=true. Elapsed: 16.047448015s
Apr 29 07:34:19.888: INFO: Pod "pod-subpath-test-projected-str6": Phase="Running", Reason="", readiness=true. Elapsed: 18.053554001s
Apr 29 07:34:21.894: INFO: Pod "pod-subpath-test-projected-str6": Phase="Running", Reason="", readiness=true. Elapsed: 20.059886266s
Apr 29 07:34:23.898: INFO: Pod "pod-subpath-test-projected-str6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.064020871s
STEP: Saw pod success
Apr 29 07:34:23.898: INFO: Pod "pod-subpath-test-projected-str6" satisfied condition "Succeeded or Failed"
Apr 29 07:34:23.900: INFO: Trying to get logs from node localhost.localdomain151 pod pod-subpath-test-projected-str6 container test-container-subpath-projected-str6: <nil>
STEP: delete the pod
Apr 29 07:34:23.933: INFO: Waiting for pod pod-subpath-test-projected-str6 to disappear
Apr 29 07:34:23.935: INFO: Pod pod-subpath-test-projected-str6 no longer exists
STEP: Deleting pod pod-subpath-test-projected-str6
Apr 29 07:34:23.935: INFO: Deleting pod "pod-subpath-test-projected-str6" in namespace "subpath-1644"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:34:23.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1644" for this suite.

• [SLOW TEST:22.370 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":191,"skipped":3701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:34:23.946: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:34:26.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5427" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":192,"skipped":3754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:34:26.048: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1537
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Apr 29 07:34:26.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6179 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Apr 29 07:34:26.234: INFO: stderr: ""
Apr 29 07:34:26.234: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
Apr 29 07:34:26.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-6179 delete pods e2e-test-httpd-pod'
Apr 29 07:34:28.263: INFO: stderr: ""
Apr 29 07:34:28.263: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:34:28.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6179" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":193,"skipped":3776,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:34:28.273: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5271
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 07:34:28.529: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 29 07:34:28.564: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:34:30.571: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:34:32.569: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:34:34.569: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:34:36.568: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:34:38.569: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:34:40.569: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:34:42.569: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:34:44.568: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:34:46.568: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:34:48.569: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:34:50.568: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 29 07:34:50.573: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Apr 29 07:34:52.590: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Apr 29 07:34:52.590: INFO: Breadth first check of 172.16.102.175 on host 123.58.176.150...
Apr 29 07:34:52.593: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.115.187:9080/dial?request=hostname&protocol=http&host=172.16.102.175&port=8083&tries=1'] Namespace:pod-network-test-5271 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 07:34:52.593: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 07:34:52.593: INFO: ExecWithOptions: Clientset creation
Apr 29 07:34:52.593: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5271/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.115.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.102.175%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Apr 29 07:34:52.674: INFO: Waiting for responses: map[]
Apr 29 07:34:52.674: INFO: reached 172.16.102.175 after 0/1 tries
Apr 29 07:34:52.674: INFO: Breadth first check of 172.16.115.161 on host 123.58.176.151...
Apr 29 07:34:52.678: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.115.187:9080/dial?request=hostname&protocol=http&host=172.16.115.161&port=8083&tries=1'] Namespace:pod-network-test-5271 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 07:34:52.678: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 07:34:52.678: INFO: ExecWithOptions: Clientset creation
Apr 29 07:34:52.678: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5271/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.115.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.115.161%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Apr 29 07:34:52.741: INFO: Waiting for responses: map[]
Apr 29 07:34:52.741: INFO: reached 172.16.115.161 after 0/1 tries
Apr 29 07:34:52.741: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:34:52.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5271" for this suite.

• [SLOW TEST:24.479 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":194,"skipped":3793,"failed":0}
SS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:34:52.752: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-dc915658-5407-440d-ae5e-c69406e4c983
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:34:52.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2888" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":195,"skipped":3795,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:34:52.824: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-226
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-226
STEP: creating replication controller externalsvc in namespace services-226
I0429 07:34:52.962482      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-226, replica count: 2
I0429 07:34:56.013133      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Apr 29 07:34:56.043: INFO: Creating new exec pod
Apr 29 07:34:58.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-226 exec execpodsqnpb -- /bin/sh -x -c nslookup nodeport-service.services-226.svc.cluster.local'
Apr 29 07:34:58.573: INFO: stderr: "+ nslookup nodeport-service.services-226.svc.cluster.local\n"
Apr 29 07:34:58.573: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-226.svc.cluster.local\tcanonical name = externalsvc.services-226.svc.cluster.local.\nName:\texternalsvc.services-226.svc.cluster.local\nAddress: 10.110.120.101\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-226, will wait for the garbage collector to delete the pods
Apr 29 07:34:58.633: INFO: Deleting ReplicationController externalsvc took: 6.581011ms
Apr 29 07:34:58.734: INFO: Terminating ReplicationController externalsvc pods took: 100.416395ms
Apr 29 07:35:00.548: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:00.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-226" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.760 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":196,"skipped":3799,"failed":0}
S
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:00.584: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Apr 29 07:35:00.638: INFO: Waiting up to 5m0s for pod "client-containers-0494f8f3-9fad-496b-bf8b-8d0d9ec92f56" in namespace "containers-1125" to be "Succeeded or Failed"
Apr 29 07:35:00.641: INFO: Pod "client-containers-0494f8f3-9fad-496b-bf8b-8d0d9ec92f56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407519ms
Apr 29 07:35:02.645: INFO: Pod "client-containers-0494f8f3-9fad-496b-bf8b-8d0d9ec92f56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006532657s
STEP: Saw pod success
Apr 29 07:35:02.645: INFO: Pod "client-containers-0494f8f3-9fad-496b-bf8b-8d0d9ec92f56" satisfied condition "Succeeded or Failed"
Apr 29 07:35:02.647: INFO: Trying to get logs from node localhost.localdomain151 pod client-containers-0494f8f3-9fad-496b-bf8b-8d0d9ec92f56 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 07:35:02.661: INFO: Waiting for pod client-containers-0494f8f3-9fad-496b-bf8b-8d0d9ec92f56 to disappear
Apr 29 07:35:02.662: INFO: Pod client-containers-0494f8f3-9fad-496b-bf8b-8d0d9ec92f56 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:02.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1125" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":197,"skipped":3800,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:02.670: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Apr 29 07:35:02.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-4375 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Apr 29 07:35:03.000: INFO: stderr: ""
Apr 29 07:35:03.000: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Apr 29 07:35:03.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-4375 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Apr 29 07:35:03.191: INFO: stderr: ""
Apr 29 07:35:03.191: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Apr 29 07:35:03.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-4375 delete pods e2e-test-httpd-pod'
Apr 29 07:35:05.355: INFO: stderr: ""
Apr 29 07:35:05.355: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:05.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4375" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":198,"skipped":3803,"failed":0}
S
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:05.364: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:35:05.871: INFO: Checking APIGroup: apiregistration.k8s.io
Apr 29 07:35:05.873: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Apr 29 07:35:05.873: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Apr 29 07:35:05.873: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Apr 29 07:35:05.873: INFO: Checking APIGroup: apps
Apr 29 07:35:05.874: INFO: PreferredVersion.GroupVersion: apps/v1
Apr 29 07:35:05.874: INFO: Versions found [{apps/v1 v1}]
Apr 29 07:35:05.874: INFO: apps/v1 matches apps/v1
Apr 29 07:35:05.874: INFO: Checking APIGroup: events.k8s.io
Apr 29 07:35:05.875: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Apr 29 07:35:05.875: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Apr 29 07:35:05.875: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Apr 29 07:35:05.875: INFO: Checking APIGroup: authentication.k8s.io
Apr 29 07:35:05.876: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Apr 29 07:35:05.876: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Apr 29 07:35:05.876: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Apr 29 07:35:05.876: INFO: Checking APIGroup: authorization.k8s.io
Apr 29 07:35:05.878: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Apr 29 07:35:05.878: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Apr 29 07:35:05.878: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Apr 29 07:35:05.878: INFO: Checking APIGroup: autoscaling
Apr 29 07:35:05.879: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Apr 29 07:35:05.879: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Apr 29 07:35:05.879: INFO: autoscaling/v2 matches autoscaling/v2
Apr 29 07:35:05.879: INFO: Checking APIGroup: batch
Apr 29 07:35:05.880: INFO: PreferredVersion.GroupVersion: batch/v1
Apr 29 07:35:05.880: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Apr 29 07:35:05.880: INFO: batch/v1 matches batch/v1
Apr 29 07:35:05.880: INFO: Checking APIGroup: certificates.k8s.io
Apr 29 07:35:05.881: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Apr 29 07:35:05.881: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Apr 29 07:35:05.881: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Apr 29 07:35:05.881: INFO: Checking APIGroup: networking.k8s.io
Apr 29 07:35:05.882: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Apr 29 07:35:05.882: INFO: Versions found [{networking.k8s.io/v1 v1}]
Apr 29 07:35:05.882: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Apr 29 07:35:05.882: INFO: Checking APIGroup: policy
Apr 29 07:35:05.883: INFO: PreferredVersion.GroupVersion: policy/v1
Apr 29 07:35:05.883: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Apr 29 07:35:05.883: INFO: policy/v1 matches policy/v1
Apr 29 07:35:05.883: INFO: Checking APIGroup: rbac.authorization.k8s.io
Apr 29 07:35:05.884: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Apr 29 07:35:05.884: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Apr 29 07:35:05.884: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Apr 29 07:35:05.884: INFO: Checking APIGroup: storage.k8s.io
Apr 29 07:35:05.885: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Apr 29 07:35:05.885: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Apr 29 07:35:05.885: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Apr 29 07:35:05.885: INFO: Checking APIGroup: admissionregistration.k8s.io
Apr 29 07:35:05.886: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Apr 29 07:35:05.886: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Apr 29 07:35:05.886: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Apr 29 07:35:05.886: INFO: Checking APIGroup: apiextensions.k8s.io
Apr 29 07:35:05.887: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Apr 29 07:35:05.887: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Apr 29 07:35:05.887: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Apr 29 07:35:05.887: INFO: Checking APIGroup: scheduling.k8s.io
Apr 29 07:35:05.888: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Apr 29 07:35:05.888: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Apr 29 07:35:05.888: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Apr 29 07:35:05.888: INFO: Checking APIGroup: coordination.k8s.io
Apr 29 07:35:05.889: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Apr 29 07:35:05.889: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Apr 29 07:35:05.889: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Apr 29 07:35:05.889: INFO: Checking APIGroup: node.k8s.io
Apr 29 07:35:05.890: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Apr 29 07:35:05.890: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Apr 29 07:35:05.890: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Apr 29 07:35:05.890: INFO: Checking APIGroup: discovery.k8s.io
Apr 29 07:35:05.891: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Apr 29 07:35:05.891: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Apr 29 07:35:05.891: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Apr 29 07:35:05.891: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Apr 29 07:35:05.892: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Apr 29 07:35:05.892: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Apr 29 07:35:05.892: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Apr 29 07:35:05.892: INFO: Checking APIGroup: cluster.kubecube.io
Apr 29 07:35:05.893: INFO: PreferredVersion.GroupVersion: cluster.kubecube.io/v1
Apr 29 07:35:05.893: INFO: Versions found [{cluster.kubecube.io/v1 v1}]
Apr 29 07:35:05.893: INFO: cluster.kubecube.io/v1 matches cluster.kubecube.io/v1
Apr 29 07:35:05.893: INFO: Checking APIGroup: crd.projectcalico.org
Apr 29 07:35:05.894: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Apr 29 07:35:05.894: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Apr 29 07:35:05.894: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Apr 29 07:35:05.895: INFO: Checking APIGroup: extension.kubecube.io
Apr 29 07:35:05.895: INFO: PreferredVersion.GroupVersion: extension.kubecube.io/v1
Apr 29 07:35:05.895: INFO: Versions found [{extension.kubecube.io/v1 v1}]
Apr 29 07:35:05.895: INFO: extension.kubecube.io/v1 matches extension.kubecube.io/v1
Apr 29 07:35:05.895: INFO: Checking APIGroup: hotplug.kubecube.io
Apr 29 07:35:05.897: INFO: PreferredVersion.GroupVersion: hotplug.kubecube.io/v1
Apr 29 07:35:05.897: INFO: Versions found [{hotplug.kubecube.io/v1 v1}]
Apr 29 07:35:05.897: INFO: hotplug.kubecube.io/v1 matches hotplug.kubecube.io/v1
Apr 29 07:35:05.897: INFO: Checking APIGroup: monitoring.coreos.com
Apr 29 07:35:05.898: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Apr 29 07:35:05.898: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Apr 29 07:35:05.898: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Apr 29 07:35:05.898: INFO: Checking APIGroup: monitoring.kubecube.io
Apr 29 07:35:05.899: INFO: PreferredVersion.GroupVersion: monitoring.kubecube.io/v1
Apr 29 07:35:05.899: INFO: Versions found [{monitoring.kubecube.io/v1 v1}]
Apr 29 07:35:05.899: INFO: monitoring.kubecube.io/v1 matches monitoring.kubecube.io/v1
Apr 29 07:35:05.899: INFO: Checking APIGroup: netease.com
Apr 29 07:35:05.900: INFO: PreferredVersion.GroupVersion: netease.com/v1
Apr 29 07:35:05.900: INFO: Versions found [{netease.com/v1 v1}]
Apr 29 07:35:05.900: INFO: netease.com/v1 matches netease.com/v1
Apr 29 07:35:05.900: INFO: Checking APIGroup: quota.kubecube.io
Apr 29 07:35:05.901: INFO: PreferredVersion.GroupVersion: quota.kubecube.io/v1
Apr 29 07:35:05.901: INFO: Versions found [{quota.kubecube.io/v1 v1}]
Apr 29 07:35:05.901: INFO: quota.kubecube.io/v1 matches quota.kubecube.io/v1
Apr 29 07:35:05.901: INFO: Checking APIGroup: tenant.kubecube.io
Apr 29 07:35:05.902: INFO: PreferredVersion.GroupVersion: tenant.kubecube.io/v1
Apr 29 07:35:05.902: INFO: Versions found [{tenant.kubecube.io/v1 v1}]
Apr 29 07:35:05.902: INFO: tenant.kubecube.io/v1 matches tenant.kubecube.io/v1
Apr 29 07:35:05.902: INFO: Checking APIGroup: user.kubecube.io
Apr 29 07:35:05.903: INFO: PreferredVersion.GroupVersion: user.kubecube.io/v1
Apr 29 07:35:05.903: INFO: Versions found [{user.kubecube.io/v1 v1}]
Apr 29 07:35:05.903: INFO: user.kubecube.io/v1 matches user.kubecube.io/v1
Apr 29 07:35:05.903: INFO: Checking APIGroup: hnc.x-k8s.io
Apr 29 07:35:05.905: INFO: PreferredVersion.GroupVersion: hnc.x-k8s.io/v1alpha2
Apr 29 07:35:05.905: INFO: Versions found [{hnc.x-k8s.io/v1alpha2 v1alpha2}]
Apr 29 07:35:05.905: INFO: hnc.x-k8s.io/v1alpha2 matches hnc.x-k8s.io/v1alpha2
Apr 29 07:35:05.905: INFO: Checking APIGroup: metrics.k8s.io
Apr 29 07:35:05.906: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Apr 29 07:35:05.906: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Apr 29 07:35:05.906: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:05.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-6009" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":199,"skipped":3804,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:05.918: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-b16cb048-f633-4736-aa12-9139c572f47e
STEP: Creating a pod to test consume configMaps
Apr 29 07:35:05.973: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9b44863e-4ab6-41ec-808b-7b3a249cbe0d" in namespace "projected-6787" to be "Succeeded or Failed"
Apr 29 07:35:05.976: INFO: Pod "pod-projected-configmaps-9b44863e-4ab6-41ec-808b-7b3a249cbe0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.213861ms
Apr 29 07:35:07.979: INFO: Pod "pod-projected-configmaps-9b44863e-4ab6-41ec-808b-7b3a249cbe0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005570819s
STEP: Saw pod success
Apr 29 07:35:07.979: INFO: Pod "pod-projected-configmaps-9b44863e-4ab6-41ec-808b-7b3a249cbe0d" satisfied condition "Succeeded or Failed"
Apr 29 07:35:07.981: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-configmaps-9b44863e-4ab6-41ec-808b-7b3a249cbe0d container agnhost-container: <nil>
STEP: delete the pod
Apr 29 07:35:07.997: INFO: Waiting for pod pod-projected-configmaps-9b44863e-4ab6-41ec-808b-7b3a249cbe0d to disappear
Apr 29 07:35:08.000: INFO: Pod pod-projected-configmaps-9b44863e-4ab6-41ec-808b-7b3a249cbe0d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:08.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6787" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3812,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:08.010: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-f792df21-c786-41c9-9305-ca2333dff7e9
STEP: Creating a pod to test consume configMaps
Apr 29 07:35:08.053: INFO: Waiting up to 5m0s for pod "pod-configmaps-9cf811ae-5e28-4243-a0eb-26f061ca3801" in namespace "configmap-9127" to be "Succeeded or Failed"
Apr 29 07:35:08.055: INFO: Pod "pod-configmaps-9cf811ae-5e28-4243-a0eb-26f061ca3801": Phase="Pending", Reason="", readiness=false. Elapsed: 2.341091ms
Apr 29 07:35:10.061: INFO: Pod "pod-configmaps-9cf811ae-5e28-4243-a0eb-26f061ca3801": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007768064s
STEP: Saw pod success
Apr 29 07:35:10.061: INFO: Pod "pod-configmaps-9cf811ae-5e28-4243-a0eb-26f061ca3801" satisfied condition "Succeeded or Failed"
Apr 29 07:35:10.063: INFO: Trying to get logs from node localhost.localdomain151 pod pod-configmaps-9cf811ae-5e28-4243-a0eb-26f061ca3801 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 07:35:10.079: INFO: Waiting for pod pod-configmaps-9cf811ae-5e28-4243-a0eb-26f061ca3801 to disappear
Apr 29 07:35:10.081: INFO: Pod pod-configmaps-9cf811ae-5e28-4243-a0eb-26f061ca3801 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:10.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9127" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":201,"skipped":3812,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:10.090: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:35:10.123: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:10.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7954" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":202,"skipped":3812,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:10.664: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:35:10.750: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5541
I0429 07:35:10.756463      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5541, replica count: 1
I0429 07:35:11.807938      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 07:35:12.808155      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 07:35:12.934: INFO: Created: latency-svc-r2lph
Apr 29 07:35:12.941: INFO: Got endpoints: latency-svc-r2lph [32.529032ms]
Apr 29 07:35:12.959: INFO: Created: latency-svc-cbbrt
Apr 29 07:35:12.981: INFO: Got endpoints: latency-svc-cbbrt [39.966604ms]
Apr 29 07:35:12.981: INFO: Created: latency-svc-9vmwm
Apr 29 07:35:13.005: INFO: Got endpoints: latency-svc-9vmwm [63.551691ms]
Apr 29 07:35:13.005: INFO: Created: latency-svc-4llwr
Apr 29 07:35:13.020: INFO: Created: latency-svc-rz4w9
Apr 29 07:35:13.020: INFO: Got endpoints: latency-svc-4llwr [78.36281ms]
Apr 29 07:35:13.036: INFO: Got endpoints: latency-svc-rz4w9 [94.852616ms]
Apr 29 07:35:13.038: INFO: Created: latency-svc-p2847
Apr 29 07:35:13.051: INFO: Got endpoints: latency-svc-p2847 [109.055431ms]
Apr 29 07:35:13.051: INFO: Created: latency-svc-nr7q5
Apr 29 07:35:13.054: INFO: Got endpoints: latency-svc-nr7q5 [112.239813ms]
Apr 29 07:35:13.065: INFO: Created: latency-svc-84z4k
Apr 29 07:35:13.081: INFO: Created: latency-svc-mfh4t
Apr 29 07:35:13.081: INFO: Got endpoints: latency-svc-84z4k [139.193295ms]
Apr 29 07:35:13.089: INFO: Got endpoints: latency-svc-mfh4t [147.910615ms]
Apr 29 07:35:13.103: INFO: Created: latency-svc-d7xwr
Apr 29 07:35:13.118: INFO: Got endpoints: latency-svc-d7xwr [176.08387ms]
Apr 29 07:35:13.120: INFO: Created: latency-svc-x4p4g
Apr 29 07:35:13.140: INFO: Created: latency-svc-8w672
Apr 29 07:35:13.140: INFO: Got endpoints: latency-svc-x4p4g [198.635005ms]
Apr 29 07:35:13.159: INFO: Got endpoints: latency-svc-8w672 [217.303201ms]
Apr 29 07:35:13.169: INFO: Created: latency-svc-czptc
Apr 29 07:35:13.173: INFO: Got endpoints: latency-svc-czptc [231.028815ms]
Apr 29 07:35:13.184: INFO: Created: latency-svc-h88zm
Apr 29 07:35:13.187: INFO: Got endpoints: latency-svc-h88zm [245.578459ms]
Apr 29 07:35:13.198: INFO: Created: latency-svc-ndklc
Apr 29 07:35:13.213: INFO: Got endpoints: latency-svc-ndklc [271.016959ms]
Apr 29 07:35:13.213: INFO: Created: latency-svc-jg5zm
Apr 29 07:35:13.224: INFO: Got endpoints: latency-svc-jg5zm [282.002686ms]
Apr 29 07:35:13.237: INFO: Created: latency-svc-dzzmq
Apr 29 07:35:13.241: INFO: Got endpoints: latency-svc-dzzmq [259.679028ms]
Apr 29 07:35:13.253: INFO: Created: latency-svc-fwxzh
Apr 29 07:35:13.260: INFO: Got endpoints: latency-svc-fwxzh [255.303228ms]
Apr 29 07:35:13.271: INFO: Created: latency-svc-kttvx
Apr 29 07:35:13.275: INFO: Got endpoints: latency-svc-kttvx [255.623832ms]
Apr 29 07:35:13.286: INFO: Created: latency-svc-clcvs
Apr 29 07:35:13.290: INFO: Got endpoints: latency-svc-clcvs [253.244771ms]
Apr 29 07:35:13.301: INFO: Created: latency-svc-rdjr2
Apr 29 07:35:13.306: INFO: Got endpoints: latency-svc-rdjr2 [255.463528ms]
Apr 29 07:35:13.315: INFO: Created: latency-svc-d9c7q
Apr 29 07:35:13.330: INFO: Got endpoints: latency-svc-d9c7q [276.155163ms]
Apr 29 07:35:13.330: INFO: Created: latency-svc-f5z4n
Apr 29 07:35:13.355: INFO: Got endpoints: latency-svc-f5z4n [273.588176ms]
Apr 29 07:35:13.367: INFO: Created: latency-svc-95l6q
Apr 29 07:35:13.380: INFO: Got endpoints: latency-svc-95l6q [290.003246ms]
Apr 29 07:35:13.380: INFO: Created: latency-svc-25v48
Apr 29 07:35:13.390: INFO: Got endpoints: latency-svc-25v48 [271.969493ms]
Apr 29 07:35:13.400: INFO: Created: latency-svc-wvw7s
Apr 29 07:35:13.403: INFO: Got endpoints: latency-svc-wvw7s [262.213231ms]
Apr 29 07:35:13.413: INFO: Created: latency-svc-2wpgg
Apr 29 07:35:13.423: INFO: Got endpoints: latency-svc-2wpgg [263.518216ms]
Apr 29 07:35:13.442: INFO: Created: latency-svc-7chzz
Apr 29 07:35:13.447: INFO: Got endpoints: latency-svc-7chzz [273.565392ms]
Apr 29 07:35:13.456: INFO: Created: latency-svc-xdqwt
Apr 29 07:35:13.465: INFO: Got endpoints: latency-svc-xdqwt [277.915334ms]
Apr 29 07:35:13.478: INFO: Created: latency-svc-nj2mf
Apr 29 07:35:13.489: INFO: Got endpoints: latency-svc-nj2mf [276.114057ms]
Apr 29 07:35:13.498: INFO: Created: latency-svc-rf87f
Apr 29 07:35:13.507: INFO: Got endpoints: latency-svc-rf87f [283.388434ms]
Apr 29 07:35:13.517: INFO: Created: latency-svc-8jsl4
Apr 29 07:35:13.527: INFO: Got endpoints: latency-svc-8jsl4 [286.105094ms]
Apr 29 07:35:13.542: INFO: Created: latency-svc-j9z49
Apr 29 07:35:13.547: INFO: Got endpoints: latency-svc-j9z49 [286.813038ms]
Apr 29 07:35:13.561: INFO: Created: latency-svc-px5vr
Apr 29 07:35:13.584: INFO: Got endpoints: latency-svc-px5vr [309.083201ms]
Apr 29 07:35:13.585: INFO: Created: latency-svc-mj4vp
Apr 29 07:35:13.602: INFO: Got endpoints: latency-svc-mj4vp [312.693373ms]
Apr 29 07:35:13.616: INFO: Created: latency-svc-ncb44
Apr 29 07:35:13.631: INFO: Created: latency-svc-plrsn
Apr 29 07:35:13.631: INFO: Got endpoints: latency-svc-ncb44 [324.432955ms]
Apr 29 07:35:13.645: INFO: Got endpoints: latency-svc-plrsn [315.476062ms]
Apr 29 07:35:13.661: INFO: Created: latency-svc-t29ds
Apr 29 07:35:13.673: INFO: Got endpoints: latency-svc-t29ds [318.657612ms]
Apr 29 07:35:13.674: INFO: Created: latency-svc-9cb6j
Apr 29 07:35:13.726: INFO: Got endpoints: latency-svc-9cb6j [346.467007ms]
Apr 29 07:35:13.727: INFO: Created: latency-svc-hb9ck
Apr 29 07:35:13.727: INFO: Created: latency-svc-dm466
Apr 29 07:35:13.727: INFO: Created: latency-svc-dtk4j
Apr 29 07:35:13.835: INFO: Got endpoints: latency-svc-dm466 [445.590846ms]
Apr 29 07:35:13.835: INFO: Got endpoints: latency-svc-hb9ck [432.735483ms]
Apr 29 07:35:13.836: INFO: Got endpoints: latency-svc-dtk4j [413.005313ms]
Apr 29 07:35:14.025: INFO: Created: latency-svc-cgqfj
Apr 29 07:35:14.025: INFO: Created: latency-svc-k4z4k
Apr 29 07:35:14.127: INFO: Created: latency-svc-djxz7
Apr 29 07:35:14.127: INFO: Created: latency-svc-qt6t8
Apr 29 07:35:14.129: INFO: Created: latency-svc-vw876
Apr 29 07:35:14.427: INFO: Created: latency-svc-hkzhn
Apr 29 07:35:14.427: INFO: Created: latency-svc-5cqww
Apr 29 07:35:14.427: INFO: Created: latency-svc-twbnr
Apr 29 07:35:14.627: INFO: Created: latency-svc-swpdt
Apr 29 07:35:14.628: INFO: Created: latency-svc-slf2z
Apr 29 07:35:14.628: INFO: Got endpoints: latency-svc-cgqfj [1.162520339s]
Apr 29 07:35:14.628: INFO: Created: latency-svc-7s2cf
Apr 29 07:35:14.631: INFO: Created: latency-svc-g6wk5
Apr 29 07:35:14.631: INFO: Created: latency-svc-l9wjh
Apr 29 07:35:14.727: INFO: Got endpoints: latency-svc-djxz7 [1.238273463s]
Apr 29 07:35:14.728: INFO: Got endpoints: latency-svc-qt6t8 [1.221293179s]
Apr 29 07:35:14.729: INFO: Got endpoints: latency-svc-k4z4k [1.282285592s]
Apr 29 07:35:14.729: INFO: Created: latency-svc-ct7dl
Apr 29 07:35:14.729: INFO: Created: latency-svc-lfc6g
Apr 29 07:35:14.732: INFO: Created: latency-svc-rg27x
Apr 29 07:35:14.732: INFO: Got endpoints: latency-svc-hkzhn [1.086697748s]
Apr 29 07:35:14.825: INFO: Got endpoints: latency-svc-5cqww [1.194234573s]
Apr 29 07:35:14.826: INFO: Got endpoints: latency-svc-twbnr [1.223657818s]
Apr 29 07:35:14.826: INFO: Got endpoints: latency-svc-vw876 [1.29892521s]
Apr 29 07:35:14.925: INFO: Created: latency-svc-tccd7
Apr 29 07:35:15.026: INFO: Got endpoints: latency-svc-swpdt [1.299682401s]
Apr 29 07:35:15.026: INFO: Created: latency-svc-zcn4d
Apr 29 07:35:15.026: INFO: Got endpoints: latency-svc-g6wk5 [1.479171392s]
Apr 29 07:35:15.026: INFO: Got endpoints: latency-svc-7s2cf [1.353227269s]
Apr 29 07:35:15.034: INFO: Created: latency-svc-r7d5g
Apr 29 07:35:15.327: INFO: Created: latency-svc-p6hvp
Apr 29 07:35:15.327: INFO: Got endpoints: latency-svc-l9wjh [1.491058133s]
Apr 29 07:35:15.327: INFO: Created: latency-svc-jnt52
Apr 29 07:35:15.327: INFO: Created: latency-svc-2vjhv
Apr 29 07:35:15.329: INFO: Created: latency-svc-mztvj
Apr 29 07:35:15.425: INFO: Got endpoints: latency-svc-slf2z [1.840582136s]
Apr 29 07:35:15.426: INFO: Got endpoints: latency-svc-lfc6g [1.590004291s]
Apr 29 07:35:15.526: INFO: Created: latency-svc-pbgj7
Apr 29 07:35:15.529: INFO: Created: latency-svc-q2lt9
Apr 29 07:35:15.529: INFO: Got endpoints: latency-svc-rg27x [901.119696ms]
Apr 29 07:35:15.529: INFO: Got endpoints: latency-svc-ct7dl [1.693608855s]
Apr 29 07:35:15.627: INFO: Created: latency-svc-4qf2q
Apr 29 07:35:15.726: INFO: Got endpoints: latency-svc-r7d5g [994.036371ms]
Apr 29 07:35:15.726: INFO: Got endpoints: latency-svc-zcn4d [997.59916ms]
Apr 29 07:35:15.726: INFO: Created: latency-svc-dvxrs
Apr 29 07:35:15.824: INFO: Got endpoints: latency-svc-tccd7 [1.095641309s]
Apr 29 07:35:16.025: INFO: Created: latency-svc-tgcp2
Apr 29 07:35:16.125: INFO: Created: latency-svc-ckvjx
Apr 29 07:35:16.126: INFO: Got endpoints: latency-svc-p6hvp [1.398205578s]
Apr 29 07:35:16.126: INFO: Got endpoints: latency-svc-jnt52 [1.299670434s]
Apr 29 07:35:16.126: INFO: Created: latency-svc-rqn4b
Apr 29 07:35:16.225: INFO: Created: latency-svc-72m4b
Apr 29 07:35:16.525: INFO: Created: latency-svc-2wbnt
Apr 29 07:35:16.525: INFO: Got endpoints: latency-svc-2vjhv [1.700477699s]
Apr 29 07:35:16.525: INFO: Created: latency-svc-5ps6j
Apr 29 07:35:16.526: INFO: Got endpoints: latency-svc-mztvj [1.699342917s]
Apr 29 07:35:16.625: INFO: Created: latency-svc-mkzl8
Apr 29 07:35:16.726: INFO: Got endpoints: latency-svc-4qf2q [1.699168588s]
Apr 29 07:35:16.825: INFO: Got endpoints: latency-svc-pbgj7 [1.798232854s]
Apr 29 07:35:16.829: INFO: Created: latency-svc-8vtlc
Apr 29 07:35:16.925: INFO: Got endpoints: latency-svc-q2lt9 [1.899322132s]
Apr 29 07:35:17.027: INFO: Created: latency-svc-6txf8
Apr 29 07:35:17.027: INFO: Got endpoints: latency-svc-tgcp2 [1.600979031s]
Apr 29 07:35:17.027: INFO: Got endpoints: latency-svc-dvxrs [1.700173138s]
Apr 29 07:35:17.125: INFO: Created: latency-svc-q87s7
Apr 29 07:35:17.227: INFO: Created: latency-svc-pzpph
Apr 29 07:35:17.325: INFO: Created: latency-svc-sw8jg
Apr 29 07:35:17.426: INFO: Created: latency-svc-zfmlt
Apr 29 07:35:17.426: INFO: Got endpoints: latency-svc-rqn4b [1.897265433s]
Apr 29 07:35:17.426: INFO: Got endpoints: latency-svc-ckvjx [1.897329539s]
Apr 29 07:35:17.629: INFO: Created: latency-svc-bqbjg
Apr 29 07:35:17.629: INFO: Got endpoints: latency-svc-72m4b [2.204051883s]
Apr 29 07:35:17.725: INFO: Got endpoints: latency-svc-5ps6j [1.998963411s]
Apr 29 07:35:17.825: INFO: Created: latency-svc-b2kft
Apr 29 07:35:17.926: INFO: Created: latency-svc-bdr5l
Apr 29 07:35:17.926: INFO: Got endpoints: latency-svc-2wbnt [2.19981123s]
Apr 29 07:35:18.224: INFO: Got endpoints: latency-svc-mkzl8 [2.399487947s]
Apr 29 07:35:18.425: INFO: Got endpoints: latency-svc-q87s7 [1.899976769s]
Apr 29 07:35:18.426: INFO: Created: latency-svc-stx4h
Apr 29 07:35:18.427: INFO: Got endpoints: latency-svc-8vtlc [2.300996331s]
Apr 29 07:35:18.428: INFO: Created: latency-svc-wrfqb
Apr 29 07:35:18.428: INFO: Created: latency-svc-hs5j8
Apr 29 07:35:18.537: INFO: Got endpoints: latency-svc-6txf8 [2.411441974s]
Apr 29 07:35:18.537: INFO: Created: latency-svc-mvcxd
Apr 29 07:35:18.625: INFO: Got endpoints: latency-svc-pzpph [2.099089409s]
Apr 29 07:35:18.726: INFO: Created: latency-svc-s2mjx
Apr 29 07:35:18.825: INFO: Created: latency-svc-ttjmn
Apr 29 07:35:18.929: INFO: Created: latency-svc-xtsrw
Apr 29 07:35:18.929: INFO: Created: latency-svc-bz6s6
Apr 29 07:35:19.026: INFO: Created: latency-svc-j5x7x
Apr 29 07:35:19.126: INFO: Got endpoints: latency-svc-bqbjg [2.200758264s]
Apr 29 07:35:19.126: INFO: Got endpoints: latency-svc-b2kft [2.099184637s]
Apr 29 07:35:19.326: INFO: Got endpoints: latency-svc-sw8jg [2.500849628s]
Apr 29 07:35:19.524: INFO: Got endpoints: latency-svc-hs5j8 [2.09810049s]
Apr 29 07:35:19.626: INFO: Created: latency-svc-d2sxl
Apr 29 07:35:19.626: INFO: Got endpoints: latency-svc-bdr5l [2.599502901s]
Apr 29 07:35:19.729: INFO: Got endpoints: latency-svc-wrfqb [2.302459543s]
Apr 29 07:35:19.827: INFO: Got endpoints: latency-svc-stx4h [2.102042555s]
Apr 29 07:35:19.829: INFO: Got endpoints: latency-svc-s2mjx [1.291578451s]
Apr 29 07:35:19.937: INFO: Got endpoints: latency-svc-bz6s6 [1.509824592s]
Apr 29 07:35:19.938: INFO: Got endpoints: latency-svc-mvcxd [2.308351699s]
Apr 29 07:35:19.938: INFO: Created: latency-svc-495fp
Apr 29 07:35:19.938: INFO: Created: latency-svc-c5hwf
Apr 29 07:35:20.135: INFO: Got endpoints: latency-svc-zfmlt [3.409774353s]
Apr 29 07:35:20.140: INFO: Created: latency-svc-fd8zq
Apr 29 07:35:20.140: INFO: Got endpoints: latency-svc-ttjmn [2.214513068s]
Apr 29 07:35:20.141: INFO: Got endpoints: latency-svc-xtsrw [1.715229785s]
Apr 29 07:35:20.143: INFO: Created: latency-svc-f4zd5
Apr 29 07:35:20.144: INFO: Created: latency-svc-sh4gj
Apr 29 07:35:20.240: INFO: Created: latency-svc-cfp4b
Apr 29 07:35:20.240: INFO: Got endpoints: latency-svc-c5hwf [715.641153ms]
Apr 29 07:35:20.240: INFO: Created: latency-svc-cpbcx
Apr 29 07:35:20.240: INFO: Created: latency-svc-sf5bg
Apr 29 07:35:20.343: INFO: Created: latency-svc-m8ltb
Apr 29 07:35:20.343: INFO: Created: latency-svc-7bbnv
Apr 29 07:35:20.344: INFO: Created: latency-svc-r6dcd
Apr 29 07:35:20.347: INFO: Created: latency-svc-mb4dr
Apr 29 07:35:20.425: INFO: Got endpoints: latency-svc-fd8zq [695.910828ms]
Apr 29 07:35:20.440: INFO: Created: latency-svc-qsg9z
Apr 29 07:35:20.442: INFO: Got endpoints: latency-svc-d2sxl [1.817554465s]
Apr 29 07:35:20.460: INFO: Got endpoints: latency-svc-f4zd5 [631.619988ms]
Apr 29 07:35:20.460: INFO: Got endpoints: latency-svc-sh4gj [633.194165ms]
Apr 29 07:35:20.538: INFO: Got endpoints: latency-svc-495fp [911.058664ms]
Apr 29 07:35:20.541: INFO: Created: latency-svc-bxkkm
Apr 29 07:35:20.541: INFO: Created: latency-svc-htd94
Apr 29 07:35:20.541: INFO: Created: latency-svc-lsjlb
Apr 29 07:35:20.642: INFO: Created: latency-svc-2h7rk
Apr 29 07:35:20.736: INFO: Got endpoints: latency-svc-cpbcx [1.610075289s]
Apr 29 07:35:20.737: INFO: Got endpoints: latency-svc-j5x7x [2.512687288s]
Apr 29 07:35:20.737: INFO: Got endpoints: latency-svc-sf5bg [1.411361957s]
Apr 29 07:35:20.737: INFO: Got endpoints: latency-svc-cfp4b [596.90369ms]
Apr 29 07:35:20.830: INFO: Got endpoints: latency-svc-7bbnv [689.659201ms]
Apr 29 07:35:20.833: INFO: Created: latency-svc-m2ct5
Apr 29 07:35:20.834: INFO: Got endpoints: latency-svc-qsg9z [897.024252ms]
Apr 29 07:35:20.834: INFO: Created: latency-svc-59zxk
Apr 29 07:35:20.838: INFO: Got endpoints: latency-svc-m8ltb [702.575964ms]
Apr 29 07:35:20.935: INFO: Created: latency-svc-m5plg
Apr 29 07:35:20.938: INFO: Got endpoints: latency-svc-mb4dr [1.811988303s]
Apr 29 07:35:20.938: INFO: Got endpoints: latency-svc-r6dcd [698.020319ms]
Apr 29 07:35:21.038: INFO: Created: latency-svc-2c95d
Apr 29 07:35:21.038: INFO: Got endpoints: latency-svc-lsjlb [1.100541949s]
Apr 29 07:35:21.038: INFO: Created: latency-svc-4j9mb
Apr 29 07:35:21.038: INFO: Created: latency-svc-nj642
Apr 29 07:35:21.038: INFO: Created: latency-svc-kzjt9
Apr 29 07:35:21.038: INFO: Created: latency-svc-6lxqf
Apr 29 07:35:21.038: INFO: Got endpoints: latency-svc-2h7rk [500.659588ms]
Apr 29 07:35:21.038: INFO: Got endpoints: latency-svc-htd94 [577.889339ms]
Apr 29 07:35:21.038: INFO: Got endpoints: latency-svc-m2ct5 [301.349288ms]
Apr 29 07:35:21.128: INFO: Got endpoints: latency-svc-nj642 [290.143551ms]
Apr 29 07:35:21.131: INFO: Created: latency-svc-wtm55
Apr 29 07:35:21.131: INFO: Got endpoints: latency-svc-59zxk [394.256381ms]
Apr 29 07:35:21.131: INFO: Got endpoints: latency-svc-kzjt9 [688.804622ms]
Apr 29 07:35:21.131: INFO: Got endpoints: latency-svc-bxkkm [670.62864ms]
Apr 29 07:35:21.132: INFO: Created: latency-svc-qvs9p
Apr 29 07:35:21.132: INFO: Created: latency-svc-22df9
Apr 29 07:35:21.132: INFO: Created: latency-svc-rvzmf
Apr 29 07:35:21.132: INFO: Created: latency-svc-c6fcj
Apr 29 07:35:21.132: INFO: Created: latency-svc-mzdvs
Apr 29 07:35:21.231: INFO: Got endpoints: latency-svc-m5plg [494.472964ms]
Apr 29 07:35:21.238: INFO: Got endpoints: latency-svc-4j9mb [404.475757ms]
Apr 29 07:35:21.238: INFO: Got endpoints: latency-svc-qvs9p [199.742783ms]
Apr 29 07:35:21.241: INFO: Created: latency-svc-jfw9j
Apr 29 07:35:21.241: INFO: Created: latency-svc-gjbxw
Apr 29 07:35:21.241: INFO: Created: latency-svc-g4zhm
Apr 29 07:35:21.241: INFO: Created: latency-svc-mtxn8
Apr 29 07:35:21.334: INFO: Got endpoints: latency-svc-6lxqf [503.970256ms]
Apr 29 07:35:21.337: INFO: Got endpoints: latency-svc-2c95d [398.976208ms]
Apr 29 07:35:21.439: INFO: Got endpoints: latency-svc-wtm55 [400.478145ms]
Apr 29 07:35:21.507: INFO: Got endpoints: latency-svc-c6fcj [468.699349ms]
Apr 29 07:35:21.509: INFO: Created: latency-svc-ccdml
Apr 29 07:35:21.509: INFO: Created: latency-svc-xbkbn
Apr 29 07:35:21.509: INFO: Created: latency-svc-v9255
Apr 29 07:35:21.509: INFO: Got endpoints: latency-svc-rvzmf [471.08647ms]
Apr 29 07:35:21.510: INFO: Got endpoints: latency-svc-22df9 [1.084628164s]
Apr 29 07:35:21.510: INFO: Got endpoints: latency-svc-mzdvs [772.804128ms]
Apr 29 07:35:21.530: INFO: Created: latency-svc-6w2bj
Apr 29 07:35:21.530: INFO: Created: latency-svc-kpjp5
Apr 29 07:35:21.533: INFO: Created: latency-svc-9rj2g
Apr 29 07:35:21.643: INFO: Got endpoints: latency-svc-gjbxw [512.333037ms]
Apr 29 07:35:21.647: INFO: Got endpoints: latency-svc-v9255 [708.496213ms]
Apr 29 07:35:21.647: INFO: Got endpoints: latency-svc-g4zhm [515.917199ms]
Apr 29 07:35:21.647: INFO: Got endpoints: latency-svc-mtxn8 [518.876738ms]
Apr 29 07:35:21.647: INFO: Got endpoints: latency-svc-jfw9j [516.136249ms]
Apr 29 07:35:21.657: INFO: Created: latency-svc-c2qnl
Apr 29 07:35:21.657: INFO: Created: latency-svc-r8q9d
Apr 29 07:35:21.657: INFO: Created: latency-svc-5629d
Apr 29 07:35:21.657: INFO: Created: latency-svc-tqgl5
Apr 29 07:35:21.657: INFO: Created: latency-svc-v5mx4
Apr 29 07:35:21.662: INFO: Got endpoints: latency-svc-9rj2g [423.811596ms]
Apr 29 07:35:21.665: INFO: Got endpoints: latency-svc-xbkbn [330.194429ms]
Apr 29 07:35:21.665: INFO: Got endpoints: latency-svc-kpjp5 [426.540392ms]
Apr 29 07:35:21.669: INFO: Got endpoints: latency-svc-6w2bj [438.426712ms]
Apr 29 07:35:21.706: INFO: Got endpoints: latency-svc-ccdml [369.228933ms]
Apr 29 07:35:21.741: INFO: Created: latency-svc-lhk8v
Apr 29 07:35:21.742: INFO: Created: latency-svc-fgx65
Apr 29 07:35:21.745: INFO: Got endpoints: latency-svc-v5mx4 [235.494383ms]
Apr 29 07:35:21.745: INFO: Got endpoints: latency-svc-5629d [235.03161ms]
Apr 29 07:35:21.746: INFO: Got endpoints: latency-svc-tqgl5 [238.442483ms]
Apr 29 07:35:21.746: INFO: Got endpoints: latency-svc-c2qnl [307.39715ms]
Apr 29 07:35:21.746: INFO: Got endpoints: latency-svc-r8q9d [236.708696ms]
Apr 29 07:35:21.757: INFO: Created: latency-svc-4qb2z
Apr 29 07:35:21.757: INFO: Got endpoints: latency-svc-lhk8v [110.067986ms]
Apr 29 07:35:21.757: INFO: Got endpoints: latency-svc-fgx65 [113.410573ms]
Apr 29 07:35:21.770: INFO: Created: latency-svc-sfhnx
Apr 29 07:35:21.770: INFO: Got endpoints: latency-svc-4qb2z [122.630646ms]
Apr 29 07:35:21.832: INFO: Got endpoints: latency-svc-sfhnx [184.739444ms]
Apr 29 07:35:21.982: INFO: Created: latency-svc-qh6bq
Apr 29 07:35:21.982: INFO: Created: latency-svc-x98mq
Apr 29 07:35:21.982: INFO: Created: latency-svc-gvfdh
Apr 29 07:35:21.982: INFO: Created: latency-svc-hbbzf
Apr 29 07:35:21.982: INFO: Created: latency-svc-wjmxs
Apr 29 07:35:21.982: INFO: Created: latency-svc-5w8b7
Apr 29 07:35:21.982: INFO: Created: latency-svc-m8jjp
Apr 29 07:35:21.982: INFO: Created: latency-svc-8m8v9
Apr 29 07:35:21.982: INFO: Created: latency-svc-lnmcx
Apr 29 07:35:21.982: INFO: Created: latency-svc-qx24n
Apr 29 07:35:21.983: INFO: Created: latency-svc-s8v2g
Apr 29 07:35:21.983: INFO: Created: latency-svc-lkbnh
Apr 29 07:35:21.983: INFO: Created: latency-svc-qpkdf
Apr 29 07:35:21.983: INFO: Created: latency-svc-mkhnp
Apr 29 07:35:21.983: INFO: Created: latency-svc-4bgrl
Apr 29 07:35:21.992: INFO: Got endpoints: latency-svc-4bgrl [322.556281ms]
Apr 29 07:35:21.995: INFO: Got endpoints: latency-svc-m8jjp [289.086486ms]
Apr 29 07:35:21.996: INFO: Got endpoints: latency-svc-qpkdf [348.294155ms]
Apr 29 07:35:21.996: INFO: Got endpoints: latency-svc-x98mq [163.833229ms]
Apr 29 07:35:21.996: INFO: Got endpoints: latency-svc-gvfdh [250.485091ms]
Apr 29 07:35:22.000: INFO: Got endpoints: latency-svc-qh6bq [253.960307ms]
Apr 29 07:35:22.004: INFO: Got endpoints: latency-svc-mkhnp [234.767931ms]
Apr 29 07:35:22.005: INFO: Got endpoints: latency-svc-8m8v9 [340.381009ms]
Apr 29 07:35:22.017: INFO: Created: latency-svc-hbzf5
Apr 29 07:35:22.018: INFO: Got endpoints: latency-svc-qx24n [271.952109ms]
Apr 29 07:35:22.018: INFO: Got endpoints: latency-svc-lnmcx [356.105586ms]
Apr 29 07:35:22.018: INFO: Got endpoints: latency-svc-wjmxs [261.327536ms]
Apr 29 07:35:22.018: INFO: Got endpoints: latency-svc-hbbzf [272.695414ms]
Apr 29 07:35:22.034: INFO: Created: latency-svc-tqrjv
Apr 29 07:35:22.036: INFO: Got endpoints: latency-svc-5w8b7 [290.946455ms]
Apr 29 07:35:22.036: INFO: Got endpoints: latency-svc-lkbnh [371.379051ms]
Apr 29 07:35:22.052: INFO: Created: latency-svc-hgh6l
Apr 29 07:35:22.053: INFO: Got endpoints: latency-svc-s8v2g [295.785278ms]
Apr 29 07:35:22.073: INFO: Created: latency-svc-lcqth
Apr 29 07:35:22.100: INFO: Got endpoints: latency-svc-hbzf5 [108.256904ms]
Apr 29 07:35:22.102: INFO: Created: latency-svc-8sfdf
Apr 29 07:35:22.116: INFO: Created: latency-svc-pq6pq
Apr 29 07:35:22.134: INFO: Created: latency-svc-wfhjk
Apr 29 07:35:22.150: INFO: Created: latency-svc-c5mnl
Apr 29 07:35:22.153: INFO: Got endpoints: latency-svc-tqrjv [157.900188ms]
Apr 29 07:35:22.166: INFO: Created: latency-svc-cx9vw
Apr 29 07:35:22.184: INFO: Created: latency-svc-bqg2q
Apr 29 07:35:22.227: INFO: Got endpoints: latency-svc-hgh6l [231.326112ms]
Apr 29 07:35:22.385: INFO: Created: latency-svc-lhpp4
Apr 29 07:35:22.385: INFO: Got endpoints: latency-svc-pq6pq [384.896157ms]
Apr 29 07:35:22.385: INFO: Got endpoints: latency-svc-lcqth [389.580857ms]
Apr 29 07:35:22.386: INFO: Got endpoints: latency-svc-8sfdf [389.696598ms]
Apr 29 07:35:22.496: INFO: Got endpoints: latency-svc-c5mnl [491.043738ms]
Apr 29 07:35:22.496: INFO: Got endpoints: latency-svc-cx9vw [478.121785ms]
Apr 29 07:35:22.496: INFO: Got endpoints: latency-svc-wfhjk [491.651105ms]
Apr 29 07:35:22.505: INFO: Created: latency-svc-lrx5w
Apr 29 07:35:22.526: INFO: Created: latency-svc-qgsfm
Apr 29 07:35:22.549: INFO: Got endpoints: latency-svc-bqg2q [530.684007ms]
Apr 29 07:35:22.550: INFO: Created: latency-svc-dhk5p
Apr 29 07:35:22.564: INFO: Created: latency-svc-57bsq
Apr 29 07:35:22.577: INFO: Created: latency-svc-jv4rh
Apr 29 07:35:22.591: INFO: Created: latency-svc-4xwbx
Apr 29 07:35:22.593: INFO: Got endpoints: latency-svc-lhpp4 [575.219107ms]
Apr 29 07:35:22.629: INFO: Created: latency-svc-cdbhl
Apr 29 07:35:22.629: INFO: Created: latency-svc-pqkzh
Apr 29 07:35:22.651: INFO: Created: latency-svc-c9v7h
Apr 29 07:35:22.652: INFO: Got endpoints: latency-svc-lrx5w [633.310562ms]
Apr 29 07:35:22.665: INFO: Created: latency-svc-qj22r
Apr 29 07:35:22.678: INFO: Created: latency-svc-ksrps
Apr 29 07:35:22.694: INFO: Got endpoints: latency-svc-qgsfm [657.661647ms]
Apr 29 07:35:22.694: INFO: Created: latency-svc-klblc
Apr 29 07:35:22.710: INFO: Created: latency-svc-sm47k
Apr 29 07:35:22.724: INFO: Created: latency-svc-6smtn
Apr 29 07:35:22.745: INFO: Got endpoints: latency-svc-dhk5p [709.006617ms]
Apr 29 07:35:22.745: INFO: Created: latency-svc-798vg
Apr 29 07:35:22.766: INFO: Created: latency-svc-7dpqs
Apr 29 07:35:22.779: INFO: Created: latency-svc-xwvgj
Apr 29 07:35:22.794: INFO: Got endpoints: latency-svc-57bsq [740.913161ms]
Apr 29 07:35:22.794: INFO: Created: latency-svc-xsffs
Apr 29 07:35:22.813: INFO: Created: latency-svc-thrk2
Apr 29 07:35:22.841: INFO: Got endpoints: latency-svc-jv4rh [741.281898ms]
Apr 29 07:35:22.867: INFO: Created: latency-svc-sf7s5
Apr 29 07:35:22.891: INFO: Got endpoints: latency-svc-4xwbx [737.578759ms]
Apr 29 07:35:22.908: INFO: Created: latency-svc-dqnbx
Apr 29 07:35:22.945: INFO: Got endpoints: latency-svc-pqkzh [560.377974ms]
Apr 29 07:35:22.971: INFO: Created: latency-svc-ljlw4
Apr 29 07:35:22.991: INFO: Got endpoints: latency-svc-cdbhl [763.991723ms]
Apr 29 07:35:23.008: INFO: Created: latency-svc-f5zhc
Apr 29 07:35:23.041: INFO: Got endpoints: latency-svc-c9v7h [655.745239ms]
Apr 29 07:35:23.066: INFO: Created: latency-svc-mcq2t
Apr 29 07:35:23.091: INFO: Got endpoints: latency-svc-qj22r [705.807819ms]
Apr 29 07:35:23.108: INFO: Created: latency-svc-bk5g7
Apr 29 07:35:23.140: INFO: Got endpoints: latency-svc-ksrps [644.25142ms]
Apr 29 07:35:23.167: INFO: Created: latency-svc-x7wkd
Apr 29 07:35:23.194: INFO: Got endpoints: latency-svc-klblc [698.319244ms]
Apr 29 07:35:23.213: INFO: Created: latency-svc-pwznh
Apr 29 07:35:23.240: INFO: Got endpoints: latency-svc-sm47k [743.652828ms]
Apr 29 07:35:23.258: INFO: Created: latency-svc-s8qbn
Apr 29 07:35:23.290: INFO: Got endpoints: latency-svc-6smtn [741.348188ms]
Apr 29 07:35:23.323: INFO: Created: latency-svc-thbgz
Apr 29 07:35:23.341: INFO: Got endpoints: latency-svc-798vg [747.143762ms]
Apr 29 07:35:23.358: INFO: Created: latency-svc-689cq
Apr 29 07:35:23.390: INFO: Got endpoints: latency-svc-7dpqs [738.333303ms]
Apr 29 07:35:23.440: INFO: Got endpoints: latency-svc-xwvgj [746.001006ms]
Apr 29 07:35:23.490: INFO: Got endpoints: latency-svc-xsffs [744.509723ms]
Apr 29 07:35:23.540: INFO: Got endpoints: latency-svc-thrk2 [746.640272ms]
Apr 29 07:35:23.591: INFO: Got endpoints: latency-svc-sf7s5 [749.267724ms]
Apr 29 07:35:23.640: INFO: Got endpoints: latency-svc-dqnbx [748.888113ms]
Apr 29 07:35:23.691: INFO: Got endpoints: latency-svc-ljlw4 [745.608073ms]
Apr 29 07:35:23.741: INFO: Got endpoints: latency-svc-f5zhc [749.744961ms]
Apr 29 07:35:23.791: INFO: Got endpoints: latency-svc-mcq2t [750.18816ms]
Apr 29 07:35:23.841: INFO: Got endpoints: latency-svc-bk5g7 [749.614765ms]
Apr 29 07:35:23.890: INFO: Got endpoints: latency-svc-x7wkd [749.876281ms]
Apr 29 07:35:23.941: INFO: Got endpoints: latency-svc-pwznh [746.772021ms]
Apr 29 07:35:23.991: INFO: Got endpoints: latency-svc-s8qbn [751.306156ms]
Apr 29 07:35:24.042: INFO: Got endpoints: latency-svc-thbgz [751.426631ms]
Apr 29 07:35:24.090: INFO: Got endpoints: latency-svc-689cq [749.66662ms]
Apr 29 07:35:24.090: INFO: Latencies: [39.966604ms 63.551691ms 78.36281ms 94.852616ms 108.256904ms 109.055431ms 110.067986ms 112.239813ms 113.410573ms 122.630646ms 139.193295ms 147.910615ms 157.900188ms 163.833229ms 176.08387ms 184.739444ms 198.635005ms 199.742783ms 217.303201ms 231.028815ms 231.326112ms 234.767931ms 235.03161ms 235.494383ms 236.708696ms 238.442483ms 245.578459ms 250.485091ms 253.244771ms 253.960307ms 255.303228ms 255.463528ms 255.623832ms 259.679028ms 261.327536ms 262.213231ms 263.518216ms 271.016959ms 271.952109ms 271.969493ms 272.695414ms 273.565392ms 273.588176ms 276.114057ms 276.155163ms 277.915334ms 282.002686ms 283.388434ms 286.105094ms 286.813038ms 289.086486ms 290.003246ms 290.143551ms 290.946455ms 295.785278ms 301.349288ms 307.39715ms 309.083201ms 312.693373ms 315.476062ms 318.657612ms 322.556281ms 324.432955ms 330.194429ms 340.381009ms 346.467007ms 348.294155ms 356.105586ms 369.228933ms 371.379051ms 384.896157ms 389.580857ms 389.696598ms 394.256381ms 398.976208ms 400.478145ms 404.475757ms 413.005313ms 423.811596ms 426.540392ms 432.735483ms 438.426712ms 445.590846ms 468.699349ms 471.08647ms 478.121785ms 491.043738ms 491.651105ms 494.472964ms 500.659588ms 503.970256ms 512.333037ms 515.917199ms 516.136249ms 518.876738ms 530.684007ms 560.377974ms 575.219107ms 577.889339ms 596.90369ms 631.619988ms 633.194165ms 633.310562ms 644.25142ms 655.745239ms 657.661647ms 670.62864ms 688.804622ms 689.659201ms 695.910828ms 698.020319ms 698.319244ms 702.575964ms 705.807819ms 708.496213ms 709.006617ms 715.641153ms 737.578759ms 738.333303ms 740.913161ms 741.281898ms 741.348188ms 743.652828ms 744.509723ms 745.608073ms 746.001006ms 746.640272ms 746.772021ms 747.143762ms 748.888113ms 749.267724ms 749.614765ms 749.66662ms 749.744961ms 749.876281ms 750.18816ms 751.306156ms 751.426631ms 763.991723ms 772.804128ms 897.024252ms 901.119696ms 911.058664ms 994.036371ms 997.59916ms 1.084628164s 1.086697748s 1.095641309s 1.100541949s 1.162520339s 1.194234573s 1.221293179s 1.223657818s 1.238273463s 1.282285592s 1.291578451s 1.29892521s 1.299670434s 1.299682401s 1.353227269s 1.398205578s 1.411361957s 1.479171392s 1.491058133s 1.509824592s 1.590004291s 1.600979031s 1.610075289s 1.693608855s 1.699168588s 1.699342917s 1.700173138s 1.700477699s 1.715229785s 1.798232854s 1.811988303s 1.817554465s 1.840582136s 1.897265433s 1.897329539s 1.899322132s 1.899976769s 1.998963411s 2.09810049s 2.099089409s 2.099184637s 2.102042555s 2.19981123s 2.200758264s 2.204051883s 2.214513068s 2.300996331s 2.302459543s 2.308351699s 2.399487947s 2.411441974s 2.500849628s 2.512687288s 2.599502901s 3.409774353s]
Apr 29 07:35:24.090: INFO: 50 %ile: 631.619988ms
Apr 29 07:35:24.090: INFO: 90 %ile: 1.899322132s
Apr 29 07:35:24.090: INFO: 99 %ile: 2.599502901s
Apr 29 07:35:24.090: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:24.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5541" for this suite.

• [SLOW TEST:13.437 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":203,"skipped":3828,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:24.102: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-3504/configmap-test-822006d7-a677-44a6-b2ef-64271e47e71b
STEP: Creating a pod to test consume configMaps
Apr 29 07:35:24.142: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb292c90-ecf6-426c-8029-7b8160684797" in namespace "configmap-3504" to be "Succeeded or Failed"
Apr 29 07:35:24.144: INFO: Pod "pod-configmaps-fb292c90-ecf6-426c-8029-7b8160684797": Phase="Pending", Reason="", readiness=false. Elapsed: 2.788866ms
Apr 29 07:35:26.149: INFO: Pod "pod-configmaps-fb292c90-ecf6-426c-8029-7b8160684797": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007256781s
STEP: Saw pod success
Apr 29 07:35:26.149: INFO: Pod "pod-configmaps-fb292c90-ecf6-426c-8029-7b8160684797" satisfied condition "Succeeded or Failed"
Apr 29 07:35:26.151: INFO: Trying to get logs from node localhost.localdomain151 pod pod-configmaps-fb292c90-ecf6-426c-8029-7b8160684797 container env-test: <nil>
STEP: delete the pod
Apr 29 07:35:26.169: INFO: Waiting for pod pod-configmaps-fb292c90-ecf6-426c-8029-7b8160684797 to disappear
Apr 29 07:35:26.171: INFO: Pod pod-configmaps-fb292c90-ecf6-426c-8029-7b8160684797 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:26.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3504" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":204,"skipped":3844,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:26.181: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 07:35:26.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-46cea4c2-71f7-49c4-ba3e-ef00b97d2d2a" in namespace "projected-4425" to be "Succeeded or Failed"
Apr 29 07:35:26.239: INFO: Pod "downwardapi-volume-46cea4c2-71f7-49c4-ba3e-ef00b97d2d2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070908ms
Apr 29 07:35:28.244: INFO: Pod "downwardapi-volume-46cea4c2-71f7-49c4-ba3e-ef00b97d2d2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006695688s
STEP: Saw pod success
Apr 29 07:35:28.244: INFO: Pod "downwardapi-volume-46cea4c2-71f7-49c4-ba3e-ef00b97d2d2a" satisfied condition "Succeeded or Failed"
Apr 29 07:35:28.250: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-46cea4c2-71f7-49c4-ba3e-ef00b97d2d2a container client-container: <nil>
STEP: delete the pod
Apr 29 07:35:28.265: INFO: Waiting for pod downwardapi-volume-46cea4c2-71f7-49c4-ba3e-ef00b97d2d2a to disappear
Apr 29 07:35:28.267: INFO: Pod downwardapi-volume-46cea4c2-71f7-49c4-ba3e-ef00b97d2d2a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:28.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4425" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":205,"skipped":3883,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:28.276: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:35:28.348: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 29 07:35:28.355: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:35:28.355: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Apr 29 07:35:28.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:35:28.374: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:35:29.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:35:29.377: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:35:30.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 29 07:35:30.379: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 29 07:35:30.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 29 07:35:30.397: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Apr 29 07:35:31.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:35:31.409: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 29 07:35:31.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:35:31.447: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:35:32.450: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:35:32.450: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:35:33.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:35:33.454: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:35:34.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:35:34.451: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:35:35.450: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 29 07:35:35.450: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2627, will wait for the garbage collector to delete the pods
Apr 29 07:35:35.512: INFO: Deleting DaemonSet.extensions daemon-set took: 4.907391ms
Apr 29 07:35:35.613: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.69174ms
Apr 29 07:35:37.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:35:37.718: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 29 07:35:37.721: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"116902"},"items":null}

Apr 29 07:35:37.724: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"116902"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:37.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2627" for this suite.

• [SLOW TEST:9.475 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":206,"skipped":3885,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:37.751: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 07:35:38.413: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 07:35:41.431: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Apr 29 07:35:42.431: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Apr 29 07:35:43.431: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Apr 29 07:35:45.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=webhook-2444 attach --namespace=webhook-2444 to-be-attached-pod -i -c=container1'
Apr 29 07:35:45.560: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:45.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2444" for this suite.
STEP: Destroying namespace "webhook-2444-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.860 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":207,"skipped":3889,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:45.611: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:35:45.643: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:35:46.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-645" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":208,"skipped":3895,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:35:46.674: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Apr 29 07:35:46.705: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:36:09.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3520" for this suite.

• [SLOW TEST:22.776 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":209,"skipped":3902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:36:09.451: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5521
STEP: creating service affinity-clusterip in namespace services-5521
STEP: creating replication controller affinity-clusterip in namespace services-5521
I0429 07:36:09.531050      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5521, replica count: 3
I0429 07:36:12.581211      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 07:36:12.592: INFO: Creating new exec pod
Apr 29 07:36:15.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-5521 exec execpod-affinitymlkrz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Apr 29 07:36:15.762: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Apr 29 07:36:15.762: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:36:15.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-5521 exec execpod-affinitymlkrz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.60.189 80'
Apr 29 07:36:15.920: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.60.189 80\nConnection to 10.109.60.189 80 port [tcp/http] succeeded!\n"
Apr 29 07:36:15.921: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:36:15.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-5521 exec execpod-affinitymlkrz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.109.60.189:80/ ; done'
Apr 29 07:36:16.139: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.60.189:80/\n"
Apr 29 07:36:16.139: INFO: stdout: "\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9\naffinity-clusterip-gz4c9"
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Received response from host: affinity-clusterip-gz4c9
Apr 29 07:36:16.139: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5521, will wait for the garbage collector to delete the pods
Apr 29 07:36:16.215: INFO: Deleting ReplicationController affinity-clusterip took: 6.355169ms
Apr 29 07:36:16.316: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.66695ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:36:18.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5521" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.195 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":210,"skipped":4024,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:36:18.647: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 29 07:36:18.683: INFO: Waiting up to 5m0s for pod "pod-93693029-b58e-4fc8-8d3b-dd76e20c4aea" in namespace "emptydir-4213" to be "Succeeded or Failed"
Apr 29 07:36:18.685: INFO: Pod "pod-93693029-b58e-4fc8-8d3b-dd76e20c4aea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597267ms
Apr 29 07:36:20.690: INFO: Pod "pod-93693029-b58e-4fc8-8d3b-dd76e20c4aea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00716131s
STEP: Saw pod success
Apr 29 07:36:20.690: INFO: Pod "pod-93693029-b58e-4fc8-8d3b-dd76e20c4aea" satisfied condition "Succeeded or Failed"
Apr 29 07:36:20.692: INFO: Trying to get logs from node localhost.localdomain151 pod pod-93693029-b58e-4fc8-8d3b-dd76e20c4aea container test-container: <nil>
STEP: delete the pod
Apr 29 07:36:20.707: INFO: Waiting for pod pod-93693029-b58e-4fc8-8d3b-dd76e20c4aea to disappear
Apr 29 07:36:20.710: INFO: Pod pod-93693029-b58e-4fc8-8d3b-dd76e20c4aea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:36:20.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4213" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":211,"skipped":4035,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:36:20.721: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Apr 29 07:36:21.818: INFO: The status of Pod kube-controller-manager-localhost.localdomain is Running (Ready = true)
E0429 07:36:21.845562      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:23.901207      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:24.925242      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:25.958506      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:26.984578      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:28.009897      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:32.112220      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:35.207839      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:37.260913      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:38.294195      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:39.322445      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:40.361816      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:42.418670      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:43.443096      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:46.198587      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:47.224861      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:48.248812      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:49.273356      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:50.306723      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:51.342247      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:52.369832      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:54.431313      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:55.146101      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:56.173021      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:57.199918      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:58.227205      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:36:59.253732      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:00.281515      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:02.332561      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:03.362192      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:06.139027      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:07.172450      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:08.196832      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:10.259479      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:12.311341      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:15.392273      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:16.416899      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:17.137610      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:19.188053      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:20.214959      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:21.245528      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:24.324091      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:26.377314      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:27.401372      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:29.173743      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:30.210980      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:31.236864      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:32.263540      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:34.340796      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:35.364791      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:38.461766      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:39.487390      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:41.558661      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:42.584864      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:43.609971      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:45.659598      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:46.684010      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:37:48.742610      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
Apr 29 07:37:48.743: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:37:48.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1109" for this suite.

• [SLOW TEST:88.032 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":212,"skipped":4045,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:37:48.754: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Apr 29 07:37:48.782: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:37:51.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9186" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":213,"skipped":4090,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:37:51.750: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:37:51.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8801" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":214,"skipped":4111,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:37:51.810: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2e7eb2d8-a4cc-4c09-8846-0822964dd118
STEP: Creating the pod
Apr 29 07:37:51.875: INFO: The status of Pod pod-projected-configmaps-e43d5b16-b274-4afb-8cff-ff677fb48364 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:37:53.878: INFO: The status of Pod pod-projected-configmaps-e43d5b16-b274-4afb-8cff-ff677fb48364 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-2e7eb2d8-a4cc-4c09-8846-0822964dd118
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:37:55.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7951" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":215,"skipped":4115,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:37:55.921: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:37:55.969: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Creating first CR 
Apr 29 07:37:58.526: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-04-29T07:37:58Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-04-29T07:37:58Z]] name:name1 resourceVersion:117940 uid:23a7e9da-6d80-483b-9568-5646fa09c3a9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Apr 29 07:38:08.532: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-04-29T07:38:08Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-04-29T07:38:08Z]] name:name2 resourceVersion:117995 uid:da75389d-5758-4120-82cd-6781cf2a9f45] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Apr 29 07:38:18.541: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-04-29T07:37:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-04-29T07:38:18Z]] name:name1 resourceVersion:118034 uid:23a7e9da-6d80-483b-9568-5646fa09c3a9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Apr 29 07:38:28.549: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-04-29T07:38:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-04-29T07:38:28Z]] name:name2 resourceVersion:118073 uid:da75389d-5758-4120-82cd-6781cf2a9f45] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Apr 29 07:38:38.557: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-04-29T07:37:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-04-29T07:38:18Z]] name:name1 resourceVersion:118112 uid:23a7e9da-6d80-483b-9568-5646fa09c3a9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Apr 29 07:38:48.567: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-04-29T07:38:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-04-29T07:38:28Z]] name:name2 resourceVersion:118151 uid:da75389d-5758-4120-82cd-6781cf2a9f45] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:38:59.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5786" for this suite.

• [SLOW TEST:63.172 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":216,"skipped":4118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:38:59.093: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Apr 29 07:38:59.126: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Apr 29 07:39:16.139: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 07:39:18.950: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:39:35.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6297" for this suite.

• [SLOW TEST:36.729 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":217,"skipped":4164,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:39:35.823: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 29 07:39:35.929: INFO: Waiting up to 5m0s for pod "pod-154a737f-9b01-4ec5-a9d9-92723ab09f45" in namespace "emptydir-7803" to be "Succeeded or Failed"
Apr 29 07:39:35.934: INFO: Pod "pod-154a737f-9b01-4ec5-a9d9-92723ab09f45": Phase="Pending", Reason="", readiness=false. Elapsed: 5.228894ms
Apr 29 07:39:37.939: INFO: Pod "pod-154a737f-9b01-4ec5-a9d9-92723ab09f45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010323535s
STEP: Saw pod success
Apr 29 07:39:37.939: INFO: Pod "pod-154a737f-9b01-4ec5-a9d9-92723ab09f45" satisfied condition "Succeeded or Failed"
Apr 29 07:39:37.942: INFO: Trying to get logs from node localhost.localdomain151 pod pod-154a737f-9b01-4ec5-a9d9-92723ab09f45 container test-container: <nil>
STEP: delete the pod
Apr 29 07:39:37.969: INFO: Waiting for pod pod-154a737f-9b01-4ec5-a9d9-92723ab09f45 to disappear
Apr 29 07:39:37.972: INFO: Pod pod-154a737f-9b01-4ec5-a9d9-92723ab09f45 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:39:37.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7803" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":218,"skipped":4181,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:39:37.982: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 29 07:39:38.241: INFO: Waiting up to 5m0s for pod "pod-bfb5ed2e-305a-4e8e-9e54-c1573c16da54" in namespace "emptydir-7660" to be "Succeeded or Failed"
Apr 29 07:39:38.244: INFO: Pod "pod-bfb5ed2e-305a-4e8e-9e54-c1573c16da54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.647732ms
Apr 29 07:39:40.249: INFO: Pod "pod-bfb5ed2e-305a-4e8e-9e54-c1573c16da54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007043768s
STEP: Saw pod success
Apr 29 07:39:40.249: INFO: Pod "pod-bfb5ed2e-305a-4e8e-9e54-c1573c16da54" satisfied condition "Succeeded or Failed"
Apr 29 07:39:40.251: INFO: Trying to get logs from node localhost.localdomain151 pod pod-bfb5ed2e-305a-4e8e-9e54-c1573c16da54 container test-container: <nil>
STEP: delete the pod
Apr 29 07:39:40.269: INFO: Waiting for pod pod-bfb5ed2e-305a-4e8e-9e54-c1573c16da54 to disappear
Apr 29 07:39:40.271: INFO: Pod pod-bfb5ed2e-305a-4e8e-9e54-c1573c16da54 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:39:40.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7660" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":219,"skipped":4185,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:39:40.282: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 29 07:39:40.345: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  c29bcdc8-4da5-41f2-bcce-8001e2493ba7 118451 0 2022-04-29 07:39:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-29 07:39:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 07:39:40.345: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  c29bcdc8-4da5-41f2-bcce-8001e2493ba7 118451 0 2022-04-29 07:39:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-29 07:39:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 29 07:39:40.351: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  c29bcdc8-4da5-41f2-bcce-8001e2493ba7 118452 0 2022-04-29 07:39:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-29 07:39:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 07:39:40.351: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  c29bcdc8-4da5-41f2-bcce-8001e2493ba7 118452 0 2022-04-29 07:39:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-29 07:39:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 29 07:39:40.358: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  c29bcdc8-4da5-41f2-bcce-8001e2493ba7 118453 0 2022-04-29 07:39:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-29 07:39:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 07:39:40.358: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  c29bcdc8-4da5-41f2-bcce-8001e2493ba7 118453 0 2022-04-29 07:39:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-29 07:39:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 29 07:39:40.394: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  c29bcdc8-4da5-41f2-bcce-8001e2493ba7 118454 0 2022-04-29 07:39:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-29 07:39:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 07:39:40.394: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1576  c29bcdc8-4da5-41f2-bcce-8001e2493ba7 118454 0 2022-04-29 07:39:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-04-29 07:39:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 29 07:39:40.399: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1576  947d9202-b19a-4a56-a284-d2d4d8d00782 118455 0 2022-04-29 07:39:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-04-29 07:39:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 07:39:40.399: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1576  947d9202-b19a-4a56-a284-d2d4d8d00782 118455 0 2022-04-29 07:39:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-04-29 07:39:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 29 07:39:50.413: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1576  947d9202-b19a-4a56-a284-d2d4d8d00782 118522 0 2022-04-29 07:39:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-04-29 07:39:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 07:39:50.413: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1576  947d9202-b19a-4a56-a284-d2d4d8d00782 118522 0 2022-04-29 07:39:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-04-29 07:39:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:40:00.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1576" for this suite.

• [SLOW TEST:20.145 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":220,"skipped":4188,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:40:00.427: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Apr 29 07:40:00.462: INFO: The status of Pod pod-hostip-b162cc3c-e405-40d2-9eeb-12acc4ceb0cf is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:40:02.467: INFO: The status of Pod pod-hostip-b162cc3c-e405-40d2-9eeb-12acc4ceb0cf is Running (Ready = true)
Apr 29 07:40:02.473: INFO: Pod pod-hostip-b162cc3c-e405-40d2-9eeb-12acc4ceb0cf has hostIP: 123.58.176.151
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:40:02.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4608" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":221,"skipped":4194,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:40:02.483: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:40:02.512: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 29 07:40:06.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7277 --namespace=crd-publish-openapi-7277 create -f -'
Apr 29 07:40:07.862: INFO: stderr: ""
Apr 29 07:40:07.862: INFO: stdout: "e2e-test-crd-publish-openapi-1711-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 29 07:40:07.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7277 --namespace=crd-publish-openapi-7277 delete e2e-test-crd-publish-openapi-1711-crds test-cr'
Apr 29 07:40:07.933: INFO: stderr: ""
Apr 29 07:40:07.933: INFO: stdout: "e2e-test-crd-publish-openapi-1711-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr 29 07:40:07.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7277 --namespace=crd-publish-openapi-7277 apply -f -'
Apr 29 07:40:08.126: INFO: stderr: ""
Apr 29 07:40:08.126: INFO: stdout: "e2e-test-crd-publish-openapi-1711-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 29 07:40:08.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7277 --namespace=crd-publish-openapi-7277 delete e2e-test-crd-publish-openapi-1711-crds test-cr'
Apr 29 07:40:08.198: INFO: stderr: ""
Apr 29 07:40:08.198: INFO: stdout: "e2e-test-crd-publish-openapi-1711-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Apr 29 07:40:08.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7277 explain e2e-test-crd-publish-openapi-1711-crds'
Apr 29 07:40:08.384: INFO: stderr: ""
Apr 29 07:40:08.384: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1711-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:40:12.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7277" for this suite.

• [SLOW TEST:10.193 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":222,"skipped":4198,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:40:12.676: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Apr 29 07:40:12.721: INFO: observed Pod pod-test in namespace pods-4734 in phase Pending with labels: map[test-pod-static:true] & conditions []
Apr 29 07:40:12.722: INFO: observed Pod pod-test in namespace pods-4734 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:12 +0000 UTC  }]
Apr 29 07:40:12.745: INFO: observed Pod pod-test in namespace pods-4734 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:12 +0000 UTC  }]
Apr 29 07:40:13.279: INFO: observed Pod pod-test in namespace pods-4734 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:12 +0000 UTC  }]
Apr 29 07:40:14.021: INFO: Found Pod pod-test in namespace pods-4734 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:40:12 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Apr 29 07:40:14.034: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Apr 29 07:40:14.055: INFO: observed event type ADDED
Apr 29 07:40:14.056: INFO: observed event type MODIFIED
Apr 29 07:40:14.056: INFO: observed event type MODIFIED
Apr 29 07:40:14.056: INFO: observed event type MODIFIED
Apr 29 07:40:14.056: INFO: observed event type MODIFIED
Apr 29 07:40:14.056: INFO: observed event type MODIFIED
Apr 29 07:40:14.057: INFO: observed event type MODIFIED
Apr 29 07:40:14.057: INFO: observed event type MODIFIED
Apr 29 07:40:16.024: INFO: observed event type MODIFIED
Apr 29 07:40:16.285: INFO: observed event type MODIFIED
Apr 29 07:40:17.030: INFO: observed event type MODIFIED
Apr 29 07:40:17.035: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:40:17.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4734" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":223,"skipped":4202,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:40:17.046: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 07:40:17.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dfa836fb-4e28-4fcb-81ef-84771a0b8aae" in namespace "projected-9560" to be "Succeeded or Failed"
Apr 29 07:40:17.129: INFO: Pod "downwardapi-volume-dfa836fb-4e28-4fcb-81ef-84771a0b8aae": Phase="Pending", Reason="", readiness=false. Elapsed: 3.895551ms
Apr 29 07:40:19.134: INFO: Pod "downwardapi-volume-dfa836fb-4e28-4fcb-81ef-84771a0b8aae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008669278s
STEP: Saw pod success
Apr 29 07:40:19.134: INFO: Pod "downwardapi-volume-dfa836fb-4e28-4fcb-81ef-84771a0b8aae" satisfied condition "Succeeded or Failed"
Apr 29 07:40:19.137: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-dfa836fb-4e28-4fcb-81ef-84771a0b8aae container client-container: <nil>
STEP: delete the pod
Apr 29 07:40:19.151: INFO: Waiting for pod downwardapi-volume-dfa836fb-4e28-4fcb-81ef-84771a0b8aae to disappear
Apr 29 07:40:19.153: INFO: Pod downwardapi-volume-dfa836fb-4e28-4fcb-81ef-84771a0b8aae no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:40:19.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9560" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":224,"skipped":4208,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:40:19.161: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8687, will wait for the garbage collector to delete the pods
Apr 29 07:40:21.290: INFO: Deleting Job.batch foo took: 6.115982ms
Apr 29 07:40:21.391: INFO: Terminating Job.batch foo pods took: 100.995549ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:40:55.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8687" for this suite.

• [SLOW TEST:36.044 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":225,"skipped":4213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:40:55.206: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:40:55.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2411" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":226,"skipped":4289,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:40:55.308: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 29 07:40:55.362: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Apr 29 07:40:55.366: INFO: starting watch
STEP: patching
STEP: updating
Apr 29 07:40:55.381: INFO: waiting for watch events with expected annotations
Apr 29 07:40:55.381: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:40:55.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4901" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":227,"skipped":4317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:40:55.418: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:40:55.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6898" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":228,"skipped":4345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:40:55.489: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Apr 29 07:40:55.637: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 29 07:41:00.642: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Apr 29 07:41:00.645: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Apr 29 07:41:00.652: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Apr 29 07:41:00.653: INFO: Observed &ReplicaSet event: ADDED
Apr 29 07:41:00.653: INFO: Observed &ReplicaSet event: MODIFIED
Apr 29 07:41:00.653: INFO: Observed &ReplicaSet event: MODIFIED
Apr 29 07:41:00.654: INFO: Observed &ReplicaSet event: MODIFIED
Apr 29 07:41:00.654: INFO: Found replicaset test-rs in namespace replicaset-6573 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 29 07:41:00.654: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Apr 29 07:41:00.654: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr 29 07:41:00.658: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Apr 29 07:41:00.660: INFO: Observed &ReplicaSet event: ADDED
Apr 29 07:41:00.660: INFO: Observed &ReplicaSet event: MODIFIED
Apr 29 07:41:00.660: INFO: Observed &ReplicaSet event: MODIFIED
Apr 29 07:41:00.660: INFO: Observed &ReplicaSet event: MODIFIED
Apr 29 07:41:00.660: INFO: Observed replicaset test-rs in namespace replicaset-6573 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 29 07:41:00.661: INFO: Observed &ReplicaSet event: MODIFIED
Apr 29 07:41:00.661: INFO: Found replicaset test-rs in namespace replicaset-6573 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Apr 29 07:41:00.661: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:41:00.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6573" for this suite.

• [SLOW TEST:5.236 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":229,"skipped":4375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:41:00.725: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-1523
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1523
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1523
Apr 29 07:41:00.950: INFO: Found 0 stateful pods, waiting for 1
Apr 29 07:41:10.956: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 29 07:41:10.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-1523 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 07:41:11.107: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 07:41:11.107: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 07:41:11.107: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 07:41:11.110: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 29 07:41:21.116: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 07:41:21.116: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 07:41:21.140: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999777s
Apr 29 07:41:22.145: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.986106888s
Apr 29 07:41:23.149: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.980919631s
Apr 29 07:41:24.153: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977422434s
Apr 29 07:41:25.158: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.973265343s
Apr 29 07:41:26.162: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.968294304s
Apr 29 07:41:27.167: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.96344166s
Apr 29 07:41:28.171: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.958935841s
Apr 29 07:41:29.176: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.954826463s
Apr 29 07:41:30.181: INFO: Verifying statefulset ss doesn't scale past 1 for another 950.641879ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1523
Apr 29 07:41:31.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-1523 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 07:41:31.337: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 07:41:31.337: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 07:41:31.337: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 07:41:31.340: INFO: Found 1 stateful pods, waiting for 3
Apr 29 07:41:41.348: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 07:41:41.348: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 07:41:41.348: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 29 07:41:41.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-1523 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 07:41:41.502: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 07:41:41.502: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 07:41:41.502: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 07:41:41.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-1523 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 07:41:41.664: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 07:41:41.664: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 07:41:41.664: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 07:41:41.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-1523 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 07:41:41.807: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 07:41:41.807: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 07:41:41.807: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 07:41:41.807: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 07:41:41.811: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 29 07:41:51.822: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 07:41:51.822: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 07:41:51.822: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 07:41:51.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999981s
Apr 29 07:41:52.840: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996827407s
Apr 29 07:41:53.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990866774s
Apr 29 07:41:54.852: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985248614s
Apr 29 07:41:55.858: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977285216s
Apr 29 07:41:56.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972534587s
Apr 29 07:41:57.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964361073s
Apr 29 07:41:58.877: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958834597s
Apr 29 07:41:59.882: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.953706181s
Apr 29 07:42:00.891: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.878332ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1523
Apr 29 07:42:01.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-1523 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 07:42:02.038: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 07:42:02.038: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 07:42:02.038: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 07:42:02.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-1523 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 07:42:02.194: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 07:42:02.194: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 07:42:02.194: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 07:42:02.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-1523 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 07:42:02.369: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 07:42:02.369: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 07:42:02.369: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 07:42:02.369: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Apr 29 07:42:12.387: INFO: Deleting all statefulset in ns statefulset-1523
Apr 29 07:42:12.390: INFO: Scaling statefulset ss to 0
Apr 29 07:42:12.407: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 07:42:12.410: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:42:12.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1523" for this suite.

• [SLOW TEST:71.718 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":230,"skipped":4404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:42:12.444: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 29 07:42:12.641: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Apr 29 07:42:12.644: INFO: starting watch
STEP: patching
STEP: updating
Apr 29 07:42:12.660: INFO: waiting for watch events with expected annotations
Apr 29 07:42:12.660: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:42:12.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6108" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":231,"skipped":4450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:42:12.702: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Apr 29 07:42:12.750: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:42:14.755: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Apr 29 07:42:14.766: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:42:16.772: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Apr 29 07:42:16.783: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 07:42:16.785: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 07:42:18.786: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 07:42:18.791: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 07:42:20.786: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 07:42:20.791: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:42:20.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7183" for this suite.

• [SLOW TEST:8.117 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":232,"skipped":4473,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:42:20.819: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 07:42:21.367: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 07:42:24.386: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:42:24.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2431" for this suite.
STEP: Destroying namespace "webhook-2431-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":233,"skipped":4483,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:42:24.470: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2474
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-2474
Apr 29 07:42:24.550: INFO: Found 0 stateful pods, waiting for 1
Apr 29 07:42:34.555: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Apr 29 07:42:34.576: INFO: Deleting all statefulset in ns statefulset-2474
Apr 29 07:42:34.579: INFO: Scaling statefulset ss to 0
Apr 29 07:42:44.601: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 07:42:44.603: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:42:44.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2474" for this suite.

• [SLOW TEST:20.159 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":234,"skipped":4493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:42:44.629: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-da24e61d-bd3a-4585-8a33-d5132402f728
STEP: Creating a pod to test consume secrets
Apr 29 07:42:44.668: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-21336867-b77f-418e-a135-c5f411b846f8" in namespace "projected-2509" to be "Succeeded or Failed"
Apr 29 07:42:44.673: INFO: Pod "pod-projected-secrets-21336867-b77f-418e-a135-c5f411b846f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.629129ms
Apr 29 07:42:46.678: INFO: Pod "pod-projected-secrets-21336867-b77f-418e-a135-c5f411b846f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009787789s
STEP: Saw pod success
Apr 29 07:42:46.678: INFO: Pod "pod-projected-secrets-21336867-b77f-418e-a135-c5f411b846f8" satisfied condition "Succeeded or Failed"
Apr 29 07:42:46.681: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-secrets-21336867-b77f-418e-a135-c5f411b846f8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 07:42:46.697: INFO: Waiting for pod pod-projected-secrets-21336867-b77f-418e-a135-c5f411b846f8 to disappear
Apr 29 07:42:46.699: INFO: Pod pod-projected-secrets-21336867-b77f-418e-a135-c5f411b846f8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:42:46.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2509" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":235,"skipped":4528,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:42:46.707: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Apr 29 07:42:46.749: INFO: Waiting up to 5m0s for pod "var-expansion-db5c0b44-77d1-48cd-a497-c69f8459a514" in namespace "var-expansion-6466" to be "Succeeded or Failed"
Apr 29 07:42:46.753: INFO: Pod "var-expansion-db5c0b44-77d1-48cd-a497-c69f8459a514": Phase="Pending", Reason="", readiness=false. Elapsed: 3.079839ms
Apr 29 07:42:48.757: INFO: Pod "var-expansion-db5c0b44-77d1-48cd-a497-c69f8459a514": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007988032s
STEP: Saw pod success
Apr 29 07:42:48.757: INFO: Pod "var-expansion-db5c0b44-77d1-48cd-a497-c69f8459a514" satisfied condition "Succeeded or Failed"
Apr 29 07:42:48.760: INFO: Trying to get logs from node localhost.localdomain151 pod var-expansion-db5c0b44-77d1-48cd-a497-c69f8459a514 container dapi-container: <nil>
STEP: delete the pod
Apr 29 07:42:48.775: INFO: Waiting for pod var-expansion-db5c0b44-77d1-48cd-a497-c69f8459a514 to disappear
Apr 29 07:42:48.777: INFO: Pod var-expansion-db5c0b44-77d1-48cd-a497-c69f8459a514 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:42:48.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6466" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":236,"skipped":4543,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:42:48.785: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:42:48.845: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 29 07:42:48.853: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 29 07:42:53.858: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 29 07:42:53.858: INFO: Creating deployment "test-rolling-update-deployment"
Apr 29 07:42:53.862: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 29 07:42:53.866: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 29 07:42:55.874: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 29 07:42:55.876: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr 29 07:42:55.883: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8627  fa2b5ec4-19d6-4d7b-8081-1cf24e89f034 120212 1 2022-04-29 07:42:53 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-04-29 07:42:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 07:42:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cfca78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-04-29 07:42:53 +0000 UTC,LastTransitionTime:2022-04-29 07:42:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-796dbc4547" has successfully progressed.,LastUpdateTime:2022-04-29 07:42:55 +0000 UTC,LastTransitionTime:2022-04-29 07:42:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 29 07:42:55.889: INFO: New ReplicaSet "test-rolling-update-deployment-796dbc4547" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-796dbc4547  deployment-8627  aa663059-a7d9-410a-a0fb-ae0f62ae5ce1 120202 1 2022-04-29 07:42:53 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment fa2b5ec4-19d6-4d7b-8081-1cf24e89f034 0xc005cfcf87 0xc005cfcf88}] []  [{kube-controller-manager Update apps/v1 2022-04-29 07:42:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa2b5ec4-19d6-4d7b-8081-1cf24e89f034\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 07:42:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 796dbc4547,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cfd038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 29 07:42:55.889: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 29 07:42:55.889: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8627  cfe4ec5e-5600-4198-b2e9-37530162f270 120211 2 2022-04-29 07:42:48 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment fa2b5ec4-19d6-4d7b-8081-1cf24e89f034 0xc005cfce57 0xc005cfce58}] []  [{e2e.test Update apps/v1 2022-04-29 07:42:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 07:42:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa2b5ec4-19d6-4d7b-8081-1cf24e89f034\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-04-29 07:42:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005cfcf18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 07:42:55.892: INFO: Pod "test-rolling-update-deployment-796dbc4547-xcw5s" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-796dbc4547-xcw5s test-rolling-update-deployment-796dbc4547- deployment-8627  234b6d5e-1f10-4de8-800f-d759ace9054e 120201 0 2022-04-29 07:42:53 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[cni.projectcalico.org/podIP:172.16.115.152/32 cni.projectcalico.org/podIPs:172.16.115.152/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-796dbc4547 aa663059-a7d9-410a-a0fb-ae0f62ae5ce1 0xc005cfd497 0xc005cfd498}] []  [{kube-controller-manager Update v1 2022-04-29 07:42:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa663059-a7d9-410a-a0fb-ae0f62ae5ce1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-04-29 07:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-04-29 07:42:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.115.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8zx2v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8zx2v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:42:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:42:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:42:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:42:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:172.16.115.152,StartTime:2022-04-29 07:42:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 07:42:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://47520b600a1a46405b62e824f26f54a7dfe6ce531c98dbf3cdbf46fcfd61b707,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.115.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:42:55.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8627" for this suite.

• [SLOW TEST:7.115 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":237,"skipped":4556,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:42:55.901: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-f20b89ca-5f01-4f3d-be1b-f8a25da02530
STEP: Creating a pod to test consume configMaps
Apr 29 07:42:55.943: INFO: Waiting up to 5m0s for pod "pod-configmaps-f83cef5e-bbda-461e-b5f0-2c16f65a03f0" in namespace "configmap-4845" to be "Succeeded or Failed"
Apr 29 07:42:55.946: INFO: Pod "pod-configmaps-f83cef5e-bbda-461e-b5f0-2c16f65a03f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.917609ms
Apr 29 07:42:57.950: INFO: Pod "pod-configmaps-f83cef5e-bbda-461e-b5f0-2c16f65a03f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007099513s
STEP: Saw pod success
Apr 29 07:42:57.950: INFO: Pod "pod-configmaps-f83cef5e-bbda-461e-b5f0-2c16f65a03f0" satisfied condition "Succeeded or Failed"
Apr 29 07:42:57.953: INFO: Trying to get logs from node localhost.localdomain151 pod pod-configmaps-f83cef5e-bbda-461e-b5f0-2c16f65a03f0 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 07:42:57.970: INFO: Waiting for pod pod-configmaps-f83cef5e-bbda-461e-b5f0-2c16f65a03f0 to disappear
Apr 29 07:42:57.972: INFO: Pod pod-configmaps-f83cef5e-bbda-461e-b5f0-2c16f65a03f0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:42:57.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4845" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":238,"skipped":4602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:42:57.980: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Apr 29 07:42:58.051: INFO: Found Service test-service-t8cp5 in namespace services-4656 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Apr 29 07:42:58.051: INFO: Service test-service-t8cp5 created
STEP: Getting /status
Apr 29 07:42:58.053: INFO: Service test-service-t8cp5 has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Apr 29 07:42:58.058: INFO: observed Service test-service-t8cp5 in namespace services-4656 with annotations: map[] & LoadBalancer: {[]}
Apr 29 07:42:58.058: INFO: Found Service test-service-t8cp5 in namespace services-4656 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Apr 29 07:42:58.058: INFO: Service test-service-t8cp5 has service status patched
STEP: updating the ServiceStatus
Apr 29 07:42:58.064: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Apr 29 07:42:58.065: INFO: Observed Service test-service-t8cp5 in namespace services-4656 with annotations: map[] & Conditions: {[]}
Apr 29 07:42:58.065: INFO: Observed event: &Service{ObjectMeta:{test-service-t8cp5  services-4656  73b39d6b-8bc6-4714-972e-862193168e30 120263 0 2022-04-29 07:42:58 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-04-29 07:42:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-04-29 07:42:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.103.55.68,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.103.55.68],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Apr 29 07:42:58.066: INFO: Found Service test-service-t8cp5 in namespace services-4656 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 29 07:42:58.066: INFO: Service test-service-t8cp5 has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Apr 29 07:42:58.073: INFO: observed Service test-service-t8cp5 in namespace services-4656 with labels: map[test-service-static:true]
Apr 29 07:42:58.073: INFO: observed Service test-service-t8cp5 in namespace services-4656 with labels: map[test-service-static:true]
Apr 29 07:42:58.073: INFO: observed Service test-service-t8cp5 in namespace services-4656 with labels: map[test-service-static:true]
Apr 29 07:42:58.073: INFO: Found Service test-service-t8cp5 in namespace services-4656 with labels: map[test-service:patched test-service-static:true]
Apr 29 07:42:58.073: INFO: Service test-service-t8cp5 patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Apr 29 07:42:58.085: INFO: Observed event: ADDED
Apr 29 07:42:58.086: INFO: Observed event: MODIFIED
Apr 29 07:42:58.086: INFO: Observed event: MODIFIED
Apr 29 07:42:58.086: INFO: Observed event: MODIFIED
Apr 29 07:42:58.086: INFO: Found Service test-service-t8cp5 in namespace services-4656 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Apr 29 07:42:58.086: INFO: Service test-service-t8cp5 deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:42:58.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4656" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":239,"skipped":4625,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:42:58.095: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-6816
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6816 to expose endpoints map[]
Apr 29 07:42:58.172: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Apr 29 07:42:59.179: INFO: successfully validated that service endpoint-test2 in namespace services-6816 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6816
Apr 29 07:42:59.189: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:43:01.194: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6816 to expose endpoints map[pod1:[80]]
Apr 29 07:43:01.204: INFO: successfully validated that service endpoint-test2 in namespace services-6816 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Apr 29 07:43:01.204: INFO: Creating new exec pod
Apr 29 07:43:04.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-6816 exec execpod74kb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Apr 29 07:43:04.356: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr 29 07:43:04.356: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:43:04.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-6816 exec execpod74kb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.59.8 80'
Apr 29 07:43:04.498: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.59.8 80\nConnection to 10.101.59.8 80 port [tcp/http] succeeded!\n"
Apr 29 07:43:04.498: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-6816
Apr 29 07:43:04.506: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:43:06.511: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6816 to expose endpoints map[pod1:[80] pod2:[80]]
Apr 29 07:43:06.523: INFO: successfully validated that service endpoint-test2 in namespace services-6816 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Apr 29 07:43:07.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-6816 exec execpod74kb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Apr 29 07:43:07.696: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr 29 07:43:07.696: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:43:07.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-6816 exec execpod74kb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.59.8 80'
Apr 29 07:43:07.835: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.59.8 80\nConnection to 10.101.59.8 80 port [tcp/http] succeeded!\n"
Apr 29 07:43:07.835: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6816
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6816 to expose endpoints map[pod2:[80]]
Apr 29 07:43:07.864: INFO: successfully validated that service endpoint-test2 in namespace services-6816 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Apr 29 07:43:08.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-6816 exec execpod74kb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Apr 29 07:43:09.021: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr 29 07:43:09.021: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:43:09.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-6816 exec execpod74kb6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.59.8 80'
Apr 29 07:43:09.162: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.59.8 80\nConnection to 10.101.59.8 80 port [tcp/http] succeeded!\n"
Apr 29 07:43:09.162: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-6816
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6816 to expose endpoints map[]
Apr 29 07:43:09.183: INFO: successfully validated that service endpoint-test2 in namespace services-6816 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:43:09.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6816" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.114 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":240,"skipped":4671,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:43:09.209: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1692.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1692.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1692.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1692.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 07:43:11.273: INFO: DNS probes using dns-1692/dns-test-4935f665-85e1-48af-933a-ffbec6e9e6f0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:43:11.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1692" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":241,"skipped":4698,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:43:11.302: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-a56a3bbe-81de-48f7-b315-e5ad9aa3f41c
STEP: Creating a pod to test consume secrets
Apr 29 07:43:11.350: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-32dd9e55-3bda-432e-8ca1-dc92eaf5579b" in namespace "projected-335" to be "Succeeded or Failed"
Apr 29 07:43:11.353: INFO: Pod "pod-projected-secrets-32dd9e55-3bda-432e-8ca1-dc92eaf5579b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.005228ms
Apr 29 07:43:13.358: INFO: Pod "pod-projected-secrets-32dd9e55-3bda-432e-8ca1-dc92eaf5579b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007518535s
STEP: Saw pod success
Apr 29 07:43:13.358: INFO: Pod "pod-projected-secrets-32dd9e55-3bda-432e-8ca1-dc92eaf5579b" satisfied condition "Succeeded or Failed"
Apr 29 07:43:13.360: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-secrets-32dd9e55-3bda-432e-8ca1-dc92eaf5579b container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 07:43:13.375: INFO: Waiting for pod pod-projected-secrets-32dd9e55-3bda-432e-8ca1-dc92eaf5579b to disappear
Apr 29 07:43:13.377: INFO: Pod pod-projected-secrets-32dd9e55-3bda-432e-8ca1-dc92eaf5579b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:43:13.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-335" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4709,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:43:13.386: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Apr 29 07:43:53.495: INFO: The status of Pod kube-controller-manager-localhost.localdomain is Running (Ready = true)
E0429 07:43:53.519003      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:43:54.546818      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:43:55.573124      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:43:56.600570      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:43:57.623177      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:43:58.653576      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:00.707821      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:04.810855      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:08.923418      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:09.950058      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:12.005930      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:14.067729      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:15.095987      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:15.809187      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:17.869370      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:19.939416      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:20.965329      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:25.074787      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:26.103818      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:27.843564      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:28.875113      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:30.928269      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:31.958415      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:32.986404      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:36.072671      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:37.813652      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:38.844414      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:39.871750      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:42.971548      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:43.998467      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:46.056120      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:49.855095      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:50.884410      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:51.913332      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:52.941494      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:53.968585      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:55.004797      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:56.033553      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:57.063636      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:58.090721      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:44:59.815265      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:00.848041      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:01.879173      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:02.908156      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:03.949654      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:07.075913      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:08.117084      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:09.141004      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:10.167832      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:10.192593      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:11.218696      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:12.245474      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:14.297233      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:17.374831      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:45:19.436461      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
Apr 29 07:45:20.464: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Apr 29 07:45:20.464: INFO: Deleting pod "simpletest.rc-2c6gv" in namespace "gc-7195"
Apr 29 07:45:20.477: INFO: Deleting pod "simpletest.rc-2f9jd" in namespace "gc-7195"
Apr 29 07:45:20.492: INFO: Deleting pod "simpletest.rc-2lmtr" in namespace "gc-7195"
Apr 29 07:45:20.509: INFO: Deleting pod "simpletest.rc-2pjx8" in namespace "gc-7195"
Apr 29 07:45:20.518: INFO: Deleting pod "simpletest.rc-4ckv4" in namespace "gc-7195"
Apr 29 07:45:20.552: INFO: Deleting pod "simpletest.rc-4hsg2" in namespace "gc-7195"
Apr 29 07:45:20.566: INFO: Deleting pod "simpletest.rc-4lq5s" in namespace "gc-7195"
Apr 29 07:45:20.587: INFO: Deleting pod "simpletest.rc-4tmng" in namespace "gc-7195"
Apr 29 07:45:20.628: INFO: Deleting pod "simpletest.rc-4wfxz" in namespace "gc-7195"
Apr 29 07:45:20.641: INFO: Deleting pod "simpletest.rc-4zklz" in namespace "gc-7195"
Apr 29 07:45:20.727: INFO: Deleting pod "simpletest.rc-5b9lb" in namespace "gc-7195"
Apr 29 07:45:20.739: INFO: Deleting pod "simpletest.rc-5mp4b" in namespace "gc-7195"
Apr 29 07:45:20.829: INFO: Deleting pod "simpletest.rc-5pq6t" in namespace "gc-7195"
Apr 29 07:45:20.839: INFO: Deleting pod "simpletest.rc-5thpv" in namespace "gc-7195"
Apr 29 07:45:20.849: INFO: Deleting pod "simpletest.rc-66kcx" in namespace "gc-7195"
Apr 29 07:45:20.858: INFO: Deleting pod "simpletest.rc-6bclp" in namespace "gc-7195"
Apr 29 07:45:20.871: INFO: Deleting pod "simpletest.rc-6hpsf" in namespace "gc-7195"
Apr 29 07:45:20.887: INFO: Deleting pod "simpletest.rc-6pj7w" in namespace "gc-7195"
Apr 29 07:45:20.900: INFO: Deleting pod "simpletest.rc-6x92n" in namespace "gc-7195"
Apr 29 07:45:20.922: INFO: Deleting pod "simpletest.rc-6x9zk" in namespace "gc-7195"
Apr 29 07:45:20.958: INFO: Deleting pod "simpletest.rc-7f8md" in namespace "gc-7195"
Apr 29 07:45:20.969: INFO: Deleting pod "simpletest.rc-7pnmb" in namespace "gc-7195"
Apr 29 07:45:20.994: INFO: Deleting pod "simpletest.rc-7rfbw" in namespace "gc-7195"
Apr 29 07:45:21.005: INFO: Deleting pod "simpletest.rc-84dgs" in namespace "gc-7195"
Apr 29 07:45:21.029: INFO: Deleting pod "simpletest.rc-99jcb" in namespace "gc-7195"
Apr 29 07:45:21.051: INFO: Deleting pod "simpletest.rc-9hswg" in namespace "gc-7195"
Apr 29 07:45:21.077: INFO: Deleting pod "simpletest.rc-9nmx6" in namespace "gc-7195"
Apr 29 07:45:21.099: INFO: Deleting pod "simpletest.rc-b2nlt" in namespace "gc-7195"
Apr 29 07:45:21.147: INFO: Deleting pod "simpletest.rc-b5sgb" in namespace "gc-7195"
Apr 29 07:45:21.172: INFO: Deleting pod "simpletest.rc-bvlbf" in namespace "gc-7195"
Apr 29 07:45:21.189: INFO: Deleting pod "simpletest.rc-chf2m" in namespace "gc-7195"
Apr 29 07:45:21.204: INFO: Deleting pod "simpletest.rc-cjxcc" in namespace "gc-7195"
Apr 29 07:45:21.235: INFO: Deleting pod "simpletest.rc-cqmbz" in namespace "gc-7195"
Apr 29 07:45:21.255: INFO: Deleting pod "simpletest.rc-d4t4l" in namespace "gc-7195"
Apr 29 07:45:21.268: INFO: Deleting pod "simpletest.rc-d98zp" in namespace "gc-7195"
Apr 29 07:45:21.315: INFO: Deleting pod "simpletest.rc-d9sjs" in namespace "gc-7195"
Apr 29 07:45:21.349: INFO: Deleting pod "simpletest.rc-dcjs7" in namespace "gc-7195"
Apr 29 07:45:21.378: INFO: Deleting pod "simpletest.rc-dd4km" in namespace "gc-7195"
Apr 29 07:45:21.393: INFO: Deleting pod "simpletest.rc-dmgbl" in namespace "gc-7195"
Apr 29 07:45:21.412: INFO: Deleting pod "simpletest.rc-dnlq6" in namespace "gc-7195"
Apr 29 07:45:21.451: INFO: Deleting pod "simpletest.rc-dntr2" in namespace "gc-7195"
Apr 29 07:45:21.467: INFO: Deleting pod "simpletest.rc-dthgw" in namespace "gc-7195"
Apr 29 07:45:21.482: INFO: Deleting pod "simpletest.rc-dz8kz" in namespace "gc-7195"
Apr 29 07:45:21.498: INFO: Deleting pod "simpletest.rc-fhpqd" in namespace "gc-7195"
Apr 29 07:45:21.511: INFO: Deleting pod "simpletest.rc-fqdc4" in namespace "gc-7195"
Apr 29 07:45:21.540: INFO: Deleting pod "simpletest.rc-g46k2" in namespace "gc-7195"
Apr 29 07:45:21.560: INFO: Deleting pod "simpletest.rc-g76f2" in namespace "gc-7195"
Apr 29 07:45:21.575: INFO: Deleting pod "simpletest.rc-g9fzq" in namespace "gc-7195"
Apr 29 07:45:21.594: INFO: Deleting pod "simpletest.rc-gjwdd" in namespace "gc-7195"
Apr 29 07:45:21.640: INFO: Deleting pod "simpletest.rc-hcbvm" in namespace "gc-7195"
Apr 29 07:45:21.680: INFO: Deleting pod "simpletest.rc-hd8rn" in namespace "gc-7195"
Apr 29 07:45:21.697: INFO: Deleting pod "simpletest.rc-hqj76" in namespace "gc-7195"
Apr 29 07:45:21.710: INFO: Deleting pod "simpletest.rc-hv4r9" in namespace "gc-7195"
Apr 29 07:45:21.748: INFO: Deleting pod "simpletest.rc-hw6mb" in namespace "gc-7195"
Apr 29 07:45:21.770: INFO: Deleting pod "simpletest.rc-j2nfm" in namespace "gc-7195"
Apr 29 07:45:21.787: INFO: Deleting pod "simpletest.rc-jjvff" in namespace "gc-7195"
Apr 29 07:45:21.805: INFO: Deleting pod "simpletest.rc-jphjj" in namespace "gc-7195"
Apr 29 07:45:21.816: INFO: Deleting pod "simpletest.rc-jttvf" in namespace "gc-7195"
Apr 29 07:45:21.850: INFO: Deleting pod "simpletest.rc-jw92c" in namespace "gc-7195"
Apr 29 07:45:21.871: INFO: Deleting pod "simpletest.rc-jzt8v" in namespace "gc-7195"
Apr 29 07:45:21.885: INFO: Deleting pod "simpletest.rc-k6vzq" in namespace "gc-7195"
Apr 29 07:45:21.903: INFO: Deleting pod "simpletest.rc-kcppb" in namespace "gc-7195"
Apr 29 07:45:21.939: INFO: Deleting pod "simpletest.rc-kkh6j" in namespace "gc-7195"
Apr 29 07:45:21.967: INFO: Deleting pod "simpletest.rc-km8ks" in namespace "gc-7195"
Apr 29 07:45:21.989: INFO: Deleting pod "simpletest.rc-kp2ph" in namespace "gc-7195"
Apr 29 07:45:22.007: INFO: Deleting pod "simpletest.rc-kr7xc" in namespace "gc-7195"
Apr 29 07:45:22.022: INFO: Deleting pod "simpletest.rc-lg96d" in namespace "gc-7195"
Apr 29 07:45:22.063: INFO: Deleting pod "simpletest.rc-lj9j4" in namespace "gc-7195"
Apr 29 07:45:22.073: INFO: Deleting pod "simpletest.rc-lkt54" in namespace "gc-7195"
Apr 29 07:45:22.083: INFO: Deleting pod "simpletest.rc-lppn9" in namespace "gc-7195"
Apr 29 07:45:22.101: INFO: Deleting pod "simpletest.rc-m54dp" in namespace "gc-7195"
Apr 29 07:45:22.115: INFO: Deleting pod "simpletest.rc-mjzxt" in namespace "gc-7195"
Apr 29 07:45:22.159: INFO: Deleting pod "simpletest.rc-n4zdk" in namespace "gc-7195"
Apr 29 07:45:22.180: INFO: Deleting pod "simpletest.rc-n9xsn" in namespace "gc-7195"
Apr 29 07:45:22.195: INFO: Deleting pod "simpletest.rc-nnfw8" in namespace "gc-7195"
Apr 29 07:45:22.207: INFO: Deleting pod "simpletest.rc-pvdkw" in namespace "gc-7195"
Apr 29 07:45:22.219: INFO: Deleting pod "simpletest.rc-pvlgt" in namespace "gc-7195"
Apr 29 07:45:22.289: INFO: Deleting pod "simpletest.rc-q6w8j" in namespace "gc-7195"
Apr 29 07:45:22.312: INFO: Deleting pod "simpletest.rc-q8496" in namespace "gc-7195"
Apr 29 07:45:22.322: INFO: Deleting pod "simpletest.rc-r4v5m" in namespace "gc-7195"
Apr 29 07:45:22.358: INFO: Deleting pod "simpletest.rc-s29nc" in namespace "gc-7195"
Apr 29 07:45:22.378: INFO: Deleting pod "simpletest.rc-s9w45" in namespace "gc-7195"
Apr 29 07:45:22.388: INFO: Deleting pod "simpletest.rc-sdbmf" in namespace "gc-7195"
Apr 29 07:45:22.406: INFO: Deleting pod "simpletest.rc-sdm2n" in namespace "gc-7195"
Apr 29 07:45:22.434: INFO: Deleting pod "simpletest.rc-sjklg" in namespace "gc-7195"
Apr 29 07:45:22.451: INFO: Deleting pod "simpletest.rc-sqwlw" in namespace "gc-7195"
Apr 29 07:45:22.471: INFO: Deleting pod "simpletest.rc-tbsfd" in namespace "gc-7195"
Apr 29 07:45:22.483: INFO: Deleting pod "simpletest.rc-trvrl" in namespace "gc-7195"
Apr 29 07:45:22.501: INFO: Deleting pod "simpletest.rc-ts2f2" in namespace "gc-7195"
Apr 29 07:45:22.513: INFO: Deleting pod "simpletest.rc-tt6gm" in namespace "gc-7195"
Apr 29 07:45:22.550: INFO: Deleting pod "simpletest.rc-v4752" in namespace "gc-7195"
Apr 29 07:45:22.578: INFO: Deleting pod "simpletest.rc-v4nrn" in namespace "gc-7195"
Apr 29 07:45:22.643: INFO: Deleting pod "simpletest.rc-wx6cw" in namespace "gc-7195"
Apr 29 07:45:22.737: INFO: Deleting pod "simpletest.rc-x59x4" in namespace "gc-7195"
Apr 29 07:45:22.840: INFO: Deleting pod "simpletest.rc-x5gcl" in namespace "gc-7195"
Apr 29 07:45:22.942: INFO: Deleting pod "simpletest.rc-z4t67" in namespace "gc-7195"
Apr 29 07:45:22.974: INFO: Deleting pod "simpletest.rc-zdt26" in namespace "gc-7195"
Apr 29 07:45:23.034: INFO: Deleting pod "simpletest.rc-zhpwr" in namespace "gc-7195"
Apr 29 07:45:23.052: INFO: Deleting pod "simpletest.rc-zwlv4" in namespace "gc-7195"
Apr 29 07:45:23.070: INFO: Deleting pod "simpletest.rc-zzfmd" in namespace "gc-7195"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:45:23.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7195" for this suite.

• [SLOW TEST:129.725 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":243,"skipped":4727,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:45:23.112: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:45:30.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5698" for this suite.

• [SLOW TEST:7.191 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":244,"skipped":4746,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:45:30.302: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 29 07:45:30.526: INFO: Waiting up to 5m0s for pod "pod-100ab0f7-2d5c-4ee8-8657-9c62c5f7112f" in namespace "emptydir-606" to be "Succeeded or Failed"
Apr 29 07:45:30.530: INFO: Pod "pod-100ab0f7-2d5c-4ee8-8657-9c62c5f7112f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.318235ms
Apr 29 07:45:32.536: INFO: Pod "pod-100ab0f7-2d5c-4ee8-8657-9c62c5f7112f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009654754s
Apr 29 07:45:34.541: INFO: Pod "pod-100ab0f7-2d5c-4ee8-8657-9c62c5f7112f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015177961s
STEP: Saw pod success
Apr 29 07:45:34.541: INFO: Pod "pod-100ab0f7-2d5c-4ee8-8657-9c62c5f7112f" satisfied condition "Succeeded or Failed"
Apr 29 07:45:34.545: INFO: Trying to get logs from node localhost.localdomain151 pod pod-100ab0f7-2d5c-4ee8-8657-9c62c5f7112f container test-container: <nil>
STEP: delete the pod
Apr 29 07:45:34.693: INFO: Waiting for pod pod-100ab0f7-2d5c-4ee8-8657-9c62c5f7112f to disappear
Apr 29 07:45:34.695: INFO: Pod pod-100ab0f7-2d5c-4ee8-8657-9c62c5f7112f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:45:34.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-606" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":245,"skipped":4746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:45:34.705: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9203
STEP: creating service affinity-nodeport in namespace services-9203
STEP: creating replication controller affinity-nodeport in namespace services-9203
I0429 07:45:34.773827      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9203, replica count: 3
I0429 07:45:37.825861      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 07:45:37.835: INFO: Creating new exec pod
Apr 29 07:45:40.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-9203 exec execpod-affinitybd9xd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Apr 29 07:45:41.004: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Apr 29 07:45:41.004: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:45:41.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-9203 exec execpod-affinitybd9xd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.198.104 80'
Apr 29 07:45:41.136: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.198.104 80\nConnection to 10.107.198.104 80 port [tcp/http] succeeded!\n"
Apr 29 07:45:41.136: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:45:41.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-9203 exec execpod-affinitybd9xd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.150 31452'
Apr 29 07:45:41.284: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.150 31452\nConnection to 123.58.176.150 31452 port [tcp/*] succeeded!\n"
Apr 29 07:45:41.284: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:45:41.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-9203 exec execpod-affinitybd9xd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 123.58.176.151 31452'
Apr 29 07:45:41.429: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 123.58.176.151 31452\nConnection to 123.58.176.151 31452 port [tcp/*] succeeded!\n"
Apr 29 07:45:41.429: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 07:45:41.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-9203 exec execpod-affinitybd9xd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://123.58.176.150:31452/ ; done'
Apr 29 07:45:41.662: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n+ echo\n+ curl -q -s --connect-timeout 2 http://123.58.176.150:31452/\n"
Apr 29 07:45:41.662: INFO: stdout: "\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7\naffinity-nodeport-6dqw7"
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Received response from host: affinity-nodeport-6dqw7
Apr 29 07:45:41.662: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9203, will wait for the garbage collector to delete the pods
Apr 29 07:45:41.734: INFO: Deleting ReplicationController affinity-nodeport took: 5.526453ms
Apr 29 07:45:41.835: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.151569ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:45:43.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9203" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.053 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":246,"skipped":4787,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:45:43.758: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 07:45:44.493: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 07:45:47.510: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:45:47.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5491" for this suite.
STEP: Destroying namespace "webhook-5491-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":247,"skipped":4789,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:45:47.561: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:45:47.736: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-250fe905-2e0d-40e5-9438-f1b7112d6c39" in namespace "security-context-test-557" to be "Succeeded or Failed"
Apr 29 07:45:47.741: INFO: Pod "busybox-readonly-false-250fe905-2e0d-40e5-9438-f1b7112d6c39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.86529ms
Apr 29 07:45:49.745: INFO: Pod "busybox-readonly-false-250fe905-2e0d-40e5-9438-f1b7112d6c39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00865873s
Apr 29 07:45:49.745: INFO: Pod "busybox-readonly-false-250fe905-2e0d-40e5-9438-f1b7112d6c39" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:45:49.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-557" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":248,"skipped":4796,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:45:49.754: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-f49z
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 07:45:49.843: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-f49z" in namespace "subpath-613" to be "Succeeded or Failed"
Apr 29 07:45:49.845: INFO: Pod "pod-subpath-test-downwardapi-f49z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.098729ms
Apr 29 07:45:51.851: INFO: Pod "pod-subpath-test-downwardapi-f49z": Phase="Running", Reason="", readiness=true. Elapsed: 2.008107331s
Apr 29 07:45:53.856: INFO: Pod "pod-subpath-test-downwardapi-f49z": Phase="Running", Reason="", readiness=true. Elapsed: 4.0127032s
Apr 29 07:45:55.860: INFO: Pod "pod-subpath-test-downwardapi-f49z": Phase="Running", Reason="", readiness=true. Elapsed: 6.017048315s
Apr 29 07:45:57.865: INFO: Pod "pod-subpath-test-downwardapi-f49z": Phase="Running", Reason="", readiness=true. Elapsed: 8.021864188s
Apr 29 07:45:59.870: INFO: Pod "pod-subpath-test-downwardapi-f49z": Phase="Running", Reason="", readiness=true. Elapsed: 10.026469738s
Apr 29 07:46:01.875: INFO: Pod "pod-subpath-test-downwardapi-f49z": Phase="Running", Reason="", readiness=true. Elapsed: 12.031893019s
Apr 29 07:46:03.880: INFO: Pod "pod-subpath-test-downwardapi-f49z": Phase="Running", Reason="", readiness=true. Elapsed: 14.036582566s
Apr 29 07:46:05.884: INFO: Pod "pod-subpath-test-downwardapi-f49z": Phase="Running", Reason="", readiness=true. Elapsed: 16.040836013s
Apr 29 07:46:07.889: INFO: Pod "pod-subpath-test-downwardapi-f49z": Phase="Running", Reason="", readiness=true. Elapsed: 18.045865225s
Apr 29 07:46:09.901: INFO: Pod "pod-subpath-test-downwardapi-f49z": Phase="Running", Reason="", readiness=true. Elapsed: 20.058012354s
Apr 29 07:46:11.906: INFO: Pod "pod-subpath-test-downwardapi-f49z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.062593941s
STEP: Saw pod success
Apr 29 07:46:11.906: INFO: Pod "pod-subpath-test-downwardapi-f49z" satisfied condition "Succeeded or Failed"
Apr 29 07:46:11.908: INFO: Trying to get logs from node localhost.localdomain151 pod pod-subpath-test-downwardapi-f49z container test-container-subpath-downwardapi-f49z: <nil>
STEP: delete the pod
Apr 29 07:46:11.923: INFO: Waiting for pod pod-subpath-test-downwardapi-f49z to disappear
Apr 29 07:46:11.935: INFO: Pod pod-subpath-test-downwardapi-f49z no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-f49z
Apr 29 07:46:11.935: INFO: Deleting pod "pod-subpath-test-downwardapi-f49z" in namespace "subpath-613"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:46:11.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-613" for this suite.

• [SLOW TEST:22.192 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":249,"skipped":4797,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:46:11.947: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 07:46:11.986: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e378fb96-103d-43bc-a4eb-c65f073de054" in namespace "downward-api-6566" to be "Succeeded or Failed"
Apr 29 07:46:11.988: INFO: Pod "downwardapi-volume-e378fb96-103d-43bc-a4eb-c65f073de054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.154953ms
Apr 29 07:46:13.994: INFO: Pod "downwardapi-volume-e378fb96-103d-43bc-a4eb-c65f073de054": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007515172s
STEP: Saw pod success
Apr 29 07:46:13.994: INFO: Pod "downwardapi-volume-e378fb96-103d-43bc-a4eb-c65f073de054" satisfied condition "Succeeded or Failed"
Apr 29 07:46:13.996: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-e378fb96-103d-43bc-a4eb-c65f073de054 container client-container: <nil>
STEP: delete the pod
Apr 29 07:46:14.011: INFO: Waiting for pod downwardapi-volume-e378fb96-103d-43bc-a4eb-c65f073de054 to disappear
Apr 29 07:46:14.013: INFO: Pod downwardapi-volume-e378fb96-103d-43bc-a4eb-c65f073de054 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:46:14.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6566" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":250,"skipped":4808,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:46:14.022: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:46:34.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2632" for this suite.

• [SLOW TEST:20.213 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":251,"skipped":4823,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:46:34.235: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 07:46:34.337: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f57816c4-ce19-4548-a7be-eb932f81456a" in namespace "projected-8490" to be "Succeeded or Failed"
Apr 29 07:46:34.372: INFO: Pod "downwardapi-volume-f57816c4-ce19-4548-a7be-eb932f81456a": Phase="Pending", Reason="", readiness=false. Elapsed: 34.422565ms
Apr 29 07:46:36.379: INFO: Pod "downwardapi-volume-f57816c4-ce19-4548-a7be-eb932f81456a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041802478s
STEP: Saw pod success
Apr 29 07:46:36.379: INFO: Pod "downwardapi-volume-f57816c4-ce19-4548-a7be-eb932f81456a" satisfied condition "Succeeded or Failed"
Apr 29 07:46:36.381: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-f57816c4-ce19-4548-a7be-eb932f81456a container client-container: <nil>
STEP: delete the pod
Apr 29 07:46:36.397: INFO: Waiting for pod downwardapi-volume-f57816c4-ce19-4548-a7be-eb932f81456a to disappear
Apr 29 07:46:36.405: INFO: Pod downwardapi-volume-f57816c4-ce19-4548-a7be-eb932f81456a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:46:36.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8490" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":252,"skipped":4843,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:46:36.415: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-326/secret-test-ce4b582d-4dcf-4eaf-b15d-9af28b1dc81e
STEP: Creating a pod to test consume secrets
Apr 29 07:46:36.454: INFO: Waiting up to 5m0s for pod "pod-configmaps-5c2af0be-d825-4df7-b974-35c8e4151905" in namespace "secrets-326" to be "Succeeded or Failed"
Apr 29 07:46:36.457: INFO: Pod "pod-configmaps-5c2af0be-d825-4df7-b974-35c8e4151905": Phase="Pending", Reason="", readiness=false. Elapsed: 2.570864ms
Apr 29 07:46:38.462: INFO: Pod "pod-configmaps-5c2af0be-d825-4df7-b974-35c8e4151905": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007801389s
STEP: Saw pod success
Apr 29 07:46:38.462: INFO: Pod "pod-configmaps-5c2af0be-d825-4df7-b974-35c8e4151905" satisfied condition "Succeeded or Failed"
Apr 29 07:46:38.465: INFO: Trying to get logs from node localhost.localdomain151 pod pod-configmaps-5c2af0be-d825-4df7-b974-35c8e4151905 container env-test: <nil>
STEP: delete the pod
Apr 29 07:46:38.481: INFO: Waiting for pod pod-configmaps-5c2af0be-d825-4df7-b974-35c8e4151905 to disappear
Apr 29 07:46:38.483: INFO: Pod pod-configmaps-5c2af0be-d825-4df7-b974-35c8e4151905 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:46:38.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-326" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":253,"skipped":4864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:46:38.494: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-9c32aabf-ef32-4a9c-9918-69e4b1a294e5
STEP: Creating the pod
Apr 29 07:46:38.546: INFO: The status of Pod pod-configmaps-1e94ab92-11b9-446c-aa05-243b4ced0617 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:46:40.552: INFO: The status of Pod pod-configmaps-1e94ab92-11b9-446c-aa05-243b4ced0617 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-9c32aabf-ef32-4a9c-9918-69e4b1a294e5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:46:42.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8474" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":254,"skipped":4920,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:46:42.585: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:46:42.681: INFO: Create a RollingUpdate DaemonSet
Apr 29 07:46:42.725: INFO: Check that daemon pods launch on every node of the cluster
Apr 29 07:46:42.734: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:46:42.734: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:46:43.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:46:43.742: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:46:44.744: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 29 07:46:44.744: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Apr 29 07:46:44.744: INFO: Update the DaemonSet to trigger a rollout
Apr 29 07:46:44.753: INFO: Updating DaemonSet daemon-set
Apr 29 07:46:47.768: INFO: Roll back the DaemonSet before rollout is complete
Apr 29 07:46:47.776: INFO: Updating DaemonSet daemon-set
Apr 29 07:46:47.776: INFO: Make sure DaemonSet rollback is complete
Apr 29 07:46:47.780: INFO: Wrong image for pod: daemon-set-pj4sq. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Apr 29 07:46:47.780: INFO: Pod daemon-set-pj4sq is not available
Apr 29 07:46:51.790: INFO: Pod daemon-set-9vp9m is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-392, will wait for the garbage collector to delete the pods
Apr 29 07:46:51.861: INFO: Deleting DaemonSet.extensions daemon-set took: 6.622217ms
Apr 29 07:46:51.962: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.660326ms
Apr 29 07:46:53.965: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:46:53.965: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 29 07:46:53.968: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"124323"},"items":null}

Apr 29 07:46:53.970: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"124323"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:46:53.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-392" for this suite.

• [SLOW TEST:11.401 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":255,"skipped":4965,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:46:53.987: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Apr 29 07:46:54.019: INFO: Creating simple deployment test-deployment-c2nj5
Apr 29 07:46:54.029: INFO: deployment "test-deployment-c2nj5" doesn't have the required revision set
STEP: Getting /status
Apr 29 07:46:56.044: INFO: Deployment test-deployment-c2nj5 has Conditions: [{Available True 2022-04-29 07:46:55 +0000 UTC 2022-04-29 07:46:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-04-29 07:46:55 +0000 UTC 2022-04-29 07:46:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-c2nj5-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
Apr 29 07:46:56.051: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.April, 29, 7, 46, 55, 0, time.Local), LastTransitionTime:time.Date(2022, time.April, 29, 7, 46, 55, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.April, 29, 7, 46, 55, 0, time.Local), LastTransitionTime:time.Date(2022, time.April, 29, 7, 46, 54, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-c2nj5-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Apr 29 07:46:56.054: INFO: Observed &Deployment event: ADDED
Apr 29 07:46:56.054: INFO: Observed Deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-29 07:46:54 +0000 UTC 2022-04-29 07:46:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-c2nj5-764bc7c4b7"}
Apr 29 07:46:56.054: INFO: Observed &Deployment event: MODIFIED
Apr 29 07:46:56.054: INFO: Observed Deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-29 07:46:54 +0000 UTC 2022-04-29 07:46:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-c2nj5-764bc7c4b7"}
Apr 29 07:46:56.054: INFO: Observed Deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-04-29 07:46:54 +0000 UTC 2022-04-29 07:46:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 29 07:46:56.055: INFO: Observed &Deployment event: MODIFIED
Apr 29 07:46:56.055: INFO: Observed Deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-04-29 07:46:54 +0000 UTC 2022-04-29 07:46:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 29 07:46:56.055: INFO: Observed Deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-29 07:46:54 +0000 UTC 2022-04-29 07:46:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-c2nj5-764bc7c4b7" is progressing.}
Apr 29 07:46:56.055: INFO: Observed &Deployment event: MODIFIED
Apr 29 07:46:56.055: INFO: Observed Deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-04-29 07:46:55 +0000 UTC 2022-04-29 07:46:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 29 07:46:56.055: INFO: Observed Deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-29 07:46:55 +0000 UTC 2022-04-29 07:46:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-c2nj5-764bc7c4b7" has successfully progressed.}
Apr 29 07:46:56.055: INFO: Observed &Deployment event: MODIFIED
Apr 29 07:46:56.055: INFO: Observed Deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-04-29 07:46:55 +0000 UTC 2022-04-29 07:46:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 29 07:46:56.055: INFO: Observed Deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-29 07:46:55 +0000 UTC 2022-04-29 07:46:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-c2nj5-764bc7c4b7" has successfully progressed.}
Apr 29 07:46:56.055: INFO: Found Deployment test-deployment-c2nj5 in namespace deployment-2136 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 29 07:46:56.055: INFO: Deployment test-deployment-c2nj5 has an updated status
STEP: patching the Statefulset Status
Apr 29 07:46:56.055: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr 29 07:46:56.061: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Apr 29 07:46:56.063: INFO: Observed &Deployment event: ADDED
Apr 29 07:46:56.063: INFO: Observed deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-29 07:46:54 +0000 UTC 2022-04-29 07:46:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-c2nj5-764bc7c4b7"}
Apr 29 07:46:56.063: INFO: Observed &Deployment event: MODIFIED
Apr 29 07:46:56.064: INFO: Observed deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-29 07:46:54 +0000 UTC 2022-04-29 07:46:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-c2nj5-764bc7c4b7"}
Apr 29 07:46:56.064: INFO: Observed deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-04-29 07:46:54 +0000 UTC 2022-04-29 07:46:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 29 07:46:56.064: INFO: Observed &Deployment event: MODIFIED
Apr 29 07:46:56.064: INFO: Observed deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-04-29 07:46:54 +0000 UTC 2022-04-29 07:46:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 29 07:46:56.064: INFO: Observed deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-29 07:46:54 +0000 UTC 2022-04-29 07:46:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-c2nj5-764bc7c4b7" is progressing.}
Apr 29 07:46:56.064: INFO: Observed &Deployment event: MODIFIED
Apr 29 07:46:56.064: INFO: Observed deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-04-29 07:46:55 +0000 UTC 2022-04-29 07:46:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 29 07:46:56.064: INFO: Observed deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-29 07:46:55 +0000 UTC 2022-04-29 07:46:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-c2nj5-764bc7c4b7" has successfully progressed.}
Apr 29 07:46:56.064: INFO: Observed &Deployment event: MODIFIED
Apr 29 07:46:56.064: INFO: Observed deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-04-29 07:46:55 +0000 UTC 2022-04-29 07:46:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 29 07:46:56.064: INFO: Observed deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-04-29 07:46:55 +0000 UTC 2022-04-29 07:46:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-c2nj5-764bc7c4b7" has successfully progressed.}
Apr 29 07:46:56.064: INFO: Observed deployment test-deployment-c2nj5 in namespace deployment-2136 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 29 07:46:56.065: INFO: Observed &Deployment event: MODIFIED
Apr 29 07:46:56.065: INFO: Found deployment test-deployment-c2nj5 in namespace deployment-2136 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Apr 29 07:46:56.065: INFO: Deployment test-deployment-c2nj5 has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr 29 07:46:56.070: INFO: Deployment "test-deployment-c2nj5":
&Deployment{ObjectMeta:{test-deployment-c2nj5  deployment-2136  bf78f786-cbc0-4ed1-b9c7-f5e49c8460d3 124364 1 2022-04-29 07:46:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-04-29 07:46:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-04-29 07:46:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-04-29 07:46:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054b14a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-c2nj5-764bc7c4b7",LastUpdateTime:2022-04-29 07:46:56 +0000 UTC,LastTransitionTime:2022-04-29 07:46:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 29 07:46:56.073: INFO: New ReplicaSet "test-deployment-c2nj5-764bc7c4b7" of Deployment "test-deployment-c2nj5":
&ReplicaSet{ObjectMeta:{test-deployment-c2nj5-764bc7c4b7  deployment-2136  8fd4f206-d0e4-4ca1-822f-5afd6329b681 124358 1 2022-04-29 07:46:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-c2nj5 bf78f786-cbc0-4ed1-b9c7-f5e49c8460d3 0xc004e9df17 0xc004e9df18}] []  [{kube-controller-manager Update apps/v1 2022-04-29 07:46:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf78f786-cbc0-4ed1-b9c7-f5e49c8460d3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 07:46:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e9dfc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 29 07:46:56.076: INFO: Pod "test-deployment-c2nj5-764bc7c4b7-zcw9m" is available:
&Pod{ObjectMeta:{test-deployment-c2nj5-764bc7c4b7-zcw9m test-deployment-c2nj5-764bc7c4b7- deployment-2136  e9c38b97-eb39-488f-a630-96aa2b60a04c 124357 0 2022-04-29 07:46:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[cni.projectcalico.org/podIP:172.16.115.134/32 cni.projectcalico.org/podIPs:172.16.115.134/32] [{apps/v1 ReplicaSet test-deployment-c2nj5-764bc7c4b7 8fd4f206-d0e4-4ca1-822f-5afd6329b681 0xc0054b1880 0xc0054b1881}] []  [{calico Update v1 2022-04-29 07:46:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-04-29 07:46:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fd4f206-d0e4-4ca1-822f-5afd6329b681\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-04-29 07:46:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.115.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdrcv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdrcv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:46:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:46:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:46:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 07:46:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:172.16.115.134,StartTime:2022-04-29 07:46:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 07:46:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://954c5f9250b4a4bf66609d0b904b42bcd649156262f95d72c297d6eb802fc0ff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.115.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:46:56.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2136" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":256,"skipped":4976,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:46:56.084: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 07:46:56.437: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 07:46:59.458: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:46:59.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7997" for this suite.
STEP: Destroying namespace "webhook-7997-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":257,"skipped":4995,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:46:59.564: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-f2032d77-f3c1-49e4-ba94-46eb9e7884c3
STEP: Creating a pod to test consume secrets
Apr 29 07:46:59.654: INFO: Waiting up to 5m0s for pod "pod-secrets-5e306855-ffd7-4c87-92c9-fb17ea44e505" in namespace "secrets-2915" to be "Succeeded or Failed"
Apr 29 07:46:59.656: INFO: Pod "pod-secrets-5e306855-ffd7-4c87-92c9-fb17ea44e505": Phase="Pending", Reason="", readiness=false. Elapsed: 1.927223ms
Apr 29 07:47:01.659: INFO: Pod "pod-secrets-5e306855-ffd7-4c87-92c9-fb17ea44e505": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004841893s
STEP: Saw pod success
Apr 29 07:47:01.659: INFO: Pod "pod-secrets-5e306855-ffd7-4c87-92c9-fb17ea44e505" satisfied condition "Succeeded or Failed"
Apr 29 07:47:01.661: INFO: Trying to get logs from node localhost.localdomain151 pod pod-secrets-5e306855-ffd7-4c87-92c9-fb17ea44e505 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 07:47:01.675: INFO: Waiting for pod pod-secrets-5e306855-ffd7-4c87-92c9-fb17ea44e505 to disappear
Apr 29 07:47:01.677: INFO: Pod pod-secrets-5e306855-ffd7-4c87-92c9-fb17ea44e505 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:47:01.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2915" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":258,"skipped":4997,"failed":0}
SSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:47:01.685: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:47:03.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5595" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":259,"skipped":5002,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:47:03.809: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr 29 07:47:03.840: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 07:47:03.848: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 07:47:03.851: INFO: 
Logging pods the apiserver thinks is on node localhost.localdomain before test
Apr 29 07:47:03.862: INFO: sample-crd-conversion-webhook-deployment-bb9577b7b-b2dfv from crd-webhook-2934 started at 2022-04-29 06:48:32 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container sample-crd-conversion-webhook ready: true, restart count 0
Apr 29 07:47:03.862: INFO: elasticsearch-master-0 from elasticsearch started at 2022-04-29 02:41:34 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container elasticsearch ready: true, restart count 0
Apr 29 07:47:03.862: INFO: hnc-controller-manager-6d47799b76-dcfk2 from hnc-system started at 2022-04-29 06:48:32 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container manager ready: true, restart count 0
Apr 29 07:47:03.862: INFO: ingress-nginx-admission-create-d6gjd from ingress-nginx started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container create ready: false, restart count 0
Apr 29 07:47:03.862: INFO: ingress-nginx-admission-patch-wlvbg from ingress-nginx started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container patch ready: false, restart count 1
Apr 29 07:47:03.862: INFO: ingress-nginx-controller-776d88d5c9-wsdhf from ingress-nginx started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container controller ready: true, restart count 0
Apr 29 07:47:03.862: INFO: calico-kube-controllers-d84f7b88b-rqbrz from kube-system started at 2022-04-29 02:39:15 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr 29 07:47:03.862: INFO: calico-node-kb4fj from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container calico-node ready: true, restart count 0
Apr 29 07:47:03.862: INFO: coredns-65c54cc984-q685t from kube-system started at 2022-04-29 02:39:15 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container coredns ready: true, restart count 0
Apr 29 07:47:03.862: INFO: coredns-65c54cc984-vcx96 from kube-system started at 2022-04-29 02:39:15 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container coredns ready: true, restart count 0
Apr 29 07:47:03.862: INFO: etcd-localhost.localdomain from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container etcd ready: true, restart count 5
Apr 29 07:47:03.862: INFO: kube-apiserver-localhost.localdomain from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container kube-apiserver ready: true, restart count 5
Apr 29 07:47:03.862: INFO: kube-controller-manager-localhost.localdomain from kube-system started at 2022-04-29 02:39:05 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container kube-controller-manager ready: true, restart count 5
Apr 29 07:47:03.862: INFO: kube-proxy-tmzt7 from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 07:47:03.862: INFO: kube-scheduler-localhost.localdomain from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container kube-scheduler ready: true, restart count 5
Apr 29 07:47:03.862: INFO: metrics-server-6d69ff96c6-vth6c from kube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container metrics-server ready: true, restart count 0
Apr 29 07:47:03.862: INFO: kubecube-monitoring-kube-state-metrics-6cf5c8c6d4-7knxg from kubecube-monitoring started at 2022-04-29 02:42:04 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 29 07:47:03.862: INFO: kubecube-monitoring-operator-5f84949849-f69dc from kubecube-monitoring started at 2022-04-29 02:42:04 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container kubecube-monitoring ready: true, restart count 0
Apr 29 07:47:03.862: INFO: kubecube-monitoring-prometheus-node-exporter-wmrth from kubecube-monitoring started at 2022-04-29 02:42:04 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container node-exporter ready: true, restart count 0
Apr 29 07:47:03.862: INFO: kubecube-thanos-query-86f669566f-vzpq5 from kubecube-monitoring started at 2022-04-29 05:02:36 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container query ready: true, restart count 0
Apr 29 07:47:03.862: INFO: prometheus-kubecube-monitoring-prometheus-0 from kubecube-monitoring started at 2022-04-29 02:42:11 +0000 UTC (2 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container config-reloader ready: true, restart count 0
Apr 29 07:47:03.862: INFO: 	Container prometheus ready: true, restart count 1
Apr 29 07:47:03.862: INFO: audit-78b7d4cf7d-fs2pm from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container kubecube-audit ready: true, restart count 0
Apr 29 07:47:03.862: INFO: cloud-shell-5c87948759-ksk75 from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container cloud-shell ready: true, restart count 0
Apr 29 07:47:03.862: INFO: frontend-7cbccf84fb-wnxk7 from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container nginx ready: true, restart count 0
Apr 29 07:47:03.862: INFO: kubecube-756775dbb-6wvmx from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container kubecube ready: true, restart count 2
Apr 29 07:47:03.862: INFO: warden-d876d9fd8-rbkr2 from kubecube-system started at 2022-04-29 02:41:18 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container warden ready: true, restart count 0
Apr 29 07:47:03.862: INFO: webconsole-7d7f989996-h84gm from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container kubecube-webconsole ready: true, restart count 0
Apr 29 07:47:03.862: INFO: local-path-provisioner-86b7fd94b5-wdtg4 from local-path-storage started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container local-path-provisioner ready: true, restart count 2
Apr 29 07:47:03.862: INFO: sonobuoy-systemd-logs-daemon-set-82ef1d0cc98c43bb-5fv6g from sonobuoy started at 2022-04-29 06:40:07 +0000 UTC (2 container statuses recorded)
Apr 29 07:47:03.862: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 07:47:03.862: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 07:47:03.862: INFO: 
Logging pods the apiserver thinks is on node localhost.localdomain151 before test
Apr 29 07:47:03.871: INFO: calico-node-tszc4 from kube-system started at 2022-04-29 02:46:05 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.871: INFO: 	Container calico-node ready: true, restart count 0
Apr 29 07:47:03.871: INFO: kube-proxy-2xqmv from kube-system started at 2022-04-29 02:46:05 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.871: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 07:47:03.871: INFO: kubecube-monitoring-prometheus-node-exporter-lrdt2 from kubecube-monitoring started at 2022-04-29 06:48:58 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.871: INFO: 	Container node-exporter ready: true, restart count 0
Apr 29 07:47:03.871: INFO: kubecube-thanos-receive-0 from kubecube-monitoring started at 2022-04-29 06:49:03 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.871: INFO: 	Container receive ready: true, restart count 0
Apr 29 07:47:03.871: INFO: sonobuoy from sonobuoy started at 2022-04-29 06:40:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:47:03.871: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 07:47:03.871: INFO: sonobuoy-e2e-job-829345ef71224423 from sonobuoy started at 2022-04-29 06:40:07 +0000 UTC (2 container statuses recorded)
Apr 29 07:47:03.871: INFO: 	Container e2e ready: true, restart count 0
Apr 29 07:47:03.871: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 07:47:03.871: INFO: sonobuoy-systemd-logs-daemon-set-82ef1d0cc98c43bb-9qkq7 from sonobuoy started at 2022-04-29 06:40:07 +0000 UTC (2 container statuses recorded)
Apr 29 07:47:03.871: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 07:47:03.871: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-851185be-d2a6-458c-90b2-58a3327876b7 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-851185be-d2a6-458c-90b2-58a3327876b7 off the node localhost.localdomain151
STEP: verifying the node doesn't have the label kubernetes.io/e2e-851185be-d2a6-458c-90b2-58a3327876b7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:47:07.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5377" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":260,"skipped":5018,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:47:07.963: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:47:08.007: INFO: The status of Pod pod-secrets-68ca2562-5ace-4ce9-85f1-fa2594a068c0 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:47:10.010: INFO: The status of Pod pod-secrets-68ca2562-5ace-4ce9-85f1-fa2594a068c0 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:47:10.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4096" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":261,"skipped":5043,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:47:10.049: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Apr 29 07:47:10.150: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:47:12.154: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Apr 29 07:47:12.164: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:47:14.169: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 29 07:47:14.186: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 07:47:14.189: INFO: Pod pod-with-poststart-http-hook still exists
Apr 29 07:47:16.190: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 07:47:16.195: INFO: Pod pod-with-poststart-http-hook still exists
Apr 29 07:47:18.189: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 07:47:18.193: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:47:18.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6096" for this suite.

• [SLOW TEST:8.153 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":262,"skipped":5061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:47:18.203: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-195
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-195
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-195
Apr 29 07:47:18.254: INFO: Found 0 stateful pods, waiting for 1
Apr 29 07:47:28.259: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 29 07:47:28.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-195 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 07:47:28.406: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 07:47:28.406: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 07:47:28.406: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 07:47:28.410: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 29 07:47:38.415: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 07:47:38.415: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 07:47:38.427: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Apr 29 07:47:38.427: INFO: ss-0  localhost.localdomain151  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:18 +0000 UTC  }]
Apr 29 07:47:38.427: INFO: 
Apr 29 07:47:38.427: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 29 07:47:39.433: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996676186s
Apr 29 07:47:40.437: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992361839s
Apr 29 07:47:41.448: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987230038s
Apr 29 07:47:42.452: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976501251s
Apr 29 07:47:43.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97200131s
Apr 29 07:47:44.462: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966851688s
Apr 29 07:47:45.467: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.962636793s
Apr 29 07:47:46.472: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957581561s
Apr 29 07:47:47.476: INFO: Verifying statefulset ss doesn't scale past 3 for another 953.182367ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-195
Apr 29 07:47:48.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-195 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 07:47:48.610: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 07:47:48.610: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 07:47:48.610: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 07:47:48.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-195 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 07:47:48.759: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 29 07:47:48.759: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 07:47:48.759: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 07:47:48.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-195 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 07:47:48.898: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 29 07:47:48.898: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 07:47:48.898: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 07:47:48.902: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Apr 29 07:47:58.908: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 07:47:58.908: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 07:47:58.908: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 29 07:47:58.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-195 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 07:47:59.077: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 07:47:59.077: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 07:47:59.077: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 07:47:59.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-195 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 07:47:59.222: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 07:47:59.222: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 07:47:59.222: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 07:47:59.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=statefulset-195 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 07:47:59.384: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 07:47:59.384: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 07:47:59.384: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 07:47:59.384: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 07:47:59.388: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 29 07:48:09.396: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 07:48:09.396: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 07:48:09.396: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 07:48:09.409: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Apr 29 07:48:09.409: INFO: ss-0  localhost.localdomain151  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:18 +0000 UTC  }]
Apr 29 07:48:09.409: INFO: ss-1  localhost.localdomain     Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:38 +0000 UTC  }]
Apr 29 07:48:09.409: INFO: ss-2  localhost.localdomain151  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:38 +0000 UTC  }]
Apr 29 07:48:09.409: INFO: 
Apr 29 07:48:09.409: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 07:48:10.413: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Apr 29 07:48:10.413: INFO: ss-0  localhost.localdomain151  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:18 +0000 UTC  }]
Apr 29 07:48:10.413: INFO: ss-2  localhost.localdomain151  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-04-29 07:47:38 +0000 UTC  }]
Apr 29 07:48:10.413: INFO: 
Apr 29 07:48:10.413: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 29 07:48:11.417: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.99198375s
Apr 29 07:48:12.423: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987119539s
Apr 29 07:48:13.427: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.982149892s
Apr 29 07:48:14.431: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.977635599s
Apr 29 07:48:15.435: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.972441712s
Apr 29 07:48:16.440: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.969547237s
Apr 29 07:48:17.445: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.964494953s
Apr 29 07:48:18.450: INFO: Verifying statefulset ss doesn't scale past 0 for another 959.397049ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-195
Apr 29 07:48:19.454: INFO: Scaling statefulset ss to 0
Apr 29 07:48:19.465: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Apr 29 07:48:19.466: INFO: Deleting all statefulset in ns statefulset-195
Apr 29 07:48:19.468: INFO: Scaling statefulset ss to 0
Apr 29 07:48:19.477: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 07:48:19.481: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:48:19.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-195" for this suite.

• [SLOW TEST:61.309 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":263,"skipped":5116,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:48:19.512: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr 29 07:48:19.543: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 07:48:19.553: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 07:48:19.556: INFO: 
Logging pods the apiserver thinks is on node localhost.localdomain before test
Apr 29 07:48:19.568: INFO: sample-crd-conversion-webhook-deployment-bb9577b7b-b2dfv from crd-webhook-2934 started at 2022-04-29 06:48:32 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container sample-crd-conversion-webhook ready: true, restart count 0
Apr 29 07:48:19.568: INFO: elasticsearch-master-0 from elasticsearch started at 2022-04-29 02:41:34 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container elasticsearch ready: true, restart count 0
Apr 29 07:48:19.568: INFO: hnc-controller-manager-6d47799b76-dcfk2 from hnc-system started at 2022-04-29 06:48:32 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container manager ready: true, restart count 0
Apr 29 07:48:19.568: INFO: ingress-nginx-admission-create-d6gjd from ingress-nginx started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container create ready: false, restart count 0
Apr 29 07:48:19.568: INFO: ingress-nginx-admission-patch-wlvbg from ingress-nginx started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container patch ready: false, restart count 1
Apr 29 07:48:19.568: INFO: ingress-nginx-controller-776d88d5c9-wsdhf from ingress-nginx started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container controller ready: true, restart count 0
Apr 29 07:48:19.568: INFO: calico-kube-controllers-d84f7b88b-rqbrz from kube-system started at 2022-04-29 02:39:15 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr 29 07:48:19.568: INFO: calico-node-kb4fj from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container calico-node ready: true, restart count 0
Apr 29 07:48:19.568: INFO: coredns-65c54cc984-q685t from kube-system started at 2022-04-29 02:39:15 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container coredns ready: true, restart count 0
Apr 29 07:48:19.568: INFO: coredns-65c54cc984-vcx96 from kube-system started at 2022-04-29 02:39:15 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container coredns ready: true, restart count 0
Apr 29 07:48:19.568: INFO: etcd-localhost.localdomain from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container etcd ready: true, restart count 5
Apr 29 07:48:19.568: INFO: kube-apiserver-localhost.localdomain from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container kube-apiserver ready: true, restart count 5
Apr 29 07:48:19.568: INFO: kube-controller-manager-localhost.localdomain from kube-system started at 2022-04-29 02:39:05 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container kube-controller-manager ready: true, restart count 5
Apr 29 07:48:19.568: INFO: kube-proxy-tmzt7 from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 07:48:19.568: INFO: kube-scheduler-localhost.localdomain from kube-system started at 2022-04-29 02:39:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container kube-scheduler ready: true, restart count 5
Apr 29 07:48:19.568: INFO: metrics-server-6d69ff96c6-vth6c from kube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container metrics-server ready: true, restart count 0
Apr 29 07:48:19.568: INFO: kubecube-monitoring-kube-state-metrics-6cf5c8c6d4-7knxg from kubecube-monitoring started at 2022-04-29 02:42:04 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 29 07:48:19.568: INFO: kubecube-monitoring-operator-5f84949849-f69dc from kubecube-monitoring started at 2022-04-29 02:42:04 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container kubecube-monitoring ready: true, restart count 0
Apr 29 07:48:19.568: INFO: kubecube-monitoring-prometheus-node-exporter-wmrth from kubecube-monitoring started at 2022-04-29 02:42:04 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container node-exporter ready: true, restart count 0
Apr 29 07:48:19.568: INFO: kubecube-thanos-query-86f669566f-vzpq5 from kubecube-monitoring started at 2022-04-29 05:02:36 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container query ready: true, restart count 0
Apr 29 07:48:19.568: INFO: prometheus-kubecube-monitoring-prometheus-0 from kubecube-monitoring started at 2022-04-29 02:42:11 +0000 UTC (2 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container config-reloader ready: true, restart count 0
Apr 29 07:48:19.568: INFO: 	Container prometheus ready: true, restart count 1
Apr 29 07:48:19.568: INFO: audit-78b7d4cf7d-fs2pm from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container kubecube-audit ready: true, restart count 0
Apr 29 07:48:19.568: INFO: cloud-shell-5c87948759-ksk75 from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container cloud-shell ready: true, restart count 0
Apr 29 07:48:19.568: INFO: frontend-7cbccf84fb-wnxk7 from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container nginx ready: true, restart count 0
Apr 29 07:48:19.568: INFO: kubecube-756775dbb-6wvmx from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container kubecube ready: true, restart count 2
Apr 29 07:48:19.568: INFO: warden-d876d9fd8-rbkr2 from kubecube-system started at 2022-04-29 02:41:18 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container warden ready: true, restart count 0
Apr 29 07:48:19.568: INFO: webconsole-7d7f989996-h84gm from kubecube-system started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container kubecube-webconsole ready: true, restart count 0
Apr 29 07:48:19.568: INFO: local-path-provisioner-86b7fd94b5-wdtg4 from local-path-storage started at 2022-04-29 02:39:30 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container local-path-provisioner ready: true, restart count 2
Apr 29 07:48:19.568: INFO: sonobuoy-systemd-logs-daemon-set-82ef1d0cc98c43bb-5fv6g from sonobuoy started at 2022-04-29 06:40:07 +0000 UTC (2 container statuses recorded)
Apr 29 07:48:19.568: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 07:48:19.568: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 07:48:19.568: INFO: 
Logging pods the apiserver thinks is on node localhost.localdomain151 before test
Apr 29 07:48:19.580: INFO: calico-node-tszc4 from kube-system started at 2022-04-29 02:46:05 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.580: INFO: 	Container calico-node ready: true, restart count 0
Apr 29 07:48:19.580: INFO: kube-proxy-2xqmv from kube-system started at 2022-04-29 02:46:05 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.580: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 07:48:19.580: INFO: kubecube-monitoring-prometheus-node-exporter-lrdt2 from kubecube-monitoring started at 2022-04-29 06:48:58 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.580: INFO: 	Container node-exporter ready: true, restart count 0
Apr 29 07:48:19.580: INFO: kubecube-thanos-receive-0 from kubecube-monitoring started at 2022-04-29 06:49:03 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.580: INFO: 	Container receive ready: true, restart count 0
Apr 29 07:48:19.580: INFO: sonobuoy from sonobuoy started at 2022-04-29 06:40:06 +0000 UTC (1 container statuses recorded)
Apr 29 07:48:19.580: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 07:48:19.580: INFO: sonobuoy-e2e-job-829345ef71224423 from sonobuoy started at 2022-04-29 06:40:07 +0000 UTC (2 container statuses recorded)
Apr 29 07:48:19.580: INFO: 	Container e2e ready: true, restart count 0
Apr 29 07:48:19.580: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 07:48:19.580: INFO: sonobuoy-systemd-logs-daemon-set-82ef1d0cc98c43bb-9qkq7 from sonobuoy started at 2022-04-29 06:40:07 +0000 UTC (2 container statuses recorded)
Apr 29 07:48:19.580: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 07:48:19.580: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-21c897e2-0575-4fbf-b00b-9f8b5f5a0960 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 123.58.176.151 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-21c897e2-0575-4fbf-b00b-9f8b5f5a0960 off the node localhost.localdomain151
STEP: verifying the node doesn't have the label kubernetes.io/e2e-21c897e2-0575-4fbf-b00b-9f8b5f5a0960
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:53:23.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3521" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.183 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":264,"skipped":5122,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:53:23.695: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1835 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1835;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1835 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1835;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1835.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1835.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1835.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1835.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1835.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1835.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1835.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1835.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1835.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1835.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1835.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1835.svc;check="$$(dig +notcp +noall +answer +search 116.72.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.72.116_udp@PTR;check="$$(dig +tcp +noall +answer +search 116.72.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.72.116_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1835 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1835;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1835 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1835;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1835.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1835.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1835.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1835.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1835.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1835.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1835.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1835.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1835.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1835.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1835.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1835.svc;check="$$(dig +notcp +noall +answer +search 116.72.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.72.116_udp@PTR;check="$$(dig +tcp +noall +answer +search 116.72.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.72.116_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 07:53:25.857: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.861: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.865: INFO: Unable to read wheezy_udp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.868: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.871: INFO: Unable to read wheezy_udp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.876: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.879: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.883: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.886: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.889: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.892: INFO: Unable to read 10.101.72.116_udp@PTR from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.895: INFO: Unable to read 10.101.72.116_tcp@PTR from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.899: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.901: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.908: INFO: Unable to read jessie_udp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.911: INFO: Unable to read jessie_tcp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.919: INFO: Unable to read jessie_udp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.924: INFO: Unable to read jessie_tcp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.930: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.932: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.935: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.938: INFO: Unable to read 10.101.72.116_udp@PTR from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.943: INFO: Unable to read 10.101.72.116_tcp@PTR from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:25.943: INFO: Lookups using dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1835 wheezy_tcp@dns-test-service.dns-1835 wheezy_udp@dns-test-service.dns-1835.svc wheezy_tcp@dns-test-service.dns-1835.svc wheezy_udp@_http._tcp.dns-test-service.dns-1835.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1835.svc wheezy_udp@_http._tcp.test-service-2.dns-1835.svc wheezy_tcp@_http._tcp.test-service-2.dns-1835.svc 10.101.72.116_udp@PTR 10.101.72.116_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1835 jessie_tcp@dns-test-service.dns-1835 jessie_udp@dns-test-service.dns-1835.svc jessie_tcp@dns-test-service.dns-1835.svc jessie_tcp@_http._tcp.dns-test-service.dns-1835.svc jessie_udp@_http._tcp.test-service-2.dns-1835.svc jessie_tcp@_http._tcp.test-service-2.dns-1835.svc 10.101.72.116_udp@PTR 10.101.72.116_tcp@PTR]

Apr 29 07:53:30.949: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:30.952: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:30.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:30.957: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:30.960: INFO: Unable to read wheezy_udp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:30.963: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:30.983: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:30.985: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:30.988: INFO: Unable to read jessie_udp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:30.991: INFO: Unable to read jessie_tcp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:30.994: INFO: Unable to read jessie_udp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:30.997: INFO: Unable to read jessie_tcp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:31.016: INFO: Lookups using dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1835 wheezy_tcp@dns-test-service.dns-1835 wheezy_udp@dns-test-service.dns-1835.svc wheezy_tcp@dns-test-service.dns-1835.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1835 jessie_tcp@dns-test-service.dns-1835 jessie_udp@dns-test-service.dns-1835.svc jessie_tcp@dns-test-service.dns-1835.svc]

Apr 29 07:53:35.950: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:35.953: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:35.956: INFO: Unable to read wheezy_udp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:35.958: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:35.961: INFO: Unable to read wheezy_udp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:35.964: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:35.983: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:35.985: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:35.988: INFO: Unable to read jessie_udp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:35.991: INFO: Unable to read jessie_tcp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:35.995: INFO: Unable to read jessie_udp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:35.998: INFO: Unable to read jessie_tcp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:36.015: INFO: Lookups using dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1835 wheezy_tcp@dns-test-service.dns-1835 wheezy_udp@dns-test-service.dns-1835.svc wheezy_tcp@dns-test-service.dns-1835.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1835 jessie_tcp@dns-test-service.dns-1835 jessie_udp@dns-test-service.dns-1835.svc jessie_tcp@dns-test-service.dns-1835.svc]

Apr 29 07:53:40.947: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:40.950: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:40.953: INFO: Unable to read wheezy_udp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:40.957: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:40.960: INFO: Unable to read wheezy_udp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:40.962: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:40.983: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:40.986: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:40.989: INFO: Unable to read jessie_udp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:40.992: INFO: Unable to read jessie_tcp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:40.995: INFO: Unable to read jessie_udp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:40.999: INFO: Unable to read jessie_tcp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:41.016: INFO: Lookups using dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1835 wheezy_tcp@dns-test-service.dns-1835 wheezy_udp@dns-test-service.dns-1835.svc wheezy_tcp@dns-test-service.dns-1835.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1835 jessie_tcp@dns-test-service.dns-1835 jessie_udp@dns-test-service.dns-1835.svc jessie_tcp@dns-test-service.dns-1835.svc]

Apr 29 07:53:45.950: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:45.953: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:45.956: INFO: Unable to read wheezy_udp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:45.959: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:45.961: INFO: Unable to read wheezy_udp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:45.964: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:45.986: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:45.990: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:45.993: INFO: Unable to read jessie_udp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:45.996: INFO: Unable to read jessie_tcp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:45.999: INFO: Unable to read jessie_udp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:46.001: INFO: Unable to read jessie_tcp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:46.023: INFO: Lookups using dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1835 wheezy_tcp@dns-test-service.dns-1835 wheezy_udp@dns-test-service.dns-1835.svc wheezy_tcp@dns-test-service.dns-1835.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1835 jessie_tcp@dns-test-service.dns-1835 jessie_udp@dns-test-service.dns-1835.svc jessie_tcp@dns-test-service.dns-1835.svc]

Apr 29 07:53:50.953: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:50.957: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:50.959: INFO: Unable to read wheezy_udp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:50.962: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:50.968: INFO: Unable to read wheezy_udp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:50.971: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:50.991: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:50.993: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:50.996: INFO: Unable to read jessie_udp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:50.999: INFO: Unable to read jessie_tcp@dns-test-service.dns-1835 from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:51.001: INFO: Unable to read jessie_udp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:51.004: INFO: Unable to read jessie_tcp@dns-test-service.dns-1835.svc from pod dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb: the server could not find the requested resource (get pods dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb)
Apr 29 07:53:51.025: INFO: Lookups using dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1835 wheezy_tcp@dns-test-service.dns-1835 wheezy_udp@dns-test-service.dns-1835.svc wheezy_tcp@dns-test-service.dns-1835.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1835 jessie_tcp@dns-test-service.dns-1835 jessie_udp@dns-test-service.dns-1835.svc jessie_tcp@dns-test-service.dns-1835.svc]

Apr 29 07:53:56.013: INFO: DNS probes using dns-1835/dns-test-5a388c2b-9618-462f-89fc-15c1829ec3eb succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:53:56.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1835" for this suite.

• [SLOW TEST:32.442 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":265,"skipped":5147,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:53:56.138: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 07:53:56.314: INFO: The status of Pod busybox-readonly-fsb744489b-8d33-47c2-99f7-6311719079ca is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:53:58.319: INFO: The status of Pod busybox-readonly-fsb744489b-8d33-47c2-99f7-6311719079ca is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:53:58.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2561" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":266,"skipped":5163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:53:58.349: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-95273723-4d4c-4b69-8e9e-aa7e8b787d36
STEP: Creating a pod to test consume secrets
Apr 29 07:53:58.392: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5bf8fdb4-6991-49c0-95d0-767e96037204" in namespace "projected-3968" to be "Succeeded or Failed"
Apr 29 07:53:58.399: INFO: Pod "pod-projected-secrets-5bf8fdb4-6991-49c0-95d0-767e96037204": Phase="Pending", Reason="", readiness=false. Elapsed: 6.804239ms
Apr 29 07:54:00.403: INFO: Pod "pod-projected-secrets-5bf8fdb4-6991-49c0-95d0-767e96037204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010949866s
STEP: Saw pod success
Apr 29 07:54:00.403: INFO: Pod "pod-projected-secrets-5bf8fdb4-6991-49c0-95d0-767e96037204" satisfied condition "Succeeded or Failed"
Apr 29 07:54:00.406: INFO: Trying to get logs from node localhost.localdomain151 pod pod-projected-secrets-5bf8fdb4-6991-49c0-95d0-767e96037204 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 07:54:00.421: INFO: Waiting for pod pod-projected-secrets-5bf8fdb4-6991-49c0-95d0-767e96037204 to disappear
Apr 29 07:54:00.423: INFO: Pod pod-projected-secrets-5bf8fdb4-6991-49c0-95d0-767e96037204 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:54:00.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3968" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":267,"skipped":5188,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:54:00.431: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Apr 29 07:54:00.460: INFO: created test-podtemplate-1
Apr 29 07:54:00.463: INFO: created test-podtemplate-2
Apr 29 07:54:00.467: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Apr 29 07:54:00.470: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Apr 29 07:54:00.481: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:54:00.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6365" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":268,"skipped":5213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:54:00.492: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-b8c0adf3-8c88-4434-8b91-c6fd051f0a79
STEP: Creating secret with name s-test-opt-upd-c1bc9984-630f-4cfc-8b38-df7f00fd93f4
STEP: Creating the pod
Apr 29 07:54:00.577: INFO: The status of Pod pod-secrets-88a240c9-34f3-45de-b924-96a8d6329d47 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:54:02.583: INFO: The status of Pod pod-secrets-88a240c9-34f3-45de-b924-96a8d6329d47 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-b8c0adf3-8c88-4434-8b91-c6fd051f0a79
STEP: Updating secret s-test-opt-upd-c1bc9984-630f-4cfc-8b38-df7f00fd93f4
STEP: Creating secret with name s-test-opt-create-1dc80cf3-bfe0-4481-ba25-37e2355ab385
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:54:06.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8637" for this suite.

• [SLOW TEST:6.188 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":269,"skipped":5249,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:54:06.681: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:55:06.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7105" for this suite.

• [SLOW TEST:60.057 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":5251,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:55:06.738: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Apr 29 07:55:07.827: INFO: The status of Pod kube-controller-manager-localhost.localdomain is Running (Ready = true)
E0429 07:55:08.883064      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:09.910556      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:10.937387      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:11.962552      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:14.019652      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:17.100596      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:23.276497      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:24.306310      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:25.330295      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:27.380239      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:29.429551      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:32.207024      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:33.235562      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:34.261416      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:36.314280      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:39.405543      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:41.160694      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:42.185849      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:43.212629      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:44.242483      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:45.268778      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:50.407371      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:53.184698      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:54.209404      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:56.260483      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:57.288487      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:55:59.339383      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:00.363347      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:01.389125      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:02.413435      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:04.183460      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:05.209847      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:07.279591      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:08.306459      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:09.333764      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:11.385501      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:14.161684      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:15.190919      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:17.247840      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:19.297931      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:20.327281      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:21.352227      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:23.401385      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:24.427654      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:24.452546      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:25.480743      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:26.507967      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:27.533979      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:28.567743      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:29.593903      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:30.620313      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 07:56:32.669680      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
Apr 29 07:56:34.723: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:56:34.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2846" for this suite.

• [SLOW TEST:87.999 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":271,"skipped":5252,"failed":0}
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:56:34.737: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-925d83bc-ce90-475d-b55c-646041c9c49d in namespace container-probe-9877
Apr 29 07:56:36.792: INFO: Started pod liveness-925d83bc-ce90-475d-b55c-646041c9c49d in namespace container-probe-9877
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 07:56:36.795: INFO: Initial restart count of pod liveness-925d83bc-ce90-475d-b55c-646041c9c49d is 0
Apr 29 07:56:56.852: INFO: Restart count of pod container-probe-9877/liveness-925d83bc-ce90-475d-b55c-646041c9c49d is now 1 (20.057525722s elapsed)
Apr 29 07:57:16.910: INFO: Restart count of pod container-probe-9877/liveness-925d83bc-ce90-475d-b55c-646041c9c49d is now 2 (40.11544063s elapsed)
Apr 29 07:57:36.971: INFO: Restart count of pod container-probe-9877/liveness-925d83bc-ce90-475d-b55c-646041c9c49d is now 3 (1m0.176458702s elapsed)
Apr 29 07:57:57.034: INFO: Restart count of pod container-probe-9877/liveness-925d83bc-ce90-475d-b55c-646041c9c49d is now 4 (1m20.239703645s elapsed)
Apr 29 07:58:17.093: INFO: Restart count of pod container-probe-9877/liveness-925d83bc-ce90-475d-b55c-646041c9c49d is now 5 (1m40.298393243s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:58:17.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9877" for this suite.

• [SLOW TEST:102.382 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":272,"skipped":5252,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:58:17.119: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Apr 29 07:58:17.250: INFO: Waiting up to 5m0s for pod "downward-api-6d9589ef-11a3-4eb6-bc0e-9c4487e4c01f" in namespace "downward-api-2971" to be "Succeeded or Failed"
Apr 29 07:58:17.252: INFO: Pod "downward-api-6d9589ef-11a3-4eb6-bc0e-9c4487e4c01f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.384617ms
Apr 29 07:58:19.256: INFO: Pod "downward-api-6d9589ef-11a3-4eb6-bc0e-9c4487e4c01f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00648017s
STEP: Saw pod success
Apr 29 07:58:19.256: INFO: Pod "downward-api-6d9589ef-11a3-4eb6-bc0e-9c4487e4c01f" satisfied condition "Succeeded or Failed"
Apr 29 07:58:19.259: INFO: Trying to get logs from node localhost.localdomain151 pod downward-api-6d9589ef-11a3-4eb6-bc0e-9c4487e4c01f container dapi-container: <nil>
STEP: delete the pod
Apr 29 07:58:19.283: INFO: Waiting for pod downward-api-6d9589ef-11a3-4eb6-bc0e-9c4487e4c01f to disappear
Apr 29 07:58:19.285: INFO: Pod downward-api-6d9589ef-11a3-4eb6-bc0e-9c4487e4c01f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:58:19.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2971" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":273,"skipped":5254,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:58:19.294: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 29 07:58:19.336: INFO: Waiting up to 5m0s for pod "pod-9cdb42f2-bd30-4730-a8c2-bee1afbb7695" in namespace "emptydir-8248" to be "Succeeded or Failed"
Apr 29 07:58:19.340: INFO: Pod "pod-9cdb42f2-bd30-4730-a8c2-bee1afbb7695": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03244ms
Apr 29 07:58:21.346: INFO: Pod "pod-9cdb42f2-bd30-4730-a8c2-bee1afbb7695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010168752s
STEP: Saw pod success
Apr 29 07:58:21.346: INFO: Pod "pod-9cdb42f2-bd30-4730-a8c2-bee1afbb7695" satisfied condition "Succeeded or Failed"
Apr 29 07:58:21.349: INFO: Trying to get logs from node localhost.localdomain151 pod pod-9cdb42f2-bd30-4730-a8c2-bee1afbb7695 container test-container: <nil>
STEP: delete the pod
Apr 29 07:58:21.365: INFO: Waiting for pod pod-9cdb42f2-bd30-4730-a8c2-bee1afbb7695 to disappear
Apr 29 07:58:21.368: INFO: Pod pod-9cdb42f2-bd30-4730-a8c2-bee1afbb7695 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:58:21.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8248" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":274,"skipped":5263,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:58:21.377: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Apr 29 07:58:21.450: INFO: Waiting up to 5m0s for pod "test-pod-b81452fd-b080-4d09-8737-b7613867447f" in namespace "svcaccounts-5481" to be "Succeeded or Failed"
Apr 29 07:58:21.453: INFO: Pod "test-pod-b81452fd-b080-4d09-8737-b7613867447f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.47439ms
Apr 29 07:58:23.456: INFO: Pod "test-pod-b81452fd-b080-4d09-8737-b7613867447f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006407311s
STEP: Saw pod success
Apr 29 07:58:23.456: INFO: Pod "test-pod-b81452fd-b080-4d09-8737-b7613867447f" satisfied condition "Succeeded or Failed"
Apr 29 07:58:23.459: INFO: Trying to get logs from node localhost.localdomain151 pod test-pod-b81452fd-b080-4d09-8737-b7613867447f container agnhost-container: <nil>
STEP: delete the pod
Apr 29 07:58:23.474: INFO: Waiting for pod test-pod-b81452fd-b080-4d09-8737-b7613867447f to disappear
Apr 29 07:58:23.477: INFO: Pod test-pod-b81452fd-b080-4d09-8737-b7613867447f no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:58:23.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5481" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":275,"skipped":5265,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:58:23.487: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr 29 07:58:23.534: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:58:28.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5076" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":5315,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:58:28.026: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:58:39.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5358" for this suite.

• [SLOW TEST:11.167 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":277,"skipped":5316,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:58:39.193: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:58:39.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5617" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":278,"skipped":5333,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:58:39.268: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 07:58:39.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:58:39.363: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:58:40.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 29 07:58:40.373: INFO: Node localhost.localdomain is running 0 daemon pod, expected 1
Apr 29 07:58:41.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 29 07:58:41.372: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
Apr 29 07:58:41.393: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"128309"},"items":null}

Apr 29 07:58:41.396: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"128309"},"items":[{"metadata":{"name":"daemon-set-gx9cl","generateName":"daemon-set-","namespace":"daemonsets-2000","uid":"49cd8e5d-275a-4847-8984-bb4f198920cd","resourceVersion":"128303","creationTimestamp":"2022-04-29T07:58:39Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"172.16.102.174/32","cni.projectcalico.org/podIPs":"172.16.102.174/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0ca025ec-8eaa-4ffe-9d2d-fd61b93596ba","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-04-29T07:58:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-04-29T07:58:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ca025ec-8eaa-4ffe-9d2d-fd61b93596ba\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-04-29T07:58:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.102.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8gdc4","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8gdc4","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"localhost.localdomain","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["localhost.localdomain"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-29T07:58:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-29T07:58:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-29T07:58:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-29T07:58:39Z"}],"hostIP":"123.58.176.150","podIP":"172.16.102.174","podIPs":[{"ip":"172.16.102.174"}],"startTime":"2022-04-29T07:58:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-04-29T07:58:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://1aa57a4e78ff2662f41128456d135188933501efe25f5094c65fd487e1f607e5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kjhst","generateName":"daemon-set-","namespace":"daemonsets-2000","uid":"fecab010-07f5-42cb-a2e6-4def3e79ea58","resourceVersion":"128307","creationTimestamp":"2022-04-29T07:58:39Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"172.16.115.130/32","cni.projectcalico.org/podIPs":"172.16.115.130/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0ca025ec-8eaa-4ffe-9d2d-fd61b93596ba","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-04-29T07:58:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-04-29T07:58:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ca025ec-8eaa-4ffe-9d2d-fd61b93596ba\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-04-29T07:58:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.115.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b9bkz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b9bkz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"localhost.localdomain151","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["localhost.localdomain151"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-29T07:58:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-29T07:58:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-29T07:58:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-04-29T07:58:39Z"}],"hostIP":"123.58.176.151","podIP":"172.16.115.130","podIPs":[{"ip":"172.16.115.130"}],"startTime":"2022-04-29T07:58:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-04-29T07:58:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://451ff8688b7a65a5afaf2b905ab2de674afa17eb61a775f5e22df062c8df93bc","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:58:41.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2000" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":279,"skipped":5354,"failed":0}
SSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:58:41.418: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Apr 29 07:58:43.465: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:58:49.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-727" for this suite.

• [SLOW TEST:8.161 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":280,"skipped":5360,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:58:49.579: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-9221
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 07:58:49.631: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 29 07:58:49.655: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:58:51.659: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:58:53.660: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:58:55.660: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:58:57.659: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:58:59.659: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:59:01.660: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:59:03.659: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:59:05.659: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:59:07.661: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:59:09.666: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 07:59:11.659: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 29 07:59:11.663: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Apr 29 07:59:13.690: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Apr 29 07:59:13.690: INFO: Going to poll 172.16.102.138 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Apr 29 07:59:13.693: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.102.138:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9221 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 07:59:13.693: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 07:59:13.693: INFO: ExecWithOptions: Clientset creation
Apr 29 07:59:13.693: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9221/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.102.138%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Apr 29 07:59:13.790: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 29 07:59:13.790: INFO: Going to poll 172.16.115.156 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Apr 29 07:59:13.793: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.115.156:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9221 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 07:59:13.793: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 07:59:13.794: INFO: ExecWithOptions: Clientset creation
Apr 29 07:59:13.794: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9221/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.115.156%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Apr 29 07:59:13.871: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:59:13.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9221" for this suite.

• [SLOW TEST:24.301 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":281,"skipped":5365,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:59:13.881: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 07:59:13.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31ce42c7-51cd-4fea-a383-533f9a4b8ac8" in namespace "downward-api-9049" to be "Succeeded or Failed"
Apr 29 07:59:13.938: INFO: Pod "downwardapi-volume-31ce42c7-51cd-4fea-a383-533f9a4b8ac8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.9149ms
Apr 29 07:59:15.942: INFO: Pod "downwardapi-volume-31ce42c7-51cd-4fea-a383-533f9a4b8ac8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008090811s
STEP: Saw pod success
Apr 29 07:59:15.942: INFO: Pod "downwardapi-volume-31ce42c7-51cd-4fea-a383-533f9a4b8ac8" satisfied condition "Succeeded or Failed"
Apr 29 07:59:15.944: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-31ce42c7-51cd-4fea-a383-533f9a4b8ac8 container client-container: <nil>
STEP: delete the pod
Apr 29 07:59:15.963: INFO: Waiting for pod downwardapi-volume-31ce42c7-51cd-4fea-a383-533f9a4b8ac8 to disappear
Apr 29 07:59:15.965: INFO: Pod downwardapi-volume-31ce42c7-51cd-4fea-a383-533f9a4b8ac8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:59:15.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9049" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":282,"skipped":5388,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:59:15.973: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 07:59:16.024: INFO: Waiting up to 5m0s for pod "downwardapi-volume-422a1095-b3cb-4259-9817-1ade06eb3683" in namespace "projected-5169" to be "Succeeded or Failed"
Apr 29 07:59:16.027: INFO: Pod "downwardapi-volume-422a1095-b3cb-4259-9817-1ade06eb3683": Phase="Pending", Reason="", readiness=false. Elapsed: 2.714166ms
Apr 29 07:59:18.033: INFO: Pod "downwardapi-volume-422a1095-b3cb-4259-9817-1ade06eb3683": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008406586s
STEP: Saw pod success
Apr 29 07:59:18.033: INFO: Pod "downwardapi-volume-422a1095-b3cb-4259-9817-1ade06eb3683" satisfied condition "Succeeded or Failed"
Apr 29 07:59:18.035: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-422a1095-b3cb-4259-9817-1ade06eb3683 container client-container: <nil>
STEP: delete the pod
Apr 29 07:59:18.050: INFO: Waiting for pod downwardapi-volume-422a1095-b3cb-4259-9817-1ade06eb3683 to disappear
Apr 29 07:59:18.053: INFO: Pod downwardapi-volume-422a1095-b3cb-4259-9817-1ade06eb3683 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:59:18.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5169" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":283,"skipped":5391,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:59:18.061: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Apr 29 07:59:18.104: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Apr 29 07:59:20.108: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 29 07:59:21.122: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 07:59:22.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5912" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":284,"skipped":5393,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 07:59:22.171: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-966
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Apr 29 07:59:22.272: INFO: Found 0 stateful pods, waiting for 3
Apr 29 07:59:32.276: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 07:59:32.276: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 07:59:32.276: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Apr 29 07:59:32.305: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 29 07:59:42.342: INFO: Updating stateful set ss2
Apr 29 07:59:42.349: INFO: Waiting for Pod statefulset-966/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Apr 29 07:59:52.386: INFO: Found 1 stateful pods, waiting for 3
Apr 29 08:00:02.391: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 08:00:02.391: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 08:00:02.391: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 29 08:00:02.414: INFO: Updating stateful set ss2
Apr 29 08:00:02.420: INFO: Waiting for Pod statefulset-966/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Apr 29 08:00:12.449: INFO: Updating stateful set ss2
Apr 29 08:00:12.454: INFO: Waiting for StatefulSet statefulset-966/ss2 to complete update
Apr 29 08:00:12.454: INFO: Waiting for Pod statefulset-966/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Apr 29 08:00:22.467: INFO: Deleting all statefulset in ns statefulset-966
Apr 29 08:00:22.469: INFO: Scaling statefulset ss2 to 0
Apr 29 08:00:32.488: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 08:00:32.490: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:00:32.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-966" for this suite.

• [SLOW TEST:70.341 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":285,"skipped":5403,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:00:32.513: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:00:32.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3315" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":286,"skipped":5409,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:00:32.625: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-a3ff3d2d-2082-46a5-bd99-635e9e857802 in namespace container-probe-3325
Apr 29 08:00:34.736: INFO: Started pod busybox-a3ff3d2d-2082-46a5-bd99-635e9e857802 in namespace container-probe-3325
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 08:00:34.739: INFO: Initial restart count of pod busybox-a3ff3d2d-2082-46a5-bd99-635e9e857802 is 0
Apr 29 08:01:24.854: INFO: Restart count of pod container-probe-3325/busybox-a3ff3d2d-2082-46a5-bd99-635e9e857802 is now 1 (50.115732618s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:01:24.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3325" for this suite.

• [SLOW TEST:52.249 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":287,"skipped":5429,"failed":0}
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:01:24.874: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 29 08:01:26.466: INFO: starting watch
STEP: patching
STEP: updating
Apr 29 08:01:26.475: INFO: waiting for watch events with expected annotations
Apr 29 08:01:26.475: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:01:26.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2521" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":288,"skipped":5430,"failed":0}

------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:01:26.520: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 29 08:01:27.062: INFO: Pod name wrapped-volume-race-6119c370-2c6a-49f9-9af6-44fe1edb4ed1: Found 0 pods out of 5
Apr 29 08:01:32.073: INFO: Pod name wrapped-volume-race-6119c370-2c6a-49f9-9af6-44fe1edb4ed1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6119c370-2c6a-49f9-9af6-44fe1edb4ed1 in namespace emptydir-wrapper-7823, will wait for the garbage collector to delete the pods
Apr 29 08:01:42.156: INFO: Deleting ReplicationController wrapped-volume-race-6119c370-2c6a-49f9-9af6-44fe1edb4ed1 took: 7.412359ms
Apr 29 08:01:42.257: INFO: Terminating ReplicationController wrapped-volume-race-6119c370-2c6a-49f9-9af6-44fe1edb4ed1 pods took: 100.477246ms
STEP: Creating RC which spawns configmap-volume pods
Apr 29 08:01:44.977: INFO: Pod name wrapped-volume-race-78b0e22f-d535-4dc8-afc1-f4b7178cfb60: Found 0 pods out of 5
Apr 29 08:01:49.988: INFO: Pod name wrapped-volume-race-78b0e22f-d535-4dc8-afc1-f4b7178cfb60: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-78b0e22f-d535-4dc8-afc1-f4b7178cfb60 in namespace emptydir-wrapper-7823, will wait for the garbage collector to delete the pods
Apr 29 08:02:02.083: INFO: Deleting ReplicationController wrapped-volume-race-78b0e22f-d535-4dc8-afc1-f4b7178cfb60 took: 10.586965ms
Apr 29 08:02:02.183: INFO: Terminating ReplicationController wrapped-volume-race-78b0e22f-d535-4dc8-afc1-f4b7178cfb60 pods took: 100.546062ms
STEP: Creating RC which spawns configmap-volume pods
Apr 29 08:02:04.705: INFO: Pod name wrapped-volume-race-968b2eb7-bf94-40a2-afc7-be35491a9c25: Found 0 pods out of 5
Apr 29 08:02:09.714: INFO: Pod name wrapped-volume-race-968b2eb7-bf94-40a2-afc7-be35491a9c25: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-968b2eb7-bf94-40a2-afc7-be35491a9c25 in namespace emptydir-wrapper-7823, will wait for the garbage collector to delete the pods
Apr 29 08:02:19.806: INFO: Deleting ReplicationController wrapped-volume-race-968b2eb7-bf94-40a2-afc7-be35491a9c25 took: 8.417226ms
Apr 29 08:02:19.907: INFO: Terminating ReplicationController wrapped-volume-race-968b2eb7-bf94-40a2-afc7-be35491a9c25 pods took: 100.158703ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:02:23.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7823" for this suite.

• [SLOW TEST:56.681 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":289,"skipped":5430,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:02:23.201: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 08:02:23.242: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 29 08:02:28.247: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 29 08:02:28.247: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 29 08:02:30.251: INFO: Creating deployment "test-rollover-deployment"
Apr 29 08:02:30.258: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 29 08:02:32.270: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 29 08:02:32.275: INFO: Ensure that both replica sets have 1 created replica
Apr 29 08:02:32.279: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 29 08:02:32.287: INFO: Updating deployment test-rollover-deployment
Apr 29 08:02:32.287: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 29 08:02:34.293: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 29 08:02:34.299: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 29 08:02:34.303: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 08:02:34.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.April, 29, 8, 2, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 08:02:36.324: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 08:02:36.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.April, 29, 8, 2, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 08:02:38.311: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 08:02:38.311: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.April, 29, 8, 2, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 08:02:40.311: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 08:02:40.311: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.April, 29, 8, 2, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 08:02:42.310: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 08:02:42.310: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.April, 29, 8, 2, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.April, 29, 8, 2, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 08:02:44.310: INFO: 
Apr 29 08:02:44.310: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr 29 08:02:44.317: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9706  672e45c5-7070-4668-9085-e560da8718c5 131146 2 2022-04-29 08:02:30 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-04-29 08:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 08:02:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e9dea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-04-29 08:02:30 +0000 UTC,LastTransitionTime:2022-04-29 08:02:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668b7f667d" has successfully progressed.,LastUpdateTime:2022-04-29 08:02:43 +0000 UTC,LastTransitionTime:2022-04-29 08:02:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 29 08:02:44.319: INFO: New ReplicaSet "test-rollover-deployment-668b7f667d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668b7f667d  deployment-9706  e8725f02-3859-4ead-b169-b33a9cc05391 131136 2 2022-04-29 08:02:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 672e45c5-7070-4668-9085-e560da8718c5 0xc003af2fe7 0xc003af2fe8}] []  [{kube-controller-manager Update apps/v1 2022-04-29 08:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"672e45c5-7070-4668-9085-e560da8718c5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 08:02:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668b7f667d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003af3098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 29 08:02:44.319: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 29 08:02:44.319: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9706  5ecd410b-ee1a-42fd-b66c-2228fc2d65c1 131145 2 2022-04-29 08:02:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 672e45c5-7070-4668-9085-e560da8718c5 0xc003af2eb7 0xc003af2eb8}] []  [{e2e.test Update apps/v1 2022-04-29 08:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 08:02:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"672e45c5-7070-4668-9085-e560da8718c5\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-04-29 08:02:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003af2f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 08:02:44.319: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-9706  da1fe508-0768-4847-b7a1-92976d813f38 131071 2 2022-04-29 08:02:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 672e45c5-7070-4668-9085-e560da8718c5 0xc003af3107 0xc003af3108}] []  [{kube-controller-manager Update apps/v1 2022-04-29 08:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"672e45c5-7070-4668-9085-e560da8718c5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 08:02:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003af31b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 08:02:44.323: INFO: Pod "test-rollover-deployment-668b7f667d-jsc5d" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668b7f667d-jsc5d test-rollover-deployment-668b7f667d- deployment-9706  ae4c4230-886d-41f0-b6e1-47a99a2d00af 131087 0 2022-04-29 08:02:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[cni.projectcalico.org/podIP:172.16.115.182/32 cni.projectcalico.org/podIPs:172.16.115.182/32] [{apps/v1 ReplicaSet test-rollover-deployment-668b7f667d e8725f02-3859-4ead-b169-b33a9cc05391 0xc001e86617 0xc001e86618}] []  [{calico Update v1 2022-04-29 08:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-04-29 08:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8725f02-3859-4ead-b169-b33a9cc05391\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-04-29 08:02:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.115.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zh5cl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zh5cl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 08:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 08:02:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 08:02:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 08:02:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:172.16.115.182,StartTime:2022-04-29 08:02:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 08:02:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://54278044c16fd9f7647381f6b344cb1469f327707eabce385327866a1e43a3df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.115.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:02:44.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9706" for this suite.

• [SLOW TEST:21.131 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":290,"skipped":5440,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:02:44.333: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 08:02:44.374: INFO: Waiting up to 5m0s for pod "downwardapi-volume-808966b4-1a86-4e5b-8235-a66b58b6aad7" in namespace "downward-api-989" to be "Succeeded or Failed"
Apr 29 08:02:44.376: INFO: Pod "downwardapi-volume-808966b4-1a86-4e5b-8235-a66b58b6aad7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.752968ms
Apr 29 08:02:46.382: INFO: Pod "downwardapi-volume-808966b4-1a86-4e5b-8235-a66b58b6aad7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008215671s
STEP: Saw pod success
Apr 29 08:02:46.382: INFO: Pod "downwardapi-volume-808966b4-1a86-4e5b-8235-a66b58b6aad7" satisfied condition "Succeeded or Failed"
Apr 29 08:02:46.385: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-808966b4-1a86-4e5b-8235-a66b58b6aad7 container client-container: <nil>
STEP: delete the pod
Apr 29 08:02:46.413: INFO: Waiting for pod downwardapi-volume-808966b4-1a86-4e5b-8235-a66b58b6aad7 to disappear
Apr 29 08:02:46.416: INFO: Pod downwardapi-volume-808966b4-1a86-4e5b-8235-a66b58b6aad7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:02:46.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-989" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5452,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:02:46.427: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Apr 29 08:02:56.511: INFO: The status of Pod kube-controller-manager-localhost.localdomain is Running (Ready = true)
E0429 08:02:56.534899      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:02:58.587225      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:01.658463      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:02.684875      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:03.717517      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:04.744683      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:05.771022      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:06.796238      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:10.914329      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:11.949780      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:15.037355      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:16.062353      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:17.086813      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:18.112555      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:18.820461      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:19.847650      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:20.888557      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:21.918157      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:22.945382      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:23.970553      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:25.004943      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:26.029700      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:29.114089      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:30.847271      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:31.874110      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:32.901515      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:35.987129      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:37.013064      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:38.039817      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:40.823378      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:42.877594      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:43.902072      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:44.927397      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:49.030172      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:51.086930      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:52.854517      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:53.887554      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:03:54.915533      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:00.051735      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:02.821555      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:03.848920      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:05.900180      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:06.925673      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:07.953754      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:10.009505      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:11.036388      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:13.096206      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:16.203524      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:17.228838      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:18.255297      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:20.306474      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:21.342277      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
E0429 08:04:22.393494      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod 9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad, uid : failed to execute portforward in network namespace \"host\": failed to connect to localhost:10257 inside namespace \"9d7befa551fdc50d50891623ef70e56a834c37fc6cfc56b29437359ce98645ad\", IPv4: dial tcp4 127.0.0.1:10257: connect: connection refused IPv6 dial tcp6 [::1]:10257: connect: connection refused "
Apr 29 08:04:23.418: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:04:23.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-288" for this suite.

• [SLOW TEST:97.003 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":292,"skipped":5453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:04:23.431: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 08:04:24.174: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 08:04:27.193: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:04:27.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5440" for this suite.
STEP: Destroying namespace "webhook-5440-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":293,"skipped":5519,"failed":0}
SS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:04:27.276: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 08:04:27.360: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-31cac59d-2659-4b5a-87e4-21f5fd9130e9" in namespace "security-context-test-9827" to be "Succeeded or Failed"
Apr 29 08:04:27.362: INFO: Pod "alpine-nnp-false-31cac59d-2659-4b5a-87e4-21f5fd9130e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.979262ms
Apr 29 08:04:29.367: INFO: Pod "alpine-nnp-false-31cac59d-2659-4b5a-87e4-21f5fd9130e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006302148s
Apr 29 08:04:29.367: INFO: Pod "alpine-nnp-false-31cac59d-2659-4b5a-87e4-21f5fd9130e9" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:04:29.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9827" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5521,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:04:29.390: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Apr 29 08:04:29.432: INFO: Waiting up to 5m0s for pod "downward-api-8b0263c4-0a58-4698-8814-fea6dbeea3a4" in namespace "downward-api-1882" to be "Succeeded or Failed"
Apr 29 08:04:29.435: INFO: Pod "downward-api-8b0263c4-0a58-4698-8814-fea6dbeea3a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.352304ms
Apr 29 08:04:31.439: INFO: Pod "downward-api-8b0263c4-0a58-4698-8814-fea6dbeea3a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00704924s
STEP: Saw pod success
Apr 29 08:04:31.439: INFO: Pod "downward-api-8b0263c4-0a58-4698-8814-fea6dbeea3a4" satisfied condition "Succeeded or Failed"
Apr 29 08:04:31.442: INFO: Trying to get logs from node localhost.localdomain151 pod downward-api-8b0263c4-0a58-4698-8814-fea6dbeea3a4 container dapi-container: <nil>
STEP: delete the pod
Apr 29 08:04:31.458: INFO: Waiting for pod downward-api-8b0263c4-0a58-4698-8814-fea6dbeea3a4 to disappear
Apr 29 08:04:31.460: INFO: Pod downward-api-8b0263c4-0a58-4698-8814-fea6dbeea3a4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:04:31.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1882" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":295,"skipped":5575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:04:31.468: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 08:04:31.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ebb8c35-b4eb-4010-a1b4-950ee7e1cd63" in namespace "downward-api-5954" to be "Succeeded or Failed"
Apr 29 08:04:31.727: INFO: Pod "downwardapi-volume-7ebb8c35-b4eb-4010-a1b4-950ee7e1cd63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.422803ms
Apr 29 08:04:33.733: INFO: Pod "downwardapi-volume-7ebb8c35-b4eb-4010-a1b4-950ee7e1cd63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008196896s
STEP: Saw pod success
Apr 29 08:04:33.733: INFO: Pod "downwardapi-volume-7ebb8c35-b4eb-4010-a1b4-950ee7e1cd63" satisfied condition "Succeeded or Failed"
Apr 29 08:04:33.735: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-7ebb8c35-b4eb-4010-a1b4-950ee7e1cd63 container client-container: <nil>
STEP: delete the pod
Apr 29 08:04:33.748: INFO: Waiting for pod downwardapi-volume-7ebb8c35-b4eb-4010-a1b4-950ee7e1cd63 to disappear
Apr 29 08:04:33.751: INFO: Pod downwardapi-volume-7ebb8c35-b4eb-4010-a1b4-950ee7e1cd63 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:04:33.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5954" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5626,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:04:33.758: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Apr 29 08:04:33.796: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 29 08:04:38.800: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:04:38.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2383" for this suite.

• [SLOW TEST:5.072 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":297,"skipped":5630,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:04:38.830: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 08:04:38.928: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 29 08:04:43.932: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Apr 29 08:04:43.939: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Apr 29 08:04:43.948: INFO: observed ReplicaSet test-rs in namespace replicaset-9300 with ReadyReplicas 1, AvailableReplicas 1
Apr 29 08:04:43.954: INFO: observed ReplicaSet test-rs in namespace replicaset-9300 with ReadyReplicas 1, AvailableReplicas 1
Apr 29 08:04:43.968: INFO: observed ReplicaSet test-rs in namespace replicaset-9300 with ReadyReplicas 1, AvailableReplicas 1
Apr 29 08:04:43.980: INFO: observed ReplicaSet test-rs in namespace replicaset-9300 with ReadyReplicas 1, AvailableReplicas 1
Apr 29 08:04:45.032: INFO: observed ReplicaSet test-rs in namespace replicaset-9300 with ReadyReplicas 2, AvailableReplicas 2
Apr 29 08:04:45.880: INFO: observed Replicaset test-rs in namespace replicaset-9300 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:04:45.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9300" for this suite.

• [SLOW TEST:7.058 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":298,"skipped":5635,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:04:45.889: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:04:45.919: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-5431
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:04:52.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-69" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:04:52.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5431" for this suite.

• [SLOW TEST:6.137 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":299,"skipped":5653,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:04:52.026: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:04:52.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1293" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":300,"skipped":5667,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:04:52.362: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 08:04:52.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-2586 version'
Apr 29 08:04:52.493: INFO: stderr: ""
Apr 29 08:04:52.493: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.5\", GitCommit:\"c285e781331a3785a7f436042c65c5641ce8a9e9\", GitTreeState:\"clean\", BuildDate:\"2022-03-16T15:58:47Z\", GoVersion:\"go1.17.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.5\", GitCommit:\"c285e781331a3785a7f436042c65c5641ce8a9e9\", GitTreeState:\"clean\", BuildDate:\"2022-03-16T15:52:18Z\", GoVersion:\"go1.17.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:04:52.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2586" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":301,"skipped":5733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:04:52.505: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-5fdbc0df-eb56-48ec-befd-6b895e561ded in namespace container-probe-7678
Apr 29 08:04:54.630: INFO: Started pod busybox-5fdbc0df-eb56-48ec-befd-6b895e561ded in namespace container-probe-7678
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 08:04:54.632: INFO: Initial restart count of pod busybox-5fdbc0df-eb56-48ec-befd-6b895e561ded is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:08:55.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7678" for this suite.

• [SLOW TEST:242.724 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":302,"skipped":5873,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:08:55.229: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-19891a21-25fc-4d6d-a1de-d3d50c5b2db6
STEP: Creating a pod to test consume secrets
Apr 29 08:08:55.343: INFO: Waiting up to 5m0s for pod "pod-secrets-1e7c21bb-ebe6-48ba-b7c8-b22655c723d4" in namespace "secrets-9723" to be "Succeeded or Failed"
Apr 29 08:08:55.348: INFO: Pod "pod-secrets-1e7c21bb-ebe6-48ba-b7c8-b22655c723d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.080116ms
Apr 29 08:08:57.352: INFO: Pod "pod-secrets-1e7c21bb-ebe6-48ba-b7c8-b22655c723d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008551759s
STEP: Saw pod success
Apr 29 08:08:57.352: INFO: Pod "pod-secrets-1e7c21bb-ebe6-48ba-b7c8-b22655c723d4" satisfied condition "Succeeded or Failed"
Apr 29 08:08:57.354: INFO: Trying to get logs from node localhost.localdomain151 pod pod-secrets-1e7c21bb-ebe6-48ba-b7c8-b22655c723d4 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 08:08:57.383: INFO: Waiting for pod pod-secrets-1e7c21bb-ebe6-48ba-b7c8-b22655c723d4 to disappear
Apr 29 08:08:57.385: INFO: Pod pod-secrets-1e7c21bb-ebe6-48ba-b7c8-b22655c723d4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:08:57.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9723" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":303,"skipped":5906,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:08:57.394: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 29 08:08:57.430: INFO: Waiting up to 5m0s for pod "pod-aebd6922-b16e-4734-ab4f-ea13e062757e" in namespace "emptydir-2959" to be "Succeeded or Failed"
Apr 29 08:08:57.433: INFO: Pod "pod-aebd6922-b16e-4734-ab4f-ea13e062757e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.722102ms
Apr 29 08:08:59.438: INFO: Pod "pod-aebd6922-b16e-4734-ab4f-ea13e062757e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007378277s
STEP: Saw pod success
Apr 29 08:08:59.438: INFO: Pod "pod-aebd6922-b16e-4734-ab4f-ea13e062757e" satisfied condition "Succeeded or Failed"
Apr 29 08:08:59.440: INFO: Trying to get logs from node localhost.localdomain151 pod pod-aebd6922-b16e-4734-ab4f-ea13e062757e container test-container: <nil>
STEP: delete the pod
Apr 29 08:08:59.458: INFO: Waiting for pod pod-aebd6922-b16e-4734-ab4f-ea13e062757e to disappear
Apr 29 08:08:59.460: INFO: Pod pod-aebd6922-b16e-4734-ab4f-ea13e062757e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:08:59.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2959" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":304,"skipped":5910,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:08:59.469: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 08:08:59.500: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Apr 29 08:09:03.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 --namespace=crd-publish-openapi-7647 create -f -'
Apr 29 08:09:04.861: INFO: stderr: ""
Apr 29 08:09:04.861: INFO: stdout: "e2e-test-crd-publish-openapi-3578-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 29 08:09:04.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 --namespace=crd-publish-openapi-7647 delete e2e-test-crd-publish-openapi-3578-crds test-foo'
Apr 29 08:09:04.935: INFO: stderr: ""
Apr 29 08:09:04.935: INFO: stdout: "e2e-test-crd-publish-openapi-3578-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr 29 08:09:04.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 --namespace=crd-publish-openapi-7647 apply -f -'
Apr 29 08:09:05.117: INFO: stderr: ""
Apr 29 08:09:05.117: INFO: stdout: "e2e-test-crd-publish-openapi-3578-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 29 08:09:05.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 --namespace=crd-publish-openapi-7647 delete e2e-test-crd-publish-openapi-3578-crds test-foo'
Apr 29 08:09:05.194: INFO: stderr: ""
Apr 29 08:09:05.194: INFO: stdout: "e2e-test-crd-publish-openapi-3578-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
Apr 29 08:09:05.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 --namespace=crd-publish-openapi-7647 create -f -'
Apr 29 08:09:05.372: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Apr 29 08:09:05.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 --namespace=crd-publish-openapi-7647 create -f -'
Apr 29 08:09:05.548: INFO: rc: 1
Apr 29 08:09:05.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 --namespace=crd-publish-openapi-7647 apply -f -'
Apr 29 08:09:05.749: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Apr 29 08:09:05.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 --namespace=crd-publish-openapi-7647 create -f -'
Apr 29 08:09:05.928: INFO: rc: 1
Apr 29 08:09:05.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 --namespace=crd-publish-openapi-7647 apply -f -'
Apr 29 08:09:06.110: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Apr 29 08:09:06.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 explain e2e-test-crd-publish-openapi-3578-crds'
Apr 29 08:09:06.305: INFO: stderr: ""
Apr 29 08:09:06.305: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3578-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Apr 29 08:09:06.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 explain e2e-test-crd-publish-openapi-3578-crds.metadata'
Apr 29 08:09:06.509: INFO: stderr: ""
Apr 29 08:09:06.509: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3578-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr 29 08:09:06.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 explain e2e-test-crd-publish-openapi-3578-crds.spec'
Apr 29 08:09:06.716: INFO: stderr: ""
Apr 29 08:09:06.716: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3578-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr 29 08:09:06.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 explain e2e-test-crd-publish-openapi-3578-crds.spec.bars'
Apr 29 08:09:06.941: INFO: stderr: ""
Apr 29 08:09:06.941: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3578-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Apr 29 08:09:06.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=crd-publish-openapi-7647 explain e2e-test-crd-publish-openapi-3578-crds.spec.bars2'
Apr 29 08:09:07.180: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:09:11.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7647" for this suite.

• [SLOW TEST:12.093 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":305,"skipped":5919,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:09:11.562: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5108.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5108.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5108.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5108.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5108.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5108.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5108.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5108.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5108.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5108.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5108.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5108.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 229.49.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.49.229_udp@PTR;check="$$(dig +tcp +noall +answer +search 229.49.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.49.229_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5108.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5108.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5108.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5108.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5108.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5108.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5108.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5108.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5108.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5108.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5108.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5108.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 229.49.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.49.229_udp@PTR;check="$$(dig +tcp +noall +answer +search 229.49.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.49.229_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 08:09:13.684: INFO: Unable to read wheezy_udp@dns-test-service.dns-5108.svc.cluster.local from pod dns-5108/dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613: the server could not find the requested resource (get pods dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613)
Apr 29 08:09:13.687: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5108.svc.cluster.local from pod dns-5108/dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613: the server could not find the requested resource (get pods dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613)
Apr 29 08:09:13.690: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5108.svc.cluster.local from pod dns-5108/dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613: the server could not find the requested resource (get pods dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613)
Apr 29 08:09:13.692: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5108.svc.cluster.local from pod dns-5108/dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613: the server could not find the requested resource (get pods dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613)
Apr 29 08:09:13.706: INFO: Unable to read jessie_udp@dns-test-service.dns-5108.svc.cluster.local from pod dns-5108/dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613: the server could not find the requested resource (get pods dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613)
Apr 29 08:09:13.708: INFO: Unable to read jessie_tcp@dns-test-service.dns-5108.svc.cluster.local from pod dns-5108/dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613: the server could not find the requested resource (get pods dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613)
Apr 29 08:09:13.711: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5108.svc.cluster.local from pod dns-5108/dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613: the server could not find the requested resource (get pods dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613)
Apr 29 08:09:13.714: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5108.svc.cluster.local from pod dns-5108/dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613: the server could not find the requested resource (get pods dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613)
Apr 29 08:09:13.716: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-5108.svc.cluster.local from pod dns-5108/dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613: the server could not find the requested resource (get pods dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613)
Apr 29 08:09:13.719: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-5108.svc.cluster.local from pod dns-5108/dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613: the server could not find the requested resource (get pods dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613)
Apr 29 08:09:13.724: INFO: Lookups using dns-5108/dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613 failed for: [wheezy_udp@dns-test-service.dns-5108.svc.cluster.local wheezy_tcp@dns-test-service.dns-5108.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5108.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5108.svc.cluster.local jessie_udp@dns-test-service.dns-5108.svc.cluster.local jessie_tcp@dns-test-service.dns-5108.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5108.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5108.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-5108.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-5108.svc.cluster.local]

Apr 29 08:09:18.775: INFO: DNS probes using dns-5108/dns-test-87b5db4d-73e9-4f7c-8919-b5e53423f613 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:09:18.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5108" for this suite.

• [SLOW TEST:7.270 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":306,"skipped":5950,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:09:18.832: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 08:09:18.871: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb3f3834-8258-4675-9110-5774ca1e1c44" in namespace "projected-9942" to be "Succeeded or Failed"
Apr 29 08:09:18.877: INFO: Pod "downwardapi-volume-fb3f3834-8258-4675-9110-5774ca1e1c44": Phase="Pending", Reason="", readiness=false. Elapsed: 5.18187ms
Apr 29 08:09:20.882: INFO: Pod "downwardapi-volume-fb3f3834-8258-4675-9110-5774ca1e1c44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010292439s
STEP: Saw pod success
Apr 29 08:09:20.882: INFO: Pod "downwardapi-volume-fb3f3834-8258-4675-9110-5774ca1e1c44" satisfied condition "Succeeded or Failed"
Apr 29 08:09:20.884: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-fb3f3834-8258-4675-9110-5774ca1e1c44 container client-container: <nil>
STEP: delete the pod
Apr 29 08:09:20.903: INFO: Waiting for pod downwardapi-volume-fb3f3834-8258-4675-9110-5774ca1e1c44 to disappear
Apr 29 08:09:20.909: INFO: Pod downwardapi-volume-fb3f3834-8258-4675-9110-5774ca1e1c44 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:09:20.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9942" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":307,"skipped":5959,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:09:20.920: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 08:09:21.495: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 08:09:24.512: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Apr 29 08:09:24.531: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:09:24.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6157" for this suite.
STEP: Destroying namespace "webhook-6157-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":308,"skipped":5971,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:09:24.606: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-qg9t
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 08:09:24.680: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qg9t" in namespace "subpath-9781" to be "Succeeded or Failed"
Apr 29 08:09:24.688: INFO: Pod "pod-subpath-test-configmap-qg9t": Phase="Pending", Reason="", readiness=false. Elapsed: 7.801878ms
Apr 29 08:09:26.693: INFO: Pod "pod-subpath-test-configmap-qg9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.012711362s
Apr 29 08:09:28.697: INFO: Pod "pod-subpath-test-configmap-qg9t": Phase="Running", Reason="", readiness=true. Elapsed: 4.017039986s
Apr 29 08:09:30.701: INFO: Pod "pod-subpath-test-configmap-qg9t": Phase="Running", Reason="", readiness=true. Elapsed: 6.020889562s
Apr 29 08:09:32.705: INFO: Pod "pod-subpath-test-configmap-qg9t": Phase="Running", Reason="", readiness=true. Elapsed: 8.024706714s
Apr 29 08:09:34.709: INFO: Pod "pod-subpath-test-configmap-qg9t": Phase="Running", Reason="", readiness=true. Elapsed: 10.028906749s
Apr 29 08:09:36.714: INFO: Pod "pod-subpath-test-configmap-qg9t": Phase="Running", Reason="", readiness=true. Elapsed: 12.0331639s
Apr 29 08:09:38.718: INFO: Pod "pod-subpath-test-configmap-qg9t": Phase="Running", Reason="", readiness=true. Elapsed: 14.037612262s
Apr 29 08:09:40.722: INFO: Pod "pod-subpath-test-configmap-qg9t": Phase="Running", Reason="", readiness=true. Elapsed: 16.041465484s
Apr 29 08:09:42.726: INFO: Pod "pod-subpath-test-configmap-qg9t": Phase="Running", Reason="", readiness=true. Elapsed: 18.045769458s
Apr 29 08:09:44.730: INFO: Pod "pod-subpath-test-configmap-qg9t": Phase="Running", Reason="", readiness=true. Elapsed: 20.04992088s
Apr 29 08:09:46.734: INFO: Pod "pod-subpath-test-configmap-qg9t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.054071192s
STEP: Saw pod success
Apr 29 08:09:46.734: INFO: Pod "pod-subpath-test-configmap-qg9t" satisfied condition "Succeeded or Failed"
Apr 29 08:09:46.737: INFO: Trying to get logs from node localhost.localdomain151 pod pod-subpath-test-configmap-qg9t container test-container-subpath-configmap-qg9t: <nil>
STEP: delete the pod
Apr 29 08:09:46.753: INFO: Waiting for pod pod-subpath-test-configmap-qg9t to disappear
Apr 29 08:09:46.756: INFO: Pod pod-subpath-test-configmap-qg9t no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qg9t
Apr 29 08:09:46.756: INFO: Deleting pod "pod-subpath-test-configmap-qg9t" in namespace "subpath-9781"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:09:46.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9781" for this suite.

• [SLOW TEST:22.161 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":309,"skipped":5986,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:09:46.767: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 08:09:46.814: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c1d88dd-4ec2-4724-9128-a4c299ee3efa" in namespace "projected-4973" to be "Succeeded or Failed"
Apr 29 08:09:46.816: INFO: Pod "downwardapi-volume-0c1d88dd-4ec2-4724-9128-a4c299ee3efa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.637816ms
Apr 29 08:09:48.821: INFO: Pod "downwardapi-volume-0c1d88dd-4ec2-4724-9128-a4c299ee3efa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00746079s
STEP: Saw pod success
Apr 29 08:09:48.821: INFO: Pod "downwardapi-volume-0c1d88dd-4ec2-4724-9128-a4c299ee3efa" satisfied condition "Succeeded or Failed"
Apr 29 08:09:48.823: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-0c1d88dd-4ec2-4724-9128-a4c299ee3efa container client-container: <nil>
STEP: delete the pod
Apr 29 08:09:48.844: INFO: Waiting for pod downwardapi-volume-0c1d88dd-4ec2-4724-9128-a4c299ee3efa to disappear
Apr 29 08:09:48.846: INFO: Pod downwardapi-volume-0c1d88dd-4ec2-4724-9128-a4c299ee3efa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:09:48.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4973" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":5987,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:09:48.865: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 29 08:09:48.938: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 08:10:48.979: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Apr 29 08:10:48.999: INFO: Created pod: pod0-0-sched-preemption-low-priority
Apr 29 08:10:49.008: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Apr 29 08:10:49.022: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Apr 29 08:10:49.026: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:01.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3185" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:72.269 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":311,"skipped":5991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:01.134: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Apr 29 08:11:01.182: INFO: Waiting up to 5m0s for pod "security-context-fe146396-a32c-4708-b480-e9cab476b32d" in namespace "security-context-3495" to be "Succeeded or Failed"
Apr 29 08:11:01.185: INFO: Pod "security-context-fe146396-a32c-4708-b480-e9cab476b32d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.07652ms
Apr 29 08:11:03.189: INFO: Pod "security-context-fe146396-a32c-4708-b480-e9cab476b32d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007032855s
STEP: Saw pod success
Apr 29 08:11:03.189: INFO: Pod "security-context-fe146396-a32c-4708-b480-e9cab476b32d" satisfied condition "Succeeded or Failed"
Apr 29 08:11:03.192: INFO: Trying to get logs from node localhost.localdomain151 pod security-context-fe146396-a32c-4708-b480-e9cab476b32d container test-container: <nil>
STEP: delete the pod
Apr 29 08:11:03.207: INFO: Waiting for pod security-context-fe146396-a32c-4708-b480-e9cab476b32d to disappear
Apr 29 08:11:03.210: INFO: Pod security-context-fe146396-a32c-4708-b480-e9cab476b32d no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:03.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3495" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":312,"skipped":6021,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:03.219: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2480.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2480.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2480.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2480.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 08:11:05.283: INFO: DNS probes using dns-2480/dns-test-18098729-c9e7-439c-ad49-e1b20226ad82 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:05.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2480" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":313,"skipped":6042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:05.318: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 29 08:11:07.369: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:07.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1014" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":314,"skipped":6074,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:07.404: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 08:11:07.465: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:13.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2349" for this suite.

• [SLOW TEST:6.471 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":315,"skipped":6080,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:13.876: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 08:11:14.688: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 08:11:17.707: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 08:11:17.711: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3064-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:20.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2588" for this suite.
STEP: Destroying namespace "webhook-2588-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.067 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":316,"skipped":6101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:20.944: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 08:11:20.975: INFO: Creating pod...
Apr 29 08:11:20.983: INFO: Pod Quantity: 1 Status: Pending
Apr 29 08:11:21.987: INFO: Pod Quantity: 1 Status: Pending
Apr 29 08:11:22.990: INFO: Pod Status: Running
Apr 29 08:11:22.990: INFO: Creating service...
Apr 29 08:11:23.000: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/pods/agnhost/proxy/some/path/with/DELETE
Apr 29 08:11:23.006: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 29 08:11:23.006: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/pods/agnhost/proxy/some/path/with/GET
Apr 29 08:11:23.010: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Apr 29 08:11:23.010: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/pods/agnhost/proxy/some/path/with/HEAD
Apr 29 08:11:23.012: INFO: http.Client request:HEAD | StatusCode:200
Apr 29 08:11:23.012: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/pods/agnhost/proxy/some/path/with/OPTIONS
Apr 29 08:11:23.014: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 29 08:11:23.014: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/pods/agnhost/proxy/some/path/with/PATCH
Apr 29 08:11:23.017: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 29 08:11:23.017: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/pods/agnhost/proxy/some/path/with/POST
Apr 29 08:11:23.019: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 29 08:11:23.019: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/pods/agnhost/proxy/some/path/with/PUT
Apr 29 08:11:23.028: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr 29 08:11:23.028: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/services/test-service/proxy/some/path/with/DELETE
Apr 29 08:11:23.031: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 29 08:11:23.031: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/services/test-service/proxy/some/path/with/GET
Apr 29 08:11:23.036: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Apr 29 08:11:23.036: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/services/test-service/proxy/some/path/with/HEAD
Apr 29 08:11:23.042: INFO: http.Client request:HEAD | StatusCode:200
Apr 29 08:11:23.042: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/services/test-service/proxy/some/path/with/OPTIONS
Apr 29 08:11:23.045: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 29 08:11:23.045: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/services/test-service/proxy/some/path/with/PATCH
Apr 29 08:11:23.048: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 29 08:11:23.048: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/services/test-service/proxy/some/path/with/POST
Apr 29 08:11:23.051: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 29 08:11:23.051: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2917/services/test-service/proxy/some/path/with/PUT
Apr 29 08:11:23.054: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:23.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2917" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":317,"skipped":6131,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:23.063: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 08:11:23.116: INFO: Creating simple deployment test-new-deployment
Apr 29 08:11:23.141: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Apr 29 08:11:25.174: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-792  1889fe41-b629-4fa4-8c49-b403c83cec54 134567 3 2022-04-29 08:11:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-04-29 08:11:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 08:11:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00388e998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-04-29 08:11:24 +0000 UTC,LastTransitionTime:2022-04-29 08:11:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-04-29 08:11:24 +0000 UTC,LastTransitionTime:2022-04-29 08:11:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 29 08:11:25.178: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-792  9ee15e09-2545-439f-bb08-0eea9ae783d8 134568 2 2022-04-29 08:11:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 1889fe41-b629-4fa4-8c49-b403c83cec54 0xc00388edb7 0xc00388edb8}] []  [{kube-controller-manager Update apps/v1 2022-04-29 08:11:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1889fe41-b629-4fa4-8c49-b403c83cec54\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-04-29 08:11:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00388ee48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 29 08:11:25.182: INFO: Pod "test-new-deployment-5d9fdcc779-z584n" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-z584n test-new-deployment-5d9fdcc779- deployment-792  9a7d0a29-5341-487e-9007-18b137860680 134560 0 2022-04-29 08:11:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:172.16.115.143/32 cni.projectcalico.org/podIPs:172.16.115.143/32] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 9ee15e09-2545-439f-bb08-0eea9ae783d8 0xc00066ad87 0xc00066ad88}] []  [{calico Update v1 2022-04-29 08:11:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-04-29 08:11:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ee15e09-2545-439f-bb08-0eea9ae783d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-04-29 08:11:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.115.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-svffr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-svffr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain151,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 08:11:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 08:11:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 08:11:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 08:11:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:123.58.176.151,PodIP:172.16.115.143,StartTime:2022-04-29 08:11:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-04-29 08:11:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://322a6af9a325166e1eadeb490ae3724590ed2cb0fa61a696429ff0e6b3dce2f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.115.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 08:11:25.182: INFO: Pod "test-new-deployment-5d9fdcc779-zcccg" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-zcccg test-new-deployment-5d9fdcc779- deployment-792  48413cbd-fa64-4e28-87ef-4637cf659ac3 134571 0 2022-04-29 08:11:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 9ee15e09-2545-439f-bb08-0eea9ae783d8 0xc00066b047 0xc00066b048}] []  [{kube-controller-manager Update v1 2022-04-29 08:11:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ee15e09-2545-439f-bb08-0eea9ae783d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drkx4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drkx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:localhost.localdomain,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-04-29 08:11:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:25.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-792" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":318,"skipped":6140,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:25.201: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 29 08:11:26.104: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 08:11:29.125: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 08:11:29.129: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:32.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4377" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.086 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":319,"skipped":6140,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:32.287: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
STEP: creating an pod
Apr 29 08:11:32.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-73 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr 29 08:11:32.430: INFO: stderr: ""
Apr 29 08:11:32.430: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Apr 29 08:11:32.430: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr 29 08:11:32.430: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-73" to be "running and ready, or succeeded"
Apr 29 08:11:32.433: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.664711ms
Apr 29 08:11:34.438: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.007342993s
Apr 29 08:11:34.438: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr 29 08:11:34.438: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Apr 29 08:11:34.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-73 logs logs-generator logs-generator'
Apr 29 08:11:34.527: INFO: stderr: ""
Apr 29 08:11:34.527: INFO: stdout: "I0429 08:11:33.298532       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/4bhl 461\nI0429 08:11:33.498877       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/btb 347\nI0429 08:11:33.699277       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/rdx 572\nI0429 08:11:33.898628       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/nph 589\nI0429 08:11:34.098979       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/d576 247\nI0429 08:11:34.299384       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/nmz 217\nI0429 08:11:34.499085       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2l6 461\n"
STEP: limiting log lines
Apr 29 08:11:34.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-73 logs logs-generator logs-generator --tail=1'
Apr 29 08:11:34.604: INFO: stderr: ""
Apr 29 08:11:34.604: INFO: stdout: "I0429 08:11:34.499085       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2l6 461\n"
Apr 29 08:11:34.604: INFO: got output "I0429 08:11:34.499085       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2l6 461\n"
STEP: limiting log bytes
Apr 29 08:11:34.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-73 logs logs-generator logs-generator --limit-bytes=1'
Apr 29 08:11:34.684: INFO: stderr: ""
Apr 29 08:11:34.684: INFO: stdout: "I"
Apr 29 08:11:34.684: INFO: got output "I"
STEP: exposing timestamps
Apr 29 08:11:34.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-73 logs logs-generator logs-generator --tail=1 --timestamps'
Apr 29 08:11:34.769: INFO: stderr: ""
Apr 29 08:11:34.769: INFO: stdout: "2022-04-29T04:11:34.700376502-04:00 I0429 08:11:34.699409       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/wtr 352\n"
Apr 29 08:11:34.769: INFO: got output "2022-04-29T04:11:34.700376502-04:00 I0429 08:11:34.699409       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/wtr 352\n"
STEP: restricting to a time range
Apr 29 08:11:37.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-73 logs logs-generator logs-generator --since=1s'
Apr 29 08:11:37.351: INFO: stderr: ""
Apr 29 08:11:37.351: INFO: stdout: "I0429 08:11:36.498653       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/fz9w 295\nI0429 08:11:36.699053       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/vwpp 274\nI0429 08:11:36.899415       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/nxg 206\nI0429 08:11:37.098689       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/m5m 443\nI0429 08:11:37.299078       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/ttk 394\n"
Apr 29 08:11:37.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-73 logs logs-generator logs-generator --since=24h'
Apr 29 08:11:37.429: INFO: stderr: ""
Apr 29 08:11:37.429: INFO: stdout: "I0429 08:11:33.298532       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/4bhl 461\nI0429 08:11:33.498877       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/btb 347\nI0429 08:11:33.699277       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/rdx 572\nI0429 08:11:33.898628       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/nph 589\nI0429 08:11:34.098979       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/d576 247\nI0429 08:11:34.299384       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/nmz 217\nI0429 08:11:34.499085       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2l6 461\nI0429 08:11:34.699409       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/wtr 352\nI0429 08:11:34.898637       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/7hs 234\nI0429 08:11:35.098988       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/mpt5 481\nI0429 08:11:35.299433       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/8qm7 484\nI0429 08:11:35.498891       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/565 469\nI0429 08:11:35.699324       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/2s5 405\nI0429 08:11:35.898647       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/nl9 287\nI0429 08:11:36.099044       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/hj62 457\nI0429 08:11:36.299424       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/7j8g 338\nI0429 08:11:36.498653       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/fz9w 295\nI0429 08:11:36.699053       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/vwpp 274\nI0429 08:11:36.899415       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/nxg 206\nI0429 08:11:37.098689       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/m5m 443\nI0429 08:11:37.299078       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/ttk 394\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
Apr 29 08:11:37.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-73 delete pod logs-generator'
Apr 29 08:11:38.739: INFO: stderr: ""
Apr 29 08:11:38.739: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:38.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-73" for this suite.

• [SLOW TEST:6.461 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1406
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":320,"skipped":6148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:38.749: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Apr 29 08:11:40.832: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7704 PodName:pod-sharedvolume-4acc2d85-c26e-4564-a0ce-f65e9049d83b ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 08:11:40.832: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
Apr 29 08:11:40.833: INFO: ExecWithOptions: Clientset creation
Apr 29 08:11:40.833: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-7704/pods/pod-sharedvolume-4acc2d85-c26e-4564-a0ce-f65e9049d83b/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
Apr 29 08:11:40.902: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:40.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7704" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":321,"skipped":6219,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:40.915: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Apr 29 08:11:40.950: INFO: The status of Pod annotationupdateac08c2d8-b16a-4944-ac04-78cc406b5243 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 08:11:42.956: INFO: The status of Pod annotationupdateac08c2d8-b16a-4944-ac04-78cc406b5243 is Running (Ready = true)
Apr 29 08:11:43.475: INFO: Successfully updated pod "annotationupdateac08c2d8-b16a-4944-ac04-78cc406b5243"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:47.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6810" for this suite.

• [SLOW TEST:6.592 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":322,"skipped":6220,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:47.507: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Apr 29 08:11:47.556: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Apr 29 08:11:49.560: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:11:50.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4841" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":323,"skipped":6224,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:11:50.581: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2635
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 08:11:50.629: INFO: Found 0 stateful pods, waiting for 1
Apr 29 08:12:00.635: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Apr 29 08:12:00.653: INFO: Found 1 stateful pods, waiting for 2
Apr 29 08:12:10.661: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 08:12:10.661: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Apr 29 08:12:10.682: INFO: Deleting all statefulset in ns statefulset-2635
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:12:10.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2635" for this suite.

• [SLOW TEST:20.118 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":324,"skipped":6229,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:12:10.699: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9209
Apr 29 08:12:10.840: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Apr 29 08:12:12.845: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Apr 29 08:12:12.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-9209 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr 29 08:12:12.996: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr 29 08:12:12.996: INFO: stdout: "iptables"
Apr 29 08:12:12.996: INFO: proxyMode: iptables
Apr 29 08:12:13.007: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 29 08:12:13.010: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-9209
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9209
I0429 08:12:13.032214      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9209, replica count: 3
I0429 08:12:16.082977      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 08:12:16.089: INFO: Creating new exec pod
Apr 29 08:12:19.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-9209 exec execpod-affinityr54hc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Apr 29 08:12:19.250: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Apr 29 08:12:19.250: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 08:12:19.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-9209 exec execpod-affinityr54hc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.239.159 80'
Apr 29 08:12:19.408: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.239.159 80\nConnection to 10.109.239.159 80 port [tcp/http] succeeded!\n"
Apr 29 08:12:19.408: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr 29 08:12:19.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-9209 exec execpod-affinityr54hc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.109.239.159:80/ ; done'
Apr 29 08:12:19.672: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n"
Apr 29 08:12:19.672: INFO: stdout: "\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr\naffinity-clusterip-timeout-wt2tr"
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Received response from host: affinity-clusterip-timeout-wt2tr
Apr 29 08:12:19.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-9209 exec execpod-affinityr54hc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.109.239.159:80/'
Apr 29 08:12:19.827: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n"
Apr 29 08:12:19.827: INFO: stdout: "affinity-clusterip-timeout-wt2tr"
Apr 29 08:12:39.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-9209 exec execpod-affinityr54hc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.109.239.159:80/'
Apr 29 08:12:40.064: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n"
Apr 29 08:12:40.064: INFO: stdout: "affinity-clusterip-timeout-wt2tr"
Apr 29 08:13:00.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=services-9209 exec execpod-affinityr54hc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.109.239.159:80/'
Apr 29 08:13:00.207: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.109.239.159:80/\n"
Apr 29 08:13:00.207: INFO: stdout: "affinity-clusterip-timeout-x9qnq"
Apr 29 08:13:00.207: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9209, will wait for the garbage collector to delete the pods
Apr 29 08:13:00.280: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.578922ms
Apr 29 08:13:00.380: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.184776ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:13:02.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9209" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:51.905 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":325,"skipped":6237,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:13:02.604: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-ccecf2a1-c319-4bb2-8b50-c38325b4a514
STEP: Creating configMap with name cm-test-opt-upd-40912d5f-2b38-4fcd-9200-89a109a445c8
STEP: Creating the pod
Apr 29 08:13:02.728: INFO: The status of Pod pod-configmaps-1708c78e-b70b-4bc2-9a9f-665abfe2656b is Pending, waiting for it to be Running (with Ready = true)
Apr 29 08:13:04.733: INFO: The status of Pod pod-configmaps-1708c78e-b70b-4bc2-9a9f-665abfe2656b is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-ccecf2a1-c319-4bb2-8b50-c38325b4a514
STEP: Updating configmap cm-test-opt-upd-40912d5f-2b38-4fcd-9200-89a109a445c8
STEP: Creating configMap with name cm-test-opt-create-f066fbdd-0cdb-42f1-9239-46b3e35aa4ff
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:13:06.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5790" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":6254,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:13:06.793: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Apr 29 08:13:06.832: INFO: Waiting up to 5m0s for pod "downward-api-1e611596-6316-4b1a-a64f-8e3756b8937c" in namespace "downward-api-7685" to be "Succeeded or Failed"
Apr 29 08:13:06.839: INFO: Pod "downward-api-1e611596-6316-4b1a-a64f-8e3756b8937c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.03004ms
Apr 29 08:13:08.843: INFO: Pod "downward-api-1e611596-6316-4b1a-a64f-8e3756b8937c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011885612s
STEP: Saw pod success
Apr 29 08:13:08.844: INFO: Pod "downward-api-1e611596-6316-4b1a-a64f-8e3756b8937c" satisfied condition "Succeeded or Failed"
Apr 29 08:13:08.846: INFO: Trying to get logs from node localhost.localdomain151 pod downward-api-1e611596-6316-4b1a-a64f-8e3756b8937c container dapi-container: <nil>
STEP: delete the pod
Apr 29 08:13:08.860: INFO: Waiting for pod downward-api-1e611596-6316-4b1a-a64f-8e3756b8937c to disappear
Apr 29 08:13:08.864: INFO: Pod downward-api-1e611596-6316-4b1a-a64f-8e3756b8937c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:13:08.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7685" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":6263,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:13:08.878: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 08:13:12.957: INFO: DNS probes using dns-2275/dns-test-ecc67ed3-2ba8-4e10-a907-515a5469cd08 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:13:12.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2275" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":328,"skipped":6294,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:13:12.981: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:15:01.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3373" for this suite.

• [SLOW TEST:108.109 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":329,"skipped":6315,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:15:01.091: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 29 08:15:01.233: INFO: Waiting up to 5m0s for pod "pod-2ce88dbc-519d-4f4a-a28d-50093b2df44c" in namespace "emptydir-9828" to be "Succeeded or Failed"
Apr 29 08:15:01.236: INFO: Pod "pod-2ce88dbc-519d-4f4a-a28d-50093b2df44c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.234801ms
Apr 29 08:15:03.240: INFO: Pod "pod-2ce88dbc-519d-4f4a-a28d-50093b2df44c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00681509s
STEP: Saw pod success
Apr 29 08:15:03.240: INFO: Pod "pod-2ce88dbc-519d-4f4a-a28d-50093b2df44c" satisfied condition "Succeeded or Failed"
Apr 29 08:15:03.242: INFO: Trying to get logs from node localhost.localdomain151 pod pod-2ce88dbc-519d-4f4a-a28d-50093b2df44c container test-container: <nil>
STEP: delete the pod
Apr 29 08:15:03.267: INFO: Waiting for pod pod-2ce88dbc-519d-4f4a-a28d-50093b2df44c to disappear
Apr 29 08:15:03.269: INFO: Pod pod-2ce88dbc-519d-4f4a-a28d-50093b2df44c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:15:03.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9828" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":330,"skipped":6318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:15:03.277: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-6e2cffe2-32dd-4c0d-aa26-fa66df56b4d6
STEP: Creating a pod to test consume secrets
Apr 29 08:15:03.317: INFO: Waiting up to 5m0s for pod "pod-secrets-cae468eb-9730-434d-afee-6ebd9f07f6c7" in namespace "secrets-3156" to be "Succeeded or Failed"
Apr 29 08:15:03.319: INFO: Pod "pod-secrets-cae468eb-9730-434d-afee-6ebd9f07f6c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064589ms
Apr 29 08:15:05.322: INFO: Pod "pod-secrets-cae468eb-9730-434d-afee-6ebd9f07f6c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005407852s
STEP: Saw pod success
Apr 29 08:15:05.322: INFO: Pod "pod-secrets-cae468eb-9730-434d-afee-6ebd9f07f6c7" satisfied condition "Succeeded or Failed"
Apr 29 08:15:05.324: INFO: Trying to get logs from node localhost.localdomain151 pod pod-secrets-cae468eb-9730-434d-afee-6ebd9f07f6c7 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 08:15:05.342: INFO: Waiting for pod pod-secrets-cae468eb-9730-434d-afee-6ebd9f07f6c7 to disappear
Apr 29 08:15:05.344: INFO: Pod pod-secrets-cae468eb-9730-434d-afee-6ebd9f07f6c7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:15:05.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3156" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":331,"skipped":6340,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:15:05.353: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-9313
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-9313
Apr 29 08:15:05.449: INFO: Found 0 stateful pods, waiting for 1
Apr 29 08:15:15.457: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Apr 29 08:15:15.473: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Apr 29 08:15:15.484: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Apr 29 08:15:15.487: INFO: Observed &StatefulSet event: ADDED
Apr 29 08:15:15.487: INFO: Found Statefulset ss in namespace statefulset-9313 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 29 08:15:15.487: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Apr 29 08:15:15.487: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr 29 08:15:15.494: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Apr 29 08:15:15.496: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Apr 29 08:15:15.496: INFO: Deleting all statefulset in ns statefulset-9313
Apr 29 08:15:15.498: INFO: Scaling statefulset ss to 0
Apr 29 08:15:25.515: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 08:15:25.519: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:15:25.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9313" for this suite.

• [SLOW TEST:20.183 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":332,"skipped":6407,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:15:25.537: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-17311319-96d5-4e05-9f72-3fdb04191139
STEP: Creating a pod to test consume secrets
Apr 29 08:15:25.626: INFO: Waiting up to 5m0s for pod "pod-secrets-cbcca132-cefb-429b-b68d-a0713befa8dc" in namespace "secrets-5437" to be "Succeeded or Failed"
Apr 29 08:15:25.628: INFO: Pod "pod-secrets-cbcca132-cefb-429b-b68d-a0713befa8dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.723421ms
Apr 29 08:15:27.638: INFO: Pod "pod-secrets-cbcca132-cefb-429b-b68d-a0713befa8dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012565907s
STEP: Saw pod success
Apr 29 08:15:27.638: INFO: Pod "pod-secrets-cbcca132-cefb-429b-b68d-a0713befa8dc" satisfied condition "Succeeded or Failed"
Apr 29 08:15:27.641: INFO: Trying to get logs from node localhost.localdomain151 pod pod-secrets-cbcca132-cefb-429b-b68d-a0713befa8dc container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 08:15:27.657: INFO: Waiting for pod pod-secrets-cbcca132-cefb-429b-b68d-a0713befa8dc to disappear
Apr 29 08:15:27.659: INFO: Pod pod-secrets-cbcca132-cefb-429b-b68d-a0713befa8dc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:15:27.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5437" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":333,"skipped":6418,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:15:27.666: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a collection of services
Apr 29 08:15:27.697: INFO: Creating e2e-svc-a-r4wp9
Apr 29 08:15:27.706: INFO: Creating e2e-svc-b-w9h48
Apr 29 08:15:27.725: INFO: Creating e2e-svc-c-nznz5
STEP: deleting service collection
Apr 29 08:15:27.796: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:15:27.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-164" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":334,"skipped":6438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:15:27.813: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:15:27.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6953" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":335,"skipped":6469,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:15:27.887: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Apr 29 08:15:28.035: INFO: Pod name sample-pod: Found 0 pods out of 3
Apr 29 08:15:33.039: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Apr 29 08:15:33.041: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:15:33.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5487" for this suite.

• [SLOW TEST:5.180 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":336,"skipped":6469,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:15:33.067: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:15:49.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9455" for this suite.

• [SLOW TEST:16.177 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":337,"skipped":6481,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:15:49.244: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Apr 29 08:15:49.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-5214 create -f -'
Apr 29 08:15:50.607: INFO: stderr: ""
Apr 29 08:15:50.607: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Apr 29 08:15:50.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-5214 diff -f -'
Apr 29 08:15:50.815: INFO: rc: 1
Apr 29 08:15:50.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-43752302 --namespace=kubectl-5214 delete -f -'
Apr 29 08:15:50.882: INFO: stderr: ""
Apr 29 08:15:50.882: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:15:50.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5214" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":338,"skipped":6486,"failed":0}

------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:15:50.890: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:15:57.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6373" for this suite.
STEP: Destroying namespace "nsdeletetest-5247" for this suite.
Apr 29 08:15:57.088: INFO: Namespace nsdeletetest-5247 was already deleted
STEP: Destroying namespace "nsdeletetest-6373" for this suite.

• [SLOW TEST:6.204 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":339,"skipped":6486,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:15:57.095: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:15:59.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7416" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":340,"skipped":6514,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:15:59.951: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Apr 29 08:15:59.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b79649b-76eb-4eb3-a618-edb4e63b13bd" in namespace "downward-api-4180" to be "Succeeded or Failed"
Apr 29 08:15:59.996: INFO: Pod "downwardapi-volume-9b79649b-76eb-4eb3-a618-edb4e63b13bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.201107ms
Apr 29 08:16:02.001: INFO: Pod "downwardapi-volume-9b79649b-76eb-4eb3-a618-edb4e63b13bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008142042s
STEP: Saw pod success
Apr 29 08:16:02.001: INFO: Pod "downwardapi-volume-9b79649b-76eb-4eb3-a618-edb4e63b13bd" satisfied condition "Succeeded or Failed"
Apr 29 08:16:02.004: INFO: Trying to get logs from node localhost.localdomain151 pod downwardapi-volume-9b79649b-76eb-4eb3-a618-edb4e63b13bd container client-container: <nil>
STEP: delete the pod
Apr 29 08:16:02.020: INFO: Waiting for pod downwardapi-volume-9b79649b-76eb-4eb3-a618-edb4e63b13bd to disappear
Apr 29 08:16:02.024: INFO: Pod downwardapi-volume-9b79649b-76eb-4eb3-a618-edb4e63b13bd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:16:02.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4180" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":341,"skipped":6528,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:16:02.032: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Apr 29 08:16:02.128: INFO: created pod
Apr 29 08:16:02.128: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5783" to be "Succeeded or Failed"
Apr 29 08:16:02.130: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501741ms
Apr 29 08:16:04.134: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005647148s
STEP: Saw pod success
Apr 29 08:16:04.134: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Apr 29 08:16:34.135: INFO: polling logs
Apr 29 08:16:34.142: INFO: Pod logs: 
2022/04/29 08:16:02 OK: Got token
2022/04/29 08:16:02 validating with in-cluster discovery
2022/04/29 08:16:02 OK: got issuer https://kubernetes.default.svc.cluster.local
2022/04/29 08:16:02 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5783:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1651220762, NotBefore:1651220162, IssuedAt:1651220162, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5783", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0e5b9201-9c57-4fa0-8ae6-ff9c397fb2ee"}}}
2022/04/29 08:16:02 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2022/04/29 08:16:02 OK: Validated signature on JWT
2022/04/29 08:16:02 OK: Got valid claims from token!
2022/04/29 08:16:02 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5783:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1651220762, NotBefore:1651220162, IssuedAt:1651220162, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5783", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0e5b9201-9c57-4fa0-8ae6-ff9c397fb2ee"}}}

Apr 29 08:16:34.142: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:16:34.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5783" for this suite.

• [SLOW TEST:32.123 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":342,"skipped":6538,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:16:34.156: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:16:34.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3492" for this suite.
STEP: Destroying namespace "nspatchtest-29a31301-6b69-40c1-939f-37bbf5ab864a-1658" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":343,"skipped":6564,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:16:34.348: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:16:34.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7803" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":344,"skipped":6575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:16:34.449: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 29 08:16:34.572: INFO: Waiting up to 5m0s for pod "pod-6c661191-c270-4123-8aef-825ea307e59c" in namespace "emptydir-8582" to be "Succeeded or Failed"
Apr 29 08:16:34.575: INFO: Pod "pod-6c661191-c270-4123-8aef-825ea307e59c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.523638ms
Apr 29 08:16:36.579: INFO: Pod "pod-6c661191-c270-4123-8aef-825ea307e59c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006944158s
STEP: Saw pod success
Apr 29 08:16:36.579: INFO: Pod "pod-6c661191-c270-4123-8aef-825ea307e59c" satisfied condition "Succeeded or Failed"
Apr 29 08:16:36.581: INFO: Trying to get logs from node localhost.localdomain151 pod pod-6c661191-c270-4123-8aef-825ea307e59c container test-container: <nil>
STEP: delete the pod
Apr 29 08:16:36.595: INFO: Waiting for pod pod-6c661191-c270-4123-8aef-825ea307e59c to disappear
Apr 29 08:16:36.600: INFO: Pod pod-6c661191-c270-4123-8aef-825ea307e59c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:16:36.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8582" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":345,"skipped":6658,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Apr 29 08:16:36.608: INFO: >>> kubeConfig: /tmp/kubeconfig-43752302
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-c7e3f6d3-f32f-4a07-bd1d-ec22f13e0859
STEP: Creating configMap with name cm-test-opt-upd-f382ce1b-bd38-4eeb-b45d-0bef7987164e
STEP: Creating the pod
Apr 29 08:16:36.663: INFO: The status of Pod pod-projected-configmaps-1af7dcea-7a6f-47e0-aedb-cc4840c63db4 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 08:16:38.720: INFO: The status of Pod pod-projected-configmaps-1af7dcea-7a6f-47e0-aedb-cc4840c63db4 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-c7e3f6d3-f32f-4a07-bd1d-ec22f13e0859
STEP: Updating configmap cm-test-opt-upd-f382ce1b-bd38-4eeb-b45d-0bef7987164e
STEP: Creating configMap with name cm-test-opt-create-8eda4559-d24a-4ffd-8abd-16fd6b3d9f99
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Apr 29 08:16:40.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4712" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":346,"skipped":6668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSApr 29 08:16:40.779: INFO: Running AfterSuite actions on all nodes
Apr 29 08:16:40.779: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
Apr 29 08:16:40.779: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Apr 29 08:16:40.779: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Apr 29 08:16:40.779: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Apr 29 08:16:40.779: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Apr 29 08:16:40.779: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Apr 29 08:16:40.779: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Apr 29 08:16:40.779: INFO: Running AfterSuite actions on node 1
Apr 29 08:16:40.779: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6696,"failed":0}

Ran 346 of 7042 Specs in 5790.401 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6696 Skipped
PASS

Ginkgo ran 1 suite in 1h36m32.864324563s
Test Suite Passed
